<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Thinkgamer的博客">
    <meta name="keyword"  content="Python,Django,爬虫,Hadoop,Maching Learning,数据挖掘,机器学习,云计算,大数据,深度学习,开发者,程序猿,程序媛,极客,编程,代码,开源,IT网站,Developer,Programmer,Coder,用户体验">
    <link rel="shortcut icon" href="/assets/img/favicon.ico">

    <title>
        
        梯度算法之梯度上升和梯度下降 - Thinkgamer的博客 | Thinkgamer&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> All In CTR、DL、ML、RL、NLP、KG </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/assets/img/head.jpg" />
        </div>
        <div class="name">
            <i>Thinkgamer</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#高数中的导数"><span class="toc-text">高数中的导数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#偏导数"><span class="toc-text">偏导数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#方向导数"><span class="toc-text">方向导数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度"><span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降与梯度上升"><span class="toc-text">梯度下降与梯度上升</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降"><span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#关于梯度下降的几个概念"><span class="toc-text">关于梯度下降的几个概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降的代数方法描述"><span class="toc-text">梯度下降的代数方法描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降的矩阵方式描述"><span class="toc-text">梯度下降的矩阵方式描述</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度上升"><span class="toc-text">梯度上升</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降的算法优化"><span class="toc-text">梯度下降的算法优化</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> All In CTR、DL、ML、RL、NLP、KG </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        梯度算法之梯度上升和梯度下降
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2017-12-14 14:11:11</span></span>
        
        <span class="attr">标签：/
            
            <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span></span>
        </span>
    </div>
    <div class="post-content no-indent">
        <blockquote>
<p>第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。</p>
</blockquote>
<a id="more"></a>
<h1 id="高数中的导数"><a href="#高数中的导数" class="headerlink" title="高数中的导数"></a>高数中的导数</h1><p>设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成</p>
<script type="math/tex; mode=display">
x_{0} + \Delta x</script><p>函数y=f(x)的增量</p>
<script type="math/tex; mode=display">
\Delta y = f(x_0 + \Delta x) - f(x_0)</script><p>与自变量的增量 $ \Delta x $ 之比：</p>
<script type="math/tex; mode=display">
\frac{ \Delta y }{ \Delta x } = \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>称为f(x)的平均变化率。<br>如 $ \Delta x \rightarrow 0 $ 平均变化率的极限</p>
<script type="math/tex; mode=display">
\lim_{\Delta x \rightarrow 0} \frac{ \Delta y }{ \Delta x } = \lim_{\Delta x  \rightarrow 0} \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。</p>
<p>关于导数的说明</p>
<p>1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都</p>
<p>2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导</p>
<p>3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数</p>
<p>4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减</p>
<p>5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率</p>
<h1 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h1><p>函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \Delta x $ 时，相应的有函数增量</p>
<script type="math/tex; mode=display">
f(x_0 + \Delta x, y_0) - f(x_0,y_0)</script><p>如果</p>
<script type="math/tex; mode=display">
\lim_{\Delta x\rightarrow 0 } \frac {f(x_0 + \Delta x, y_0) - f(x_0,y_0)}{\Delta x}</script><p>存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $</p>
<p>如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做</p>
<script type="math/tex; mode=display">
\frac{ \partial z }{ \partial x } , \frac{ \partial f }{ \partial x } , z_x , f_x(x,y),</script><p>偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处</p>
<script type="math/tex; mode=display">
f_x(x,y,z)=\lim_{\Delta x \rightarrow 0} \frac{f(x + \Delta x,y,z) -f(x,y,z)}{\Delta x}</script><script type="math/tex; mode=display">
f_y(x,y,z)=\lim_{\Delta y \rightarrow 0} \frac{f(x,y + \Delta y,z) -f(x,y,z)}{\Delta y}</script><script type="math/tex; mode=display">
f_z(x,y,z)=\lim_{\Delta z \rightarrow 0} \frac{f(x,y,z + \Delta z) -f(x,y,z)}{\Delta z}</script><p>可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。</p>
<p>区别：<br>导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；<br>偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。</p>
<p>偏导数的几何意义：<br>偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率<br>偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率</p>
<p>例子：<br>求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。</p>
<script type="math/tex; mode=display">
\frac{ \partial z}{\partial x} = 2x +3y</script><script type="math/tex; mode=display">
\frac{ \partial z}{\partial y} = 2y +3x</script><p>所以:<br>$z_x(x=1,y=2) = 8$<br>$z_y(x=1,y=2) = 7$</p>
<h1 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h1><script type="math/tex; mode=display">
\frac{ \partial }{ \partial l }  f(x_0,x_1,...,x_n) = \lim_{\rho \rightarrow 0} \frac{\Delta y}{ \Delta x } = \lim_{\rho \rightarrow 0} \frac{ f(x_0 + \Delta x_0,...,x_j + \Delta x_j,...,x_n + \Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \rho }</script><script type="math/tex; mode=display">
\rho = \sqrt{ (\Delta x_0)^{2} +...+(\Delta x_j)^{2}+...+(\Delta x_n)^{2}}</script><p>前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。</p>
<p>通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 
　</p>
<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $<br>( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y })^T<br>$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y },\frac{ \partial f }{ \partial z })^T$,以此类推。</p>
<p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p>
<p>例如：<br>函数 $f(x,y) = \frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：</p>
<script type="math/tex; mode=display">
 \frac{ \partial f }{ \partial x}=-\frac{2x}{ (x^2+y^2)^2}</script><script type="math/tex; mode=display">
 \frac{ \partial f }{ \partial y}=-\frac{2y}{ (x^2+y^2)^2}</script><p>所以</p>
<script type="math/tex; mode=display">
grad( \frac{1}{x^2+y^2} ) = (-\frac{2x}{ (x^2+y^2)^2} ,-\frac{2y}{ (x^2+y^2)^2})</script><p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 </p>
<p>注意点：<br>1）梯度是一个向量<br>2）梯度的方向是最大方向导数的方向<br>3）梯度的值是最大方向导数的值</p>
<h1 id="梯度下降与梯度上升"><a href="#梯度下降与梯度上升" class="headerlink" title="梯度下降与梯度上升"></a>梯度下降与梯度上升</h1><p>在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="关于梯度下降的几个概念"><a href="#关于梯度下降的几个概念" class="headerlink" title="关于梯度下降的几个概念"></a>关于梯度下降的几个概念</h3><p>1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度<br>2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y<br>3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。<br>4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为：</p>
<script type="math/tex; mode=display">
\jmath (\theta _0,\theta _1)=\sum_{i=0}^{m}( h_\theta(x_i)-y_i )^2</script><p>其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\theta(x_i)$ 为假设函数。</p>
<h3 id="梯度下降的代数方法描述"><a href="#梯度下降的代数方法描述" class="headerlink" title="梯度下降的代数方法描述"></a>梯度下降的代数方法描述</h3><ol>
<li><p>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。</p>
<p>则对应选定得损失函数为：</p>
<script type="math/tex; mode=display">
\jmath (\theta _0,\theta _1,...,,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2</script></li>
<li><p>算法相关参数的初始化<br>主要是初始化 $ \theta _0,\theta _1…,\theta _n$，算法终止距离 $\varepsilon $ 以及步长 $ \alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。</p>
</li>
<li><p>算法过程</p>
</li>
</ol>
<ul>
<li><p>1)：确定当前损失函数的梯度，对于$\theta _i $，其梯度表达式为：</p>
<script type="math/tex; mode=display">
\frac{\partial }{\partial \theta _i}\jmath (\theta _1,\theta _2,...,\theta _n)</script></li>
<li><p>2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即</p>
<script type="math/tex; mode=display">
\alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script></li>
<li><p>3)：确定是否所有的$\theta _i$ ，梯度下降的距离都小于 $ \varepsilon $，如果小于$ \varepsilon $，则算法停止，当前所有的 $\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。</p>
</li>
<li><p>4)：更新所有的 $\theta$，对于$\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。</p>
<script type="math/tex; mode=display">
\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script><h3 id="梯度下降的矩阵方式描述"><a href="#梯度下降的矩阵方式描述" class="headerlink" title="梯度下降的矩阵方式描述"></a>梯度下降的矩阵方式描述</h3><ol>
<li>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。<br>假设函数对应的矩阵表示为：$ h_\theta (x) = X \theta $，假设函数 $h_\theta(x)$ 为mx1的向量，$\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。<br>则对应选定得损失函数为：<script type="math/tex; mode=display">
\jmath (\theta)=(X \theta −Y)^T (X \theta−Y)</script>其中YY是样本的输出向量，维度为m*1<br><br><br>2.算法相关参数初始化:<br>$\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\varepsilon $ ，步长 $\alpha$ 和 “梯度下降的代数方法”描述中一致。<br><br><br>3.算法过程</li>
</ol>
</li>
<li><p>1)：确定当前位置的损失函数的梯度，对于 $ \theta $ 向量,其梯度表达式如下：</p>
<script type="math/tex; mode=display">
\frac{ \partial }{\partial \theta } \jmath (\theta)</script></li>
<li>2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\alpha \frac{ \partial }{\partial \theta } \jmath (\theta)$ </li>
<li>3)：确定 $\theta$ 向量里面的每个值,梯度下降的距离都小于 $\varepsilon$，如果小于 $\varepsilon$ 则算法终止，当前 $\theta$ 向量即为最终结果。否则进入步骤4)</li>
<li>4)：更新 $\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)<script type="math/tex; mode=display">
\theta =\theta - \alpha \frac{ \partial }{\partial \theta } \jmath (\theta)</script></li>
</ul>
<h2 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h2><p>梯度上升和梯度下降的分析方式是一致的，只不过把 $ \theta $ 的更新中 减号变为加号。</p>
<h2 id="梯度下降的算法优化"><a href="#梯度下降的算法优化" class="headerlink" title="梯度下降的算法优化"></a>梯度下降的算法优化</h2><ol>
<li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>
</li>
<li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
</li>
</ol>
<p>3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\bar{x}$ 和标准差std(x)，然后转化为：</p>
<script type="math/tex; mode=display">
\frac{x - \bar{x}}{std(x)}</script><p>这样特征的新期望为0，新方差为1，迭代次数可以大大加快。</p>
<hr>
<p><a href="http://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="external">http://blog.csdn.net/walilk/article/details/50978864</a></p>
<p><a href="https://www.zhihu.com/question/24658302" target="_blank" rel="external">https://www.zhihu.com/question/24658302</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></p>
<p><a href="http://www.doc88.com/p-7844239247737.html" target="_blank" rel="external">http://www.doc88.com/p-7844239247737.html</a></p>
<hr>
<center>
【技术服务】，详情点击查看：
<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>
</center>

<hr>
<center>
<img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg">
<br>
扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！
</center>

<hr>
<center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>

<center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>
        
            <div class="donate-container">
    <div class="donate-button">
        <button id="donate-button">赞赏</button>
    </div>
    <div class="donate-img-container hide" id="donate-img-container">
        <img id="donate-img" src="" data-src="/assets/img/weixin.jpeg">
        <p> 你的支持是我进步的最大动力！ </p>
    </div>
</div>
        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/thinkgamer">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank" href="http://weibo.com/5352480017">
                            <span class="fa-stack fa-lg">
                                  <i class="iconfont icon-weibo"></i>
                            </span>
            </a>
        </li>
        

        

        
        <li>
            <a target="_blank"  href="https://github.com/thinkgamer">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        


        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://item.jd.com/12671716.html">处女作：推荐系统开发实战</a></span>
        <span>/</span>
        
        <span><a href="https://mp.weixin.qq.com/s/vkDfg3v5C7QPrLOTvTRH2w">搜索与推荐Wiki</a></span>
        <span>/</span>
        
        <span><a href="https://blog.csdn.net/gamer_gyt">CSDN博客</a></span>
        <span>/</span>
        
        <span><a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg">商务合作</a></span>
        <span>/</span>
        
        <span><a href="https://www.a2data.cn/">牛人之A2Data</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        || Created By <a href="https://blog.csdn.net/gamer_gyt">Thinkgamer</a></p>
</footer><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
