<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Thinkgamer的博客">
    <meta name="keyword"  content="Python,Django,爬虫,Hadoop,Maching Learning,数据挖掘,机器学习,云计算,大数据,深度学习,开发者,程序猿,程序媛,极客,编程,代码,开源,IT网站,Developer,Programmer,Coder,用户体验">
    <link rel="shortcut icon" href="/assets/img/favicon.ico">

    <title>
        
        常见的五种神经网络(5)-生成对抗网络（下）篇 - Thinkgamer的博客 | Thinkgamer&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> All In CTR、DL、ML、RL、NLP、KG </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/assets/img/head.jpg" />
        </div>
        <div class="name">
            <i>Thinkgamer</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#KL散度、JS散度、Wassertein距离"><span class="toc-text">KL散度、JS散度、Wassertein距离</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#KL散度"><span class="toc-text">KL散度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JS散度"><span class="toc-text">JS散度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Wassertrin距离"><span class="toc-text">Wassertrin距离</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#显式和隐式密度模型"><span class="toc-text">显式和隐式密度模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#网络分解与训练"><span class="toc-text">网络分解与训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#判别网络"><span class="toc-text">判别网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#生成网络"><span class="toc-text">生成网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网络训练"><span class="toc-text">网络训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DCGAN"><span class="toc-text">DCGAN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DCGAN介绍"><span class="toc-text">DCGAN介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型分析"><span class="toc-text">模型分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型坍塌"><span class="toc-text">模型坍塌</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#前向和逆向KL散度"><span class="toc-text">前向和逆向KL散度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-GAN"><span class="toc-text">W-GAN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> All In CTR、DL、ML、RL、NLP、KG </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        常见的五种神经网络(5)-生成对抗网络（下）篇
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-04-23 16:36:49</span></span>
        
        <span class="attr">标签：/
            
            <a class="tag" href="/tags/#常见的五种神经网络" title="常见的五种神经网络">常见的五种神经网络</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span></span>
        </span>
    </div>
    <div class="post-content no-indent">
        <p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/103231385" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/103231385</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p>
<p>该系列的其他文章：</p>
<ul>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li>
<li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/103231385" target="_blank" rel="external">常见的五种神经网络(4)-深度信念网络(上篇)</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/103437985" target="_blank" rel="external">常见的五种神经网络(4)-深度信念网络(下篇)</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/103754752" target="_blank" rel="external">常见的五种神经网络(5)-生成对抗网络（上篇）</a></li>
<li><a href="https://blog.csdn.net/Gamer_gyt/article/details/103754752" target="_blank" rel="external">常见的五种神经网络(5)-生成对抗网络（下篇）</a></li>
</ul>
<blockquote>
<p>在上一篇文章中介绍了<a href="https://blog.csdn.net/Gamer_gyt/article/details/103754752" target="_blank" rel="external">生成模型的基本结构、功能和变分自动编码器</a>，在本篇文章中主要介绍一下生成对抗网络（Generative Adversaarial Networks，GAN）</p>
<h1 id="KL散度、JS散度、Wassertein距离"><a href="#KL散度、JS散度、Wassertein距离" class="headerlink" title="KL散度、JS散度、Wassertein距离"></a>KL散度、JS散度、Wassertein距离</h1><h2 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h2><p>KL散度又称相对熵，信息散度，信息增益。KL散度是两个概率分布P和Q差别的非对称性的度量。在经典境况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布。</p>
</blockquote>
<script type="math/tex; mode=display">
D_{KL}(P \parallel Q)= \sum_{i=1}^{n}P_i log(\frac{P_i}{Q_i})</script><h2 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a>JS散度</h2><p>JS散度是度量两个概率分布的相似度，是基于KL散度的变体，解决了KL散度非对称的问题。</p>
<script type="math/tex; mode=display">
D_{JS}(P \parallel Q)=\frac{1}{2} D_{KL}(P \parallel \frac{P+Q}{2}) + \frac{1}{2} D_{KL}(Q \parallel \frac{P+Q}{2})</script><p>KL散度和JS散度度量的时候都有一个问题：如果两个分布P,Q距离较远，完全没有重叠的时候，KL散度是没有意义的，在学习的时候，这就意味着在这一点的梯度为0，即梯度消失了。</p>
<h2 id="Wassertrin距离"><a href="#Wassertrin距离" class="headerlink" title="Wassertrin距离"></a>Wassertrin距离</h2><p>Wasserstein距离度量的是两个管理分布之间的距离。</p>
<script type="math/tex; mode=display">
W(P,Q) = \underset{\gamma  \sim \prod (P, Q) }{inf} E_{(x,y) \sim \gamma} \left [ ||x-y|| \right ]</script><p>其中$\prod (P, Q)$是边际分布为$P$和$Q$的所有可能的联合分布集合。</p>
<h1 id="显式和隐式密度模型"><a href="#显式和隐式密度模型" class="headerlink" title="显式和隐式密度模型"></a>显式和隐式密度模型</h1><p>在上一篇文章中介绍的变分自动编码器，之前介绍的深度信念网络都是显式的构建样本的密度函数$p(x|\theta)$，并通过最大似然估计来求解参数，称之为<strong>显式密度模型（Explicit Density Model）</strong>。</p>
<p>如果只是希望有一个模型能生成符合数据分布$p_r(x)$的样本，那么可以不显式地估计出数据分布的密度函数。假设在低维空间$Z$中有一个简单容易采样的分布$p(z)$，$p(z)$通常为标准多元正态分布$N(0,1)$。我们使用神经网络构建一个映射函数$G: Z \rightarrow X$称为生成网络。利用神经网络强大的拟合能力，使得$G(z)$服从数据分布$p_r(x)$。这种模型就称为<strong>隐式密度模型（Implicit Density Model）</strong>。所谓隐式模型就是指并不显示地建模$p_r(x)$，而是建模生成过程。</p>
<h1 id="网络分解与训练"><a href="#网络分解与训练" class="headerlink" title="网络分解与训练"></a>网络分解与训练</h1><h2 id="判别网络"><a href="#判别网络" class="headerlink" title="判别网络"></a>判别网络</h2><p>隐式密度模型的一个关键是如何确保生成网络产生的样本一定是服从真实的数据分布。既然我们不构建显式密度模型，就无法通过最大似然估计等方法来训练。</p>
<p>生成对抗网络（Generative Adversarial Networks，GAN）是通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布。在生成对抗网络中，有两个网络进行对抗训练。一个是<strong>判别网络</strong>，目标是尽量准确地判断一个样本是来自于真实数据还是生成网络产生的；另一个是<strong>生成网络</strong>，目标是尽量生成判别网络无法区分来源的样本。这两个目标相反的网络不断地进行交替训练。当最后收敛时，如果判别网络再也无法判断出一个样本的来源，那么也就等价于生成网络可以生成符合真实数据分布的样本。生成对抗网络的流程图如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20191230233737148.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="生成对抗网络的流程图"></p>
<p>判别网络（Discriminator Network）$D(x,\phi)$ 的目标是区分出一个样本$x$是来自于真实分布$p_r(x)$还是来自于生成模型$p_{\theta}(x)$，因此判别网络实际上一个两类分类器。用标签$y=1$来表示样本来自于真实分布，$y=0$表示样本来自模型，判别网络的$D(x,\phi)$的输出为$x$属于真实数据分布的概率，即：</p>
<script type="math/tex; mode=display">
p(y=1|x)=D(x, \phi)</script><p>则样本来自模型生成的概率位$p(y=0|x)=1-D(x, \phi)$</p>
<p>给定一个样本$(x,y),y=\{1,0\}$表示其是来自于$p_r(x)$还是$p_{\theta}(x)$，判别网络的目标函数为最小化交叉熵，即最大化对数函数（公式1-1）。</p>
<script type="math/tex; mode=display">
\underset{\phi}{min} = \left ( E_x[y log \,\, p(y=1 | x) + (1-y) log \,\, p(y=0|x) ]  \right )
\\
= \underset{\phi}{max}  \left (   E_{x\sim p_r(x)} [log \,\, D(x, \phi)] + E_{z\sim p(z)}[ log (1-D(G(z,\theta), \phi))]   \right )</script><p>其中$\theta, \phi$分别是生成网络和判别网络的参数。</p>
<h2 id="生成网络"><a href="#生成网络" class="headerlink" title="生成网络"></a>生成网络</h2><p><strong>生成网络（Generator Network）</strong> 的目标刚好和判别网络相反，即让判别网络将自己生成的样本判别为真实样本。其目标函数如下（公式1-2）：</p>
<script type="math/tex; mode=display">
 \underset{\theta}{max} \left (   E_{z\sim p(z)} [log \,\, D(G(z , \theta), \phi)]   \right )
\\
=  \underset{\theta}{min} \left (   E_{z\sim p(z)} [log \,\, (1-D(G(z , \theta), \phi))]   \right )</script><p>上面的这两个目标函数是等价的。但是在实际训练时，一般使用前者，因为其梯度性质更好。我们知道，函数$log(x), x\in (0,1)$在$x$接近1时的梯度要比接近0时的梯度小很多，接近饱和区间。这样，当判别网络$D$以很高的概率认为生成网络$G$产生的样本是“假”样本，即$(1-D(G(z, \theta), \phi)) \rightarrow 1$。这时目标函数关于$\theta$的梯度反而很小，从而不利于优化。</p>
<h2 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h2><p>在生成对抗网络的训练过程中，需要平衡两个网络的能力。对于判别网络来说，一开始的判别能力不能太强，否则难以提升生成网络的能力。然后也不能太弱，否则针对他训练的生产网络也不会太好。在训练时需要使用一些技巧，使得在每次迭代中，判别网络臂生成网络的能力强一些，但又不能强太多。</p>
<p>生成对抗网络的训练流程如下所示，每次迭代时，判别网络更新$K$次而生成网络更新一次，即首先要保证判别网络足够强才能开始训练生成网络。在实践中$K$是一个超参数，其取值一般取决于具体任务。</p>
<p><img src="https://img-blog.csdnimg.cn/20191231090237628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="生成对抗网络的训练流程"></p>
<h1 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h1><h2 id="DCGAN介绍"><a href="#DCGAN介绍" class="headerlink" title="DCGAN介绍"></a>DCGAN介绍</h2><p>生成对抗网络是指一类采用对抗训练方式来进行学习的深度生成模型，其包含的判别网络和生成网络都可以根据不同的生成任务使用不同的网络结构。</p>
<p>在深度卷积生成对抗网络（Dee Convolutional Generative Adversarial Neteorks，DCGAN）中，判别网络是一个传统的深度卷积网络，但使用了带步长的卷积来实现下采样操作，不用最大汇聚（pooling）操作。生成网络使用一个特殊的深度卷积网络来实现，如下图所示，使用微步卷积来生成64x63大小的图像。</p>
<p><img src="https://img-blog.csdnimg.cn/20191231091304687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="DCGAN中的生成网络"></p>
<p>上图中，第一层是全连接层，输入是从均匀分布中随机采样的100维向量$z$，输出是4x4x1024的向量，重塑为4x4x1024的张量，然后是四层的微步卷积，没有汇聚层。</p>
<p>DCGAN的主要优点是通过一些经验性的网络结构设计使得对抗训练更加稳定。比如：</p>
<ul>
<li>（1）使用代步长的卷积（在判别网络中）和微步卷积（在生成网络中）来代替汇聚操作，以免损失信息</li>
<li>（2）使用批量归一化</li>
<li>（3）去除卷积层之后的全连接层</li>
<li>（4）在生成网络中，除了最后一层使用Tanh激活函数外，其余层都使用ReLU函数</li>
<li>（5）在判别网络中，都适用LeakyReLU激活函数</li>
</ul>
<h2 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h2><p>将判别网络和生成网络合并，整个生成对抗网络得整个目标函数看作最小最大化游戏（Minimax Game），表达式如下（1-3）：</p>
<script type="math/tex; mode=display">
\underset{\theta}{min} \, \underset{\phi}{max}\left (  E_{x \sim p_r{(x)}} \left [  log\,\, D(x, \phi) \right ] + E_{x \sim p_{\theta}(x)} \left [  log\,\,(1- D(x, \phi)) \right ]  \right )</script><p>因为之前提到的生成网络梯度问题，这个最小化最大化形式的目标函数一般用来进行理论分析，并不是实际训练时的目标函数。</p>
<p>假设$p_r(x)$和$p_{\theta}(x)$已知，则最优得判别器为：</p>
<script type="math/tex; mode=display">
D^*(x) =  \frac{p_r(x)}{p_r(x) + p_{\theta}(x)}</script><p>将最优得判别器$D^*(x)$代入公式1-3，则目标函数变为（公式1-4）：</p>
<script type="math/tex; mode=display">
L(G|D^*) =  E_{x \sim p_r{(x)}} \left [  log\,\, D^*(x) \right ] + E_{x \sim p_r{(\theta})} \left [  log\,\,(1- D^*(x)) \right ] 
\\
= 2D_{JS}(p_r||p_{\theta}) - 2log2</script><p>其中$D_{JS}$为JS散度。</p>
<p>在生成对抗网络中，当判别网络为最优时，生成网络的优化目标是最小化真实分布$p_r$和模型分布$p_{\theta}$之间得JS散度。当两个分布相同时，JS散度为0，最优生成网络$G^<em>$对应得损失为$L(G^</em>|D^*)=-2log2$。</p>
<p>然而JS散度的一个问题是：当两个分布没有重叠时，他们之间得JS散度恒等于常数log2。对生成网络来说，目标函数关于参数的梯度为0。</p>
<script type="math/tex; mode=display">
\frac{ \partial L(G|D^*) }{ \partial \theta} =0</script><p>下图给出了生成对抗网络中的梯度消失问题的示例。当真实分布$p_r$和模型分布$p_{\theta}$没有重叠，最优的判断网对对所有生成数据得输出都为0，$D^*(G(z, \theta))=0, \forall  z$。因此生成网络得梯度消失。</p>
<p><img src="https://img-blog.csdnimg.cn/20191231134841397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="生成网络中得梯度消失问题"></p>
<p>在实际训练生成对抗网络时，我们一般不会将判别网络训练到最优，只进行一步或多步梯度下降，使得生成网络的梯度依然存在。然而，判别网络也不能太差，否则生成网络的梯度为错误的梯度。如何使得判别网络在梯度消失和梯度错误之间取得平衡并不是一件容易的事。</p>
<h2 id="模型坍塌"><a href="#模型坍塌" class="headerlink" title="模型坍塌"></a>模型坍塌</h2><p>如果使用公式1-2作为生成网络的目标函数，将最优判断网络$D^*$代入，得到：</p>
<script type="math/tex; mode=display">
L'(G|D^*) = E_{x \sim p_{\theta}(x)} \left [ log \, D^*(x) \right ]
\\
= E_{x \sim p_{\theta}(x)} \left [ log \, \frac {p_r(x)}{p_r(x) + p_{\theta}(x) } \cdot  \frac{p_{\theta}(x) }{p_{\theta}(x) } \right ]
\\
= - E_{x \sim p_{\theta}(x)}\left [ log \, \frac{p_{\theta}(x)}{p_r(x)} \right ] + E_{x \sim p_{\theta}(x)} \left [  log \, \frac {p_r(x)}{p_r(x) + p_{\theta}(x) }  \right ]
\\
= -D_{KL}(p_{\theta} || p_r) +  E_{x \sim p_{\theta}(x)} \left [ log (1-D^*(x)) \right ]
\\ 
 = -D_{KL}(p_{\theta} || p_r) +  2D_{JS}(p_r || p_{\theta})-2log2 - E_{x \sim p_r(x)} \left [ log \, D^* (x) \right ]</script><p>其中后两项和生成网络无关，因此：</p>
<script type="math/tex; mode=display">
\underset{ \theta }{ max } L'(G | D^*) =\underset{ \theta }{ min } D_{KL} (p_{\theta} || p_r) - 2D_{JS} (p_r || p_{\theta} )</script><p>其中JS散度和$D_{JS}(p_{\theta} || p_r) \in [0, log 2]$为有界函数，因此生成网络的目标是为更多的受逆向KL散度$D_{KL}(p_{\theta} || p_r)$影响，使得生成网络更倾向于生成一些更安全的样本，从而造成<strong>模型坍塌（Model Collapse）</strong>问题</p>
<h2 id="前向和逆向KL散度"><a href="#前向和逆向KL散度" class="headerlink" title="前向和逆向KL散度"></a>前向和逆向KL散度</h2><p>KL散度是一种非对称的散度，在计算真实分布$p_r$和模型分布$p_{\theta}$之间得KL散度时，按照顺序不同，有两种KL散度：前向KL散度（Forward KL divergence）$D_{KL}(p_r || p_{\theta})$ 和逆向KL散度（Reverse KL divergence）$D_{KL}(p_{\theta} || p_r)$</p>
<p>前向和逆向KL散度分别定义为：</p>
<script type="math/tex; mode=display">
D_{KL}(p_r || p_{\theta}) = \int p_r(x) log\, \frac{p_r(x)}{p_{\theta}(x)}dx
\\
D_{KL}(p_{\theta} || p_r) = \int p_{\theta}(x) log\, \frac{p_{\theta}(x)}{p_r(x)}dx</script><p>在前向KL散度中：</p>
<ul>
<li>（1）当$p_r(x) \rightarrow 0$而 $p_{\theta}(x)&gt; 0$时，$p_r(x) log\, \frac{p_r(x)}{p_{\theta}(x)} \rightarrow  0$。不管$p_{\theta}(x)$如何取值，都对前向KL散度的计算没有贡献。</li>
<li>（2）当$p_r(x) &gt; 0$而 $p_{\theta}(x) \rightarrow 0$时，$p_r(x) log\, \frac{p_r(x)}{p_{\theta}(x)} \rightarrow  \infty$。前向KL散度会变得非常大。</li>
</ul>
<p>因此，前向KL散度会鼓励模型分布$p_{\theta}(x)$尽可能的覆盖所有真实分布$p_r(x)&gt;0$的点，而不用回避$p_r(x)\approx 0$的点。</p>
<p>在逆向KL散度中：</p>
<ul>
<li>（1）当$p_r(x) \rightarrow 0$而 $p_{\theta}(x)&gt; 0$时，$p_{\theta}(x) log\, \frac{p_{\theta}(x)}{p_r(x)} \rightarrow  \infty$。即当$p_{\theta}(x)$接近于0，而$p_{\theta}(x)$有一定的密度时，逆向KL散度会变得非常大。</li>
<li>（2）当$p_{\theta}(x) \rightarrow 0$, 不管 $p_r(x)$如何取值，$p_{\theta}(x) log\, \frac{p_{\theta}(x)}{p_r(x)} \rightarrow 0$。</li>
</ul>
<p>因此逆向KL散度会鼓励模型分布$p_{\theta}(x)$尽可能避开所有真实分布$p_r(x)\approx 0$的点，而不需要考虑是否覆盖所有分布为$p_r(x) &gt; 0$的点。</p>
<p>下图给出数据真实分布为一个高斯混合分布，模型分布为一个旦高斯分布时，使用前向和逆向KL散度来进行模型优化的示例，蓝色曲线为真实分布$p_r$的等高线，红色曲线为模型分布$p_{\theta}$的等高线。</p>
<p><img src="https://img-blog.csdnimg.cn/2019123114461746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="前向和逆向KL散度"></p>
<h2 id="W-GAN"><a href="#W-GAN" class="headerlink" title="W-GAN"></a>W-GAN</h2><p>W-GAN是一种通过使用Wassertein距离替代JS散度来优化训练的生成对抗网络。对于真实分布$p_r$和模型分布$p_{\theta}$，他们的1st-wassertein距离为：</p>
<script type="math/tex; mode=display">
W^1(p_r,p_{\theta}) = \underset{\gamma  \sim \prod (P_r, P_g) }{inf} E_{(x,y) \sim \gamma} \left [ ||x-y|| \right ]</script><p>其中$\prod (P_r, P_g)$是边际分布为$p_r$和$p_{\theta}$的所有可能的联合分布集合。</p>
<p>当两个分布没有重叠或者重叠非常少时，他们之间的KL散度为$+ \infty$，JS散度为log2，并不随着两个分布之间的距离而变化。而1st-wassertein距离可以依然衡量两个没有重叠分布的距离。</p>
<p>W-GAN的目标函数为：</p>
<script type="math/tex; mode=display">
\underset{\theta}{ max } E_{z \sim p(z)} \left [ f(G(z, \theta), \phi) \right ]</script><p>因为$f(x, \phi)$为不饱和函数，所以生成网络参数$\theta$的梯度不会消失，理论上解决了原始GAN训练不稳定的问题。并且W-GAN中生成网络的目标函数不再是两个分布的比率，在一定程度上缓解了模型坍塌问题，使得生成的样本具有多样性。</p>
<p>下图给出了W-GAN的训练过程，和原始GAN相比，W-GAN的评价网络最后一层不使用sigmoid函数，损失函数不取对数。</p>
<p><img src="https://img-blog.csdnimg.cn/20191231151625579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="W-GAN的训练过程"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>深度生成模型是一种有机地融合神经网络和概率图模型的生成模型,将神经网络作为一个概率分布的逼近器,可以拟合非常复杂的数据分布。</p>
<p>变分自编码器是一个有意义的深度生成模型,可以有效地解决含隐变量的概率模型中后验分布难以估计的问题。</p>
<p>生成对抗网络是一个具有开创意义的深度生成模型,突破了以往的概率模型必须通过最大似然估计来学习参数的限制。DC-GAN是一个生成对抗网络的成功实现,可以生成十分逼真的自然图像。对抗生成网络的训练不稳定问题的一种有效解决方法是W-GAN,通过用 Wassertein 距离替代 JS 散度来进行训练。</p>
<p>虽然深度生成模型取得巨大的成功,但是作为一种无监督模型,其主要的缺点是缺乏有效的客观评价,因此不同模型之间的比较很难客观衡量。</p>
<hr>
<center>
【技术服务】，详情点击查看：
<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>
</center>

<hr>
<center>
<img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg">
<br>
扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！
</center>

<hr>
<center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>

<center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>
        
            <div class="donate-container">
    <div class="donate-button">
        <button id="donate-button">赞赏</button>
    </div>
    <div class="donate-img-container hide" id="donate-img-container">
        <img id="donate-img" src="" data-src="/assets/img/weixin.jpeg">
        <p> 你的支持是我进步的最大动力！ </p>
    </div>
</div>
        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/thinkgamer">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank" href="http://weibo.com/5352480017">
                            <span class="fa-stack fa-lg">
                                  <i class="iconfont icon-weibo"></i>
                            </span>
            </a>
        </li>
        

        

        
        <li>
            <a target="_blank"  href="https://github.com/thinkgamer">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        


        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://item.jd.com/12671716.html">处女作：推荐系统开发实战</a></span>
        <span>/</span>
        
        <span><a href="https://mp.weixin.qq.com/s/vkDfg3v5C7QPrLOTvTRH2w">搜索与推荐Wiki</a></span>
        <span>/</span>
        
        <span><a href="https://blog.csdn.net/gamer_gyt">CSDN博客</a></span>
        <span>/</span>
        
        <span><a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg">商务合作</a></span>
        <span>/</span>
        
        <span><a href="https://www.a2data.cn/">牛人之A2Data</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        || Created By <a href="https://blog.csdn.net/gamer_gyt">Thinkgamer</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
