<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Thinkgamer的博客">
    <meta name="keyword"  content="Python,Django,爬虫,Hadoop,Maching Learning,数据挖掘,机器学习,云计算,大数据,深度学习,开发者,程序猿,程序媛,极客,编程,代码,开源,IT网站,Developer,Programmer,Coder,用户体验">
    <link rel="shortcut icon" href="/assets/img/favicon.ico">

    <title>
        
        线性模型篇之softmax数学公式推导 - Thinkgamer的博客 | Thinkgamer&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> All In CTR、DL、ML、RL、NLP、KG </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/assets/img/head.jpg" />
        </div>
        <div class="name">
            <i>Thinkgamer</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SoftMax回归简介"><span class="toc-text">SoftMax回归简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参数学习"><span class="toc-text">参数学习</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> All In CTR、DL、ML、RL、NLP、KG </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        线性模型篇之softmax数学公式推导
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-04-07 07:24:46</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#SoftMax" title="SoftMax">SoftMax</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#线性回归" title="线性回归">线性回归</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <blockquote>
<p>Softmax回归也称多项（multinomial）或者多类（multi-class）的Logistic回归，是Logistic回归在多类分类问题上的推广。和逻辑回归一样属于线性模型。</p>
</blockquote>
<a id="more"></a>
<h1 id="SoftMax回归简介"><a href="#SoftMax回归简介" class="headerlink" title="SoftMax回归简介"></a>SoftMax回归简介</h1><p>对于多类问题，类别标签</p>
<script type="math/tex; mode=display">
y \in {1,2,3,...,C}</script><p>可以用C个取值，给定一个样本x，softmax回归预测的是属于类别c的概率为(公式-1)：</p>
<script type="math/tex; mode=display">
p(y=c|x)=softmax(w_c^Tx)=\frac{exp(w_c^Tx)}{\sum_{c=1}^{C}exp(w_c^Tx)}</script><p>其中w_c是第c类的权重向量。</p>
<p>softmax回归的决策函数可以表示为(公式-2)：</p>
<script type="math/tex; mode=display">
\hat{y}=  \underset{c=1}{ \overset{C}{arg max} } \ p(y=c|x) =\underset{c=1}{ \overset{C}{arg max} } \ w_c^T x</script><hr>
<p>softMax与Logistic回归的关系：</p>
<p>当类别个C=2时，softMax回归的决策函数为(公式-3)：</p>
<script type="math/tex; mode=display">
\hat{y} = \underset{y\in {0,1}}{ arg max } \ w_y^Tx=I(w_1^Tx - w_0^Tx >0 )=I((w_1 - w_0)^Tx >0 )</script><p>其中I(.)是指示函数，对比二分类决策函数(公式-4)</p>
<script type="math/tex; mode=display">
g(f(x,w))=sgn(f(x,w))=\begin{cases}
 & +1 \text{ if } f(x,w)>0 \\ 
 & -1 \text{ if } f(x,w)<0 
\end{cases}</script><p>其中sgn表示符号函数(sign function)，可以发现两类分类中的权重向量w=w1-w0</p>
<hr>
<p>向量表示：</p>
<p>公式-1用向量形式可以写为(公式-5)</p>
<script type="math/tex; mode=display">
\hat{y}=softmax(W^Tx)=\frac{erp(W^Tx)}{1^Texp(W^Tx)}</script><p>其中W=[w_1,w_2,…,w_C]是由C个类的权重向量组成的矩阵，1为全1的向量，</p>
<script type="math/tex; mode=display">
\hat{y}\in  R^C</script><p>为所有类别的预测条件概率组成的向量，第c维的值是第c类的预测条件概率。</p>
<h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>给定N个训练样本{(x^n, y^n)},n&lt;=N，softmax回归使用交叉熵损失函数来学习最优的参数矩阵W。</p>
<p>这里用C维的one-hot向量</p>
<script type="math/tex; mode=display">
y \in {0,1} ^C</script><p>来表示类别标签，其向量表示为(公式-6)：</p>
<script type="math/tex; mode=display">
y = [I(1=c),I(2=c),...,I(C=c)]^T</script><p>其中I(.)为指示函数。</p>
<p>采用交叉熵损失函数，softmax的经验风险函数为(公式-7)：</p>
<script type="math/tex; mode=display">
R(W)=-\frac{1}{N}\sum_{n=1}^{N}\sum_{c=1}^{C}y_c^nlog\hat{y}_c^n
R(W)=-\frac{1}{N}\sum_{n=1}^{N} (y^n)^Tlog\hat{y}^n</script><p>其中</p>
<script type="math/tex; mode=display">
\hat{y}^n = softmax(W^Tx^n)</script><p>为样本x^n在每个类别的后验概率。</p>
<p>==说明：公式-7第一个式变换到第二个式是因为y_c类别中只有一个为1，其余为0，所以将第二个求和去除。==</p>
<p>风险函数R(W)关于W的梯度为(公式-8)：</p>
<script type="math/tex; mode=display">
\frac{\partial R(W)}{\partial W} = -\frac{1}{N}\sum_{n=1}^{N}x^n(y^n-\hat{y}^n)^T</script><p>==<strong>证明：</strong>==</p>
<p>计算公式-8中的梯度，关键在于计算每个样本的损失函数</p>
<script type="math/tex; mode=display">
L^n(W)=-(y^n)^Tlog\hat{y}^n</script><p>关于参数W的梯度，其中需要用到两个导数公式为：</p>
<ul>
<li>若y=softmax(z)，则</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial z}=diag(y)-yy^T</script><ul>
<li>若</li>
</ul>
<script type="math/tex; mode=display">
z=W^Tx=[w_1^Tx,w_2^Tx,...,w_C^Tx]^T</script><p>则</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial w_c}</script><p>为第c列为x，其余为0的矩阵。</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial w_c} = [ \frac{\partial w_1^Tx}{\partial w_c},\frac{\partial w_2^Tx}{\partial w_c},...,\frac{\partial w_C^Tx}{\partial w_c} ]
=[0,0,..,x,...,0]
=M_c(x)</script><p>根据链式法则，</p>
<script type="math/tex; mode=display">
L^n(W) = -(y^n)^T log\hat{y}^n</script><p>关于w_c的偏导数为(公式-12)：</p>
<script type="math/tex; mode=display">
\frac{\partial L^n(W) }{\partial w_c}
= -\frac{ \partial ((y^n)^T log \hat{y}^n) }{\partial w_c}</script><script type="math/tex; mode=display">
= -\frac{\partial z^n}{ \partial w_c } \frac{\partial \hat{y}^n}{ \partial z^n }\frac{\partial log \hat{y}^n}{ \partial \hat{y}^n } y^n</script><script type="math/tex; mode=display">
=-M_c(x^n)(diag(\hat{y}^n)-\hat{y}^n(\hat{y}^n)^T)(diag(\hat{y}^n))^{-1} y^n</script><script type="math/tex; mode=display">
=-M_c(x^n)(I-\hat{y}^n1^T)y^n</script><script type="math/tex; mode=display">
=-M_c(x^n)(y^n-\hat{y}^n1^Ty^n)</script><script type="math/tex; mode=display">
=-M_c(x^n)(y^n-\hat{y}^n)</script><script type="math/tex; mode=display">
=-x^n[y^n-\hat{y}^n]_c</script><p>公式-12也可以表示为非向量形式(公式-13)：</p>
<script type="math/tex; mode=display">
\frac{\partial L^n(W) }{\partial w_c}= -x^n(I(y^n=c)-\hat{y}_c^n)</script><p>其中I(.)为指示函数，根据公式-12可以得到(公式-14)</p>
<script type="math/tex; mode=display">
\frac{\partial L^n(W) }{\partial W} = -x^n(y^n-\hat{y}^n)^T</script><p>采用梯度下降法，softmax回归的训练过程为：初始化W_0 &lt;- 0，然后通过下式进行迭代更新。</p>
<script type="math/tex; mode=display">
W_{t+1} = W_t + \alpha (\frac{1}{N} \sum_{n=1}^{N}x^n(y^n - \hat{y}_{W_t} ^ n)^T)</script><p>其中a是学习率，</p>
<script type="math/tex; mode=display">
\hat{y}_{W_t}^n</script><p>是当参数为W_t时，softmax回归模型的输出。</p>
<hr>
<p><strong>注意：</strong></p>
<blockquote>
<p>softmax回归中使用的C个权重向量是冗余的，即对所有权重向量都减去一个同样的向量v，不改变其输出结果。因此，softmax往往需要正则化来约束参数。此外，可以利用这个特性来避免计算softmax函数时在数值计算上溢出问题。</p>
</blockquote>
<hr>
<center>
<img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
</center>
<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>

        
            <div class="donate-container">
    <div class="donate-button">
        <button id="donate-button">赞赏</button>
    </div>
    <div class="donate-img-container hide" id="donate-img-container">
        <img id="donate-img" src="" data-src="/assets/img/weixin.jpeg">
        <p> 你的支持是我进步的最大动力！ </p>
    </div>
</div>
        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/thinkgamer">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank" href="http://weibo.com/5352480017">
                            <span class="fa-stack fa-lg">
                                  <i class="iconfont icon-weibo"></i>
                            </span>
            </a>
        </li>
        

        

        
        <li>
            <a target="_blank"  href="https://github.com/thinkgamer">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        


        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        || Created By <a href="https://blog.csdn.net/gamer_gyt">Thinkgamer</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
