<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Thinkgamer的博客">
    <meta name="keyword"  content="Python,Django,爬虫,Hadoop,Maching Learning,数据挖掘,机器学习,云计算,大数据,深度学习,开发者,程序猿,程序媛,极客,编程,代码,开源,IT网站,Developer,Programmer,Coder,用户体验">
    <link rel="shortcut icon" href="/assets/img/favicon.ico">

    <title>
        
        神经网络中的网络优化和正则化（四）之正则化 - Thinkgamer的博客 | Thinkgamer&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> All In CTR、DL、ML、RL、NLP、KG </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/assets/img/head.jpg" />
        </div>
        <div class="name">
            <i>Thinkgamer</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#引言"><span class="toc-text">引言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#l-1-l-2-正则"><span class="toc-text">$l_1,l_2$正则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#权重衰减"><span class="toc-text">权重衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#提前终止"><span class="toc-text">提前终止</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#丢弃法"><span class="toc-text">丢弃法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据增强"><span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#标签平滑"><span class="toc-text">标签平滑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> All In CTR、DL、ML、RL、NLP、KG </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        神经网络中的网络优化和正则化（四）之正则化
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-09-22 21:25:04</span></span>
        
        <span class="attr">标签：/
            
            <a class="tag" href="/tags/#神经网络" title="神经网络">神经网络</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span></span>
        </span>
    </div>
    <div class="post-content no-indent">
        <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍（如下所示），本篇为最后一篇主要介绍神经网络中的网络正则化。</p>
<ul>
<li>第一篇包括<ul>
<li>网络优化和正则化概述</li>
<li>优化算法介绍</li>
</ul>
</li>
<li>第二篇包括<ul>
<li>参数初始化 </li>
<li>数据预处理</li>
<li>逐层归一化</li>
</ul>
</li>
<li>第三篇包括<ul>
<li>超参数优化</li>
</ul>
</li>
<li>第四篇包括<ul>
<li>网络正则化</li>
</ul>
</li>
</ul>
<p>机器学习模型中的关键是泛化问题，即样本在真实数据集上的期望风险最小化，而在训练集上的经验风险最小化和期望风险并不一致。由于神经网络的拟合能力很强，其在训练集上的训练误差会降的很小，从而导致过拟合。</p>
<p><strong>正则化（Regularization）</strong>是一类通过限制模型复杂度，从而避免过拟合，提高模型泛化能力的一种方法，包括引入一些约束规则，增加先验，提前终止等。</p>
<p>在传统的机器学习模型中，提高模型泛化能力的主要方法是限制模型复杂度，比如$l_1,l_2$正则，但是在训练深层神经网络时，特别是在过度参数（OverParameterized）时，$l_1,l_2$正则化不如机器学习模型中效果明显，因此会引入其他的一些方法，比如：数据增强，提前终止，丢弃法，继承法等。</p>
<h3 id="l-1-l-2-正则"><a href="#l-1-l-2-正则" class="headerlink" title="$l_1,l_2$正则"></a>$l_1,l_2$正则</h3><p>$l_1,l_2$正则是机器学习中常用的正则化方法，通过约束参数的$l_1,l_2$范数来减少模型在训练数据上的过拟合现象。</p>
<p>通过引入$l_1,l_2$正则，优化问题变为：</p>
<script type="math/tex; mode=display">
a\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta))+\lambda l_p(\theta)</script><p>$L$为损失函数，$N$为训练的样本数量，$f(.)$为待学习的神经网络，$\theta$为参数，$l_p$为$l_1,l_2$正则中的一个，$\lambda$为正则项系数。</p>
<p>带正则化的优化问题等价于下面带约束条件的优化问题：</p>
<script type="math/tex; mode=display">
\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta))
\\
subject \, to \, l_p(\theta) \leq 1</script><p>下图给出了不同范数约束条件下的最优化问题示例：<br><img src="https://img-blog.csdnimg.cn/20190926205404238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="不同范数约束条件下的最优化问题示例"></p>
<p>上图中红线表示$l_p$范数，黑线表示$f(\theta)$的等高线（简单起见，这里用直线表示）</p>
<p>从上图最左侧图可以看出，$l_1$范数的约束条件往往会使最优解位于坐标轴上，从而使用最终的参数为稀疏向量，此外$l_1$范数在零点不可导，常用下式来代替：</p>
<script type="math/tex; mode=display">
l_1(\theta) = \sum_{i} \sqrt{\theta_i ^2 + \epsilon }</script><p>其中$\epsilon$为一个非常小的常数。</p>
<p>一种折中的方法是<strong>弹性网络正则化（Elastic Net Regularization）</strong> ，同时加入$l_1, l_2$正则，如下：</p>
<script type="math/tex; mode=display">
a\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta)_+\lambda_1 l_1(\theta) + \lambda_2 l_2(\theta)</script><p>其中$\lambda_1, \lambda_2$分别是正则化项的参数。</p>
<h3 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h3><p><strong>权重衰减（Weight Deacy）</strong> 也是一种有效的正则化方法，在每次调参时，引入一个衰减系数，表示式为：</p>
<script type="math/tex; mode=display">
\theta_t \leftarrow (1-w)\theta_{t-1} - \alpha g_t</script><p>其中$g_t$为第t次更新时的梯度，$\alpha$为学习率，$w$为权重衰减系数，一般取值比较小，比如0.0005。在标准的随机梯度下降中，权重衰减和$l_2$正则达到的效果相同，因此，权重衰减在一些深度学习框架中用$l_2$正则来代替。但是在较为复杂的优化方法中，两者并不等价。</p>
<h3 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h3><p><strong>提前终止（early stop）</strong> 对于深层神经网络而言是一种简单有效的正则化方法，由于深层神经网络拟合能力很强，比较容易在训练集上过拟合，因此在实际操作时往往产出一个和训练集独立的验证集，并用在验证集上的错误来代表期望错误，当验证集上的错误不再下降时，停止迭代。</p>
<p>然而在实际操作中，验证集上的错误率变化曲线并不是一条平衡的曲线，很可能是先升高再降低，因此提前停止的具体停止标准需要根据实际任务上进行优化。</p>
<h3 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h3><p>当训练一个深层神经网络时，可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法称为 <strong>丢弃法（Dropout Method）</strong>。每次丢弃的神经元为随机的，对于每一个神经元都以一个概率p来判断要不要停留，对于每一个神经层 $y=f(Wx + b)$，我们可以引入一个丢弃函数$d(.)$使得$y=f(Wd(x)+b)$。丢弃函数的定义为：</p>
<script type="math/tex; mode=display">
d(x) = \left\{\begin{matrix}
m \odot x, When \, Train\\ 
px, When \,  Test
\end{matrix}\right.</script><p>其中$m \in \{0,1\}^d$是丢弃掩码（dropout mask），通过以概率为p的贝努力分布随机生成，$p$可以通过一个验证集选取一个最优值，也可以设置为0.5， 这样对大部分网络和任务比较有效。在训练时，神经元的平均数量为原来的$p$倍，而在测试时，所有的神经元都可以是激活的，这会造成训练时和测试时的网络结构不一致，为了缓解这个问题，在测试时，需要将每一个神经元的输出乘以$p$，也相当于把不同的神经网络做了一个平均。</p>
<p>下图给出了一个网络经过dropout的示例。<br><img src="https://img-blog.csdnimg.cn/20190926170817842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="丢弃法示例"></p>
<p>一般来讲，对于隐藏层的神经元，丢弃率$p=0.5$时最好，这样当训练时有一半的神经元是丢弃的，随机生成的网络结构具有多样性。对于输入层的神经元，其丢弃率往往设置为更接近于1的数，使得输入变化不会太大，对输入层的神经元进行丢弃时，相当于给数据增加噪声，提高网络的鲁棒性。</p>
<p>丢弃法一般是针对神经元进行随机丢弃，但是也可以扩展到神经元之间的连接进行随机丢弃，或每一层进行随机丢弃。</p>
<p>丢弃法有两种解释：</p>
<p>（1）集成学习的解释<br>每做一次丢弃，相当于从原始的网络中采样得到一个子网络，如果一个神经网络有n个神经元，那么可以采样出$2^n$个子网络，每次训练都相当于是训练一个不同的子网络，这些子网络都共享最开始的参数。那么最终的网络可以看成是集成了指数级个不同风格的组合模型。</p>
<p>（2）贝叶斯学习的解释</p>
<p>丢弃法也可以解释为一个贝叶斯学习的近似，用$y=f(x,\theta)$表示一个要学习的网络，贝叶斯学习是假设参数$\theta$为随机向量，并且先验分布为$q(\theta)$，贝叶斯方法的预测为：</p>
<script type="math/tex; mode=display">
E_{q(\theta)}[y] = \int_{q}f(x,\theta)q(\theta)d\theta \approx \frac{1}{M}\sum_{m=1}^{M}f(x, \theta_m)</script><p>其中$f(x, \theta_m)$为第m次应用丢弃方法后的网络，其参数$\theta_m$为全部参数$\theta$的一次采样。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>深层神经网络的训练需要大量的样本才能取得不错的效果，因为在数据量有限的情况下，可以通过 <strong>数据增强（Data Augmentation）</strong>来增加数据量，提高模型鲁棒性，避免过拟合。目前数据增强主要应用在图像数据上，在文本等其他类型的数据还没有太好的方法。</p>
<p>图像数据增强主要通过算法对图像进行转换，引入噪声方法增强数据的多样性，增强的方法主要有：</p>
<ul>
<li>转换（Rotation）：将图像按照顺时针或者逆时针方向随机旋转一定的角度；</li>
<li>翻转（Flip）：将图像沿水平或者垂直方向随机翻转一定的角度；</li>
<li>缩放（Zoom in/out）：将图像放大或者缩小一定的比例；</li>
<li>平移（Shift）：将图像按照水平或者垂直的方法平移一定步长；</li>
<li>加噪声（Noise）：加入随机噪声。</li>
</ul>
<h3 id="标签平滑"><a href="#标签平滑" class="headerlink" title="标签平滑"></a>标签平滑</h3><p>在数据增强中，可以通过给样本加入随机噪声来避免过拟合，同样也可以给样本的标签引入一定的噪声。假设在训练数据集中，有一些样本的标签是被错误标注的，那么最小化这些样本上的损失函数会导致过拟合。一种改善的正则化方法是<strong>标签平滑（label smothing）</strong>，即在输出标签中随机加入噪声，来避免模型过拟合。</p>
<p>一个样本$x$的标签一般用onehot向量表示，如下：</p>
<script type="math/tex; mode=display">
y = [0,...,0,1,.....,1]^T</script><p>这种标签可以看作<strong>硬目标（hard targets）</strong>，如果使用softmax分类器并使用交叉熵损失函数，最小化损失函数会使得正确类和其他类权重差异很大。根据softmax函数的性质可以知道，如果要使得某一类的输出概率接近于1，其未归一化的得分要远大于其他类的得分，这样可能会导致其权重越来越大，并导致过拟合。i</p>
<p>此外如果标签是错误的，会导致严重的过拟合现象，为了改善这种情况，我们可以引入一个噪声会标签进行平滑，即假设样本以$\epsilon$的概率为其他类，平滑后的标签为：</p>
<script type="math/tex; mode=display">
\tilde{y} =[ \frac{ \epsilon }{K-1} ,...,\frac{ \epsilon }{K-1} ,1- \epsilon,\frac{ \epsilon }{K-1},....,\frac{ \epsilon }{K-1}]^T</script><p>其中$K$为标签数量，这种标签可以看作是<strong>软目标（soft targets）</strong>。标签平滑可以避免模型的输出过拟合到硬目标上，并且通常不会降低其分类能力。</p>
<p>上边的标签平滑方法是给其他$K-1$个标签相同的概率$\frac{\epsilon}{K-1}$，没有考虑目标之间的相关性。一种更好的做法是按照类别相关性来赋予其他标签不同的概率，比如先训练另外一个更复杂的教师网络，并使用大网络的输出作为软目标进行训练学生网络，这种方法也称为知识精炼（Knowledge Distillation）。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>至此，神经网络中的网络优化和正则化（一）（二）（三）（四）篇已经完成，如下：</p>
<ul>
<li><a href="https://thinkgamer.blog.csdn.net/article/details/100996744" target="_blank" rel="external">神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向</a></li>
<li><a href="https://thinkgamer.blog.csdn.net/article/details/101026786" target="_blank" rel="external">神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化</a></li>
<li><a href="https://thinkgamer.blog.csdn.net/article/details/101033047" target="_blank" rel="external">神经网络中的网络优化和正则化（三）之超参数优化</a></li>
<li><a href="">神经网络中的网络优化和正则化（四）之正则化</a></li>
</ul>
<p>神经网络中的网络优化和正则化即是对立又统一的关系，一方面我们希望找到一个最优解使得模型误差最小，另一方面又不希望得到一个最优解，可能陷入过拟合。优化和正则化的目标是期望风险最小化。</p>
<p>目前在深层神经网络中泛化能力还没有很好的理论支持，在传统的机器学习上比较有效的$l_1,l_2$正则化在深层神经网络中作用也比较有限，而一些经验性的做法，比如随机梯度下降和提前终止，会更加有效。</p>
<hr>
<center>
【技术服务】，详情点击查看：
<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>
</center>

<hr>
<center>
<img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg">
<br>
扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！
</center>

<hr>
<center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>

<center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>
        
            <div class="donate-container">
    <div class="donate-button">
        <button id="donate-button">赞赏</button>
    </div>
    <div class="donate-img-container hide" id="donate-img-container">
        <img id="donate-img" src="" data-src="/assets/img/weixin.jpeg">
        <p> 你的支持是我进步的最大动力！ </p>
    </div>
</div>
        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/thinkgamer">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank" href="http://weibo.com/5352480017">
                            <span class="fa-stack fa-lg">
                                  <i class="iconfont icon-weibo"></i>
                            </span>
            </a>
        </li>
        

        

        
        <li>
            <a target="_blank"  href="https://github.com/thinkgamer">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        


        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://item.jd.com/12671716.html">处女作：推荐系统开发实战</a></span>
        <span>/</span>
        
        <span><a href="https://mp.weixin.qq.com/s/vkDfg3v5C7QPrLOTvTRH2w">搜索与推荐Wiki</a></span>
        <span>/</span>
        
        <span><a href="https://blog.csdn.net/gamer_gyt">CSDN博客</a></span>
        <span>/</span>
        
        <span><a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg">商务合作</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        || Created By <a href="https://blog.csdn.net/gamer_gyt">Thinkgamer</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
