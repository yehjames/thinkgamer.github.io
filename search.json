[{"title":"【技术服务】和【商务合作】","url":"/2066/06/06/随缘/商务合作介绍/","content":"\n---\n<center>\n<br><br>\n<font size=\"8\" style=\"font-weight:bold; color: green;\">WelCome To “Thinkgamer 小站”</font>\n<br><br>\n</center>\n\n----\n\n全网唯一ID：Thinkgamer,个人微信公众号”搜索与推荐Wiki“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，数据挖掘，数据分析，全站开发，大数据。\n\n### About\n\nCyanScikit科技团队成立于2019年，由一群热爱技术，追求极致的Coders和Managers组成！\n\n### Leader\n全球唯一ID：Thinkgamer。《推荐系统开发实战》作者，CSDN博客技术专家，原Top电商算法工程师。擅长领域：\n- 推荐系统\n- 数据开发/分析/挖掘\n\n### Service\n<center>\n\t<img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"99%\">\n</center>\n\n- 技术咨询、学习路线定制\n- 推荐系统、技术培训\n- 数据存储（方案/规范等）、数据开发、数据分析、数据挖掘\n- 数据可视化、小程序/H5、全栈开发\n- 爬虫、广告接入\n\n### Purpose\n\n使用技术去更好的服务于客户！\n\n### Team\n<center>\n\t<img src=\"https://img-blog.csdnimg.cn/20191105121227125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"99%\">\n</center>\n\n### Contact Me\n\n<center>\n\t<img src=\"https://img-blog.csdnimg.cn/20191105121227125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"99%\">\n</center>\n\n<center>\n\t<img src=\"https://img-blog.csdnimg.cn/20191105121446131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"99%\">\n</center>","tags":["随手记"]},{"title":"《文章推荐系统》系列之14、推荐中心.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之14、推荐中心/","content":"\n在前面的文章中，我们实现了召回和排序，接下来将进入推荐逻辑处理阶段，通常称为推荐中心，推荐中心负责接收应用系统的推荐请求，读取召回和排序的结果并进行调整，最后返回给应用系统。推荐中心的调用流程如下所示：\n\n![](https://upload-images.jianshu.io/upload_images/12790782-84442ee3d9e5a3ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 推荐接口设计\n通常推荐接口包括 Feed 流推荐和相似文章推荐\n- Feed 流推荐：根据用户偏好，获取推荐文章列表（这里的时间戳用于区分是刷新推荐列表还是查看历史推荐列表）\n参数：用户 ID，频道 ID，推荐文章数量，请求推荐的时间戳\n结果：曝光参数，每篇文章的行为埋点参数，上一条推荐的时间戳\n- 相似文章推荐：当用户浏览某文章时，获取该文章的相似文章列表\n参数：文章 ID，推荐文章数量\n结果：文章 ID 列表\n\n行为埋点参数：\n```\n{\n    \"param\": '{\"action\": \"exposure\", \"userId\": 1, \"articleId\": [1,2,3,4],  \"algorithmCombine\": \"c1\"}',\n    \"recommends\": [\n        {\"article_id\": 1, \"param\": {\"click\": \"{\"action\": \"click\", \"userId\": \"1\", \"articleId\": 1, \"algorithmCombine\": 'c1'}\", \"collect\": \"...\", \"share\": \"...\",\"read\":\"...\"}},\n        {\"article_id\": 2, \"param\": {\"click\": \"...\", \"collect\": \"...\", \"share\": \"...\", \"read\":\"...\"}},\n        {\"article_id\": 3, \"param\": {\"click\": \"...\", \"collect\": \"...\", \"share\": \"...\", \"read\":\"...\"}},\n        {\"article_id\": 4, \"param\": {\"click\": \"...\", \"collect\": \"...\", \"share\": \"...\", \"read\":\"...\"}}\n    ]\n    \"timestamp\": 1546391572\n}\n```\n这里接口采用 gRPC 框架，在 user_reco.proto 文件中定义 Protobuf 序列化协议，其中定义了 Feed 流推荐接口：`rpc user_recommend(User) returns (Track) {}` 和相似文章接口：`rpc article_recommend(Article) returns(Similar) {}`\n```\nsyntax = \"proto3\";\n\nmessage User {\n    string user_id = 1;\n    int32 channel_id = 2;\n    int32 article_num = 3;\n    int64 time_stamp = 4;\n}\n// int32 ---> int64 article_id\nmessage Article {\n    int64 article_id = 1;\n    int32 article_num = 2;\n\n}\n\nmessage param2 {\n    string click = 1;\n    string collect = 2;\n    string share = 3;\n    string read = 4;\n}\n\nmessage param1 {\n    int64 article_id = 1;\n    param2 params = 2;\n}\n\nmessage Track {\n    string exposure = 1;\n    repeated param1 recommends = 2;\n    int64 time_stamp = 3;\n}\n\nmessage Similar {\n    repeated int64 article_id = 1;\n}\n\nservice UserRecommend {\n    rpc user_recommend(User) returns (Track) {}\n    rpc article_recommend(Article) returns(Similar) {}\n}\n```\n接着，通过如下命令生成服务端文件 user_reco_pb2.py 和客户端文件 user_reco_pb2_grpc.py\n```\npython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. user_reco.proto\n```\n定义参数解析类，用于解析推荐请求的参数，包括用户 ID、频道 ID、文章数量、请求时间戳以及算法名称\n```\nclass Temp(object):\n    user_id = -10\n    channel_id = -10\n    article_num = -10\n    time_stamp = -10\n    algo = \"\"\n```\n定义封装埋点参数方法，其中参数 `res` 为推荐结果，参数 `temp` 为用户请求参数，将推荐结果封装为在 user_reco.proto 文件中定义的 Track 结构，其中携带了文章对埋点参数，包括了事件名称、算法名称以及时间等等，方便后面解析用户对文章对行为信息\n```\ndef add_track(res, temp):\n    \"\"\"\n    封装埋点参数\n    :param res: 推荐文章id列表\n    :param temp: rpc参数\n    :return: 埋点参数\n        文章列表参数\n        单文章参数\n    \"\"\"\n    # 添加埋点参数\n    track = {}\n\n    # 准备曝光参数\n    # 全部字符串形式提供，在hive端不会解析问题\n    _exposure = {\"action\": \"exposure\", \"userId\": temp.user_id, \"articleId\": json.dumps(res),\n                 \"algorithmCombine\": temp.algo}\n\n    track['param'] = json.dumps(_exposure)\n    track['recommends'] = []\n\n    # 准备其它点击参数\n    for _id in res:\n        # 构造字典\n        _dic = {}\n        _dic['article_id'] = _id\n        _dic['param'] = {}\n\n        # 准备click参数\n        _p = {\"action\": \"click\", \"userId\": temp.user_id, \"articleId\": str(_id),\n              \"algorithmCombine\": temp.algo}\n\n        _dic['param']['click'] = json.dumps(_p)\n        # 准备collect参数\n        _p[\"action\"] = 'collect'\n        _dic['param']['collect'] = json.dumps(_p)\n        # 准备share参数\n        _p[\"action\"] = 'share'\n        _dic['param']['share'] = json.dumps(_p)\n        # 准备detentionTime参数\n        _p[\"action\"] = 'read'\n        _dic['param']['read'] = json.dumps(_p)\n\n        track['recommends'].append(_dic)\n\n    track['timestamp'] = temp.time_stamp\n    return track\n```\n# AB Test 流量切分\n由于推荐算法和策略是需要不断改进和完善等，所以 ABTest 也是推荐系统不可或缺的功能。可以根据用户 ID 将流量切分为多个桶（Bucket），每个桶对应一种排序策略，桶内流量将使用相应的策略进行排序，使用 ID 进行流量切分能够保证用户体验的一致性。通常 ABTest 过程如下所示：\n\n![](https://upload-images.jianshu.io/upload_images/12790782-a422277ff8f14b37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n通过定义 AB Test 参数，可以实现为不同的用户使用不同的推荐算法策略，其中 `COMBINE` 为融合方式，`RECALL` 为召回方式，`SORT` 为排序方式，`CHANNEL` 为频道数量，`BYPASS` 为分桶设置，`sort_dict` 为不同的排序服务对象。可以看到 Algo-1 使用 LR 进行排序，而 Algo-2 使用 Wide&Deep 进行排序\n```python\nfrom collections import namedtuple\n\n# ABTest参数信息\nparam = namedtuple('RecommendAlgorithm', ['COMBINE',\n                                          'RECALL',\n                                          'SORT',\n                                          'CHANNEL',\n                                          'BYPASS']\n                   )\n\nRAParam = param(\n    COMBINE={\n        'Algo-1': (1, [100, 101, 102, 103, 104], [200]),  # 首页推荐，所有召回结果读取+LR排序\n        'Algo-2': (2, [100, 101, 102, 103, 104], [201])  # 首页推荐，所有召回结果读取 排序\n    },\n    RECALL={\n        100: ('cb_recall', 'als'),  # 离线模型ALS召回，recall:user:1115629498121 column=als:18\n        101: ('cb_recall', 'content'),  # 离线word2vec的画像内容召回 'recall:user:5', 'content:1'\n        102: ('cb_recall', 'online'),  # 在线word2vec的画像召回 'recall:user:1', 'online:1'\n        103: 'new_article',  # 新文章召回 redis当中    ch:18:new\n        104: 'popular_article',  # 基于用户协同召回结果 ch:18:hot\n        105: ('article_similar', 'similar')  # 文章相似推荐结果 '1' 'similar:2'\n    },\n    SORT={\n        200: 'LR',\n        201: 'WDL‘\n    },\n    CHANNEL=25,\n    BYPASS=[\n            {\n                \"Bucket\": ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd'],\n                \"Strategy\": \"Algo-1\"\n            },\n            {\n                \"BeginBucket\": ['e', 'f'],\n                \"Strategy\": \"Algo-2\"\n            }\n        ]\n)\n\nsort_dict = {\n    \"LR\": lr_sort_service,\n    \"WDL\": wdl_sort_service\n}\n```\n\n流量切分，将用户 ID 进行哈希，然后取哈希结果的第一个字符，将包含该字符的策略桶所对应的算法编号赋值到此用户请求参数的 `algo` 属性中，后面将调用该编号对应的算法策略为此用户计算推荐数据\n```\nimport hashlib\nfrom setting.default import DefaultConfig, RAParam\n\n# 进行分桶实现分流，制定不同的实验策略\nbucket = hashlib.md5(user_id.encode()).hexdigest()[:1]\nif bucket in RAParam.BYPASS[0]['Bucket']:\n    temp.algo = RAParam.BYPASS[0]['Strategy']\nelse:\n    temp.algo = RAParam.BYPASS[1]['Strategy']\n```\n# 推荐中心逻辑\n推荐中心逻辑主要包括：\n- 接收应用系统发送的推荐请求，解析请求参数\n- 进行 ABTest 分流，为用户分配推荐策略\n- 根据分配的算法调用召回服务和排序服务，读取推荐结果\n- 根据业务进行调整，如过滤、补足、合并信息等\n- 封装埋点参数，返回推荐结果\n\n![](https://upload-images.jianshu.io/upload_images/12790782-dd571ececce3d264.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n首先，在 Hbase 中创建历史推荐结果表 history_recommend，用于存储用户历史推荐结果\n```\ncreate 'history_recommend', {NAME=>'channel', TTL=>7776000, VERSIONS=>999999}   86400\n# 每次指定一个时间戳,可以达到不同版本的效果\nput 'history_recommend', 'reco:his:1', 'channel:18', [17283, 140357, 14668, 15182, 17999, 13648, 12884,18135]\n```\n继续在 Hbase 中创建待推荐结果表 wait_recommend，用于存储经过多路召回并且排序之后的待推荐结果，当 wait_recommend 没有数据时，才再次调用排序服务计算出新的待推荐结果并写入到 wait_recommend，所以不需设置多个版本。注意该表与 cb_recall 的区别，cb_recall 存储的是还未经排序的召回结果。\n```\ncreate 'wait_recommend', 'channel'\n\nput 'wait_recommend', 'reco:1', 'channel:18', [17283, 140357, 14668, 15182, 17999, 13648, 12884,18135]\nput 'wait_recommend', 'reco:1', 'channel:0', [17283, 140357, 14668, 15182, 17999, 13648, 12884, 17302, 13846]\n```\n用户获取 Feed 流推荐数据时，如果用户向下滑动，发出的是刷新推荐列表的请求，需要传入当前时间作为请求时间戳参数，该请求时间戳必然大于 Hbase 历史推荐结果表中的请求时间戳，那么程序将获取新的推荐列表，并返回 Hbase 历史推荐结果表中最近一次推荐的请求时间戳，用于查询历史推荐结果；如果用户向上滑动，发出的是查看历史推荐结果的请求，需要传入前面刷新推荐列表时返回的最近一次推荐的请求时间戳，该请求时间戳必然小于等于 Hbase 历史推荐结果中最近一次推荐的时间戳，那么程序将获取小于等于该请求时间戳的最近一次历史推荐结果，并返回小于该推荐结果最近一次推荐的时间戳，也就是上一次推荐的时间戳，下面是具体实现。\n\n在获取推荐列表时，首先获取用户的历史数据库中最近一次时间戳 `last_stamp`，没有则将 `last_stamp` 置为 0\n```python\ntry:\n    last_stamp = self.hbu.get_table_row('history_recommend',\n                                        'reco:his:{}'.format(temp.user_id).encode(),\n                                        'channel:{}'.format(temp.channel_id).encode(),\n                                        include_timestamp=True)[1]\nexcept Exception as e:\n    last_stamp = 0\n```\n- 如果用户请求的时间戳小于历史推荐结果中最近一次请求的时间戳 `last_stamp`，那么该请求为用户获取历史推荐结果\n 1.如果没有历史推荐结果，则返回时间戳 `0` 以及空列表 `[]`\n 2.如果历史推荐结果只有一条，则返回这一条历史推荐结果并返回时间戳 `0`，表示已经没有历史推荐结果（APP 可以显示已经没有历史推荐记录了）\n 3.如果历史推荐结果有多条，则返回历史推荐结果中第一条推荐结果（最近一次），然后返回历史推荐结果中第二条推荐结果的时间戳\n\n```\nif temp.time_stamp < last_stamp:\n    try:\n        row = self.hbu.get_table_cells('history_recommend',\n                                       'reco:his:{}'.format(temp.user_id).encode(),\n                                       'channel:{}'.format(temp.channel_id).encode(),\n                                       timestamp=temp.time_stamp + 1,\n                                       include_timestamp=True)\n    except Exception as e:\n        row = []\n        res = []\n\n    if not row:\n        temp.time_stamp = 0\n        res = []\n    elif len(row) == 1 and row[0][1] == temp.time_stamp:\n        res = eval(row[0][0])\n        temp.time_stamp = 0\n    elif len(row) >= 2:\n        res = eval(row[0][0])\n        temp.time_stamp = int(row[1][1])\n\n    res = list(map(int, res))\n    # 封装推荐结果\n    track = add_track(res, temp)\n    # 曝光参数设置为空\n    track['param'] = ''\n```\n（注意：这里将用户请求的时间戳 +1，因为 Hbase 只能获取小于该时间戳的历史推荐结果）\n\n- 如果用户请求的时间戳大于 Hbase 历史推荐结果中最近一次请求的时间戳 `last_stamp`，那么该请求为用户刷新推荐列表，需要读取推荐结果并返回。如果结果为空，需要调用 `user_reco_list()` 方法，再次计算推荐结果，再返回。\n```\nif temp.time_stamp > last_stamp:\n    # 获取缓存\n    res = redis_cache.get_reco_from_cache(temp, self.hbu)\n    # 如果结果为空，需要再次计算推荐结果 进行召回+排序，同时写入到hbase待推荐结果列表\n    if not res:\n        res = self.user_reco_list(temp)\n\n    temp.time_stamp = int(last_stamp)\n    track = add_track(res, temp)\n```\n定义 `user_reco_list()` 方法，首先要读取多路召回结果，根据为用户分配的算法策略，读取相应路径的召回结果，并进行重后合并\n```\nreco_set = []\n# (1, [100, 101, 102, 103, 104], [])\nfor number in RAParam.COMBINE[temp.algo][1]:\n    if number == 103:\n        _res = self.recall_service.read_redis_new_article(temp.channel_id)\n        reco_set = list(set(reco_set).union(set(_res)))\n    elif number == 104:\n        _res = self.recall_service.read_redis_hot_article(temp.channel_id)\n        reco_set = list(set(reco_set).union(set(_res)))\n    else:\n        # 100, 101, 102召回结果读取\n        _res = self.recall_service.read_hbase_recall(RAParam.RECALL[number][0],\n                                                     'recall:user:{}'.format(temp.user_id).encode(),\n                                                     '{}:{}'.format(RAParam.RECALL[number][1],\n                                                                    temp.channel_id).encode())\n        reco_set = list(set(reco_set).union(set(_res)))\n```\n接着，过滤当前该请求频道的历史推荐结果，如果不是 0 频道还需过滤 0 频道的历史推荐结果\n```\nhistory_list = []\ndata = self.hbu.get_table_cells('history_recommend',\n                                'reco:his:{}'.format(temp.user_id).encode(),\n                                'channel:{}'.format(temp.channel_id).encode())\n\nfor _ in data:\n    history_list = list(set(history_list).union(set(eval(_))))\n\ndata = self.hbu.get_table_cells('history_recommend',\n                                'reco:his:{}'.format(temp.user_id).encode(),\n                                'channel:{}'.format(0).encode())\n\nfor _ in data:\n    history_list = list(set(history_list).union(set(eval(_))))\n\nreco_set = list(set(reco_set).difference(set(history_list)))\n```\n最后，根据分配的算法策略，调用排序服务，将分数最高的 N 个推荐结果返回，并写入历史推荐结果表，如果还有剩余的排序结果，将其余写入待推荐结果表\n```\n# 使用指定模型对召回结果进行排序\n# temp.user_id， reco_set\n_sort_num = RAParam.COMBINE[temp.algo][2][0]\n# 'LR'\nreco_set = sort_dict[RAParam.SORT[_sort_num]](reco_set, temp, self.hbu)\n\nif not reco_set:\n    return reco_set\nelse:\n\n    # 如果reco_set小于用户需要推荐的文章\n    if len(reco_set) <= temp.article_num:\n        res = reco_set\n    else:\n        # 大于要推荐的文章结果\n        res = reco_set[:temp.article_num]\n\n        # 将剩下的文章列表写入待推荐的结果\n        self.hbu.get_table_put('wait_recommend',\n                               'reco:{}'.format(temp.user_id).encode(),\n                               'channel:{}'.format(temp.channel_id).encode(),\n                               str(reco_set[temp.article_num:]).encode(),\n                               timestamp=temp.time_stamp)\n\n    # 直接写入历史记录当中，表示这次又成功推荐一次\n    self.hbu.get_table_put('history_recommend',\n                           'reco:his:{}'.format(temp.user_id).encode(),\n                           'channel:{}'.format(temp.channel_id).encode(),\n                           str(res).encode(),\n                           timestamp=temp.time_stamp)\n\n    return res\n```\n到这里，推荐中心的基本逻辑已经结束。下面是读取多路召回结果的实现细节：通过指定列族，读取基于模型、离线内容以及在线的召回结果，并删除 cb_recall 的召回结果\n```\ndef read_hbase_recall_data(self, table_name, key_format, column_format):\n    \"\"\"\n    读取cb_recall当中的推荐数据\n    读取的时候可以选择列族进行读取als, online, content\n\n    :return:\n    \"\"\"\n    recall_list = []\n    data = self.hbu.get_table_cells(table_name, key_format, column_format)\n    # data是多个版本的推荐结果[[],[],[],]\n    for _ in data:\n        recall_list = list(set(recall_list).union(set(eval(_))))\n    self.hbu.get_table_delete(table_name, key_format, column_format)\n    return recall_list\n```\n读取 redis 中的新文章\n```\ndef read_redis_new_article(self, channel_id):\n    \"\"\"\n    读取新文章召回结果\n    :param channel_id: 提供频道\n    :return:\n    \"\"\"\n    _key = \"ch:{}:new\".format(channel_id)\n    try:\n        res = self.client.zrevrange(_key, 0, -1)\n    except Exception as e:\n        res = []\n\n    return list(map(int, res))\n```\n读取 redis 中的热门文章，并选取热度最高的前 K 个文章\n```\ndef read_redis_hot_article(self, channel_id):\n    \"\"\"\n    读取热门文章召回结果\n    :param channel_id: 提供频道\n    :return:\n    \"\"\"\n    _key = \"ch:{}:hot\".format(channel_id)\n    try:\n        res = self.client.zrevrange(_key, 0, -1)\n    except Exception as e:\n\n    # 由于每个频道的热门文章有很多，因为 保留文章点击次数\n    res = list(map(int, res))\n    if len(res) > self.hot_num:\n        res = res[:self.hot_num]\n    return res\n```\n读取相似文章\n```\ndef read_hbase_article_similar(self, table_name, key_format, article_num):\n    \"\"\"获取文章相似结果\n    :param article_id: 文章id\n    :param article_num: 文章数量\n    :return:\n    \"\"\"\n    try:\n        _dic = self.hbu.get_table_row(table_name, key_format)\n\n        res = []\n        _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True)\n        if len(_srt) > article_num:\n            _srt = _srt[:article_num]\n        for _ in _srt:\n            res.append(int(_[0].decode().split(':')[1]))\n    except Exception as e:\n        res = []\n    return res\n```\n# 使用缓存策略\n- 如果 redis 缓存中存在数据，就直接从 redis 缓存中获取推荐结果\n- 如果 redis 缓存为空而 Hbase 的待推荐结果表 wait_recommend 不为空，则从 wait_recommend 中获取推荐结果，并将一定数量的待推荐结果放入 redis 缓存中\n- 若 redis 和 wait_recommend 都为空，则需读取召回结果并进行排序，将排序结果写入 Hbase 的待推荐结果表 wait_recommend 中及 redis 中\n\n（每次读取的推荐结果都要将其写入 Hbase 的历史推荐结果表 history_recommend 中）\n\n读取 redis 缓存\n```\n#读取redis对应的键\nkey = 'reco:{}:{}:art'.format(temp.user_id, temp.channel_id)\n# 读取，删除，返回结果\npl = cache_client.pipeline()\n\n# 读取redis数据\nres = cache_client.zrevrange(key, 0, temp.article_num - 1)\nif res:\n    # 手动删除读取出来的缓存结果\n    pl.zrem(key, *res)\n```\n如果 redis 缓存为空\n```\nelse:\n    # 删除键\n    cache_client.delete(key)\n    try:\n        # 从wait_recommend中读取\n        wait_cache = eval(hbu.get_table_row('wait_recommend',\n                                            'reco:{}'.format(temp.user_id).encode(),\n                                            'channel:{}'.format(temp.channel_id).encode()))\n    except Exception as e:\n        wait_cache = []\n\n    # 如果为空则直接返回空\n    if not wait_cache:\n        return wait_cache\n\n    # 如果wait_recommend中有数据\n    if len(wait_cache) > 100:\n        cache_redis = wait_cache[:100]\n\n        # 前100个数据放入redis\n        pl.zadd(key, dict(zip(cache_redis, range(len(cache_redis)))))\n\n        # 100个后面的数据，在放回wait_recommend\n        hbu.get_table_put('wait_recommend',\n                          'reco:{}'.format(temp.user_id).encode(),\n                          'channel:{}'.format(temp.channel_id).encode(),\n                          str(wait_cache[100:]).encode())\n\n    else:\n        # 清空wait_recommend数据\n        hbu.get_table_put('wait_recommend',\n                          'reco:{}'.format(temp.user_id).encode(),\n                          'channel:{}'.format(temp.channel_id).encode(),\n                          str([]).encode())\n\n        # 所有不足100个数据，放入redis\n        pl.zadd(key, dict(zip(wait_cache, range(len(wait_cache)))))\n\n    res = cache_client.zrange(key, 0, temp.article_num - 1)\n```\n最后，在 Supervisor 中配置 gRPC 实时推荐程序\n```\n[program:online]\nenvironment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python ,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python\ncommand=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/reco_sys/abtest/routing.py\ndirectory=/root/toutiao_project/reco_sys/abtest\nuser=root\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/root/logs/recommendsuper.log\nloglevel=info\nstopsignal=KILL\nstopasgroup=true\nkillasgroup=true\n```\n\n# 想说的话\n文章推荐系统系列到此就完结啦～ 撒花 🎉🎉🎉\n若有疏漏的地方，欢迎各位多多指正，感谢关注，love & peace. 🙏🙏🙏\n\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之13、基于Wide&Deep模型的在线排序.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之13、基于Wide&Deep模型的在线排序/","content":"\n![](https://upload-images.jianshu.io/upload_images/12790782-66263ecac4ac1688.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n上图是 Wide&Deep 模型的网络结构，深度学习可以通过嵌入（Embedding）表达出更精准的用户兴趣及物品特征，不仅能减少人工特征工程的工作量，还能提高模型的泛化能力，使得用户行为预估更加准确。Wide&Deep 模型适合高维稀疏特征的推荐场景，兼有稀疏特征的可解释性和深模型的泛化能力。通常将类别特征做 Embedding 学习，再将 Embedding 稠密特征输入深模型中。Wide 部分的输入特征包括：类别特征和离散化的数值特征，Deep部分的输入特征包括：数值特征和 Embedding 后的类别特征。其中，Wide 部分使用 FTRL + L1；Deep 部分使用 AdaGrad，并且两侧是一起联合进行训练的。\n\n# 离线训练\nTensorFlow 实现了很多深度模型，其中就包括 Wide&Deep，API 接口为 `tf.estimator.DNNLinearCombinedClassifier`，我们可以直接使用。在上篇文章中已经实现了将训练数据写入 TFRecord 文件，在这里可以直接读取\n```\n@staticmethod\ndef read_ctr_records():\n    dataset = tf.data.TFRecordDataset([\"./train_ctr_201905.tfrecords\"])\n    dataset = dataset.map(parse_tfrecords)\n    dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.repeat(10000)\n    return dataset.make_one_shot_iterator().get_next()\n```\n解析每个样本，将 TFRecord 中序列化的 feature 列，解析成 channel_id (1), article_vector (100), user_weights (10), article_weights (10)\n```\ndef parse_tfrecords(example):\n    features = {\n        \"label\": tf.FixedLenFeature([], tf.int64),\n        \"feature\": tf.FixedLenFeature([], tf.string)\n    }\n    parsed_features = tf.parse_single_example(example, features)\n\n    feature = tf.decode_raw(parsed_features['feature'], tf.float64)\n    feature = tf.reshape(tf.cast(feature, tf.float32), [1, 121])\n    # 特征顺序 1 channel_id,  100 article_vector, 10 user_weights, 10 article_weights\n    # 1 channel_id类别型特征， 100维文章向量求平均值当连续特征，10维用户权重求平均值当连续特征\n    channel_id = tf.cast(tf.slice(feature, [0, 0], [1, 1]), tf.int32)\n    vector = tf.reduce_sum(tf.slice(feature, [0, 1], [1, 100]), axis=1, keep_dims=True)\n    user_weights = tf.reduce_sum(tf.slice(feature, [0, 101], [1, 10]), axis=1, keep_dims=True)\n    article_weights = tf.reduce_sum(tf.slice(feature, [0, 111], [1, 10]), axis=1, keep_dims=True)\n\n    label = tf.reshape(tf.cast(parsed_features['label'], tf.float32), [1, 1])\n\n    # 构造字典 名称-tensor\n    FEATURE_COLUMNS = ['channel_id', 'vector', 'user_weigths', 'article_weights']\n    tensor_list = [channel_id, vector, user_weights, article_weights]\n\n    feature_dict = dict(zip(FEATURE_COLUMNS, tensor_list))\n\n    return feature_dict, label\n```\n指定输入特征的数据类型，并定义 Wide&Deep 模型 model\n```\n# 离散类型\nchannel_id = tf.feature_column.categorical_column_with_identity('channel_id', num_buckets=25)\n# 连续类型\nvector = tf.feature_column.numeric_column('vector')\nuser_weigths = tf.feature_column.numeric_column('user_weigths')\narticle_weights = tf.feature_column.numeric_column('article_weights')\n\nwide_columns = [channel_id]\n\n# embedding_column用来表示类别型的变量\ndeep_columns = [tf.feature_column.embedding_column(channel_id, dimension=25),\n                vector, user_weigths, article_weights]\n\nestimator = tf.estimator.DNNLinearCombinedClassifier(model_dir=\"./ckpt/wide_and_deep\",\n                                                     linear_feature_columns=wide_columns,\n                                                     dnn_feature_columns=deep_columns,\n                                                     dnn_hidden_units=[1024, 512, 256])\n```\n通过调用 read_ctr_records() 方法，来读取 TFRecod 文件中的训练数据，并设置训练步长，对定义好的 FTRL 模型进行训练及预估\n```\nmodel.train(read_ctr_records, steps=1000)\nresult = model.evaluate(read_ctr_records)\n```\n可以用上一次模型的参数作为当前模型的初始化参数，训练完成后，通常会进行离线指标分析，若符合预期即可导出模型\n```\ncolumns = wide_columns + deep_columns\nfeature_spec = tf.feature_column.make_parse_example_spec(columns)\nserving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\nmodel.export_savedmodel(\"./serving_model/wdl/\", serving_input_receiver_fn)\n```\n# TFServing 部署\n安装\n```\ndocker pull tensorflow/serving\n```\n启动\n```\ndocker run -p 8501:8501 -p 8500:8500 --mount type=bind,source=/root/toutiao_project/reco_sys/server/models/serving_model/wdl,target=/models/wdl -e MODEL_NAME=wdl -t tensorflow/serving\n```\n- -p 8501:8501 为端口映射（-p 主机端口 : docker 容器程序）\n- TFServing 使用 8501 端口对外提供 HTTP 服务，使用8500对外提供 gRPC 服务，这里同时开放了两个端口的使用\n- --mount type=bind,source=/home/ubuntu/detectedmodel/wdl,target=/models/wdl 为文件映射，将主机（source）的模型文件映射到 docker 容器程序（target）的位置，以便 TFServing 使用模型，target 参数为 /models/模型名称\n- -e MODEL_NAME= wdl 设置了一个环境变量，名为 MODEL_NAME，此变量被 TFServing 读取，用来按名字寻找模型，与上面 target 参数中的模型名称对应\n- -t 为 TFServing 创建一个伪终端，供程序运行\n- tensorflow/serving 为镜像名称\n\n# 在线排序\n通常在线排序是根据用户实时的推荐请求，对召回结果进行 CTR 预估，进而计算出排序结果并返回。我们需要根据召回结果构造测试集，其中每个测试样本包括用户特征和文章特征。首先，根据用户 ID 和频道 ID 读取用户特征（用户在每个频道的特征不同，所以是分频道存储的）\n```\ntry:\n    user_feature = eval(hbu.get_table_row('ctr_feature_user',\n                              '{}'.format(temp.user_id).encode(),\n                              'channel:{}'.format(temp.channel_id).encode()))\nexcept Exception as e:\n    user_feature = []\n```\n再根据用户 ID 读取召回结果\n```\nrecall_set = read_hbase_recall('cb_recall', \n                'recall:user:{}'.format(temp.user_id).encode(), \n                'als:{}'.format(temp.channel_id).encode())\n```\n接着，遍历召回结果，获取文章特征，并将用户特征合并，构建样本\n```\nexamples = []\nfor article_id in recall_set:\n    try:\n        article_feature = eval(hbu.get_table_row('ctr_feature_article',\n                                  '{}'.format(article_id).encode(),\n                                  'article:{}'.format(article_id).encode()))\n    except Exception as e:\n        article_feature = []\n\n    if not article_feature:\n        article_feature = [0.0] * 111\n    \n    channel_id = int(article_feature[0])\n    # 计算后面若干向量的平均值\n    vector = np.mean(article_feature[11:])\n    # 用户权重特征\n    user_feature = np.mean(user_feature)\n    # 文章权重特征\n    article_feature = np.mean(article_feature[1:11])\n\n    # 构建example\n    example = tf.train.Example(features=tf.train.Features(feature={\n                \"channel_id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[channel_id])),\n                \"vector\": tf.train.Feature(float_list=tf.train.FloatList(value=[vector])),\n                'user_weigths': tf.train.Feature(float_list=tf.train.FloatList(value=[user_feature])),\n                'article_weights': tf.train.Feature(float_list=tf.train.FloatList(value=[article_feature])),\n            }))\n\n    examples.append(example)\n```\n调用 TFServing 的模型服务，获取排序结果\n```\nwith grpc.insecure_channel(\"127.0.0.1:8500\") as channel:\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    request = classification_pb2.ClassificationRequest()\n    # 构造请求，指定模型名称，指定输入样本\n    request.model_spec.name = 'wdl'\n    request.input.example_list.examples.extend(examples)\n    # 发送请求，获取排序结果\n    response = stub.Classify(request, 10.0)\n```\n这样，我们就实现了 Wide&Deep 模型的离线训练和 TFServing 模型部署以及在线排序服务的调用。使用这种方式，线上服务需要将特征发送给TF Serving，这不可避免引入了网络 IO，给带宽和预估时延带来压力。可以通过并发请求，召回多个召回结果集合，然后并发请求 TF Serving 模型服务，这样可以有效降低整体预估时延。还可以通过特征 ID 化，将字符串类型的特征名哈希到 64 位整型空间，这样有效减少传输的数据量，降低使用的带宽。\n\n# 模型同步\n实际环境中，我们可能还要经常将离线训练好的模型同步到线上服务机器，大致同步过程如下：\n- 同步前，检查模型 md5 文件，只有该文件更新了，才需要同步\n- 同步时，随机链接 HTTPFS 机器并限制下载速度\n- 同步后，校验模型文件 md5 值并备份旧模型\n\n同步过程中，需要处理发生错误或者超时的情况，可以设定触发报警或重试机制。通常模型的同步时间都在分钟级别。\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之12、基于FTRL优化方法的模型在线排序.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之12、基于FTRL优化方法的模型在线排序/","content":"\n# 构造 TFRecord 训练集\n和前面的 LR 离线模型一样，FTRL 模型首先也是要完成训练集的构建。在上篇文章中，我们已经知道，可以通过读取用户历史行为数据，及文章特征和用户特征，构建出训练集 `train`，其中包括 features 和 label 两列数据，features 是文章特征和用户特征的组合。在 TensorFlow 通常使用 TFRecord 文件进行数据的存取。接下来，我们就要将 `train` 保存到 TFRecord 文件中。首先开启会话，将 `train` 中的特征和标签分别传入 `write_to_tfrecords()` 方法，并利用多线程执行\n```\nimport tensorflow as tf\n\nwith tf.Session() as sess:\n    # 创建线程协调器\n    coord = tf.train.Coordinator()\n    # 开启子线程去读取数据\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    # 存入数据\n    write_to_tfrecords(train.iloc[:, 0], train.iloc[:, 1])\n    # 关闭子线程，回收\n    coord.request_stop()\n    coord.join(threads)\n```\n接着，在 `write_to_tfrecords()` 方法中，遍历训练集数据，将每个样本构造为 `tf.train.Example`，其中 feature 为 BytesList 类型，label 为 Int64List 类型，并保存到 TFRecords 文件中\n```\ndef write_to_tfrecords(feature_batch, click_batch):\n    \"\"\"将用户与文章的点击日志构造的样本写入TFRecords文件\n    \"\"\"\n    # 1、构造tfrecords的存储实例\n    writer = tf.python_io.TFRecordWriter(\"./train_ctr_20190605.tfrecords\")\n\n    # 2、循环将所有样本一个个封装成example，写入文件\n    for i in range(len(click_batch)):\n        # 取出第i个样本的特征值和目标值，格式转换\n        click = click_batch[i]\n        feature = feature_batch[i].tostring()\n        # 构造example\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \"feature\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[feature])),\n            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[click]))\n        }))\n        # 序列化example,写入文件\n        writer.write(example.SerializeToString())\n\n    writer.close()\n```\n\n# 离线训练\nFTRL（Follow The Regularized Leader）模型是一种获得稀疏模型的优化方法，我们利用构建好的 TFRecord 样本数据对 FTRL 模型进行离线训练。首先，定义 `read_ctr_records()` 方法来读取 TFRecord 文件，并通过调用 `parse_tfrecords()` 方法遍历解析每个样本，并设置了批大小和迭代次数\n```\ndef read_ctr_records():\n    train = tf.data.TFRecordDataset([\"./train_ctr_20190605.tfrecords\"])\n    train = train.map(parse_tfrecords)\n    train = train.batch(64)\n    train = train.repeat(10000)\n```\n解析每个样本，将 TFRecord 中序列化的 feature 列，解析成 channel_id (1), article_vector (100), user_weights (10), article_weights (10)\n```\nFEATURE_COLUMNS = ['channel_id', 'article_vector', 'user_weigths', 'article_weights']\n\ndef parse_tfrecords(example):\n    features = {\n        \"feature\": tf.FixedLenFeature([], tf.string),\n        \"label\": tf.FixedLenFeature([], tf.int64)\n    }\n    parsed_features = tf.parse_single_example(example, features)\n    feature = tf.decode_raw(parsed_features['feature'], tf.float64)\n    feature = tf.reshape(tf.cast(feature, tf.float32), [1, 121])\n    \n    channel_id = tf.cast(tf.slice(feature, [0, 0], [1, 1]), tf.int32)\n    article_vector = tf.reduce_sum(tf.slice(feature, [0, 1], [1, 100]), axis=1)\n    user_weights = tf.reduce_sum(tf.slice(feature, [0, 101], [1, 10]), axis=1)\n    article_weights = tf.reduce_sum(tf.slice(feature, [0, 111], [1, 10]), axis=1)\n\n    label = tf.cast(parsed_features['label'], tf.float32)\n\n    # 构造字典 名称-tensor\n    tensor_list = [channel_id, article_vector, user_weights, article_weights]\n    feature_dict = dict(zip(FEATURE_COLUMNS, tensor_list))\n\n    return feature_dict, label\n```\n指定输入特征的数据类型，并定义 FTRL 模型 `model` \n```\n# 定义离散类型特征\narticle_id = tf.feature_column.categorical_column_with_identity('channel_id', num_buckets=25)\n# 定义连续类型特征\narticle_vector = tf.feature_column.numeric_column('article_vector')\nuser_weigths = tf.feature_column.numeric_column('user_weigths')\narticle_weights = tf.feature_column.numeric_column('article_weights')\n\nfeature_columns = [article_id, article_vector, user_weigths, article_weights]\n\nmodel = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n                                           optimizer=tf.train.FtrlOptimizer(learning_rate=0.1,\n                                                                            l1_regularization_strength=10,\n                                                                            l2_regularization_strength=10))\n```\n通过调用 `read_ctr_records()` 方法，来读取 TFRecod 文件中的训练数据，并设置训练步长，对定义好的 FTRL 模型进行训练及预估\n```\nmodel.train(read_ctr_records, steps=1000)\nresult = model.evaluate(read_ctr_records)\n```\n通常需要编写离线任务，定时读取用户行为数据作为训练集和验证集，对训练集及验证集进行 CTR 预估，并根据离线指标对结果进行分析，决定是否更新模型。\n\n# 在线排序\n通常在线排序是根据用户实时的推荐请求，对召回结果进行 CTR 预估，进而计算出排序结果并返回。我们需要根据召回结果构造测试集，其中每个测试样本包括用户特征和文章特征。首先，根据用户 ID 和频道 ID 读取用户特征（用户在每个频道的特征不同，所以是分频道存储的）\n```\ntry:\n    user_feature = eval(hbu.get_table_row('ctr_feature_user',\n                              '{}'.format(temp.user_id).encode(),\n                              'channel:{}'.format(temp.channel_id).encode()))\nexcept Exception as e:\n    user_feature = []\n```\n再根据用户 ID 读取召回结果\n```\nrecall_set = read_hbase_recall('cb_recall', \n                'recall:user:{}'.format(temp.user_id).encode(), \n                'als:{}'.format(temp.channel_id).encode())\n```\n接着，遍历召回结果，获取文章特征，并将用户特征合并，作为测试样本\n```\ntest = []\nfor article_id in recall_set:\n    try:\n        article_feature = eval(hbu.get_table_row('ctr_feature_article',\n                                  '{}'.format(article_id).encode(),\n                                  'article:{}'.format(article_id).encode()))\n    except Exception as e:\n        article_feature = []\n\n    if not article_feature:\n        article_feature = [0.0] * 111\n    feature = []\n    feature.extend(user_feature)\n    feature.extend(article_feature)\n\n    test.append(f)\n```\n加载本地 FTRL 模型并对测试样本进行 CTR 预估\n```\ntest_array = np.array(test)\nmodel.load_weights('/root/toutiao_project/reco_sys/offline/models/ckpt/ctr_lr_ftrl.h5')\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    predictions = self.model.predict(sess.run(tf.constant(test_array)))\n```\n对结果进行排序并提取 CTR 最高的前 K 个文章，这样就得到了 FTRL 模型在线排序的结果。\n```\nres = pd.DataFrame(np.concatenate((np.array(recall_set).reshape(len(recall_set), 1), predictions),\n                                 axis=1), columns=['article_id', 'prob'])\n\nres_sort = res.sort_values(by=['prob'], ascending=True)\n\n# 排序后，只将排名在前100个文章ID作为推荐结果返回给用户\nif len(res_sort) > 100:\n    recall_set = list(res_sort.iloc[:100, 0])\nrecall_set = list(res_sort.iloc[:, 0])\n```\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之11、基于FTRL优化方法的模型在线排序.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之11、基于FTRL优化方法的模型在线排序/","content":"\n前面，我们已经完成了召回阶段的全部工作，通过召回，我们可以从数百万甚至上亿的原始物品数据中，筛选出和用户相关的几百、几千个可能感兴趣的物品。接下来，我们将要进入到排序阶段，对召回的几百、几千个物品进行进一步的筛选和排序。\n\n排序流程包括离线排序和在线排序：\n\n- 离线排序\n读取前天（第 T - 2 天）之前的用户行为数据作为训练集，对离线模型进行训练；训练完成后，读取昨天（第 T - 1 天）的用户行为数据作为验证集进行预测，根据预测结果对离线模型进行评估；若评估通过，当天（第 T 天）即可将离线模型更新到定时任务中，定时执行预测任务；明天（第 T + 1 天）就能根据今天的用户行为数据来观察更新后离线模型的预测效果。（注意：数据生产有一天时间差，第 T 天生成第 T - 1 天的数据）\n\n- 在线排序\n读取前天（第 T - 2 天）之前的用户行为数据作为训练集，对在线模型进行训练；训练完成后，读取昨天（第 T - 1 天）的用户行为数据作为验证集进行预测，根据预测结果对在线模型进行评估；若评估通过，当天（第 T 天）即可将在线模型更新到线上，实时执行排序任务；明天（第 T + 1 天）就能根据今天的用户行为数据来观察更新后在线模型的预测效果。\n\n这里再补充一个数据集划分的小技巧：可以横向划分，随机或按用户或其他样本选择策略；也可以纵向划分，按照时间跨度，比如一周的数据中，周一到周四是训练集，周五周六是测试集，周日是验证集。\n\n利用排序模型可以进行评分预测和用户行为预测，通常推荐系统利用排序模型进行用户行为预测，比如点击率（CTR）预估，进而根据点击率对物品进行排序，目前工业界常用的点击率预估模型有如下 3 种类型：\n\n- 宽模型 + 特征⼯程\nLR / MLR + 非 ID 类特征（⼈⼯离散 / GBDT / FM），可以使用 Spark 进行训练\n- 宽模型 + 深模型\nWide&Deep，DeepFM，可以使用 TensorFlow 进行训练\n- 深模型：\nDNN + 特征 Embedding，可以使用 TensorFlow 进行训练\n\n这里的宽模型即指线性模型，线性模型的优点包括：\n- 相对简单，训练和预测的计算复杂度都相对较低\n- 可以集中精力发掘新的有效特征，且可以并行化工作\n- 解释性较好，可以根据特征权重做解释\n\n本文我们将采用逻辑回归作为离线模型，进行点击率预估。逻辑回归（Logistic Regression，LR）是基础的二分类模型，也是监督学习的一种，通过对有标签的训练集数据进行特征学习，进而可以对测试集（新数据）的标签进行预测。我们这里的标签就是指用户是否对文章发生了点击行为。\n\n# 构造训练集\n读取用户历史行为数据，将 clicked 作为训练集标签\n```\nspark.sql(\"use profile\")\nuser_article_basic = spark.sql(\"select * from user_article_basic\").select(['user_id', 'article_id', 'clicked'])\n```\n`user_article_basic` 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-3e75d46d736f96ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n之前我们已经计算好了文章特征和用户特征，并存储到了 Hbase 中。这里我们遍历用户历史行为数据，根据其中文章 ID 和用户 ID 分别获取文章特征和用户特征，再将标签转为 int 类型，这样就将一条用户行为数据构造成为了一个样本，再将所有样本加入到训练集中\n```\ntrain = []\nfor user_id, article_id, clicked in user_article_basic:\n    try:\n        article_feature = eval(hbu.get_table_row('ctr_feature_article', '{}'.format(article_id).encode(), 'article:{}'.format(article_id).encode()))\n    except Exception as e:\n        article_feature = []\n    try:\n        user_feature = eval(hbu.get_table_row('ctr_feature_user', '{}'.format(temp.user_id).encode(), 'channel:{}'.format(temp.channel_id).encode()))\n    except Exception as e:\n        user_feature = []\n\n    if not article_feature:\n        article_feature = [0.0] * 111\n    if not user_feature:\n        user_feature = [0.0] * 10\n\n    sample = []\n    sample.append(user_feature)\n    sample.append(article_feature)\n    sample.append(int(clicked))\n\n    train.append(sample)\n```\n接下来，还需要利用 Spark 的 Vectors 将 `array<double>` 类型的 article_feature 和 user_feature 转为 `vector` 类型\n```\ncolumns = ['article_feature', 'user_feature', 'clicked']\n\ndef list_to_vector(row):\n    from pyspark.ml.linalg import Vectors\n    \n    return Vectors.dense(row[0]), Vectors.dense(row[1]), row[2]\n\ntrain = train.rdd.map(list_to_vector).toDF(columns) \n```\n再将 article_feature, user_feature 合并为统一输入到 LR 模型的特征列 features，这样就完成训练集的构建\n```\ntrain = VectorAssembler().setInputCols(columns[0:1]).setOutputCol('features').transform(train)\n```\n\n# 模型训练\nSpark 已经实现好了 LR 模型，通过指定训练集 train 的特征列 features 和标签列 clicked，即可对 LR 模型进行训练，再将训练好的模型保存到 HDFS\n```\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression()\nmodel = lr.setLabelCol(\"clicked\").setFeaturesCol(\"features\").fit(train)\nmodel.save(\"hdfs://hadoop-master:9000/headlines/models/lr.obj\")\n```\n加载训练好的 LR 模型，调用 `transform()` 对训练集做出预测（实际场景应该对验证集和训练集进行预测）\n```\nfrom pyspark.ml.classification import LogisticRegressionModel\n\nonline_model = LogisticRegressionModel.load(\"hdfs://hadoop-master:9000/headlines/models/lr.obj\")\nsort_res = online_model.transform(train)\n```\n预测结果 `sort_res` 中包括 clicked 和 probability 列，其中 clicked 为样本标签的真实值，probability 是包含两个元素的列表，第一个元素是预测的不点击概率，第二个元素则是预测的点击概率，可以提取点击率（CTR）\n```\ndef get_ctr(row):\n    return float(row.clicked), float(row.probability[1]) \n\nscore_label = sort_res.select([\"clicked\", \"probability\"]).rdd.map(get_ctr)\n```\n\n# 模型评估\n离线模型评估指标包括：\n- 评分准确度\n通常是均方根误差（RMSE），用来评估预测评分的效果\n- 排序能力\n通常采用 AUC（Area Under the Curve），即 ROC 曲线下方的面积\n- 分类准确率（Precision）\n表示在 Top K 推荐列表中，用户真实点击的物品所占的比例\n- 分类召回率（Recall）\n表示在用户真实点击的物品中，出现在 Top K 推荐列表中所占的比例\n\n当模型更新后，还可以根据商业指标进行评估，比例类的包括： 点击率（CTR）、转化率（CVR），绝对类的包括：社交关系数量、用户停留时长、成交总额（GMV）等。\n\n推荐系统的广度评估指标包括：\n- 覆盖率\n表示被有效推荐（推荐列表长度大于 c）的用户占全站用户的比例，公式如下：\n$$Con_{UV}=\\frac{N_{l >c}}{N_{UV}}$$\n- 失效率\n表示被无效推荐（推荐列表长度为 0）的用户占全站用户的比例，公式如下：\n$$Lost_{UV}=\\frac{N_{l =0}}{N_{UV}}$$\n- 新颖性\n- 更新率\n表示推荐列表的变化程度，当前周期与上个周期相比，推荐列表中不同物品的比例\n$$Update=\\frac{N_{diff}}{N_{last}}$$\n\n推荐系统的健康评估指标包括：\n- 个性化\n用于衡量推荐的个性化程度，是否大部分用户只消费小部分物品，可以计算所有用户推荐列表的平均相似度\n- 基尼系数\n用于衡量推荐系统的马太效应，反向衡量推荐的个性化程度。将物品按照累计推荐次数排序，排序位置为 i，推荐次数占总推荐次数的比例为 $P_i$，推荐次数越不平均，基尼系数越接近 1，公式为：\n$$Gini=\\frac{1}{n}\\sum_{i=1}^n P_i(2i-n-i)$$\n- 多样性\n通常是在类别维度上衡量推荐结果的多样性，可以衡量各个类别在推荐时的熵\n$$Div=\\frac{\\sum_{i=1}^n-P_i\\log(P_i)}{n\\log(n)}$$\n其中，物品共包括 n 个类别，类别 i 被推荐次数占总推荐次数的比例为 $P_i$，分母是各个类别最均匀时对应的熵，分子是实际推荐结果的类别分布熵。这是整体推荐的多样性，还可以计算每次推荐和每个用户推荐的多样性。\n\n我们这里主要根据 AUC 进行评估，首先利用 `model.summary.roc` 绘制 ROC 曲线\n```\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(model.summary.roc.select('FPR').collect(),\n         model.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n```\nROC 曲线如下所示，曲线下面的面积即为 AUC（Area Under the Curve），AUC 值越大，排序效果越好\n\n![](https://upload-images.jianshu.io/upload_images/12790782-ebdcbb926acd114a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n利用 Spark 的 `BinaryClassificationMetrics()` 计算 AUC\n```\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nmetrics = BinaryClassificationMetrics(score_label)\nmetrics.areaUnderROC\n```\n也可以利用 sklearn 的 `roc_auc_score()` 计算 AUC，`accuracy_score()` 计算准确率\n```\nfrom sklearn.metrics import accuracy_score, roc_auc_score,\nimport numpy as np\n\narr = np.array(score_label.collect())\n# AUC\nroc_auc_score(arr[:, 0], arr[:, 1]) # 0.719274521004087\n\n# 准确率\naccuracy_score(arr[:, 0], arr[:, 1].round()) # 0.9051438053097345\n```\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://book.douban.com/subject/34872145/](https://book.douban.com/subject/34872145/)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之10、基于热门文章和新文章的在线召回.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之10、基于热门文章和新文章的在线召回/","content":"\n在上篇文章中我们实现了基于内容的在线召回，接下来，我们将实现基于热门文章和新文章的在线召回。主要思路是根据点击次数，统计每个频道下的热门文章，根据发布时间统计每个频道下的新文章，当推荐文章不足时，可以根据这些文章进行补足。\n\n由于数据量较小，这里采用 Redis 存储热门文章和新文章的召回结果，数据结构如下所示\n\n热门文章召回|\t结构|\t示例\n-|-|-\npopular_recall|\tch:{}:hot|\tch:18:hot\n\n新文章召回|\t结构|\t示例\n-|-|-\nnew_article|\tch:{}:new|\tch:18:new\n\n热门文章存储，键为 `ch:频道ID:hot` 值为 `分数` 和 `文章ID`\n```python\n# ZINCRBY key increment member\n# ZSCORE\n# 为有序集 key 的成员 member 的 score 值加上增量 increment 。\nclient.zincrby(\"ch:{}:hot\".format(row['channelId']), 1, row['param']['articleId'])\n\n# ZREVRANGE key start stop [WITHSCORES]\nclient.zrevrange(ch:{}:new, 0, -1)\n```\n新文章存储，键为 `ch:{频道ID}:new` 值为 `文章ID:时间戳`\n```python\n# ZADD ZRANGE\n# ZADD key score member [[score member] [score member] ...]\n# ZRANGE page_rank 0 -1\nclient.zadd(\"ch:{}:new\".format(channel_id), {article_id: time.time()})\n```\n\n# 热门文章在线召回\n首先，添加 Spark Streaming 和 Kafka 的配置，热门文章读取由业务系统发送到 Kafka 的 click-trace 主题中的用户实时行为数据\n```\nKAFKA_SERVER = \"192.168.19.137:9092\"\nclick_kafkaParams = {\"metadata.broker.list\": KAFKA_SERVER}\nHOT_DS = KafkaUtils.createDirectStream(stream_c, ['click-trace'], click_kafkaParams)\n```\n接下来，利用 Spark Streaming 读取 Kafka 中的用户行为数据，筛选出被点击过的文章，将 Redis 中的文章热度分数进行累加即可\n```\nclient = redis.StrictRedis(host=DefaultConfig.REDIS_HOST, port=DefaultConfig.REDIS_PORT, db=10)\n\ndef update_hot_redis(self):\n    \"\"\"\n    收集用户行为，更新热门文章分数\n    :return:\n    \"\"\"\n    def update_hot_article(rdd):\n        for data in rdd.collect():\n            # 过滤用户行为\n            if data['param']['action'] in ['exposure', 'read']:\n                pass\n            else:\n                client.zincrby(\"ch:{}:hot\".format(data['channelId']), 1, data['param']['articleId'])\n\n    HOT_DS.map(lambda x: json.loads(x[1])).foreachRDD(update_hot_article)\n```\n测试，写入用户行为日志\n```shell\necho {\\\"actionTime\\\":\\\"2019-04-10 21:04:39\\\",\\\"readTime\\\":\\\"\\\",\\\"channelId\\\":18,\\\"param\\\":{\\\"action\\\": \\\"click\\\", \\\"userId\\\": \\\"2\\\", \\\"articleId\\\": \\\"14299\\\", \\\"algorithmCombine\\\": \\\"C2\\\"}} >> userClick.log\n```\n查询热门文章\n```\n127.0.0.1:6379[10]> keys *\n1) \"ch:18:hot\"\n127.0.0.1:6379[10]> ZRANGE \"ch:18:hot\" 0 -1\n1) \"14299\"\n```\n\n# 新文章在线召回\n首先，添加 Spark Streaming 和 Kafka 的配置，新文章读取由业务系统发送到 Kafka 的 new-article 主题中的最新发布文章数据\n```\nNEW_ARTICLE_DS = KafkaUtils.createDirectStream(stream_c, ['new-article'], click_kafkaParams)\n```\n接下来，利用 Spark Streaming 读取 Kafka 的新文章，将其按频道添加到 Redis 中，Redis 的值为当前时间\n```\ndef  update_new_redis(self):\n    \"\"\"更新频道最新文章\n    :return:\n    \"\"\"\n    def add_new_article(rdd):\n        for row in rdd.collect():\n            channel_id, article_id = row.split(',')\n            client.zadd(\"ch:{}:new\".format(channel_id), {article_id: time.time()})\n\n    NEW_ARTICLE_DS.map(lambda x: x[1]).foreachRDD(add_new_article)\n```\n\n还需要在 Kafka 的启动脚本中添加 new-article 主题监听配置，这样就可以收到业务系统发送过来的新文章了，重新启动 Flume 和 Kafka\n```\n/root/bigdata/kafka/bin/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic new-article --partitions 1\n```\n\n测试，向 Kafka 发送新文章数据\n```\nfrom kafka import KafkaProducer \n\n# kafka消息生产者\nkafka_producer = KafkaProducer(bootstrap_servers=['192.168.19.137:9092'])\n\n# 构造消息并发送\nmsg = '{},{}'.format(18, 13891)\nkafka_producer.send('new-article', msg.encode())\n```\n查看新文章\n```shell\n127.0.0.1:6379[10]> keys *\n1) \"ch:18:hot\"\n2) \"ch:18:new\"\n127.0.0.1:6379[10]> ZRANGE \"ch:18:new\" 0 -1\n1) \"13890\"\n2) \"13891\"\n```\n最后，修改 `online_update.py`，加入基于热门文章和新文章的在线召回逻辑，开启实时运行即可\n```\nif __name__ == '__main__':\n    ore = OnlineRecall()\n    ore.update_content_recall()\n    ore.update_hot_redis()\n    ore.update_new_redis()\n    stream_sc.start()\n    # 使用 ctrl+c 可以退出服务\n    _ONE_DAY_IN_SECONDS = 60 * 60 * 24\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        pass\n```\n到这里，我们就完成了召回阶段的全部工作，包括基于模型和基于内容的离线召回，以及基于内容、热门文章和新文章的在线召回。通过召回，我们可以从数百万甚至上亿的原始物品数据中，筛选出和用户相关的几百、几千个可能感兴趣的物品，后面，我们将要进入到排序阶段，对召回的几百、几千个物品进行进一步的筛选和排序。\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之9、基于内容的离线及在线召回.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之9、基于内容的离线及在线召回/","content":"\n在上篇文章中，我们实现了基于模型的离线召回，属于基于协同过滤的召回算法。接下来，本文就讲一下另一个经典的召回方式，那就是如何实现基于内容的离线召回。相比于协同过滤来说，基于内容的召回会简单很多，主要思路就是召回用户点击过的文章的相似文章，通常也被叫做 u2i2i。\n\n# 离线召回\n首先，读取用户历史行为数据，得到用户历史点击过的文章\n```\nspark.sql('use profile')\nuser_article_basic = spark.sql(\"select * from user_article_basic\")\nuser_article_basic = user_article_basic.filter('clicked=True')\n```\n`user_article_basic` 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-349e372abc54e502.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，遍历用户历史点击过的文章，获取与之相似度最高的 K 篇文章即可。可以根据之前计算好的文章相似度表 article_similar 进行相似文章查询，接着根据历史召回结果进行过滤，防止重复推荐。最后将召回结果按照频道分别存入召回结果表及历史召回结果表\n```\nuser_article_basic.foreachPartition(get_clicked_similar_article)\n\ndef get_clicked_similar_article(partition):\n    \"\"\"召回用户点击文章的相似文章\n    \"\"\"\n    import happybase\n    pool = happybase.ConnectionPool(size=10, host='hadoop-master')\n    \n    with pool.connection() as conn:\n        similar_table = conn.table('article_similar')\n        for row in partition:\n            # 读取文章相似度表,根据文章ID获取相似文章\n            similar_article = similar_table.row(str(row.article_id).encode(),\n                                                columns=[b'similar'])\n            # 按照相似度进行排序\n            similar_article_sorted = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)\n            if similar_article_sorted:\n                # 每次行为推荐10篇文章\n                similar_article_topk = [int(i[0].split(b':')[1]) for i in similar_article_sorted][:10]\n\n                # 根据历史召回结果进行过滤\n                history_table = conn.table('history_recall')\n                history_article_data = history_table.cells('reco:his:{}'.format(row.user_id).encode(), 'channel:{}'.format(row.channel_id).encode())\n                # 将多个版本都加入历史文章ID列表\n                history_article = []\n                if len(history_article_data) >= 2:\n                    for article in history_article_data[:-1]:\n                        history_article.extend(eval(article))\n                else:\n                    history_article = []\n\n                # 过滤history_article\n                recall_article = list(set(similar_article_topk) - set(history_article))\n\n                # 存储到召回结果表及历史召回结果表\n                if recall_article:\n                    content_table = conn.table('cb_recall')\n                    content_table.put(\"recall:user:{}\".format(row.user_id).encode(), {'content:{}'.format(row.channel_id).encode(): str(recall_article).encode()})\n\n                    # 放入历史召回结果表\n                    history_table.put(\"reco:his:{}\".format(row.user_id).encode(), {'channel:{}'.format(row.channel_id).encode(): str(recall_article).encode()})\n```\n可以根据用户 ID 和频道 ID 来查询召回结果\n```\nhbase(main):028:0> get 'cb_recall', 'recall:user:2'\nCOLUMN                     CELL                                                                        \ncontent:13                    timestamp=1558041569201, value=[141431,14381, 17966, 17454, 14125, 16174]   \n```\n\n最后，使用 Apscheduler 定时更新。在用户召回方法 `update_user_recall()` 中，增加基于内容的离线召回方法 `update_content_recall()`，首先读取用户行为日志，并筛选用户点击的文章，接着读取文章相似表，获取相似度最高的 K 篇文章，然后根据历史召回结果进行过滤，防止重复推荐，最后，按频道分别存入召回结果表及历史召回结果表\n```\ndef update_user_recall():\n    \"\"\"\n    用户的频道推荐召回结果更新逻辑\n    :return:\n    \"\"\"\n    ur = UpdateRecall(500)\n    ur.update_als_recall()\n    ur.update_content_recall()\n```\n之前已经添加好了定时更新用户召回结果的任务，每隔 3 小时运行一次，这样就完成了基于内容的离线召回。\n```\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.executors.pool import ProcessPoolExecutor\n\n# 创建scheduler，多进程执行\nexecutors = {\n    'default': ProcessPoolExecutor(3)\n}\n\nscheduler = BlockingScheduler(executors=executors)\n\n# 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次\nscheduler.add_job(update_article_profile, trigger='interval', hours=1)\n# 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次\nscheduler.add_job(update_user_profile, trigger='interval', hours=2)\n# 添加一个定时运行用户召回更新的任务，每隔3小时运行一次\nscheduler.add_job(update_user_recall, trigger='interval', hours=3)\n# 添加一个定时运行特征中心平台的任务，每隔4小时更新一次\nscheduler.add_job(update_ctr_feature, trigger='interval', hours=4)\n\nscheduler.start()\n```\n\n# 在线召回\n前面我们实现了基于内容的离线召回，接下来我们将实现基于内容的在线召回。在线召回的实时性更好，能够根据用户的线上行为实时反馈，快速跟踪用户的偏好，也能够解决用户冷启动问题。离线召回和在线召回唯一的不同就是，离线召回读取的是用户历史行为数据，而在线召回读取的是用户实时的行为数据，从而召回用户当前正在阅读的文章的相似文章。\n\n首先，我们通过 Spark Streaming 读取 Kafka 中的用户实时行为数据，Spark Streaming 配置如下\n```\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\nfrom setting.default import DefaultConfig\nimport happybase\n\nSPARK_ONLINE_CONFIG = (\n        (\"spark.app.name\", \"onlineUpdate\"), \n        (\"spark.master\", \"yarn\"),\n        (\"spark.executor.instances\", 4)\n    )\n\nKAFKA_SERVER = \"192.168.19.137:9092\"\n\n# 用于读取hbase缓存结果配置\npool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)\nconf = SparkConf()\nconf.setAll(SPARK_ONLINE_CONFIG)\nsc = SparkContext(conf=conf)\nstream_c = StreamingContext(sc, 60)\n\n# 基于内容召回配置,用于收集用户行为\nsimilar_kafkaParams = {\"metadata.broker.list\": DefaultConfig.KAFKA_SERVER, \"group.id\": 'similar'}\nSIMILAR_DS = KafkaUtils.createDirectStream(stream_c, ['click-trace'], similar_kafkaParams)\n```\nKafka 中的用户行为数据，如下所示\n```\n{\"actionTime\":\"2019-12-10 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"116644\", \"algorithmCombine\": \"C2\"}}\n```\n接下来，利用 Spark Streaming 将用户行为数据传入到 `get_similar_online_recall()` 方法中，这里利用 `json.loads()` 方法先将其转换为了 json 格式，注意用户行为数据在每条 Kafka 消息的第二个位置\n```python\nSIMILAR_DS.map(lambda x: json.loads(x[1])).foreachRDD(get_similar_online_recall)\n```\n接着，遍历用户行为数据，这里可能每次读取到多条用户行为数据。筛选出被点击、收藏或分享过的文章，并获取与其相似度最高的 K 篇文章，再根据历史召回结果表进行过滤，防止重复推荐，最后，按频道分别存入召回结果表及历史召回结果表\n```\ndef get_online_similar_recall(rdd):\n    \"\"\"\n    获取在线相似文章\n    :param rdd:\n    :return:\n    \"\"\"\n    import happybase\n\n    topk = 10\n    # 初始化happybase连接\n    pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)\n    for data in rdd.collect():\n\n        # 根据用户行为筛选文章\n        if data['param']['action'] in [\"click\", \"collect\", \"share\"]:\n            with pool.connection() as conn:\n                similar_table = conn.table(\"article_similar\")\n\n                # 根据用户行为数据涉及文章找出与之最相似文章(基于内容的相似)\n                similar_article = similar_table.row(str(data[\"param\"][\"articleId\"]).encode(), columns=[b\"similar\"])\n                similar_article = sorted(similar_article.items(), key=lambda x: x[1], reverse=True)  # 按相似度排序\n\n                if similar_article:\n                    similar_article_topk = [int(i[0].split(b\":\")[1]) for i in similar_article[:topk]] # 选取K篇作为召回推荐结果\n\n                    # 根据历史召回结果进行过滤\n                    history_table = conn.table('history_recall')\n                    history_article_data = history_table.cells(b\"reco:his:%s\" % data[\"param\"][\"userId\"].encode(), b\"channel:%d\" % data[\"channelId\"])\n                    # 将多个版本都加入历史文章ID列表\n                    history_article = []\n                    if len(history_article_data) >1:\n                        for article in history_article_data[:-1]:\n                            history_article.extend(eval(article))\n                    else:\n                        history_article = []\n\n                    # 过滤history_article\n                    recall_article = list(set(similar_article_topk) - set(history_article))\n\n                    # 如果有召回结果,按频道分别存入召回结果表及历史召回结果表\n                    if recall_article:\n                        recall_table = conn.table(\"cb_recall\")\n                        recall_table.put(b\"recall:user:%s\" % data[\"param\"][\"userId\"].encode(), {b\"online:%d\" % data[\"channelId\"]: str(recall_article).encode()})\n                        history_table.put(b\"reco:his:%s\" % data[\"param\"][\"userId\"].encode(), {b\"channel:%d\" % data[\"channelId\"]: str(recall_article).encode()})\n\n                conn.close()\n```\n可以根据用户 ID 和频道 ID 来查询召回结果\n```\nhbase(main):028:0> get 'cb_recall', 'recall:user:2'\nCOLUMN                     CELL                                                                        \nonline:13                    timestamp=1558041569201, value=[141431,14381, 17966, 17454, 14125, 16174]   \n```\n创建 `online_update.py`，加入基于内容的在线召回逻辑\n```\nif __name__ == '__main__':\n    ore = OnlineRecall()\n    ore.update_content_recall()\n    stream_sc.start()\n    _ONE_DAY_IN_SECONDS = 60 * 60 * 24\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        pass\n```\n利用 Supervisor 进行进程管理，并开启实时运行，配置如下，其中 environment 需要指定运行所需环境\n```\n[program:online]\nenvironment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python ,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_SUBMIT_ARGS='--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.2 pyspark-shell'\ncommand=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/reco_sys/online/online_update.py\ndirectory=/root/toutiao_project/reco_sys/online\nuser=root\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/root/logs/onlinesuper.log\nloglevel=info\nstopsignal=KILL\nstopasgroup=true\nkillasgroup=true\n```\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之8、基于模型的离线召回.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之8、基于模型的离线召回/","content":"\n前面我们完成了所有的数据准备，接下来，就要开始召回阶段的工作了，可以做离线召回，也可以做在线召回，召回算法通常包括基于内容的召回和基于协同过滤的召回。ALS 模型是一种基于模型的协同过滤召回算法，本文将通过 ALS 模型实现离线召回。\n\n首先，我们在 Hbase 中创建召回结果表 cb_recall，这里用不同列族来存储不同方式的召回结果，其中 als 表示模型召回，content 表示内容召回，online 表示在线召回。通过设置多个版本来存储多次召回结果，通过设置生存期来清除长时间未被使用的召回结果。\n```\ncreate 'cb_recall', {NAME=>'als', TTL=>7776000, VERSIONS=>999999}\nalter 'cb_recall', {NAME=>'content', TTL=>7776000, VERSIONS=>999999}\nalter 'cb_recall', {NAME=>'online', TTL=>7776000, VERSIONS=>999999}\n\n# 插入样例\nput 'cb_recall', 'recall:user:5', 'als:2',[1,2,3,4,5,6,7,8,9,10]\nput 'cb_recall', 'recall:user:2', 'content:1',[45,3,5,10,289,11,65,52,109,8]\nput 'cb_recall', 'recall:user:2', 'online:2',[1,2,3,4,5,6,7,8,9,10]\n```\n在 Hive 中建立外部表，用于离线分析\n```\ncreate external table cb_recall_hbase\n(\n    user_id STRING comment \"userID\",\n    als     map<string, ARRAY<BIGINT>> comment \"als recall\",\n    content map<string, ARRAY<BIGINT>> comment \"content recall\",\n    online  map<string, ARRAY<BIGINT>> comment \"online recall\"\n)\n    COMMENT \"user recall table\"\n    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n        WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,als:,content:,online:\")\n    TBLPROPERTIES (\"hbase.table.name\" = \"cb_recall\");\n```\n接着，在 Hbase 中创建历史召回结果表，用于过滤已历史召回结果，避免重复推荐。这里同样设置了多个版本来存储多次历史召回结果，设置了生存期来清除很长时间以前的历史召回结果\n```\ncreate 'history_recall', {NAME=>'channel', TTL=>7776000, VERSIONS=>999999}\n\n# 插入示例\nput 'history_recall', 'recall:user:5', 'als:1',[1,2,3]\nput 'history_recall', 'recall:user:5', 'als:1',[4,5,6,7]\nput 'history_recall', 'recall:user:5', 'als:1',[8,9,10]\n```\n# ALS 原理\n\n![](https://upload-images.jianshu.io/upload_images/12790782-e608ec3e758c49d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n我们先简单了解一下 ALS 模型，上图为用户和物品的关系矩阵，其中，每一行代表一个用户，每一列代表一个物品。蓝色元素代表用户查看过该物品，灰色元素代表用户未查看过该物品，假设有 m 个用户，n 个物品，为了得到用户对物品的评分，我们可以利用矩阵分解将原本较大的稀疏矩阵拆分成两个较小的稠密矩阵，即 m x k 维的用户隐含矩阵和 k x n 维的物品隐含矩阵，如下所示：\n\n![](https://upload-images.jianshu.io/upload_images/12790782-26825ddee138897c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n其中，用户矩阵的每一行就包括了影响用户偏好的 k 个隐含因子，物品矩阵的每一列就包括了影响物品内容的 k 个隐含因子。这里用户矩阵和物品矩阵中每个隐含因子的值就是利用交替最小二乘（Alternating Least Squares，ALS）优化算法计算而得的，所以叫做 ALS 模型。接下来，再将用户矩阵和物品矩阵相乘即可得到用户对物品的  m x n  维的评分矩阵，其中就包括每一个用户对每一个物品的评分了，进而可以根据评分进行推荐。\n\n# ALS 模型训练和预测\nSpark 已经实现了 ALS 模型，我们可以直接调用。首先，我们读取用户历史点击行为，构造训练集数据，其中只需要包括用户 ID、文章 ID 以及是否点击\n```\nspark.sql('use profile')\nuser_article_basic = spark.sql(\"select user_id, article_id, clicked from user_article_basic\")\n```\n`user_article_basic` 结果如下所示，其中 clicked 表示用户对文章是否发生过点击\n\n![](https://upload-images.jianshu.io/upload_images/12790782-abb6c53b98c88373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n我们需要将 clicked 由 boolean 类型转成 int 类型，即 true 为 1，false 为 0\n```\ndef convert_boolean_int(row):\n    return row.user_id, row.article_id, int(row.clicked)\n\nuser_article_basic = user_article_basic.rdd.map(convert_boolean_int).toDF(['user_id', 'article_id', 'clicked'])\n```\n`user_article_basic` 结果如下所示，clicked 已经是 int 类型\n\n![](https://upload-images.jianshu.io/upload_images/12790782-f0075684b769f328.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n另外，Spark 的 ALS 模型还要求输入的用户 ID 和文章 ID 必须是从 1 开始递增的连续数字，所以需要利用 Spark 的 `Pipeline` 和 `StringIndexer`，将用户 ID 和文章 ID 建立从 1 开始递增的索引\n```\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\nuser_indexer = StringIndexer(inputCol='user_id', outputCol='als_user_id')\narticle_indexer = StringIndexer(inputCol='article_id', outputCol='als_article_id')\npip = Pipeline(stages=[user_indexer, article_indexer])\npip_model = pip.fit(user_article_basic)\nals_user_article = pip_model.transform(user_article_basic)\n```\n`als_user_article` 结果如下所示，als_user_id 和 als_article_id 即是 ALS 模型所需的用户索引和文章索引\n\n![](https://upload-images.jianshu.io/upload_images/12790782-98ff5968a2cdd731.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，将用户行为数据中的 als_user_id, als_article_id, clicked 三列作为训练集，对 ALS 模型进行训练，并利用 ALS 模型计算用户对文章的偏好得分，这里可以指定为每个用户保留偏好得分最高的 K 篇文章\n```\nfrom pyspark.ml.recommendation import ALS\n\ntop_k = 100\nals = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked')\nals_model = als.fit(als_user_article)\n\nrecall_res = als_model.recommendForAllUsers(top_k)\n```\n`recall_res` 结果如下所示，其中，als_user_id 为用户索引，recommendations 为每个用户的推荐列表，包括文章索引和偏好得分，如 [[255,0.1], [10,0.08], ...]\n\n![](https://upload-images.jianshu.io/upload_images/12790782-a966583c33d30c06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 预测结果处理\n接着，我们要将推荐结果中的用户索引和文章索引还原为用户 ID 和文章 ID，这就需要建立用户 ID 与用户索引的映射及文章 ID 与文章索引的映射，可以将前面包含用户索引和文章索引的用户行为数据 `als_user_article` 分别按照 user_id 和 article_id 分组，即可得到用户 ID 与用户索引的映射以及文章 ID 与文章索引的映射\n```\nuser_real_index = als_user_article.groupBy(['user_id']).max('als_user_id').withColumnRenamed('max(als_user_id)', 'als_user_id')\narticle_real_index = als_user_article.groupBy(['article_id']).max('als_article_id').withColumnRenamed('max(als_article_id)', 'als_article_id')\n```\n`user_real_index` 结果如下所示，即用户 ID 与用户索引的映射\n\n![](https://upload-images.jianshu.io/upload_images/12790782-4517cce545326e63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n再利用 `als_user_id` 将 `recall_res` 和 `user_real_index` 进行连接，加入用户 ID\n```\nrecall_res = recall_res.join(user_real_index, on=['als_user_id'], how='left').select(['als_user_id', 'recommendations', 'user_id'])\n```\n`recall_res` 结果如下所示，得到用户索引，推荐列表和用户 ID\n\n![](https://upload-images.jianshu.io/upload_images/12790782-6939d47b670c5716.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)存储\n\n接下来，我们要构建出用户和文章的关系，利用 `explode()` 方法将 recommendations 中的每篇文章都转换为单独的一条记录，并只保留用户 ID 和文章索引这两列数据\n```\nimport pyspark.sql.functions as F\n\nrecall_res = recall_res.withColumn('als_article_id', F.explode('recommendations')).drop('recommendations').select(['user_id', 'als_article_id'])\n```\n`recall_res` 结果如下所示，als_article_id 包括文章索引和偏好得分\n\n![](https://upload-images.jianshu.io/upload_images/12790782-4f010be4233f8f6d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n我们将 als_article_id 中的偏好得分去除，只保留文章索引\n```\ndef get_article_index(row):\n    return row.user_id, row.als_article_id[0]\n\nrecall_res = recall_res.rdd.map(get_article_index).toDF(['user_id', 'als_article_id'])\n```\n`recall_res` 结果如下所示，得到用户 ID 和文章索引\n\n![](https://upload-images.jianshu.io/upload_images/12790782-251a47b794797f2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n之前我们将文章 ID 和文章索引保存到了 `article_real_index`，这里利用 als_article_id 将 recall_res 和 `article_real_index` 进行连接，得到文章 ID\n```\nrecall_res = recall_res.join(article_real_index, on=['als_article_id'], how='left').select(['user_id', 'article_id'])\n```\n`recall_res` 结果如下所示，得到用户 ID 和要向其推荐的文章 ID\n\n![](https://upload-images.jianshu.io/upload_images/12790782-d42ac7e8e540cdb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 推荐结果存储\n为了方便查询，我们需要将推荐结果按频道分别进行存储。首先，读取文章完整信息，得到频道 ID\n```\nspark.sql('use article')\narticle_data = spark.sql(\"select article_id, channel_id from article_data\")\n```\n利用 article_id 将 `recall_res` 和 `article_data` 进行连接，在推荐结果中加入频道 ID\n```\nrecall_res = recall_res.join(article_data, on=['article_id'], how='left')\n```\n`recall_res` 结果如下所示，推荐结果加入了频道 ID\n\n![](https://upload-images.jianshu.io/upload_images/12790782-d23360fcb5da1901.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n将推荐结果按照 user_id 和 channel_id 进行分组，利用 `collect_list()` 方法将文章 ID 合并为文章列表\n```\nrecall_res = recall_res.groupBy(['user_id', 'channel_id']).agg(F.collect_list('article_id')).withColumnRenamed('collect_list(article_id)', 'article_list')\n```\n`recall_res` 结果如下所示，article_list 为某用户在某频道下的推荐文章列表\n\n![](https://upload-images.jianshu.io/upload_images/12790782-29f5ad6c579f4ee4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n最后，将推荐结果按频道分别存入召回结果表 cb_recall 及历史召回结果表 history_recall。注意，在保存新的召回结果之前需要根据历史召回结果进行过滤，防止重复推荐\n```\nrecall_res = recall_res.dropna()\nrecall_res.foreachPartition(save_offline_recall_hbase)\n\ndef save_offline_recall_hbase(partition):\n    \"\"\"ALS模型离线召回结果存储\n    \"\"\"\n    import happybase\n    pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)\n    for row in partition:\n        with pool.connection() as conn:\n            # 读取历史召回结果表\n            history_table = conn.table('history_recall')\n            # 读取包含多个版本的历史召回结果\n            history_article_data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),\n                                       'channel:{}'.format(row.channel_id).encode())\n\n            # 合并多个版本历史召回结果\n            history_article = []（比如有的用户会比较怀旧）\n            if len(history_article_data) >= 2:\n                for article in history_article_data[:-1]:\n                    history_article.extend(eval(article))\n            else:\n                history_article = []\n\n            # 过滤history_article\n            recall_article = list(set(row.article_list) - set(history_article))\n\n            if recall_article:\n                table = conn.table('cb_recall')\n                table.put('recall:user:{}'.format(row.user_id).encode(), {'als:{}'.format(row.channel_id).encode(): str(recall_article).encode()})\n                history_table.put(\"reco:his:{}\".format(row.user_id).encode(), {'channel:{}'.format(row.channel_id): str(recall_article).encode()})\n            conn.close()\n```\n可以根据用户 ID 和频道 ID 来查询召回结果\n```\nhbase(main):028:0> get 'cb_recall', 'recall:user:2'\nCOLUMN                     CELL                                                                        \nals:13                    timestamp=1558041569201, value=[141431,14381, 17966, 17454, 14125, 16174]   \n```\n# Apscheduler 定时更新\n在用户召回方法 `update_user_recall()` 中，增加基于模型的离线召回方法 `update_content_recall()`，首先读取用户行为日志，进行数据预处理，构建训练集，接着对 ALS 模型进行训练和预测，最后对预测出的推荐结果进行解析并按频道分别存入召回结果表和历史召回结果表\n```\ndef update_user_recall():\n    \"\"\"\n    用户的频道推荐召回结果更新逻辑\n    :return:\n    \"\"\"\n    ur = UpdateRecall(500)\n    ur.update_als_recall()\n```\n添加定时更新用户召回结果的任务，每隔 3 小时运行一次\n```\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.executors.pool import ProcessPoolExecutor\n\n# 创建scheduler，多进程执行\nexecutors = {\n    'default': ProcessPoolExecutor(3)\n}\n\nscheduler = BlockingScheduler(executors=executors)\n\n# 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次\nscheduler.add_job(update_article_profile, trigger='interval', hours=1)\n# 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次\nscheduler.add_job(update_user_profile, trigger='interval', hours=2)\n# 添加一个定时运行用户召回更新的任务，每隔3小时运行一次\nscheduler.add_job(update_user_recall, trigger='interval', hours=3)\n# 添加一个定时运行特征中心平台的任务，每隔4小时更新一次\nscheduler.add_job(update_ctr_feature, trigger='interval', hours=4)\n\nscheduler.start()\n```\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之7、构建离线文章特征和用户特征.md","url":"/2019/12/05/RecSys/文章推荐系统/文章推荐系统》系列之7、构建离线文章特征和用户特征/","content":"\n前面我们完成了文章画像和用户画像的构建，画像数据主要是提供给召回阶段的各种召回算法使用。接下来，我们还要为排序阶段的各种排序模型做数据准备，通过特征工程将画像数据进一步加工为特征数据，以供排序模型直接使用。\n\n我们可以将特征数据存储到 Hbase 中，这里我们先在 Hbase 中创建好 ctr_feature_article 表 和 ctr_feature_user 表，分别存储文章特征数据和用户特征数据\n```sql\n-- 文章特征表\ncreate 'ctr_feature_article', 'article'\n-- 如 article:13401 timestamp=1555635749357, value=[18.0,0.08196639249252607,0.11217275332895373,0.1353835167902181,0.16086650318453152,0.16356418791892943,0.16740082750337945,0.18091837445730974,0.1907214431716628,0.2........................-0.04634634410271921,-0.06451843378804649,-0.021564142420785692,0.10212902152136256]\n\n-- 用户特征表\ncreate 'ctr_feature_user', 'channel'\n-- 如 4 column=channel:13, timestamp=1555647172980, value=[]\n```\n\n# 构建文章特征\n文章特征包括文章关键词权重、文章频道以及文章向量，我们首先读取文章画像\n```\nspark.sql(\"use article\")\narticle_profile = spark.sql(\"select * from article_profile\")\n```\n在文章画像中筛选出权重最高的 K 个关键词的权重，作为文章关键词的权重向量\n```\ndef article_profile_to_feature(row):\n    try:\n        article_weights = sorted(row.keywords.values())[:10]\n    except Exception as e:\n        article_weights = [0.0] * 10\n    return row.article_id, row.channel_id, article_weights\narticle_profile = article_profile.rdd.map(article_profile_to_feature).toDF(['article_id', 'channel_id', 'weights'])\n```\n`article_profile` 结果如下所示，weights 即为文章关键词的权重向量\n\n![](https://upload-images.jianshu.io/upload_images/12790782-bdc45d1e743a3c1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，读取文章向量信息，再将频道 ID 和文章向量加入进来，利用 article_id 将 article_profile 和 article_vector 进行内连接，并将 weights 和 articlevector 转为 `vector` 类型\n```\narticle_vector = spark.sql(\"select * from article_vector\")\narticle_feature = article_profile.join(article_vector, on=['article_id'], how='inner')\n\ndef feature_to_vector(row):\n\n    from pyspark.ml.linalg import Vectors\n\n    return row.article_id, row.channel_id, Vectors.dense(row.weights), Vectors.dense(row.articlevector)\n\narticle_feature = article_feature.rdd.map(feature_to_vector).toDF(['article_id', 'channel_id', 'weights', 'articlevector'])\n```\n最后，我们将 channel_id, weights, articlevector 合并为一列 features 即可（通常 channel_id 可以进行 one-hot 编码，我们这里先省略了）\n```\nfrom pyspark.ml.feature import VectorAssembler\n\ncolumns = ['article_id', 'channel_id', 'weights', 'articlevector']\narticle_feature = VectorAssembler().setInputCols(columns[1:4]).setOutputCol(\"features\").transform(article_feature)\n```\n`article_feature` 结果如下所示，features 就是我们准备好的文章特征\n\n![](https://upload-images.jianshu.io/upload_images/12790782-1d9a977180832173.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n最后，将文章特征结果保存到 Hbase 中\n```\ndef save_article_feature_to_hbase(partition):\n    import happybase\n    pool = happybase.ConnectionPool(size=10, host='hadoop-master')\n    with pool.connection() as conn:\n        table = conn.table('ctr_feature_article')\n        for row in partition:\n            table.put('{}'.format(row.article_id).encode(),\n                     {'article:{}'.format(row.article_id).encode(): str(row.features).encode()})\n\narticle_feature.foreachPartition(save_article_feature_to_hbase)\n```\n\n# 构建用户特征\n由于用户在不同频道的偏好差异较大，所以我们要计算用户在每个频道的特征。首先读取用户画像，将空值列删除\n```\nspark.sql(\"use profile\")\n\nuser_profile_hbase = spark.sql(\"select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase\")\n```\n`user_profile_hbase` 结果如下所示，其中 article_partial 为用户标签及权重，如 (['18:vars': 0.2, '18: python':0.2, ...], ['19:java': 0.2, '19: javascript':0.2, ...], ...) 表示某个用户在 18 号频道的标签包括 var、python 等，在 19 号频道的标签包括 java、javascript等。\n\n![](https://upload-images.jianshu.io/upload_images/12790782-655ec8ea6979ab94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n由于 gender 和 birthday 两列空值较多，我们将这两列去除（实际场景中也可以根据数据情况选择填充）\n```\n# 去除空值列\nuser_profile_hbase = user_profile_hbase.drop('env', 'birthday', 'gender')\n```\n提取用户 ID，获取 user_id 列的内容中 `:` 后面的数值即为用户 ID\n```\ndef get_user_id(row):\n    return int(row.user_id.split(\":\")[1]), row.article_partial\n\nuser_profile_hbase = user_profile_hbase.rdd.map(get_user_id)\n```\n将 `user_profile_hbase` 转为 DataFrame 类型\n```\nfrom pyspark.sql.types import *\n\n_schema = StructType([\n    StructField(\"user_id\", LongType()),\n    StructField(\"weights\", MapType(StringType(), DoubleType()))\n])\n\nuser_profile_hbase = spark.createDataFrame(user_profile_hbase, schema=_schema)\n```\n接着，将每个频道内权重最高的 K 个标签的权重作为用户标签权重向量\n```\ndef frature_preprocess(row):\n\n    from pyspark.ml.linalg import Vectors\n\n    user_weights = []\n    for i in range(1, 26):\n        try:\n            channel_weights = sorted([row.weights[key] for key in row.weights.keys() if key.split(':')[0] == str(i)])[:10]\n            user_weights.append(channel_weights)\n        except:\n            user_weights.append([0.0] * 10)\n    return row.user_id, user_weights\n\nuser_features = user_profile_hbase.rdd.map(frature_preprocess).collect()\n```\n`user_features` 就是我们计算好的用户特征，数据结构类似 (10, [[0.2, 2.1, ...], [0.2, 2.1, ...]], ...)，其中元组第一个元素 10 即为用户 ID，第二个元素是长度为 25 的用户频道标签权重列表，列表中每个元素是长度为 K 的用户标签权重列表，代表用户在某个频道下的标签权重向量。\n\n最后，将用户特征结果保存到 Hbase，利用 Spark 的 `batch()` 方法，按频道批量存储用户特征\n```\nimport happybase\n\n# 批量插入Hbase数据库中\npool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)\nwith pool.connection() as conn:\n    ctr_feature = conn.table('ctr_feature_user')\n    with ctr_feature.batch(transaction=True) as b:\n        for i in range(len(user_features)):\n            for j in range(25):\n                b.put(\"{}\".format(res[i][0]).encode(),{\"channel:{}\".format(j+1).encode(): str(res[i][1][j]).encode()})\n    conn.close()\n```\n# Apscheduler 定时更新\n定义文章特征和用户特征的离线更新方法\n```\ndef update_ctr_feature():\n    \"\"\"\n    更新文章特征和用户特征\n    :return:\n    \"\"\"\n    fp = FeaturePlatform()\n    fp.update_user_ctr_feature_to_hbase()\n    fp.update_article_ctr_feature_to_hbase()\n```\n在 Apscheduler 中添加定时更新文章特征和用户特征的任务，每隔 4 小时运行一次\n```\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.executors.pool import ProcessPoolExecutor\n\n# 创建scheduler，多进程执行\nexecutors = {\n    'default': ProcessPoolExecutor(3)\n}\n\nscheduler = BlockingScheduler(executors=executors)\n\n# 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次\nscheduler.add_job(update_article_profile, trigger='interval', hours=1)\n# 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次\nscheduler.add_job(update_user_profile, trigger='interval', hours=2)\n# 添加一个定时运行特征中心平台的任务，每隔4小时更新一次\nscheduler.add_job(update_ctr_feature, trigger='interval', hours=4)\n\nscheduler.start()\n```\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘， 提取码：eakp）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之6、构建离线用户画像.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之6、构建离线用户画像/","content":"\n前面我们完成了文章画像的构建以及文章相似度的计算，接下来，我们就要实现用户画像的构建了。用户画像往往是大型网站的重要模块，基于用户画像不仅可以实现个性化推荐，还可以实现用户分群、精准推送、精准营销以及用户行为预测、商业化转化分析等，为商业决策提供数据支持。通常用户画像包括用户属性信息（性别、年龄、出生日期等）、用户行为信息（浏览、收藏、点赞等）以及环境信息（时间、地理位置等）。\n\n# 处理用户行为数据\n在数据准备阶段，我们通过 Flume 已经可以将用户行为数据收集到 Hive 的 user_action 表的 HDFS 路径中，先来看一下这些数据长什么样子，我们读取当天的用户行为数据，注意读取之前要先关联分区\n```python\n_day = time.strftime(\"%Y-%m-%d\", time.localtime())\n_localions = '/user/hive/warehouse/profile.db/user_action/' + _day\nif fs.exists(_localions):\n    # 如果有该文件直接关联，捕获关联重复异常\n    try:\n        self.spark.sql(\"alter table user_action add partition (dt='%s') location '%s'\" % (_day, _localions))\n    except Exception as e:\n        pass\n\n    self.spark.sql(\"use profile\")\n    user_action = self.spark.sql(\"select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt>=\" + _day)\n```\n`user_action` 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-04c3417b7c70dacd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n可以发现，上面的一条记录代表用户对文章的一次行为，但通常我们需要查询某个用户对某篇文章的所有行为，所以，我们要将这里用户对文章的多条行为数据合并为一条，其中包括用户对文章的所有行为。我们需要新建一个 Hive 表 user_article_basic，这张表包括了用户 ID、文章 ID、是否曝光、是否点击、阅读时间等等，随后我们将处理好的用户行为数据存储到此表中\n```sql\ncreate table user_article_basic\n(\n    user_id     BIGINT comment \"userID\",\n    action_time STRING comment \"user actions time\",\n    article_id  BIGINT comment \"articleid\",\n    channel_id  INT comment \"channel_id\",\n    shared      BOOLEAN comment \"is shared\",\n    clicked     BOOLEAN comment \"is clicked\",\n    collected   BOOLEAN comment \"is collected\",\n    exposure    BOOLEAN comment \"is exposured\",\n    read_time   STRING comment \"reading time\"\n)\n    COMMENT \"user_article_basic\"\n    CLUSTERED by (user_id) into 2 buckets\n    STORED as textfile\n    LOCATION '/user/hive/warehouse/profile.db/user_article_basic';\n```\n遍历每一条原始用户行为数据，判断用户对文章的行为，在 user_action_basic 中将该用户与该文章对应的行为设置为 True\n```\nif user_action.collect():\n    def _generate(row):\n        _list = []\n        if row.action == 'exposure':\n            for article_id in eval(row.articleId):\n                # [\"user_id\", \"action_time\",\"article_id\", \"channel_id\", \"shared\", \"clicked\", \"collected\", \"exposure\", \"read_time\"]\n                _list.append(\n                    [row.userId, row.actionTime, article_id, row.channelId, False, False, False, True, row.readTime])\n            return _list\n        else:\n            class Temp(object):\n                shared = False\n                clicked = False\n                collected = False\n                read_time = \"\"\n\n            _tp = Temp()\n            if row.action == 'click':\n                _tp.clicked = True\n            elif row.action == 'share':\n                _tp.shared = True\n            elif row.action == 'collect':\n                _tp.collected = True\n            elif row.action == 'read':\n                _tp.clicked = True\n\n            _list.append(\n                [row.userId, row.actionTime, int(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,\n                 True, row.readTime])\n            return _list\n\n    user_action_basic = user_action.rdd.flatMap(_generate)\n    user_action_basic = user_action_basic.toDF(\n        [\"user_id\", \"action_time\", \"article_id\", \"channel_id\", \"shared\", \"clicked\", \"collected\", \"exposure\",\n         \"read_time\"])\n```\n`user_action_basic` 结果如下所示，这里的一条记录包括了某个用户对某篇文章的所有行为\n\n![](https://upload-images.jianshu.io/upload_images/12790782-651af8651abfca3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n由于 Hive 目前还不支持 pyspark 的原子性操作，所以 user_article_basic 表的用户行为数据只能全量更新（实际场景中可以选择其他语言或数据库实现）。这里，我们需要将当天的用户行为与 user_action_basic 的历史用户行为进行合并\n```\nold_data = uup.spark.sql(\"select * from user_article_basic\")\nnew_data = old_data.unionAll(user_action_basic)\n```\n合并后又会产生一个新的问题，那就是用户 ID 和文章 ID 可能重复，因为今天某个用户对某篇文章的记录可能在历史数据中也存在，而 `unionAll()` 方法并没有去重，这里我们可以按照用户 ID 和文章 ID 进行分组，利用 `max()` 方法得到 action_time, channel_id, shared, clicked, collected, exposure, read_time 即可，去重后直接存储到 user_article_basic 表中\n```\nnew_data.registerTempTable(\"temptable\")\n\nself.spark.sql('''insert overwrite table user_article_basic select user_id, max(action_time) as action_time, \n        article_id, max(channel_id) as channel_id, max(shared) as shared, max(clicked) as clicked, \n        max(collected) as collected, max(exposure) as exposure, max(read_time) as read_time from temptable \n        group by user_id, article_id''')\n```\n表 user_article_basic 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-508d84da9f16c36f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 计算用户画像\n我们选择将用户画像存储在 Hbase 中，因为 Hbase 支持原子性操作和快速读取，并且 Hive 也可以通过创建外部表关联到 Hbase，进行离线分析，如果要删除 Hive 外部表的话，对 Hbase 也没有影响。首先，在 Hbase 中创建用户画像表\n```sql\ncreate 'user_profile', 'basic','partial','env'\n```\n在 Hive 中创建 Hbase 外部表，注意字段类型设置为 map\n```sql\ncreate external table user_profile_hbase\n(\n    user_id         STRING comment \"userID\",\n    information     MAP<STRING, DOUBLE> comment \"user basic information\",\n    article_partial MAP<STRING, DOUBLE> comment \"article partial\",\n    env             MAP<STRING, INT> comment \"user env\"\n)\n    COMMENT \"user profile table\"\n    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n        WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,basic:,partial:,env:\")\n    TBLPROPERTIES (\"hbase.table.name\" = \"user_profile\");\n```\n创建外部表之后，还需要导入一些依赖包\n```\ncp -r /root/bigdata/hbase/lib/hbase-*.jar /root/bigdata/spark/jars/\ncp -r /root/bigdata/hive/lib/h*.jar /root/bigdata/spark/jars/\n```\n接下来，读取处理好的用户行为数据，由于日志中的 channel_id 有可能是来自于推荐频道（0），而不是文章真实的频道，所以这里要将 channel_id 列删除\n```\nspark.sql(\"use profile\")\nuser_article_basic = spark.sql(\"select * from user_article_basic\").drop('channel_id')\n```\n通过文章 ID，将用户行为数据与文章画像数据进行连接，从而得到文章频道 ID 和文章主题词\n```\nspark.sql('use article')\narticle_topic = spark.sql(\"select article_id, channel_id, topics from article_profile\")\nuser_article_topic = user_article_basic.join(article_topic, how='left', on=['article_id'])\n```\n`user_article_topic` 结果如下图所示，其中 topics 列即为文章主题词列表，如 ['补码', '字符串', '李白', ...]\n\n![](https://upload-images.jianshu.io/upload_images/12790782-8c0d357cbea1d7e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，我们需要计算每一个主题词对于用户的权重，所以需要将 topics 列中的每个主题词都拆分为单独的一条记录。可以利用 Spark 的 `explode()` 方法，达到类似“爆炸”的效果\n```\nimport pyspark.sql.functions as F\n\nuser_article_topic = user_topic.withColumn('topic', F.explode('topics')).drop('topics')\n```\n`user_article_topic` 如下图所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-4c1b56aad8ec412a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n我们通过用户对哪些文章发生了行为以及该文章有哪些主题词，计算出了用户对哪些主题词发生了行为。这样，我们就可以根据用户对主题词的行为来计算主题词对用户的权重，并且将这些主题词作为用户的标签。那么，用户标签权重的计算公式为：用户标签权重 =（用户行为分值之和）x 时间衰减。其中，时间衰减公式为：时间衰减系数 = 1 / (log(t) + 1)，其中 t 为发生行为的时间距离当前时间的大小\n\n不同的用户行为对应不同的权重，如下所示\n\n用户行为|分值\n-|-\n阅读时间(<1000)\t|1\n阅读时间(>=1000)\t|2\n收藏\t|2\n分享\t|3\n点击\t|5\n\n计算用户标签及权重，并存储到 Hbase 中 user_profile 表的 partial 列族中。注意，这里我们将频道 ID 和标签一起作为 partial 列族的键存储，这样我们就方便查询不同频道的标签及权重了\n```\ndef compute_user_label_weights(partitions):\n    \"\"\" 计算用户标签权重\n    \"\"\"\n    action_weight = {\n        \"read_min\": 1,\n        \"read_middle\": 2,\n        \"collect\": 2,\n        \"share\": 3,\n        \"click\": 5\n    }\n\n    from datetime import datetime\n    import numpy as np\n    \n    # 循环处理每个用户对应的每个主题词\n    for row in partitions:\n        # 计算时间衰减系数\n        t = datetime.now() - datetime.strptime(row.action_time, '%Y-%m-%d %H:%M:%S')\n        alpha = 1 / (np.log(t.days + 1) + 1)\n        \n        if row.read_time  == '':\n            read_t = 0\n        else:\n            read_t = int(row.read_time)\n        \n        # 计算阅读时间的行为分数\n        read_score = action_weight['read_middle'] if read_t > 1000 else action_weight['read_min']\n        \n        # 计算各种行为的权重和并乘以时间衰减系数\n        weights = alpha * (row.shared * action_weight['share'] + row.clicked * action_weight['click'] +\n                          row.collected * action_weight['collect'] + read_score)\n        \n        # 更新到user_profilehbase表\n        with pool.connection() as conn:\n            table = conn.table('user_profile')\n            table.put('user:{}'.format(row.user_id).encode(),\n                      {'partial:{}:{}'.format(row.channel_id, row.topic).encode(): json.dumps(\n                          weights).encode()})\n            conn.close()\n\nuser_topic.foreachPartition(compute_user_label_weights)\n```\n在 Hive 中查询用户标签及权重\n```\nhive> select * from user_profile_hbase limit 1;\nOK\nuser:1  {\"birthday\":0.0,\"gender\":null}  {\"18:##\":0.25704484358604845,\"18:&#\":0.25704484358604845,\"18:+++\":0.23934588700996243,\"18:+++++\":0.23934588700996243,\"18:AAA\":0.2747964402379244,\"18:Animal\":0.2747964402379244,\"18:Author\":0.2747964402379244,\"18:BASE\":0.23934588700996243,\"18:BBQ\":0.23934588700996243,\"18:Blueprint\":1.6487786414275463,\"18:Code\":0.23934588700996243,\"18:DIR......\n```\n接下来，要将用户属性信息加入到用户画像中。读取用户基础信息，存储到用户画像表的 basic 列族即可\n```\ndef update_user_info():\n    \"\"\"\n    更新用户画像的属性信息\n    :return:\n    \"\"\"\n    spark.sql(\"use toutiao\")\n    user_basic = spark.sql(\"select user_id, gender, birthday from user_profile\")\n\n    def udapte_user_basic(partition):\n\n        import happybase\n        #  用于读取hbase缓存结果配置\n        pool = happybase.ConnectionPool(size=10, host='172.17.0.134', port=9090)\n        for row in partition:\n            from datetime import date\n            age = 0\n            if row.birthday != 'null':\n                born = datetime.strptime(row.birthday, '%Y-%m-%d')\n                today = date.today()\n                age = today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n\n            with pool.connection() as conn:\n                table = conn.table('user_profile')\n                table.put('user:{}'.format(row.user_id).encode(),\n                          {'basic:gender'.encode(): json.dumps(row.gender).encode()})\n                table.put('user:{}'.format(row.user_id).encode(),\n                          {'basic:birthday'.encode(): json.dumps(age).encode()})\n                conn.close()\n\n    user_basic.foreachPartition(udapte_user_basic)\n```\n到这里，我们的用户画像就计算完成了。\n\n# Apscheduler 定时更新\n定义更新用户画像方法，首先处理用户行为日志，拆分文章主题词，接着计算用户标签的权重，最后再将用户属性信息加入到用户画像中\n```\ndef update_user_profile():\n    \"\"\"\n    定时更新用户画像的逻辑\n    :return:\n    \"\"\"\n    up = UpdateUserProfile()\n    if up.update_user_action_basic():\n        up.update_user_label()\n        up.update_user_info()\n```\n在 Apscheduler 中添加定时更新用户画像任务，设定每隔 2 个小时更新一次\n```\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.executors.pool import ProcessPoolExecutor\n\n# 创建scheduler，多进程执行\nexecutors = {\n    'default': ProcessPoolExecutor(3)\n}\n\nscheduler = BlockingScheduler(executors=executors)\n\n# 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次\nscheduler.add_job(update_article_profile, trigger='interval', hours=1)\n# 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次\nscheduler.add_job(update_user_profile, trigger='interval', hours=2)\n\nscheduler.start()\n```\n另外说一下，在实际场景中，用户画像往往是非常复杂的，下面是电商场景的用户画像，可以了解一下。\n\n![](https://upload-images.jianshu.io/upload_images/12790782-c44e8dbbbe82fd26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 参考\n[https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n[https://pan.baidu.com/s/1SPtttZZK5Nzh5wY7ryaBtg](https://pan.baidu.com/s/1SPtttZZK5Nzh5wY7ryaBtg)（学习资源已保存至网盘）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之5、计算文章相似度","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之5、计算文章相似度/","content":"\n在上篇文章中，我们已经完成了离线文章画像的构建，接下来，我们要为相似文章推荐做准备，那就是计算文章之间的相似度。首先，我们要计算出文章的词向量，然后利用文章的词向量来计算文章的相似度。\n\n# 计算文章词向量\n我们可以通过大量的历史文章数据，训练文章中每个词的词向量，由于文章数据过多，通常是分频道进行词向量训练，即每个频道训练一个词向量模型，我们包括的频道如下所示\n```\nchannel_info = {\n            1: \"html\",\n            2: \"开发者资讯\",\n            3: \"ios\",\n            4: \"c++\",\n            5: \"android\",\n            6: \"css\",\n            7: \"数据库\",\n            8: \"区块链\",\n            9: \"go\",\n            10: \"产品\",\n            11: \"后端\",\n            12: \"linux\",\n            13: \"人工智能\",\n            14: \"php\",\n            15: \"javascript\",\n            16: \"架构\",\n            17: \"前端\",\n            18: \"python\",\n            19: \"java\",\n            20: \"算法\",\n            21: \"面试\",\n            22: \"科技动态\",\n            23: \"js\",\n            24: \"设计\",\n            25: \"数码产品\",\n        }\n```\n接下来，分别对各自频道内的文章进行分词处理，这里先选取 18 号频道内的所有文章，进行分词处理\n```\nspark.sql(\"use article\")\narticle_data = spark.sql(\"select * from article_data where channel_id=18\")\nwords_df = article_data.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])\n\ndef segmentation(partition):\n    import os\n    import re\n    import jieba\n    import jieba.analyse\n    import jieba.posseg as pseg\n    import codecs\n\n    abspath = \"/root/words\"\n\n    # 结巴加载用户词典\n    userDict_path = os.path.join(abspath, \"ITKeywords.txt\")\n    jieba.load_userdict(userDict_path)\n\n    # 停用词文本\n    stopwords_path = os.path.join(abspath, \"stopwords.txt\")\n\n    def get_stopwords_list():\n        \"\"\"返回stopwords列表\"\"\"\n        stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()]\n        return stopwords_list\n\n    # 所有的停用词列表\n    stopwords_list = get_stopwords_list()\n\n    # 分词\n    def cut_sentence(sentence):\n        \"\"\"对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词\"\"\"\n        # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]\n        seg_list = pseg.lcut(sentence)\n        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n        filtered_words_list = []\n        for seg in seg_list:\n            if len(seg.word) <= 1:\n                continue\n            elif seg.flag == \"eng\":\n                if len(seg.word) <= 2:\n                    continue\n                else:\n                    filtered_words_list.append(seg.word)\n            elif seg.flag.startswith(\"n\"):\n                filtered_words_list.append(seg.word)\n            elif seg.flag in [\"x\", \"eng\"]:  # 是自定一个词语或者是英文单词\n                filtered_words_list.append(seg.word)\n        return filtered_words_list\n\n    for row in partition:\n        sentence = re.sub(\"<.*?>\", \"\", row.sentence)    # 替换掉标签数据\n        words = cut_sentence(sentence)\n        yield row.article_id, row.channel_id, words\n```\n`words_df` 结果如下所示，words 为分词后的词语列表\n\n![](https://upload-images.jianshu.io/upload_images/12790782-4cbedc535c6b965c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接着，使用分词后的所有词语，对 Word2Vec 模型进行训练并将模型保存到 HDFS，其中 vectorSize 为词向量的长度，minCount 为词语的最小出现次数，windowSize 为训练窗口的大小，inputCol 为输入的列名，outputCol 为输出的列名\n```\nfrom pyspark.ml.feature import Word2Vec\n\nw2v_model = Word2Vec(vectorSize=100, inputCol='words', outputCol='vector', minCount=3)\nmodel = w2v_model.fit(words_df)\nmodel.save(\"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_18_python.word2vec\")\n```\n加载训练好的 Word2Vec 模型\n```\nfrom pyspark.ml.feature import Word2VecModel\n\nw2v_model = Word2VecModel.load(\"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_18_python.word2vec\")\nvectors = w2v_model.getVectors()\n```\n`vectors` 结果如下所示，其中 vector 是训练后的每个词的 100 维词向量，是 vector 类型格式的，如 [0.2 -0.05 -0.1 ...]\n\n![](https://upload-images.jianshu.io/upload_images/12790782-ee792e4abfe00e75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n这里，我们计算出了所有词语的词向量，接下来，还要得到关键词的词向量，因为我们需要通过关键词的词向量来计算文章的词向量。那么，首先通过读取频道内的文章画像来得到关键词（实际场景应该只读取新增文章画像）\n```\narticle_profile = spark.sql(\"select * from article_profile where channel_id=18\")\n```\n在文章画像表中，关键词和权重是存储在同一列的，我们可以利用 `LATERAL VIEW explode()` 方法，将 map 类型的 keywords 列中的关键词和权重转换成单独的两列数据\n```\narticle_profile.registerTempTable('profile')\nkeyword_weight = spark.sql(\"select article_id, channel_id, keyword, weight from profile LATERAL VIEW explode(keywords) AS keyword, weight\")\n```\n`keyword_weight` 结果如下所示，keyword 为关键词，weight 为对应的权重\n\n![](https://upload-images.jianshu.io/upload_images/12790782-39f3605d616afc31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n这时就可以利用关键词 keyword 列，将文章关键词 `keyword_weight` 与词向量结果 `vectors` 进行内连接，从而得到每个关键词的词向量\n```\nkeywords_vector = keyword_weight.join(vectors, vectors.word==keyword_weight.keyword, 'inner')\n```\n`keywords_vector` 结果如下所示，vector 即对应关键词的 100 维词向量\n\n![](https://upload-images.jianshu.io/upload_images/12790782-8cc8e64b8f642fec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，将文章每个关键词的词向量加入权重信息，这里使每个关键词的词向量 = 关键词的权重 x 关键词的词向量，即 weight_vector = weight x vector，注意这里的 `vector` 为 vector 类型，所以 weight x vector 是权重和向量的每个元素相乘，向量的长度保持不变\n```\ndef compute_vector(row):\n    return row.article_id, row.channel_id, row.keyword, row.weight * row.vector\n\narticle_keyword_vectors = keywords_vector.rdd.map(compute_vector).toDF([\"article_id\", \"channel_id\", \"keyword\", \"weightingVector\"])\n```\n`article_keyword_vectors` 结果如下所示，weightingVector 即为加入权重信息后的关键词的词向量\n\n![](https://upload-images.jianshu.io/upload_images/12790782-cb450dceb634a754.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n再将上面的结果按照 article_id 进行分组，利用 `collect_set()` 方法，将一篇文章内所有关键词的词向量合并为一个列表\n```\narticle_keyword_vectors.registerTempTable('temptable')\narticle_keyword_vectors = spark.sql(\"select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from temptable group by article_id\")\n```\n`article_keyword_vectors` 结果如下所示，vectors 即为文章内所有关键词向量的列表，如 [[0.6 0.2 ...], [0.1 -0.07 ...], ...]\n\n![](https://upload-images.jianshu.io/upload_images/12790782-9e68516da76d0189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，利用上面得出的二维列表，计算每篇文章内所有关键词的词向量的平均值，作为文章的词向量。注意，这里的 `vectors` 是包含多个词向量的列表，词向量列表的平均值等于其中每个词向量的对应元素相加再除以词向量的个数\n```\ndef compute_avg_vectors(row):\n    x = 0\n    for i in row.vectors:\n        x += i\n    # 求平均值\n    return row.article_id, row.channel_id, x / len(row.vectors)\n\narticle_vector = article_keyword_vectors.rdd.map(compute_avg_vectors).toDF(['article_id', 'channel_id', 'vector'])\n```\n`article_vector` 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-a86e19255d39bf47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n此时，`article_vector` 中的 `vector` 列还是 vector 类型，而 Hive 不支持该数据类型，所以需要将 vector 类型转成 array 类型（list）\n```\ndef to_list(row):\n    return row.article_id, row.channel_id, [float(i) for i in row.vector.toArray()]\n\narticle_vector = article_vector.rdd.map(to_list).toDF(['article_id', 'channel_id', 'vector'])\n```\n在 Hive 中创建文章词向量表 article_vector\n```sql\nCREATE TABLE article_vector\n(\n    article_id INT comment \"article_id\",\n    channel_id INT comment \"channel_id\",\n    articlevector ARRAY comment \"keyword\"\n);\n```\n最后，将 18 号频道内的所有文章的词向量存储到 Hive 的文章词向量表 article_vector 中\n```python\narticle_vector.write.insertInto(\"article_vector\")\n```\n这样，我们就计算出了 18 号频道下每篇文章的词向量，在实际场景中，我们还要分别计算出其他所有频道下每篇文章的词向量。\n\n# 计算文章相似度\n前面我们计算出了文章的词向量，接下来就可以根据文章的词向量来计算文章的相似度了。通常我们会有几百万、几千万甚至上亿规模的文章数据，为了优化计算性能，我们可以只计算每个频道内文章之间的相似度，因为通常只有相同频道的文章关联性较高，而不同频道之间的文章通常关联性较低。在每个频道内，我们还可以用聚类或局部敏感哈希对文章进行分桶，将文章相似度的计算限制在更小的范围，只计算相同分类内或相同桶内的文章相似度。\n- 聚类（Clustering），对每个频道内的文章进行聚类，可以使用 KMeans 算法，需要提前设定好类别个数 K，聚类算法的时间复杂度并不小，也可以使用一些优化的聚类算法，比如二分聚类、层次聚类等。但通常聚类算法也比较耗时，所以通常被使用更多的是局部敏感哈希。\n\nSpark 的 BisectingKMeans 模型训练代码示例\n```\nfrom pyspark.ml.clustering import BisectingKMeans\n\nbkmeans = BisectingKMeans(k=100, minDivisibleClusterSize=50, featuresCol=\"articlevector\", predictionCol='group')\nbkmeans_model = bkmeans.fit(article_vector)\nbkmeans_model.save(\"hdfs://hadoop-master:9000/headlines/models/articleBisKmeans/channel_%d_%s.bkmeans\" % (channel_id, channel))\n```\n- 局部敏感哈希 LSH（Locality Sensitive Hashing），LSH 算法是基于一个假设，如果两个文本在原有的数据空间是相似的，那么经过哈希函数转换以后，它们仍然具有很高的相似度，即越相似的文本在哈希之后，落到相同的桶内的概率就越高。所以，我们只需要将目标文章进行哈希映射并得到其桶号，然后取出该桶内的所有文章，再进行线性匹配即可查找到与目标文章相邻的文章。其实 LSH 并不能保证一定能够查找到与目标文章最相邻的文章，而是在减少需要匹配的文章个数的同时，保证查找到最近邻的文章的概率很大。\n\n下面我们将使用 LSH 模型来计算文章相似度，首先，读取 18 号频道内所有文章的 ID 和词向量作为训练集\n```\narticle_vector = spark.sql(\"select article_id, articlevector from article_vector where channel_id=18\")\ntrain = articlevector.select(['article_id', 'articlevector'])\n```\n文章词向量表中的词向量是被存储为 array 类型的，我们利用 Spark 的 `Vectors.dense()` 方法，将 array 类型（list）转为 vector 类型\n```\nfrom pyspark.ml.linalg import Vectors\n\ndef list_to_vector(row):\n    return row.article_id, Vectors.dense(row.articlevector)\n\ntrain = train.rdd.map(list_to_vector).toDF(['article_id', 'articlevector'])\n```\n使用训练集 `train` 对 Spark 的 `BucketedRandomProjectionLSH` 模型进行训练，其中 inputCol 为输入特征列，outputCol 为输出特征列，numHashTables 为哈希表数量，bucketLength 为桶的数量，数量越多，相同数据进入到同一个桶的概率就越高\n```\nfrom pyspark.ml.feature import BucketedRandomProjectionLSH\n\nbrp = BucketedRandomProjectionLSH(inputCol='articlevector', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)\nmodel = brp.fit(train)\n```\n训练好模型后，调用 `approxSimilarityJoin()` 方法即可计算数据之间的相似度，如 `model.approxSimilarityJoin(df1, df2, 2.0, distCol='EuclideanDistance')` 就是利用欧几里得距离作为相似度，计算在 df1 与 df2 每条数据的相似度，这里我们计算训练集中所有文章之间的相似度\n```\nsimilar = model.approxSimilarityJoin(train, train, 2.0, distCol='EuclideanDistance')\n```\n`similar` 结果如下所示，EuclideanDistance 就是两篇文章的欧几里得距离，即相似度\n\n![](https://upload-images.jianshu.io/upload_images/12790782-8e98744a01f7d121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n在后面的推荐流程中，会经常查询文章相似度，所以出于性能考虑，我们选择将文章相似度结果存储到  Hbase 中。首先创建文章相似度表\n```\ncreate 'article_similar', 'similar'\n```\n然后存储文章相似度结果\n```\ndef save_hbase(partition):\n    import happybase\n    pool = happybase.ConnectionPool(size=3, host='hadoop-master')\n\n    with pool.connection() as conn:\n        # 建立表连接\n        table = conn.table('article_similar')\n        for row in partition:\n            if row.datasetA.article_id != row.datasetB.article_id:\n                table.put(str(row.datasetA.article_id).encode(), {\"similar:{}\".format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})\n                \n        # 手动关闭所有的连接\n        conn.close()\n\nsimilar.foreachPartition(save_hbase)\n```\n\n# Apscheduler 定时更新\n将文章相似度计算加入到文章画像更新方法中，首先合并最近一个小时的文章完整信息，接着计算 TF-IDF 和 TextRank 权重，并根据 TF-IDF 和 TextRank 权重计算得出关键词和主题词，最后计算文章的词向量及文章的相似度\n```\ndef update_article_profile():\n    \"\"\"\n    定时更新文章画像及文章相似度\n    :return:\n    \"\"\"\n    ua = UpdateArticle()\n    sentence_df = ua.merge_article_data()\n    if sentence_df.rdd.collect():\n        textrank_keywords_df, keywordsIndex = ua.generate_article_label()\n        article_profile = ua.get_article_profile(textrank_keywords_df, keywordsIndex)\n        ua.compute_article_similar(article_profile)\n```\n\n> 原文出自（已授权）：https://www.jianshu.com/u/ac833cc5146e\n\n\n# 参考\n- [https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n- [https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘，提取码 EakP）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之4、构建离线文章画像","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之4、构建离线文章画像/","content":"\n在上述步骤中，我们已经将业务数据和用户行为数据同步到了推荐系统数据库当中，接下来，我们就要对文章数据和用户数据进行分析，构建文章画像和用户画像，本文我们主要讲解如何构建文章画像。文章画像由关键词和主题词组成，我们将每个词的 TF-IDF 权重和 TextRank 权重的乘积作为关键词权重，筛选出权重最高的 K 个词作为关键词，将 TextRank 权重最高的 K 个词与 TF-IDF 权重最高的 K 个词的共现词作为主题词。\n\n首先，在 Hive 中创建文章数据库 article 及相关表，其中表 article_data 用于存储完整的文章信息，表 idf_keywords_values 用于存储关键词和索引信息，表 tfidf_keywords_values 用于存储关键词和 TF-IDF 权重信息，表 textrank_keywords_values 用于存储关键词和 TextRank 权重信息，表 article_profile 用于存储文章画像信息。\n```sql\n-- 创建文章数据库\ncreate database if not exists article comment \"artcile information\" location '/user/hive/warehouse/article.db/';\n-- 创建文章信息表\nCREATE TABLE article_data\n(\n    article_id   BIGINT comment \"article_id\",\n    channel_id   INT comment \"channel_id\",\n    channel_name STRING comment \"channel_name\",\n    title        STRING comment \"title\",\n    content      STRING comment \"content\",\n    sentence     STRING comment \"sentence\"\n)\n    COMMENT \"toutiao news_channel\"\n    LOCATION '/user/hive/warehouse/article.db/article_data';\n-- 创建关键词索引信息表\nCREATE TABLE idf_keywords_values\n(\n    keyword STRING comment \"article_id\",\n    idf     DOUBLE comment \"idf\",\n    index   INT comment \"index\"\n);\n-- 创建关键词TF-IDF权重信息表\nCREATE TABLE tfidf_keywords_values\n(\n    article_id INT comment \"article_id\",\n    channel_id INT comment \"channel_id\",\n    keyword    STRING comment \"keyword\",\n    tfidf      DOUBLE comment \"tfidf\"\n);\n-- 创建关键词TextRank权重信息表\nCREATE TABLE textrank_keywords_values\n(\n    article_id INT comment \"article_id\",\n    channel_id INT comment \"channel_id\",\n    keyword    STRING comment \"keyword\",\n    textrank   DOUBLE comment \"textrank\"\n);\n-- 创建文章画像信息表\nCREATE TABLE article_profile\n(\n    article_id INT comment \"article_id\",\n    channel_id INT comment \"channel_id\",\n    keyword    map comment \"keyword\",\n    topics     array comment \"topics\"\n); \n```\n# 计算文章完整信息\n为了计算文章画像，需要将文章信息表（news_article_basic）、文章内容表（news_article_content）及频道表（news_channel）进行合并，从而得到完整的文章信息，通常使用 Spark SQL 进行处理。\n\n通过关联表 news_article_basic, news_article_content 和 news_channel 获得文章完整信息，包括 article_id, channel_id, channel_name, title, content，这里获取一个小时内的文章信息。\n```python\nspark.sql(\"use toutiao\")\n_now = datetime.today().replace(minute=0, second=0, microsecond=0)\nstart = datetime.strftime(_now + timedelta(days=0, hours=-1, minutes=0), \"%Y-%m-%d %H:%M:%S\")\nend = datetime.strftime(_now, \"%Y-%m-%d %H:%M:%S\")\nbasic_content = spark.sql(\n            \"select a.article_id, a.channel_id, a.title, b.content from news_article_basic a \"\n            \"inner join news_article_content b on a.article_id=b.article_id where a.review_time >= '{}' \"\n            \"and a.review_time < '{}' and a.status = 2\".format(start, end))\nbasic_content.registerTempTable(\"temp_article\")\nchannel_basic_content = spark.sql(\n            \"select t.*, n.channel_name from temp_article t left join news_channel n on t.channel_id=n.channel_id\")\n```\n`channel_basic_content` 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-96f4fc864f469275.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n利用 `concat_ws()` 方法，将 channel_name, title, content 这 3 列数据合并为一列 sentence，并将结果写入文章完整信息表 article_data 中\n```\nimport pyspark.sql.functions as F\n\nspark.sql(\"use article\")\nsentence_df = channel_basic_content.select(\"article_id\", \"channel_id\", \"channel_name\", \"title\", \"content\", \\\n                                           F.concat_ws(\n                                               \",\",\n                                               channel_basic_content.channel_name,\n                                               channel_basic_content.title,\n                                               channel_basic_content.content\n                                           ).alias(\"sentence\")\n                                           )\ndel basic_content\ndel channel_basic_content\ngc.collect() # 垃圾回收\n\nsentence_df.write.insertInto(\"article_data\")\n```\n`sentence_df` 结果如下所示，文章完整信息包括 article_id, channel_id, channel_name, title, content, sentence，其中 sentence 为 channel_name, title, content 合并而成的长文本内容\n\n![](https://upload-images.jianshu.io/upload_images/12790782-b87fe4016480095b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 计算 TF-IDF\n前面我们得到了文章的完整内容信息，接下来，我们要先对文章进行分词，然后计算每个词的 TF-IDF 权重，将 TF-IDF 权重最高的 K 个词作为文章的关键词。首先，读取文章信息\n```\nspark.sql(\"use article\")\narticle_dataframe = spark.sql(\"select * from article_data\")\n```\n利用 `mapPartitions()` 方法，对每篇文章进行分词，这里使用的是 `jieba` 分词器\n```\nwords_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([\"article_id\", \"channel_id\", \"words\"])\n\ndef segmentation(partition):\n    import os\n    import re\n    import jieba\n    import jieba.analyse\n    import jieba.posseg as pseg\n    import codecs\n\n    abspath = \"/root/words\"\n\n    # 结巴加载用户词典\n    userdict_path = os.path.join(abspath, \"ITKeywords.txt\")\n    jieba.load_userdict(userdict_path)\n\n    # 停用词文本\n    stopwords_path = os.path.join(abspath, \"stopwords.txt\")\n\n    def get_stopwords_list():\n        \"\"\"返回stopwords列表\"\"\"\n        stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()]\n        return stopwords_list\n\n    # 所有的停用词列表\n    stopwords_list = get_stopwords_list()\n\n    # 分词\n    def cut_sentence(sentence):\n        \"\"\"对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词\"\"\"\n        seg_list = pseg.lcut(sentence)\n        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n        filtered_words_list = []\n        for seg in seg_list:\n            if len(seg.word) <= 1:\n                continue\n            elif seg.flag == \"eng\":\n                if len(seg.word) <= 2:\n                    continue\n                else:\n                    filtered_words_list.append(seg.word)\n            elif seg.flag.startswith(\"n\"):\n                filtered_words_list.append(seg.word)\n            elif seg.flag in [\"x\", \"eng\"]:  # 是自定一个词语或者是英文单词\n                filtered_words_list.append(seg.word)\n        return filtered_words_list\n\n    for row in partition:\n        sentence = re.sub(\"<.*?>\", \"\", row.sentence)    # 替换掉标签数据\n        words = cut_sentence(sentence)\n        yield row.article_id, row.channel_id, words\n```\n`words_df` 结果如下所示，words 为将 sentence 分词后的单词列表\n\n![](https://upload-images.jianshu.io/upload_images/12790782-55862171336abc52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n使用分词结果对词频统计模型（CountVectorizer）进行词频统计训练，并将 CountVectorizer 模型保存到 HDFS 中\n```\nfrom pyspark.ml.feature import CountVectorizer\n# vocabSize是总词汇的大小，minDF是文本中出现的最少次数\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"countFeatures\", vocabSize=200*10000, minDF=1.0)\n# 训练词频统计模型\ncv_model = cv.fit(words_df)\ncv_model.write().overwrite().save(\"hdfs://hadoop-master:9000/headlines/models/CV.model\")\n```\n加载 CountVectorizer 模型，计算词频向量\n```\nfrom pyspark.ml.feature import CountVectorizerModel\ncv_model = CountVectorizerModel.load(\"hdfs://hadoop-master:9000/headlines/models/CV.model\")\n# 得出词频向量结果\ncv_result = cv_model.transform(words_df)\n```\n`cv_result` 结果如下所示，countFeatures 为词频向量，如 (986, [2, 4, ...], [3.0, 5.0, ...]) 表示总词汇的大小为 986 个，索引为 2 和 4 的词在某篇文章中分别出现 3 次和 5 次，\n\n![](https://upload-images.jianshu.io/upload_images/12790782-760b54a16a9e780b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n得到词频向量后，再利用逆文本频率模型（ IDF ），根据词频向量进行 IDF 统计训练，并将 IDF 模型保存到 HDFS\n```\nfrom pyspark.ml.feature import IDF\nidf = IDF(inputCol=\"countFeatures\", outputCol=\"idfFeatures\")\nidf_model = idf.fit(cv_result)\nidf_model.write().overwrite().save(\"hdfs://hadoop-master:9000/headlines/models/IDF.model\")\n```\n我们已经分别计算了文章信息中每个词的 TF 和 IDF，这时就可以加载 CountVectorizer 模型和 IDF 模型，计算每个词的 TF-IDF\n```\nfrom pyspark.ml.feature import CountVectorizerModel\ncv_model = CountVectorizerModel.load(\"hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model\")\nfrom pyspark.ml.feature import IDFModel\nidf_model = IDFModel.load(\"hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model\")\n\ncv_result = cv_model.transform(words_df)\ntfidf_result = idf_model.transform(cv_result)\n```\n`tfidf_result` 结果如下所示，idfFeatures 为 TF-IDF 权重向量，如 (986, [2, 4, ...], [0.3, 0.5, ...]) 表示总词汇的大小为 986 个，索引为 2 和 4 的词在某篇文章中的 TF-IDF 值分别为 0.3 和 0.5\n\n![](https://upload-images.jianshu.io/upload_images/12790782-44cdf662e1e2adb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n对文章的每个词都根据 TF-IDF 权重排序，保留 TF-IDF 权重最高的前 K 个词作为关键词\n```\ndef sort_by_tfidf(partition):\n    TOPK = 20\n    for row in partition:\n        # 找到索引与IDF值并进行排序\n        _dict = list(zip(row.idfFeatures.indices, row.idfFeatures.values))\n        _dict = sorted(_dict, key=lambda x: x[1], reverse=True)\n        result = _dict[:TOPK]\n        for word_index, tfidf in result:\n            yield row.article_id, row.channel_id, int(word_index), round(float(tfidf), 4)\n\nkeywords_by_tfidf = tfidf_result.rdd.mapPartitions(sort_by_tfidf).toDF([\"article_id\", \"channel_id\", \"index\", \"weights\"])\n```\n`keywords_by_tfidf` 结果如下所示，每篇文章保留了权重最高的 K 个单词，index 为单词索引，weights 为对应单词的 TF-IDF 权重\n\n![](https://upload-images.jianshu.io/upload_images/12790782-566af87857d1c5bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n接下来，我们需要知道每个词的对应的 TF-IDF 值，可以利用 `zip()` 方法，将所有文章中的每个词及其 TF-IDF 权重组成字典，再加入索引列，由此得到每个词对应的 TF-IDF 值，将该结果保存到 idf_keywords_values 表\n```\nkeywords_list_with_idf = list(zip(cv_model.vocabulary, idf_model.idf.toArray()))\ndef append_index(data):\n    for index in range(len(data)):\n        data[index] = list(data[index]) # 将元组转为list\n        data[index].append(index)       # 加入索引 \n        data[index][1] = float(data[index][1])\nappend_index(keywords_list_with_idf)\nsc = spark.sparkContext\nrdd = sc.parallelize(keywords_list_with_idf) # 创建rdd\nidf_keywords = rdd.toDF([\"keywords\", \"idf\", \"index\"])\n\nidf_keywords.write.insertInto('idf_keywords_values')\n```\n`idf_keywords` 结果如下所示，包含了所有单词的名称、TF-IDF 权重及索引\n\n![](https://upload-images.jianshu.io/upload_images/12790782-bbf3babed9a9ccfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n通过 index 列，将 `keywords_by_tfidf` 与表 idf_keywords_values 进行连接，选取文章 ID、频道 ID、关键词、TF-IDF 权重作为结果，并保存到 TF-IDF 关键词表 tfidf_keywords_values\n```\nkeywords_index = spark.sql(\"select keyword, index idx from idf_keywords_values\")\nkeywords_result = keywords_by_tfidf.join(keywords_index, keywords_index.idx == keywords_by_tfidf.index).select([\"article_id\", \"channel_id\", \"keyword\", \"weights\"])\nkeywords_result.write.insertInto(\"tfidf_keywords_values\")\n```\n`keywords_result` 结果如下所示，keyword 和 weights 即为所有词在每个文章中的 TF-IDF 权重\n\n![](https://upload-images.jianshu.io/upload_images/12790782-7c39214d62083c30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 计算 TextRank\n前面我们已经计算好了每个词的 TF-IDF 权重，为了计算关键词，还需要得到每个词的 TextRank 权重，接下来，还是先读取文章完整信息\n```\nspark.sql(\"use article\")\narticle_dataframe = spark.sql(\"select * from article_data\")\n```\n对文章 sentence 列的内容进行分词，计算每个词的 TextRank 权重，并将每篇文章 TextRank 权重最高的 K 个词保存到 TextRank 结果表 textrank_keywords_values\n```\ntextrank_keywords_df = article_dataframe.rdd.mapPartitions(textrank).toDF([\"article_id\", \"channel_id\", \"keyword\", \"textrank\"])\n\ntextrank_keywords_df.write.insertInto(\"textrank_keywords_values\")\n```\nTextRank 计算细节：分词后只保留指定词性的词，滑动截取长度为 K 的窗口，计算窗口内的各个词的投票数\n```\ndef textrank(partition):\n    import os\n    import jieba\n    import jieba.analyse\n    import jieba.posseg as pseg\n    import codecs\n\n    abspath = \"/root/words\"\n\n    # 结巴加载用户词典\n    userDict_path = os.path.join(abspath, \"ITKeywords.txt\")\n    jieba.load_userdict(userDict_path)\n\n    # 停用词文本\n    stopwords_path = os.path.join(abspath, \"stopwords.txt\")\n\n    def get_stopwords_list():\n        \"\"\"返回stopwords列表\"\"\"\n        stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()]\n        return stopwords_list\n\n    # 所有的停用词列表\n    stopwords_list = get_stopwords_list()\n\n    class TextRank(jieba.analyse.TextRank):\n        def __init__(self, window=20, word_min_len=2):\n            super(TextRank, self).__init__()\n            self.span = window  # 窗口大小\n            self.word_min_len = word_min_len  # 单词的最小长度\n            # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac\n            self.pos_filt = frozenset(\n                ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', \"nw\", \"nz\", \"PER\", \"LOC\", \"ORG\"))\n\n        def pairfilter(self, wp):\n            \"\"\"过滤条件，返回True或者False\"\"\"\n\n            if wp.flag == \"eng\":\n                if len(wp.word) <= 2:\n                    return False\n\n            if wp.flag in self.pos_filt and len(wp.word.strip()) >= self.word_min_len \\\n                    and wp.word.lower() not in stopwords_list:\n                return True\n    # TextRank过滤窗口大小为5，单词最小为2\n    textrank_model = TextRank(window=5, word_min_len=2)\n    allowPOS = ('n', \"x\", 'eng', 'nr', 'ns', 'nt', \"nw\", \"nz\", \"c\")\n\n    for row in partition:\n        tags = textrank_model.textrank(row.sentence, topK=20, withWeight=True, allowPOS=allowPOS, withFlag=False)\n        for tag in tags:\n            yield row.article_id, row.channel_id, tag[0], tag[1]\n```\n`textrank_keywords_df` 结果如下所示，keyword 和 textrank 即为每个单词在文章中的 TextRank 权重\n\n![](https://upload-images.jianshu.io/upload_images/12790782-4453fb49ff5e39e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n# 画像计算\n我们计算出 TF-IDF 和 TextRank 后，就可以计算关键词和主题词了，读取 TF-IDF 权重\n```\nidf_keywords_values = oa.spark.sql(\"select * from idf_keywords_values\")\n```\n读取 TextRank 权重\n```\ntextrank_keywords_values = oa.spark.sql(\"select * from textrank_keywords_values\")\n```\n通过 `keyword` 关联 TF-IDF 权重和 TextRank 权重\n```\nkeywords_res = textrank_keywords_values.join(idf_keywords_values, on=['keyword'], how='left')\n```\n计算 TF-IDF 权重和 TextRank 权重的乘积作为关键词权重\n```\nkeywords_weights = keywords_res.withColumn('weights', keywords_res.textrank * keywords_res.idf).select([\"article_id\", \"channel_id\", \"keyword\", \"weights\"])\n```\n`keywords_weights` 结果如下所示\n\n![](https://upload-images.jianshu.io/upload_images/12790782-bd60d34728a7aff8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n这里，我们需要将相同文章的词都合并到一条记录中，将 `keywords_weights` 按照 article_id 分组，并利用 `collect_list()` 方法，分别将关键词和权重合并为列表\n```\nkeywords_weights.registerTempTable('temp')\n\nkeywords_weights = spark.sql(\"select article_id, min(channel_id) channel_id, collect_list(keyword) keywords, collect_list(weights) weights from temp group by article_id\")`\n```\n`keywords_weights` 结果如下所示，keywords 为每篇文章的关键词列表，weights 为关键词对应的权重列表\n\n![](https://upload-images.jianshu.io/upload_images/12790782-562748d12a395121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n为了方便查询，我们需要将关键词和权重合并为一列，并存储为 map 类型，这里利用 `dict()` 和 `zip()` 方法，将每个关键词及其权重组合成字典\n```\ndef to_map(row):\n    return row.article_id, row.channel_id, dict(zip(row.keywords, row.weights))\n\narticle_keywords = keywords_weights.rdd.map(to_map).toDF(['article_id', 'channel_id', 'keywords'])\n```\n`article_keywords` 结果如下所示，keywords 即为每篇文章的关键词和对应权重\n\n![](https://upload-images.jianshu.io/upload_images/12790782-5a02c70d3f93eb78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n前面我们计算完了关键词，接下来我们将 TF-IDF 和 TextRank 的共现词作为主题词，将 TF-IDF 权重表 tfidf_keywords_values 和 TextRank 权重表 textrank_keywords_values 进行关联，并利用 `collect_set()` 对结果进行去重，即可得到 TF-IDF 和 TextRank 的共现词，即主题词\n```\ntopic_sql = \"\"\"\n                select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t\n                inner join \n                textrank_keywords_values r\n                where t.keyword=r.keyword\n                group by article_id2\n                \"\"\"\narticle_topics = spark.sql(topic_sql)\n```\n`article_topics` 结果如下所示，topics 即为每篇文章的主题词列表\n\n![](https://upload-images.jianshu.io/upload_images/12790782-ca3825c269a0581b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n最后，将主题词结果和关键词结果合并，即为文章画像，保存到表 `article_profile`\n```\narticle_profile = article_keywords.join(article_topics, article_keywords.article_id==article_topics.article_id2).select([\"article_id\", \"channel_id\", \"keywords\", \"topics\"])\n\narticle_profile.write.insertInto(\"article_profile\")\n```\n文章画像数据查询测试\n```\nhive> select * from article_profile limit 1;\nOK\n26      17      {\"策略\":0.3973770571351729,\"jpg\":0.9806348975390871,\"用户\":1.2794959063944176,\"strong\":1.6488457985625076,\"文件\":0.28144603583387057,\"逻辑\":0.45256526469610714,\"形式\":0.4123994242601279,\"全自\":0.9594604850547191,\"h2\":0.6244481634710125,\"版本\":0.44280276959510817,\"Adobe\":0.8553618185108718,\"安装\":0.8305037437573172,\"检查更新\":1.8088946300014435,\"产品\":0.774842382276899,\"下载页\":1.4256311032544344,\"过程\":0.19827163395829256,\"json\":0.6423301791599972,\"方式\":0.582762869780791,\"退出应用\":1.2338671268242603,\"Setup\":1.004399549339134}   [\"Electron\",\"全自动\",\"产品\",\"版本号\",\"安装包\",\"检查更新\",\"方案\",\"版本\",\"退出应用\",\"逻辑\",\"安装过程\",\"方式\",\"定性\",\"新版本\",\"Setup\",\"静默\",\"用户\"]\nTime taken: 0.322 seconds, Fetched: 1 row(s)\n```\n\n# Apscheduler 定时更新\n定义离线更新文章画像的方法，首先合并最近一个小时的文章信息，接着计算每个词的 TF-IDF 和 TextRank 权重，并根据 TF-IDF 和 TextRank 权重计算得出文章关键词和主题词，最后将文章画像信息保存到 Hive\n```\ndef update_article_profile():\n    \"\"\"\n    定时更新文章画像\n    :return:\n    \"\"\"\n    ua = UpdateArticle()\n    # 合并文章信息\n    sentence_df = ua.merge_article_data()\n    if sentence_df.rdd.collect():\n        textrank_keywords_df, keywordsIndex = ua.generate_article_label()\n        ua.get_article_profile(textrank_keywords_df, keywordsIndex)\n```\n利用 Apscheduler 添加定时更新文章画像任务，设定每隔 1 个小时更新一次\n```\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.executors.pool import ProcessPoolExecutor\n\n# 创建scheduler,多进程执行\nexecutors = {\n    'default': ProcessPoolExecutor(3)\n}\n\nscheduler = BlockingScheduler(executors=executors)\n\n# 添加一个定时更新文章画像的任务,每隔1个小时运行一次\nscheduler.add_job(update_article_profile, trigger='interval', hours=1)\n\nscheduler.start()\n```\n利用 Supervisor 进行进程管理，配置文件如下\n```shell\n[program:offline]\nenvironment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python\ncommand=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py\ndirectory=/root/toutiao_project/scheduler\nuser=root\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/root/logs/offlinesuper.log\nloglevel=info\nstopsignal=KILL\nstopasgroup=true\nkillasgroup=true\n```\n\n> 原文出自（已授权）：https://www.jianshu.com/u/ac833cc5146e\n\n# 参考\n- [https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n- [https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘，提取码 EakP）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之3、收集用户行为数据","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之3、收集用户行为数据/","content":"\n在上一篇文章中，我们完成了业务数据的同步，在推荐系统中另一个必不可少的数据就是用户行为数据，可以说用户行为数据是推荐系统的基石，巧妇难为无米之炊，所以接下来，我们就要将用户的行为数据同步到推荐系统数据库中。\n\n在文章推荐系统中，用户行为包括曝光、点击、停留、收藏、分享等，所以这里我们定义的用户行为数据的字段包括：发生时间（actionTime）、停留时间（readTime）、频道 ID（channelId）、事件名称（action）、用户 ID（userId）、文章 ID（articleId）以及算法 ID（algorithmCombine），这里采用 json 格式，如下所示\n```\n# 曝光的参数\n{\"actionTime\":\"2019-04-10 18:15:35\",\"readTime\":\"\",\"channelId\":0,\"param\":{\"action\": \"exposure\", \"userId\": \"2\", \"articleId\": \"[18577, 14299]\", \"algorithmCombine\": \"C2\"}}\n\n# 对文章触发行为的参数\n{\"actionTime\":\"2019-04-10 18:15:36\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"18577\", \"algorithmCombine\": \"C2\"}}\n{\"actionTime\":\"2019-04-10 18:15:38\",\"readTime\":\"1621\",\"channelId\":18,\"param\":{\"action\": \"read\", \"userId\": \"2\", \"articleId\": \"18577\", \"algorithmCombine\": \"C2\"}}\n{\"actionTime\":\"2019-04-10 18:15:39\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"1\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}}\n{\"actionTime\":\"2019-04-10 18:15:39\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}}\n{\"actionTime\":\"2019-04-10 18:15:41\",\"readTime\":\"914\",\"channelId\":18,\"param\":{\"action\": \"read\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}}\n{\"actionTime\":\"2019-04-10 18:15:47\",\"readTime\":\"7256\",\"channelId\":18,\"param\":{\"action\": \"read\", \"userId\": \"1\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}}\n```\n# 用户离线行为数据\n由于用户行为数据规模庞大，通常是每天更新一次，以供离线计算使用。首先，在 Hive 中创建用户行为数据库 profile 及用户行为表 user_action，设置按照日期进行分区，并匹配 json 格式\n```sql\n-- 创建用户行为数据库\ncreate database if not exists profile comment \"use action\" location '/user/hive/warehouse/profile.db/';\n-- 创建用户行为信息表\ncreate table user_action\n(\n    actionTime STRING comment \"user actions time\",\n    readTime   STRING comment \"user reading time\",\n    channelId  INT comment \"article channel id\",\n    param map comment \"action parameter\"\n)\n    COMMENT \"user primitive action\"\n    PARTITIONED BY (dt STRING) # 按照日期分区\n    ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' # 匹配json格式\n    LOCATION '/user/hive/warehouse/profile.db/user_action';\n```\n通常用户行为数据被保存在应用服务器的日志文件中，我们可以利用 Flume 监听应用服务器上的日志文件，将用户行为数据收集到 Hive 的 user_action 表对应的 HDFS 目录中，Flume 配置如下\n```\na1.sources = s1\na1.sinks = k1\na1.channels = c1\n\na1.sources.s1.channels= c1\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /root/logs/userClick.log\na1.sources.s1.interceptors=i1 i2\na1.sources.s1.interceptors.i1.type=regex_filter\na1.sources.s1.interceptors.i1.regex=\\\\{.*\\\\}\na1.sources.s1.interceptors.i2.type=timestamp\n\n# c1\na1.channels.c1.type=memory\na1.channels.c1.capacity=30000\na1.channels.c1.transactionCapacity=1000\n\n# k1\na1.sinks.k1.type=hdfs\na1.sinks.k1.channel=c1\na1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d\na1.sinks.k1.hdfs.useLocalTimeStamp = true\na1.sinks.k1.hdfs.fileType=DataStream\na1.sinks.k1.hdfs.writeFormat=Text\na1.sinks.k1.hdfs.rollInterval=0\na1.sinks.k1.hdfs.rollSize=10240\na1.sinks.k1.hdfs.rollCount=0\na1.sinks.k1.hdfs.idleTimeout=60\n```\n编写 Flume 启动脚本 collect_click.sh\n```\n#!/usr/bin/env bash\n\nexport JAVA_HOME=/root/bigdata/jdk\nexport HADOOP_HOME=/root/bigdata/hadoop\nexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin\n\n/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1\n```\nFlume 自动生成目录后，需要手动关联 Hive 分区后才能加载到数据\n```sql\nalter table user_action add partition (dt='2019-11-11') location \"/user/hive/warehouse/profile.db/user_action/2011-11-11/\"\n```\n# 用户实时行为数据\n为了提高推荐的实时性，我们也需要收集用户的实时行为数据，以供在线计算使用。这里利用 Flume 将日志收集到 Kafka，在线计算任务可以从 Kafka 读取用户实时行为数据。首先，开启 zookeeper，以守护进程运行\n```\n/root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties\n```\n开启 Kafka\n```shell\n/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties\n\n# 开启消息生产者\n/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.19092 --sync --topic click-trace\n# 开启消费者\n/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace\n```\n修改 Flume 的日志收集配置文件，添加 c2 和 k2 ，将日志数据收集到 Kafka\n```\na1.sources = s1\na1.sinks = k1 k2\na1.channels = c1 c2\n\na1.sources.s1.channels= c1 c2\na1.sources.s1.type = exec\na1.sources.s1.command = tail -F /root/logs/userClick.log\na1.sources.s1.interceptors=i1 i2\na1.sources.s1.interceptors.i1.type=regex_filter\na1.sources.s1.interceptors.i1.regex=\\\\{.*\\\\}\na1.sources.s1.interceptors.i2.type=timestamp\n\n# c1\na1.channels.c1.type=memory\na1.channels.c1.capacity=30000\na1.channels.c1.transactionCapacity=1000\n\n# c2\na1.channels.c2.type=memory\na1.channels.c2.capacity=30000\na1.channels.c2.transactionCapacity=1000\n\n# k1\na1.sinks.k1.type=hdfs\na1.sinks.k1.channel=c1\na1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d\na1.sinks.k1.hdfs.useLocalTimeStamp = true\na1.sinks.k1.hdfs.fileType=DataStream\na1.sinks.k1.hdfs.writeFormat=Text\na1.sinks.k1.hdfs.rollInterval=0\na1.sinks.k1.hdfs.rollSize=10240\na1.sinks.k1.hdfs.rollCount=0\na1.sinks.k1.hdfs.idleTimeout=60\n\n# k2\na1.sinks.k2.channel=c2\na1.sinks.k2.type=org.apache.flume.supervisorctl\n我们可以利用supervisorctl来管理supervisor。sink.kafka.KafkaSink\na1.sinks.k2.kafka.bootstrap.servers=192.168.19.137:9092\na1.sinks.k2.kafka.topic=click-trace\na1.sinks.k2.kafka.batchSize=20\na1.sinks.k2.kafka.producer.requiredAcks=1\n```\n编写 Kafka 启动脚本 start_kafka.sh\n```\n#!/usr/bin/env bash\n# 启动zookeeper\n/root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties\n# 启动kafka\n/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties\n# 增加topic\n/root/bigdata/kafka/bin/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic click-trace --partitions 1\n```\n# 进程管理\n我们这里使用 Supervisor 进行进程管理，当进程异常时可以自动重启，Flume 进程配置如下\n```shell\n[program:collect-click]\ncommand=/bin/bash /root/toutiao_project/scripts/collect_click.sh\nuser=root\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/root/logs/collect.log\nloglevel=info\nstopsignal=KILL\nstopasgroup=true\nkillasgroup=true\n```\nKafka 进程配置如下\n```shell\n[program:kafka]\ncommand=/bin/bash /root/toutiao_project/scripts/start_kafka.sh\nuser=root\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/root/logs/kafka.log\nloglevel=info\nstopsignal=KILL\nstopasgroup=true\nkillasgroup=true\n```\n启动 Supervisor\n```\nsupervisord -c /etc/supervisord.conf\n```\n启动 Kafka 消费者，并在应用服务器日志文件中写入日志数据，Kafka 消费者即可收集到实时行为数据\n```shell\n# 启动Kafka消费者\n/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace\n\n# 写入日志数据\necho {\\\"actionTime\\\":\\\"2019-04-10 21:04:39\\\",\\\"readTime\\\":\\\"\\\",\\\"channelId\\\":18,\\\"param\\\":{\\\"action\\\": \\\"click\\\", \\\"userId\\\": \\\"2\\\", \\\"articleId\\\": \\\"14299\\\", \\\"algorithmCombine\\\": \\\"C2\\\"}} >> userClick.log\n\n# 消费者接收到日志数据\n{\"actionTime\":\"2019-04-10 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}}\n```\nSupervisor 常用命令如下\n```\nsupervisorctl\n\n> status              # 查看程序状态\n> start apscheduler   # 启动apscheduler单一程序\n> stop toutiao:*      # 关闭toutiao组程序\n> start toutiao:*     # 启动toutiao组程序\n> restart toutiao:*   # 重启toutiao组程序\n> update              # 重启配置文件修改过的程序\n```\n\n> 原文出自（已授权）：https://www.jianshu.com/u/ac833cc5146e\n\n# 参考\n- [https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n- [https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘，提取码 EakP）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之2、同步业务数据.md","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之2、同步业务数据/","content":"\n在推荐系统架构中，推荐系统的数据库和业务系统的数据库是分离的，这样才能有效避免推荐系统的数据读写、计算等对业务系统的影响，所以同步业务数据往往也是推荐系统要做的第一件事情。通常业务数据是存储在关系型数据库中，比如 MySQL，而推荐系统通常需要使用大数据平台，比如 Hadoop，我们可以利用 Sqoop 将 MySQL 中的业务数据同步到 Hive 中，在我们的文章推荐系统中，需要同步的数据包括：\n\n- 用户信息表（user_profile）：包括 user_id | gender | birthday   | real_name | create_time | update_time   | register_media_time | id_number | id_card_front | id_card_back | id_card_handheld | area | company | career 等字段。\n- 用户基础信息表（user_basic）：包括 user_id | mobile      | password | user_name       | profile_photo                | last_login          | is_media | article_count | following_count | fans_count | like_count | read_count | introduction    | certificate | is_verified | account | email       | status 等字段。\n- 文章信息表（news_article_basic）：包括 article_id | user_id | channel_id | title                         | cover                   | is_advertising | create_time         | status | reviewer_id | review_time | delete_time | comment_count | allow_comment | update_time         | reject_reason 等字段。\n- 文章内容表（news_article_content）：包括 article_id | content | update_time 等字段。\n- 频道信息表（news_channel）：包括 channel_id | channel_name | create_time         | update_time         | sequence | is_visible | is_default 等字段。\n\n首先，在 Hive 中创建业务数据库 toutiao，并创建相关表\n```sql\ncreate database if not exists toutiao comment \"user,news information of mysql\" location '/user/hive/warehouse/toutiao.db/';\n-- 用户信息表\ncreate table user_profile\n(\n    user_id             BIGINT comment \"userid\",\n    gender              BOOLEAN comment \"gender\",\n    birthday            STRING comment \"birthday\",\n    real_name           STRING comment \"real_name\",\n    create_time         STRING comment \"create_time\",\n    update_time         STRING comment \"update_time\",\n    register_media_time STRING comment \"register_media_time\",\n    id_number           STRING comment \"id_number\",\n    id_card_front       STRING comment \"id_card_front\",\n    id_card_back        STRING comment \"id_card_back\",\n    id_card_handheld    STRING comment \"id_card_handheld\",\n    area                STRING comment \"area\",\n    company             STRING comment \"company\",\n    career              STRING comment \"career\"\n)\n    COMMENT \"toutiao user profile\"\n    row format delimited fields terminated by ',' # 指定分隔符\n    LOCATION '/user/hive/warehouse/toutiao.db/user_profile';\n-- 用户基础信息表\ncreate table user_basic\n(\n    user_id         BIGINT comment \"user_id\",\n    mobile          STRING comment \"mobile\",\n    password        STRING comment \"password\",\n    profile_photo   STRING comment \"profile_photo\",\n    last_login      STRING comment \"last_login\",\n    is_media        BOOLEAN comment \"is_media\",\n    article_count   BIGINT comment \"article_count\",\n    following_count BIGINT comment \"following_count\",\n    fans_count      BIGINT comment \"fans_count\",\n    like_count      BIGINT comment \"like_count\",\n    read_count      BIGINT comment \"read_count\",\n    introduction    STRING comment \"introduction\",\n    certificate     STRING comment \"certificate\",\n    is_verified     BOOLEAN comment \"is_verified\"\n)\n    COMMENT \"toutiao user basic\"\n    row format delimited fields terminated by ',' # 指定分隔符\n    LOCATION '/user/hive/warehouse/toutiao.db/user_basic';\n-- 文章基础信息表\ncreate table news_article_basic\n(\n    article_id  BIGINT comment \"article_id\",\n    user_id     BIGINT comment \"user_id\",\n    channel_id  BIGINT comment \"channel_id\",\n    title       STRING comment \"title\",\n    status      BIGINT comment \"status\",\n    update_time STRING comment \"update_time\"\n)\n    COMMENT \"toutiao news_article_basic\"\n    row format delimited fields terminated by ',' # 指定分隔符\n    LOCATION '/user/hive/warehouse/toutiao.db/news_article_basic';\n-- 文章频道表\ncreate table news_channel\n(\n    channel_id   BIGINT comment \"channel_id\",\n    channel_name STRING comment \"channel_name\",\n    create_time  STRING comment \"create_time\",\n    update_time  STRING comment \"update_time\",\n    sequence     BIGINT comment \"sequence\",\n    is_visible   BOOLEAN comment \"is_visible\",\n    is_default   BOOLEAN comment \"is_default\"\n)\n    COMMENT \"toutiao news_channel\"\n    row format delimited fields terminated by ',' # 指定分隔符\n    LOCATION '/user/hive/warehouse/toutiao.db/news_channel';\n-- 文章内容表\ncreate table news_article_content\n(\n    article_id BIGINT comment \"article_id\",\n    content    STRING comment \"content\",\n    update_time STRING comment \"update_time\"\n)\n    COMMENT \"toutiao news_article_content\"\n    row format delimited fields terminated by ',' # 指定分隔符\n    LOCATION '/user/hive/warehouse/toutiao.db/news_article_content';\n```\n查看 Sqoop 连接到 MySQL 的数据库列表\n```shell\nsqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P\n\nmysql\nsys\ntoutiao\n```\nSqoop 可以指定全量导入和增量导入，通常我们可以先执行一次全量导入，将历史数据全部导入进来，后面再通过定时任务执行增量导入，来保持 MySQL 和 Hive 的数据同步，全量导入不需要提前创建 Hive 表，可以自动创建\n```shell\narray=(user_profile user_basic news_article_basic news_channel news_article_content)\n\nfor table_name in ${array[@]};\ndo\n    sqoop import \\\n        --connect jdbc:mysql://192.168.19.137/toutiao \\\n        --username root \\\n        --password password \\\n        --table $table_name \\\n        --m 5 \\ # 线程数\n        --hive-home /root/bigdata/hive \\ # hive路径\n        --hive-import \\ # 导入形式\n        --create-hive-table  \\ # 自动创建表\n        --hive-drop-import-delims \\\n        --warehouse-dir /user/hive/warehouse/toutiao.db \\ # 导入地址\n        --hive-table toutiao.$table_name \ndone\n```\n增量导入，有 append 和 lastmodified 两种模式\n- append：通过指定一个递增的列进行更新，只能追加，不能修改\n```shell\nnum=0\ndeclare -A check\ncheck=([user_profile]=user_id [user_basic]=user_id [news_article_basic]=article_id [news_channel]=channel_id [news_article_content]=channel_id)\n\nfor k in ${!check[@]}\ndo\n    sqoop import \\\n        --connect jdbc:mysql://192.168.19.137/toutiao \\\n        --username root \\\n        --password password \\\n        --table $k \\\n        --m 4 \\\n        --hive-home /root/bigdata/hive \\ # hive路径\n        --hive-import \\ # 导入到hive\n        --create-hive-table  \\ # 自动创建表\n        --incremental append \\ # 按照id更新\n        --check-column ${check[$k]} \\ # 指定id列\n        --last-value ${num} # 指定最后更新的id\ndone\n```\n- lastmodified：按照最后修改时间更新，支持追加和修改\n```shell\ntime=`date +\"%Y-%m-%d\" -d \"-1day\"`\ndeclare -A check\ncheck=([user_profile]=update_time [user_basic]=last_login [news_article_basic]=update_time [news_channel]=update_time)\ndeclare -A merge\nmerge=([user_profile]=user_id [user_basic]=user_id [news_article_basic]=article_id [news_channel]=channel_id)\n\nfor k in ${!check[@]}\ndo\n    sqoop import \\\n        --connect jdbc:mysql://192.168.19.137/toutiao \\\n        --username root \\\n        --password password \\\n        --table $k \\\n        --m 4 \\\n        --target-dir /user/hive/warehouse/toutiao.db/$k \\ # hive路径\n        --incremental lastmodified \\ # 按照最后修改时间更新\n        --check-column ${check[$k]} \\ # 指定时间列\n        --merge-key ${merge[$k]} \\ # 根据指定列合并重复或修改数据\n        --last-value ${time} # 指定最后修改时间\ndone\n```\nSqoop 可以直接导入到 Hive，自动创建 Hive 表，但是 lastmodified 模式不支持\nSqoop 也可以先导入到 HDFS，然后再建立 Hive 表关联，为了使用 lastmodified 模式，通常使用这种方法\n```\n--target-dir /user/hive/warehouse/toutiao.db/user_profile # 指定导入的HDFS目录\n```\n若先导入到 HDFS，需要注意 HDFS 默认分隔符为 `,` 而 Hive 默认分隔符为 `/u0001`，所以需要在创建 Hive 表时指定分隔符为 HDFS 分隔符 `,`，若不指定分隔符，查询结果将全部为 NULL\n```\nrow format delimited fields terminated by ',' # 指定分隔符\n```\n如果 MySQL 数据中存在特殊字符，如 `, \\t \\n` 都会导致 Hive 读取失败（但不影响导入到 HDFS 中），可以利用 `--query` 参数，在读取时使用 `REPLACE() CHAR() CHR()` 进行字符替换。如果特殊字符过多，比如 news_article_content 表，可以选择全量导入\n```\n--query 'select article_id, user_id, channel_id, REPLACE(REPLACE(REPLACE(title, CHAR(13),\"\"),CHAR(10),\"\"), \",\", \" \") title, status, update_time from news_article_basic WHERE $CONDITIONS' \\\n```\n如果 MySQL 数据中存在 tinyint 类型,必须在 `--connet` 中加入 `?tinyInt1isBit=false`，防止 Hive 将 tinyint 类型默认转化为 boolean 类型\n```\n--connect jdbc:mysql://192.168.19.137/toutiao?tinyInt1isBit=false\n```\nMySQL 与 Hive 对应类型如下\nMySQL|Hive\n-|-\nbigint|bigint\ntinyint|boolean\nint|int\ndouble|double\nbit|boolean\nvarchar|string\ndecimal|double\ndate / timestamp|string\n我们可以利用 crontab 来创建定时任务，将更新命令写入脚本 import_incremental.sh，输入 `crontab -e` 打开编辑界面，输入如下内容，即可定时执行数据同步任务\n```\n# 每隔半小时增量导入一次\n*/30 * * * * /root/toutiao_project/scripts/import_incremental.sh\n```\ncrontab 命令格式为 `* * * * * shell`，其中前五个 `*` 分别代表分钟 (0-59)、小时(0-59)、月内的某天(1-31)、月(1-12)、周内的某天(0-7，周日为 0 或 7)，`shell` 表示要执行的命令或脚本，使用方法如下\n```\n# 每隔5分钟运行一次backupscript脚本\n*/5 * * * * /root/backupscript.sh\n# 每天的凌晨1点运行backupscript脚本\n0 1 * * * /root/backupscript.sh\n# 每月的第一个凌晨3:15运行backupscript脚本\n15 3 1 * * /root/backupscript.sh\n```\ncrontab 常用命令如下\n```shell\ncrontab -e      # 修改 crontab 文件，如果文件不存在会自动创建。 \ncrontab -l      # 显示 crontab 文件。 \ncrontab -r      # 删除 crontab 文件。\ncrontab -ir     # 删除 crontab 文件前提醒用户。\n\nservice crond status      # 查看crontab服务状态\nservice crond start       # 启动服务 \nservice crond stop        # 关闭服务 \nservice crond restart     # 重启服务 \nservice crond reload      # 重新载入配置\n```\n> 原文出自（已授权）：https://www.jianshu.com/u/ac833cc5146e\n\n\n# 参考\n- [https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n- [https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘，提取码 EakP）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"《文章推荐系统》系列之1、推荐流程设计","url":"/2019/12/05/RecSys/文章推荐系统/《文章推荐系统》系列之1、推荐流程设计/","content":"\n推荐系统主要解决的是信息过载的问题，目标是从海量物品筛选出不同用户各自喜欢的物品，从而为每个用户提供个性化的推荐。推荐系统往往架设在大规模的业务系统之上，面临着用户的不断增长，物品的不断变化，并且有着全面的推荐评价指标和严格的性能要求（Netflix 的请求时间在 250 ms 以内，今日头条的请求时间在 200ms 以内），所以推荐系统很难一次性地快速计算出用户所喜好的物品，并且同时满足准确度、多样性等评价指标。为了解决如上这些问题，推荐系统通常被设计为三个阶段：召回、排序和调整，如下图所示：\n\n![召回、排序和调整](https://upload-images.jianshu.io/upload_images/12790782-ee044ed9dd9cc759.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n在召回阶段，首先筛选出和用户直接相关或间接相关的物品，将原始数据从万、百万、亿级别缩小到万、千级别；在排序阶段，通常使用二分类算法来预测用户对物品的喜好程度（或者是点击率），然后将物品按照喜好程序从大到小依次排列，筛选出用户最有可能喜欢的物品，这里又将召回数据从万、千级别缩小到千、百级别；最后在调整阶段，需要过滤掉重复推荐的、已经购买或阅读的、已经下线的物品，当召回和排序结果不足时，需要使用热门物品进行补充，最后合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。\n\n这里以文章推荐系统为例，讲述一下推荐系统的完整流程，如下图所示：\n\n![推荐系统的完整流程](https://upload-images.jianshu.io/upload_images/12790782-40327d59055eba56.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n1. 同步业务数据\n为了避免推荐系统的数据读写、计算等对应用产生影响，我们首先要将业务数据从应用数据库 MySQL 同步到推荐系统数据库 Hive 中，这里利用 Sqoop 先将 MySQL 中的业务数据同步到推荐系统的 HDFS 中，再关联到指定的 Hive 表中，这样就可以在推荐系统数据库 Hive 中使用用户数据和文章数据了，并且不会对应用产生任何影响。\n\n2. 收集用户行为数据\n除了用户数据和文章数据，我们还需要得到用户对文章的行为数据，比如曝光、点击、阅读、点赞、收藏、分享、评论等。我们的用户行为数据是记录在应用服务器的日志文件中的，所以可以利用 Flume 对应用服务器的日志文件进行监听，一方面将收集到的用户行为数据同步到 HDFS 中，并关联到 Hive 的用户行为表，每天更新一次，以供离线计算使用。另一方面将 Flume 收集到的用户行为数据同步到 Kafka，实时更新，以供在线计算使用。\n\n3. 构建离线画像和特征\n- 文章画像由关键词和主题词组成，我们首先读取 Hive 中的文章数据，将文章内容进行分词，根据 TF-IDF 模型计算每个词的权重，将 TF-IDF 权重最高的 K 个词作为关键词，再根据 TextRank 模型计算每个词的权重，将 TextRank 权重最高的 K 个词与 TF-IDF 权重最高的 K 个词的共现词作为主题词，将关键词和主题词存储到 Hive 的文章画像表中。接下来，利用 Word2Vec 模型，计算得到所有关键词的平均向量，作为文章的词向量，存储到 Hive 的文章向量表中，并利用 BucketedRandomProjectionLSH 模型计算得到文章的相似度，将每篇文章相似度最高的 K 篇文章，存储到 Hbase 的文章相似表中。这样我们就得到了每篇文章的画像、词向量以及相似文章列表。\n\n- 构建离线用户画像\n我们可以将用户喜欢的文章的主题词作为用户标签，以便后面根据用户标签来推荐符合其偏好的文章。首先读取用户行为数据和文章画像数据，计算在用户产生过行为的所有文章中，每个主题词的权重，不同的行为，权重不同，计算公式为：用户标签权重 =（用户行为分值之和）x 时间衰减，这样就计算得到了用户的标签及标签权重，接着读取用户数据，得到用户基础信息，将用户标签、标签权重及用户基础信息一并存储到 Hbase 的用户画像表中。到这里我们已经通过机器学习算法，基于用户和文章的业务数据得到了用户和文章的画像，但为了后面可以更方便地将数据提供给深度学习模型进行训练，我们还需要将画像数据进一步抽象为特征数据。\n\n- 构建离线文章特征\n由于已经有了画像信息，特征构造就变得简单了。读取文章画像数据，将文章权重最高的 K 个关键词的权重作为文章关键词权重向量，将频道 ID、关键词权重向量、词向量作为文章特征存储到 Hbase 的文章特征表。\n\n- 构建离线用户特征\n读取用户画像数据，将权重最高的 K 个标签的权重作为用户标签权重向量，将用户标签权重向量作为用户特征存储到 Hbase 的用户特征表。\n\n4. 多路召回\n\n- 基于模型的离线召回\n我们可以根据用户的历史点击行为来预测相似用户，并利用相似用户的点击行为来预测对文章的偏好得分，这种召回方式称为 u2u2i。获取用户历史点击行为数据，利用 ALS 模型计算得到用户对文章的偏好得分及文章列表，读取并过滤历史召回结果，防止重复推荐，将过滤后的偏好得分最高的 K 篇文章存入 Hbase 的召回结果表中，key 为 als，表明召回类型为 ALS 模型召回，并记录到 Hbase 的历史召回结果表。\n\n- 基于内容的离线召回\n我们可以根据用户的历史点击行为，向用户推荐其以前喜欢的文章的相似文章，这种方式称为 u2i2i。读取用户历史行为数据，获取用户历史发生过点击、阅读、收藏、分享等行为的文章，接着读取文章相似表，获取与发生行为的每篇文章相似度最高的 K 篇文章，然后读取并过滤历史召回结果，防止重复推荐，最后将过滤后的文章存入 Hbase 的召回结果表中，key 为 content，表明召回类型为内容召回，并记录到 Hbase 的历史召回结果表。\n\n- 基于内容的在线召回\n和上面一样，还是根据用户的点击行为，向用户推荐其喜欢的文章的相似文章，不过这里是用户实时发生的行为，所以叫做在线召回。读取 Kafka 中的用户实时行为数据，获取用户实时发生点击、阅读、收藏、分享等行为的文章，接着读取文章相似表，获取与发生行为的每篇文章相似度最高的 K 篇文章，然后读取并过滤历史召回结果，防止重复推荐，最后将过滤后的文章存入 Hbase 的召回结果表中，key 为 online，表明召回类型为在线召回，并记录到 Hbase 的历史召回结果表。\n\n- 基于热门文章的在线召回\n读取 Kafka 中的用户实时行为数据，获取用户当前发生点击、阅读、收藏、分享等行为的文章，增加这些文章在 Redis 中的热度分数。\n\n- 基于新文章的在线召回\n读取 Kafka 中的实时用户行为数据，获取新发布的文章，将其加入到 Redis 中，并设置过期时间。\n\n5. 排序\n\n不同模型的做法大致相同，这里以 LR 模型为例。\n\n- 基于 LR 模型的离线训练\n读取 Hive 的用户历史行为数据，并切分为训练集和测试集，根据其中的用户 ID 和文章 ID，读取 Hbase 的用户特征数据和文章特征数据，将二者合并作为训练集的输入特征，将用户对文章是否点击作为训练集的标签，将上一次的模型参数作为 LR 模型的初始化参数，进行点击率预估训练，计算得出 AUC 等评分指标并进行推荐效果分析。\n\n- 基于 LR 模型的在线排序\n当推荐中心读取 Hbase 的推荐结果表无数据时，推荐中心将调用在线排序服务来重新获取推荐结果。排序服务首先读取 Hbase 的召回结果作为测试集，读取 Hbase 的用户特征数据和文章特征数据，将二者合并作为测试集的输入特征，使用 LR 模型进行点击率预估，计算得到点击率最高的前 K 个文章，然后读取并过滤历史推荐结果，防止重复推荐，最后将过滤后的文章列表存入 Hbase 的推荐结果表中，key 为 lr，表明排序类型为 LR 排序。\n\n6. 推荐中心\n- 流量切分（ABTest）\n我们可以根据用户 ID 进行哈希分桶，将流量切分到多个桶，每个桶对应一种排序策略，从而对比不同排序策略在线上环境的效果。\n\n- 推荐数据读取逻辑\n优先读取 Redis 和 Hbase 中缓存的推荐结果，若 Redis 和 Hbase 都为空，则调用在线排序服务获得推荐结果。\n\n- 兜底补足（超时截断）\n当调用排序服务无结果，或者读取超时的时候，推荐中心会截断当前请求，直接读取 Redis 中的热门文章和新文章作为推荐结果。\n\n- 合并信息\n合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。\n\n> 原文出自（已授权）：https://www.jianshu.com/u/ac833cc5146e\n\n# 参考\n- [https://space.bilibili.com/61036655/channel/detail?cid=91348](https://space.bilibili.com/61036655/channel/detail?cid=91348)（强烈推荐，蚂蚁大神的视频讲得很棒）\n- [https://www.bilibili.com/video/av68356229](https://www.bilibili.com/video/av68356229)\n- [https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw](https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw)（学习资源已保存至网盘，提取码 EakP）\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["文章推荐系统"],"categories":["技术篇"]},{"title":"Django3.0和Python3.7连接Mysql报：Error loading MySQLdb module. Did you install mysqlclient","url":"/2019/12/04/Python/Django3.0和Python3.7连接Mysql报：Error loading MySQLdb module. Did you install mysqlclient/","content":"\n\n# 环境说明\n- Python 3.7.3\n- Django 3.0\n    - 安装：pip3 install -U Django\n    - 文档：https://docs.djangoproject.com/zh-hans/3.0/contents/\n\n\n# 项目说明\n\n创建项目\n\n```\ndjango-admin startproject mysite \n```\n\n配置Mysql\n```\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.mysql',\n        'NAME': 'mysite',\n        'USER': 'root',\n        'PASSWORD': '123456',\n        'HOST': '127.0.0.1',\n        'PORT': '3306',\n    }\n}\n```\n\n创建视图\n```\npython3 manage.py startapp index\n```\n\n报错\n```\n[MeetUpRec] python3 manage.py startapp index                                                                                                           14:45:34  ☁  master ☂ ✭\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/django/db/backends/mysql/base.py\", line 16, in <module>\n    import MySQLdb as Database\nModuleNotFoundError: No module named 'MySQLdb'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"manage.py\", line 21, in <module>\n    main()\n  File \"manage.py\", line 17, in main\n    execute_from_command_line(sys.argv)\n  File \"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py\", line 401, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py\", line 377, in execute\n    django.setup()\n  File \"/usr/local/lib/python3.7/site-packages/django/__init__.py\", line 24, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/usr/local/lib/python3.7/site-packages/django/apps/registry.py\", line 114, in populate\n    app_config.import_models()\n  File \"/usr/local/lib/python3.7/site-packages/django/apps/config.py\", line 211, in import_models\n    self.models_module = import_module(models_module_name)\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.7/site-packages/django/contrib/auth/models.py\", line 2, in <module>\n    from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager\n  File \"/usr/local/lib/python3.7/site-packages/django/contrib/auth/base_user.py\", line 47, in <module>\n    class AbstractBaseUser(models.Model):\n  File \"/usr/local/lib/python3.7/site-packages/django/db/models/base.py\", line 121, in __new__\n    new_class.add_to_class('_meta', Options(meta, app_label))\n  File \"/usr/local/lib/python3.7/site-packages/django/db/models/base.py\", line 325, in add_to_class\n    value.contribute_to_class(cls, name)\n  File \"/usr/local/lib/python3.7/site-packages/django/db/models/options.py\", line 208, in contribute_to_class\n    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())\n  File \"/usr/local/lib/python3.7/site-packages/django/db/__init__.py\", line 28, in __getattr__\n    return getattr(connections[DEFAULT_DB_ALIAS], item)\n  File \"/usr/local/lib/python3.7/site-packages/django/db/utils.py\", line 207, in __getitem__\n    backend = load_backend(db['ENGINE'])\n  File \"/usr/local/lib/python3.7/site-packages/django/db/utils.py\", line 111, in load_backend\n    return import_module('%s.base' % backend_name)\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/usr/local/lib/python3.7/site-packages/django/db/backends/mysql/base.py\", line 21, in <module>\n    ) from err\ndjango.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module.\nDid you install mysqlclient?\n\n```\n原因：在Python2中用的是mysqldb，但在Python3中用的是 mysqlclient\n\n尝试解决：执行mysqlclient安装命令\n```\npip3 install mysqlclient \n```\n\n报错：\n```\nCollecting mysqlclient\n  Using cached https://files.pythonhosted.org/packages/d0/97/7326248ac8d5049968bf4ec708a5d3d4806e412a42e74160d7f266a3e03a/mysqlclient-1.4.6.tar.gz\n    ERROR: Complete output from command python setup.py egg_info:\n    ERROR: /bin/sh: mysql_config: command not found\n    /bin/sh: mariadb_config: command not found\n    /bin/sh: mysql_config: command not found\n    Traceback (most recent call last):\n      File \"<string>\", line 1, in <module>\n      File \"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup.py\", line 16, in <module>\n        metadata, options = get_config()\n      File \"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup_posix.py\", line 61, in get_config\n        libs = mysql_config(\"libs\")\n      File \"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup_posix.py\", line 29, in mysql_config\n        raise EnvironmentError(\"%s not found\" % (_mysql_config_path,))\n    OSError: mysql_config not found\n    ----------------------------------------\nERROR: Command \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/\n```\n没找到mysql配置，解决如下：\n\n```\nexport PATH=$PATH:/usr/local/mysql/bin \n```\n再次安装，成功\n\n\n\n----\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<center>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n\n---\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Python"],"categories":["技术篇"]},{"title":"Spark使用Libsvm格式数据构造LabeledPoint格错误","url":"/2019/11/29/Spark/Spark使用Libsvm格式数据构造LabeledPoint格错误/","content":"\n### 背景\n\n使用libsvm格式的数据构造LabeledPoint格式，例如我的libsvm格式数据如下(索引下标最大值为，3000)：\n```\n790718 1:1 2:1 4:1 5:1 6:1 7:1 9:1 11:1 13:1 16:1 19:1 21:1 28:1 31:1 43:1 64:1 65:1 140:1 164:1 184:1 296:1 463:1 481:1 642:1 813:1 1093:1 2288:1\n692384 9:1 10:1 16:1 19:1 30:1 31:1 54:1 56:1 69:1 140:1 142:1 224:1 232:1 307:1 601:1 649:1 692:1 2851:1\n```\n\n但是在构造LabeledPoint格式数据的时候忽略的应该创建的数组长度，使用如下代码：\n```\nval dataset = rdd\n    .map( l => ( l._1, l._2.split(\" \").map(_.split(\":\")).map(e => (e(0).toInt-1, e(1).toDouble)) ) )\n    .map(l => LabeledPoint(l._1.toDouble, Vectors.sparse(l._2.length, l._2.filter(_._2!=0).map(_._1), l._2.filter(_._2!=0).map(_._2))))\n    .toDF(\"label\", \"features\")\n```\n\n所以报了 java.lang.IllegalArgumentException: requirement failed: Index 2287 out of bounds for vector of size 27 错误\n\n### 解决办法\n\n将创建LabeledPoint数据的长度改为3000即可，如下：\n\n```\nval dataset = rdd\n    .map( l => ( l._1, l._2.split(\" \").map(_.split(\":\")).map(e => (e(0).toInt-1, e(1).toDouble)) ) )\n    .map(l => LabeledPoint(l._1.toDouble, Vectors.sparse(3000, l._2.filter(_._2!=0).map(_._1), l._2.filter(_._2!=0).map(_._2))))\n    .toDF(\"label\", \"features\")\n```\n打印信息显示如下：\n```\nroot\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n\n+---------+--------------------+\n|    label|            features|\n+---------+--------------------+\n| 790718.0|(3275,[0,1,3,4,5,...|\n| 692384.0|(3275,[8,9,15,18,...|\n| 672331.0|(3275,[0,1,2,7,8,...|\n|1646601.0|   (3275,[30],[1.0])|\n|1740585.0|(3275,[0,3,6,9,11...|\n| 615659.0|(3275,[2,4,5,30,4...|\n| 169763.0|(3275,[1,2,3,4,7,...|\n| 639653.0|(3275,[1,2,4,10,1...|\n|1774993.0|(3275,[6,11,13,14...|\n|1680621.0|(3275,[11,16,31],...|\n+---------+--------------------+\nonly showing top 10 rows\n```\n\n完美解决问题！！！希望本文能够帮助到你！\n\n不得不说对于Spark中一些格式的数据使用还是不太熟悉！\n\n\n----\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<center>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n\n---\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"NLP实战之基于TFIDF的文本相似度计算","url":"/2019/11/26/NLP/NLP实战之基于TFIDF的文本相似度计算/","content":"\n### TFIDF算法介绍\n\nTF-IDF（Term Frequency–InverseDocument Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n\nTF-IDF实际是TF*IDF，其中TF（Term Frequency）表示词条$t$在文档$D_i$中的出现的频率，TF的计算公式如下所示：\n$$\nTF_{t,D_i} = \\frac{count(t)}{D_i}\n$$\n\n其中IDF（InverseDocument Frequency）表示总文档与包含词条t的文档的比值求对数，IDF的计算公式如下所示： \n$$\nIDF_t = log \\frac{N}{ \\sum_{i=1}^{N} I(t,D_i) }\n$$\n其中$N$为所有的文档总数，$I(t,D_i)$表示文档$D_i$是否包含词条$t$，若包含为1，不包含为0。但此处存在一个问题，即当词条$t$在所有文档中都没有出现的话IDF计算公式的分母为0，此时就需要对IDF做平滑处理，改善后的IDF计算公式如下所示：\n$$\nIDF_t = log \\frac{N}{ 1 + \\sum_{i=1}^{N} I(t,D_i) }\n$$\n那么最终词条$t$在文档$D_i$中的TF-IDF值为：$TF-IDF_{t,D_i} = TF_{t,D_i} * IDF_t$ 。\n\n从上述的计算词条$t$在文档$D_i$中的TF-IDF值计算可以看出：当一个词条在文档中出现的频率越高，且新鲜度低（即普遍度低），则其对应的TF-IDF值越高。\n比如现在有一个预料库，包含了100篇（$N$）论文，其中涉及包含推荐系统（$t$）这个词条的有20篇，在第一篇论文（$D1$）中总共有200个技术词汇，其中推荐系统出现了15次，则词条推荐系统的在第一篇论文（$D1$）中的TF-IDF值为：\n$$\nTF-IDF_{推荐系统} = \\frac {15} {200} * log \\frac{200}{20+1} = 0.051\n$$\n\n更多详细的关于TFIDF的介绍可以参考\n- [搜索引擎：文本分类——TF/IDF算法](https://thinkgamer.blog.csdn.net/article/details/48811033)\n\n关于TF-IDF的其他实战：\n- [基于TF-IDF算法的短标题关键词提取](https://thinkgamer.blog.csdn.net/article/details/85690389)\n\n### 基于TFIDF计算文本相似度\n\n这里需要注意的是在spark2.x中默认不支持dataframe的笛卡尔积操作，需要在创建Spark对象时开启。\n\n创建spark对象，并设置日志等级\n```\n// spark.sql.crossJoin.enabled=true spark 2.0 x不支持笛卡尔积操作，需要开启支持\nval spark = SparkSession\n    .builder()\n    .appName(\"docSimCalWithTFIDF\")\n    .config(\"spark.sql.crossJoin.enabled\",\"true\")\n    .master(\"local[10]\")\n    .enableHiveSupport()\n    .getOrCreate()\nLogger.getRootLogger.setLevel(Level.WARN)\n```\n\n这里以官方样例代码中的三行英文句子为例，创建数据集，并进行分词（spark中的中文分词包有很多，比如jieba，han，ansj，fudannlp等，这里不展开介绍）\n```\nval sentenceData = spark.createDataFrame(Seq(\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic regression models are neat\")\n)).toDF(\"label\", \"sentence\")\n\nval tokenizer = new Tokenizer()\n    .setInputCol(\"sentence\")\n    .setOutputCol(\"words\")\n\nval wordsData = tokenizer.transform(sentenceData)\nwordsData.show(10)\n```\n展示的结果为：\n```\n+-----+--------------------+--------------------+\n|label|            sentence|               words|\n+-----+--------------------+--------------------+\n|    0|Hi I heard about ...|[hi, i, heard, ab...|\n|    1|I wish Java could...|[i, wish, java, c...|\n|    2|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n```\n\n调用官方的tfidf包计算向量：\n```\n// setNumFeatures(5)表示将Hash分桶的数量设置为5个,可以根据你的词语数量来调整，一般来说，这个值越大不同的词被计算为一个Hash值的概率就越小，数据也更准确，但需要消耗更大的内存\n\nval hashingTF = new HashingTF()\n    .setInputCol(\"words\")\n    .setOutputCol(\"rawFeatures\")\n    .setNumFeatures(5)\nval featurizedData = hashingTF\n    .transform(wordsData)\n\nfeaturizedData.show(10)\n\nval idf = new IDF()\n    .setInputCol(\"rawFeatures\")\n    .setOutputCol(\"features\")\nval idfModel = idf.fit(featurizedData)\nval rescaledData = idfModel.transform(featurizedData)\n\nrescaledData.show(10)\nrescaledData.select(\"label\", \"features\").show()\n```\n展示的结果为：\n```\n+-----+--------------------+--------------------+--------------------+\n|label|            sentence|               words|         rawFeatures|\n+-----+--------------------+--------------------+--------------------+\n|    0|Hi I heard about ...|[hi, i, heard, ab...|(5,[0,2,4],[2.0,2...|\n|    1|I wish Java could...|[i, wish, java, c...|(5,[0,2,3,4],[1.0...|\n|    2|Logistic regressi...|[logistic, regres...|(5,[0,1,3,4],[1.0...|\n+-----+--------------------+--------------------+--------------------+\n\n+-----+--------------------+--------------------+--------------------+--------------------+\n|label|            sentence|               words|         rawFeatures|            features|\n+-----+--------------------+--------------------+--------------------+--------------------+\n|    0|Hi I heard about ...|[hi, i, heard, ab...|(5,[0,2,4],[2.0,2...|(5,[0,2,4],[0.0,0...|\n|    1|I wish Java could...|[i, wish, java, c...|(5,[0,2,3,4],[1.0...|(5,[0,2,3,4],[0.0...|\n|    2|Logistic regressi...|[logistic, regres...|(5,[0,1,3,4],[1.0...|(5,[0,1,3,4],[0.0...|\n+-----+--------------------+--------------------+--------------------+--------------------+\n\n+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|    0|(5,[0,2,4],[0.0,0...|\n|    1|(5,[0,2,3,4],[0.0...|\n|    2|(5,[0,1,3,4],[0.0...|\n+-----+--------------------+\n```\n\n其中$(5,[0,2,4],[0.0,0...$ 是向量的一种压缩表示格式，例如$(3,[0,1],[0.1,0.2])$表示的是 向量的长度为3，其中第 1位和第2位的值为0.1 和0.3，第3位的值为0。\n\n---\n\n这里需要将其转化为向量的形式，方便后续进行计算，可以直接通过dataframe进行转化，也可以先将dataframe转化为rdd，再进行转化。\ndatafram通过自定义UDF进行转化如下：\n```\nimport spark.implicits._\n// 解析数据 转化为denseVector格式 datafra\nval sparseVectorToDenseVector = udf { \n    features: SV => features.toDense \n}\nval df = rescaledData\n    .select($\"label\", sparseVectorToDenseVector($\"features\"))\n    .withColumn(\"tag\",lit(1))\ndf.show(10)\n```\n展示结果为：\n```\n+------+--------------------+---+\n|label1|           features1|tag|\n+------+--------------------+---+\n|     0|[0.0,0.0,0.575364...|  1|\n|     1|[0.0,0.0,0.575364...|  1|\n|     2|[0.0,0.6931471805...|  1|\n+------+--------------------+---+\n```\n\n先转化为RDD，再进行转化如下：\n```\nval selectedRDD = rescaledData.select(\"label\", \"features\").rdd\n    .map( l=>( l.get(0).toString, l.getAs[SV](1).toDense))\nselectedRDD.take(10).foreach(println)\n```\n展示结果为：\n```\n(0,[0.0,0.0,0.5753641449035617,0.0,0.0])\n(1,[0.0,0.0,0.5753641449035617,0.28768207245178085,0.0])\n(2,[0.0,0.6931471805599453,0.0,0.5753641449035617,0.0])\n```\n\n----\n\n当然也可以在进行相似度计算时进行转化，实现代码如下：\n```\n// 定义相似度计算udf\nimport spark.implicits._\nval df1 = rescaledData\n    .select($\"label\".alias(\"id1\"), $\"features\".alias(\"f1\"))\n    .withColumn(\"tag\",lit(1))\n\nval df2 = rescaledData\n    .select($\"label\".alias(\"id2\"), $\"features\".alias(\"f2\"))\n    .withColumn(\"tag\",lit(1))\n    \nval simTwoDoc = udf{ \n    (f1: SV, f2: SV) => calTwoDocSim(f1,f2) \n}\nval df =  df1.join(df2, Seq(\"tag\"), \"inner\")\n    .where(\"id1 != id2\")\n    .withColumn(\"simscore\",simTwoDoc(col(\"f1\"), col(\"f2\")))\n    .where(\"simscore > 0.0\")\n    .select(\"id1\",\"id2\",\"simscore\")\ndf.printSchema()\ndf.show(20)\n```\n其中calTwoDocSim 函数实现如下：\n```\n/**\n  * @Author: GaoYangtuan\n  * @Description: 自定义计算两个文本的距离\n  * @Thinkgamer: 《推荐系统开发实战》作者，「搜索与推荐Wiki」公号负责人，算法工程师\n  * @Param: [f1, f2]\n  * @return: double\n  **/\ndef calTwoDocSim(f1: SV, f2: SV): Double = {\n    val breeze1 =new SparseVector(f1.indices,f1.values, f1.size)\n    val breeze2 =new SparseVector(f2.indices,f2.values, f2.size)\n    val cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))\n   cosineSim\n}\n```\n打印结果如下：\n```\nroot\n |-- id1: integer (nullable = false)\n |-- id2: integer (nullable = false)\n |-- simscore: double (nullable = false)\n\n+---+---+------------------+\n|id1|id2|          simscore|\n+---+---+------------------+\n|  0|  1|0.8944271909999159|\n|  1|  0|0.8944271909999159|\n|  1|  2|0.2856369296406274|\n|  2|  1|0.2856369296406274|\n+---+---+------------------+\n```\n\n最后进行排序和保存，代码如下：\n```\nval sortAndSlice = udf { simids: Seq[Row] =>\n    simids.map{\n        case Row(id2: Int,  simscore: Double) => (id2,simscore)\n    }\n    .sortBy(_._2)\n    .reverse\n    .slice(0,100)\n    .map(e => e._1 + \":\" + e._2.formatted(\"%.3f\"))\n    .mkString(\",\")\n}\n\nval result = df\n    .groupBy($\"id1\")\n    .agg(collect_list(struct($\"id2\", $\"simscore\")).as(\"simids\"))\n    .withColumn(\"simids\", sortAndSlice(sort_array($\"simids\", asc = false)))\n\nresult.show(10)\nresult.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(\"data/tfidf\")\n```\n打印结果如下：\n```\n+---+---------------+\n|id1|         simids|\n+---+---------------+\n|  1|0:0.894,2:0.286|\n|  2|        1:0.286|\n|  0|        1:0.894|\n+---+---------------+\n```\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["NLP"],"categories":["技术篇"]},{"title":"模型的独立学习方式","url":"/2019/11/12/深度学习/神经网络笔记/模型的独立学习方式/","content":"\n### 概述\n\n针对一个给定的任务，通常采取的步骤是：准确一定非规模的数据集，这些数据要和真实数据集的分布一致；然后设定一个优化目标和方法；然后在训练集上训练模型。\n\n不同的模型往往都是从零开始训练的，一切知识都需要从训练集中得到，这就意味着每个任务都需要大量的训练数据。在实际应用中，我们面对的任务很难满足上述需求，比如训练任务和目标任务的数据分布不一致，训练数据集过少等。这时机器学习的任务就会受到限制，因此人们开始关注一些新的任务学习方式。\n本篇文章主要介绍一些“模型独立的学习方式”，比如：集成学习、协同学习、自学习、多任务学习、迁移学习、终身学习、小样本学习、元学习等。\n\n### 集成学习\n\n集成学习（Ensemble Learning）就是通过某种策略将多个模型集成起来，通过群体决策来提高决策准确率。集成学习首要的问题是如何集成多个模型，比较常用的集成策略有直接平均、加权平均等。\n\n**集成学习可以分为：Boosting、Bagging、Stacking**，这三种的详细区分和流程可以参考《[推荐系统开发实战](https://item.jd.com/12671716.html)》一书中第八章 点击率预估部分。本文中主要介绍集成学习中的Boosting学习和AdaBoost算法。\n\n集成学习的思想可以采用一句古老的谚语来描述：“三个臭皮匠，顶个诸葛亮”。但是一个有效的集成需要各个基模型的差异尽可能的大。为了增加模型之间的差异性，可以采取Bagging类和Boosting类两类方法。\n\n#### Bagging类方法\nBagging类方法是通过随机构造训练样本、随机选择特征等方法来提高每个基模型的独立性，代表性方法有Bagging和随机森林。\n\n- Bagging（Bootstrap Aggregating）是一个通过不同模型的训练数据集的独立性来提高不同模型之间的独立性。我们在原始训练集上进行有放回的随机采样，得到M比较小的训练集并训练M个模型，然后通过投票的方法进行模型集成。\n- 随机森林（Random Forest）是在Bagging的基础上再引入了随机特征，进一步提升每个基模型之间的独立性。在随机森林中，每个基模型都是一棵树。\n\n随机森林的算法步骤如下：\n\n- 从样本集中通过重采样的方式产生n个样本\n- 假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点\n- 重复m次，产生m棵决策树\n-多数投票机制来进行预测\n> 需要注意的一点是，这里m是指循环的次数，n是指样本的数目，n个样本构成训练的样本集，而m次循环中又会产生m个这样的样本集\n\n#### Boosting类方法\n\nBoosting类方法是按照一定的顺序来先后训练不同的基模型，每个模型都针对前续模型的错误进行专门训练。根据前序模型的结果，来提高训练样本的权重，从而增加不同基模型之间的差异性。Boosting类方法的代表性方法有AbaBoost，GBDT，XGB，LightGBM等。\n\n关于GBDT的介绍同样可以参考《[推荐系统开发实战](https://item.jd.com/12671716.html)》一书。\n\nBoosting类集成模型的目标是学习一个加性模型（additive model） ，其表达式如下：\n$$\nF(x) = \\sum_{m=1}^{M} a_m f_m(x)\n$$\n其中$f_m(x)$为弱分类器，或基分类器，$a_m$为弱分类器的集成权重，$F(x)$称为强分类器。\n\nBoosting类方法的关键是如何训练每个弱分类器$f_m(x)$以及对应的权重$a_m$。为了提高集成的效果，应尽可能使得每个弱分类器的差异尽可能大。一种有效的方法是采用迭代的策略来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。\n\n在学习了第m个弱分类器之后，增加分错样本的权重，使得第$m+1$个弱分类器“更关注”于前边弱分类器分错的样本。这样增加每个弱分类器的差异，最终提升的集成分类器的准确率。这种方法称为AdaBoost（其实其他的Boost模型采用的也是类似的策略，根据前m-1颗树的误差迭代第m颗树）。\n\n#### AdaBoost算法\n\nAdaBoost算法是一种迭代式的训练算法，通过改变数据分布来提高弱分类器的差异。在每一轮训练中，增加分错样本的权重，减少对分对样本的权重，从而得到一个新的数据分布。\n\n以两类分类为例，弱分类器$f_m(x) \\in \\{ +1, -1\\}$，AdaBoost算法的训练过程如下所示，最初赋予每个样本同样的权重。在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器。然后根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。\n\n![AdaBoost算法流程](https://img-blog.csdnimg.cn/20191106144746194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n**AdaBoost算法的统计学解释**\n\nAdaBoost算法可以看做是一种分步优化的加性模型，其损失函数定义为：\n$$\nL(F) = exp(-y F(x))\n\\\\\n= exp(-y \\sum_{m=1}^{M} a_m f_m(x))\n$$\n其中$y,f_m(x)\\in \\{ +1,-1\\}$\n\n假设经过$m-1$次迭代，得到：\n$$\nF_{m-1}(x) = \\sum_{t=1}^{T} a_t f_t(x)\n$$\n则第$m$次迭代的目标是找一个$a_m$和$f_m(x)$使得下面的损失函数最小。\n$$\nL(a_m,f_m(x)) = \\sum_{n=1}^{N}exp(-y^{(n)} (F_{m-1} (x^{(n)}) + a_m f_m(x^{(n)})))\n$$\n令 $w_m^{(n)}=exp(-y^{(n)} F_{m-1}(x^{(n)}))$，则损失函数可以表示为：\n$$\nL(a_m,f_m(x)) = \\sum_{n=1}^{N} w_m^{(n)} exp(-y^{(n)} a_m f_m(x^{(n)}))\n$$\n因为$y,f_m(x) \\in {+1, -1}$，有：\n$$\nyf_m(x) = 1-2I(y\\neq f_m(x))\n$$\n其中$I(x)$为指示函数。\n\n将损失函数在$f_m(x)=0$处进行二阶泰勒展开，有：\n$$\nL(a_m, f_m(x)) = \\sum_{n=1}^{N} w_m^{(n)} (  1 - a_m y^{(n)}f_m(x^{(n)}) + \\frac{1}{2}a_m^2  ) \n\\\\\n\\propto a_m \\sum_{n=1}^{N} w_n^{(n)} I(y^{(n} \\neq f_m(x^{(n)})\n$$\n从上式可以看出，当$a_m>0$时，最优的分类器$f_m(x)$为使得在样本权重为$w_m^{(n)}, 1 \\leq n \\leq N$时的加权错误率最小的分类器。\n\n在求解出$f_m(x)$之后，上述的损失函数可以写为：\n$$\nL(a_m, f_m(x)) = \\sum_{y^{(n)}=f_m(x^{(n)})} w_m^{(n)} exp(-a_m) + \\sum_{y^{(n)} \\neq f_m(x^{(n)})}  w_m^{(n)} exp(a_m) \n\\\\\n\\propto (1-\\epsilon _m) exp(-a_m) + \\epsilon_m exp(a_m)\n$$\n其中$\\epsilon_m$为分类器$f_m(x)$的加权错误率\n$$\n\\epsilon_m = \\frac { \\sum_{ y^{(n)} \\neq f_m(x^{(n)}) } w_m^{(n)} } {\\sum_{n} w_m^{(n)}}\n$$\n求上式关于$a_m$的导数并令其为0，得到\n$$\na_m = \\frac {1}{2} log \\frac {1-\\epsilon_m}{\\epsilon_m}\n$$\n\n\n**AdaBoost算法的优缺点**\n优点：\n\n- 作为分类器精度很高\n- 可以使用各种算法构建子分类器，AdaBoost提供的是一个框架\n- 使用简单分类器时，计算出的结果可理解，且构造简单\n- 不需要做特征筛选\n- 不同担心过拟合\n\n缺点：\n\n- 容易收到噪声干扰\n- 训练时间长，因为需要遍历所有特征\n- 执行效果依赖于弱分类器的选择\n\n### 自训练和协同训练\n\n监督学习虽然准确度上有一定的保证，但往往需要大量的训练数据，但在一些场景中标注数据的成本是非常高的，因此如何利用大量的无标注数据提高监督学习的效率，有着十分重要的意义。这种利用少量样本标注数据和大量样本标注数据进行学习的方式称之为半监督学习（Semi-Supervised Learning，SSL）。\n\n本节介绍两种无监督的学习算法：自训练和协同训练。\n\n#### 自训练\n自训练（Slef-Training）也叫自训练（Self-teaching）或者自举法（boostStrapping）。\n\n自训练的思路是：利用已知的标注数据训练一个模型，利用该模型去预测无标注的样本数据，然后将置信度较高的样本以及其伪标签加入训练集，然后重新训练模型，进行迭代。下图给出了自训练的算法过程。\n\n![自训练算法过程](https://img-blog.csdnimg.cn/20191109150434700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n自训练和密度估计中EM算法有一定的相似之处，通过不断地迭代来提高模型能力。但自训练的缺点是无法保证每次加入训练集的样本的伪标签是正确的。如果选择样本的伪标签是错误的，反而会损害模型的预测能力。因此自训练最关键的步骤是如何设置挑选样本的标准。\n\n#### 协同训练\n协同训练（Co-Training）是自训练的一种改进方法，通过两个基于不同视角的分类器来相互促进。很多数据都有相对独立的不同视角。比如互联网上的每个网页都由两种视角组成：文字内容和指向其他网页的链接。如果要确定一个网页的类别，可以根据文字内容来判断，也可以根据网页之间的链条关系来判断。\n\n假设一个样本$x=[x_1,x_2]$，其中$x_1,x_2$分别表示两种不同视角$V_1,V_2$的特征，并满足下面两个假设：\n- （1）：条件独立性，即给定样本标签y时，两种特征条件独立$p(x_1,x_2|y)=p(x_1|y)p(x_2|y)$\n- （2）：充足和冗余性，即当数据充分时，每种视角的特征都可以足以单独训练出一个正确的分类器。\n\n令$y=g(x)$为需要学习的真实映射函数，$f_1$和$f_2$分别为两个视角的分类器，有：\n$$\n\\exists f_1, f_2,    \\forall x \\in X,\\,\\,\\,\\,\\, f_1(x_1) = f_2(x_2) = g(x)\n$$\n其中$X$为样本$x$的取值空间。\n\n由于不同视角的条件独立性，在不同视角上训练出来的模型就相当于从不同的视角来理解问题，具有一定的互补性。协同训练就是利用这种互补行来进行自训练的一种方法。首先在训练集上根据不同视角分别训练两个模型$f_1$和$f_2$然后用$f_1$和$f_2$在无标记数据集上进行预测，各选取预测置信度比较高的样本加入到训练集，重新训练两个不同视角的模型，并不断重复这个过程（需要注意的是协同算法要求两种视图时条件独立的，如果两种视图完全一样，则协同训练退化成自训练算法）。\n\n协同训练的算法过程如下：\n![协同训练的算法过程](https://img-blog.csdnimg.cn/20191109153310892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n### 多任务学习\n一般的机器学习模型是针对单个任务进行的，不同任务的模型需要在各自的训练集上单独学习得到。而多任务学习（Multi-task learning）是指同时学习多个相关任务，让 这些任务在学习的过程中共享知识，利用多个任务之间的相关性来改进模型的性能和泛化能力。\n\n多任务学习可以看做时一种归纳迁移学习（Inductive Transfer Learning），即通过利用包含在相关任务中的信息作为归纳偏置（Inductive Bias）来提高泛化能力。\n\n**多任务学习的主要挑战在于如何设计多任务之间的共享机制**，在传统的机器学习任务中很难引入共享信息，但是在神经网络中就变得简单了许多，常见的以下四种：\n\n- **硬共享模式**：让不同任务的神经网络模型共同使用一些共享模块来提取一些通用的特征，然后再针对每个不同的任务设置一些私有模块来提取一些任务特定的特征。\n- **软共享模式**：不显式设置共享模块，但每个任务都可以从其他任务中“窃取”一些信息来提高自己的能力。窃取的方式包括直接复制使用其他任务的隐状态，或使用注意力机制来主动选择有用的信息。\n- **层次共享模式**：一般神经网络中不同层抽取的特征类型不同，底层一般抽取一些低级的局部特征，高层抽取一些高级的抽象语义特征。因此如果多任务学习中不同任务也有级别高低之分，那么一个合理的共享模式是让低级任务在底层输出，高级任务在高层输出。\n- **共享-私有模式**：一个更加分工明确的方式是将共享模块和任务特定（私有）模块的责任分开。共享模块捕捉一些跨任务的共享特征，而私有模块只捕捉和特点任务相关的特征。最终的表示由共享特征和私有特征共同构成。\n\n在多任务学习中，每个任务都可以有自己单独的训练集。为了让所有任务同时学习，我们通常会使用交替训练的方式来“近似”的实现同时学习，下图给出了四种常见的共享模式图\n\n![四种常见的共享模式图](https://img-blog.csdnimg.cn/20191112101743882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n多任务学习的流程可以分为两个阶段：\n\n- （1）联合训练阶段：每次迭代时，随机挑选一个任务，然后从这个任务中随机选择一些训练样本，计算梯度并更新参数\n- （2）单任务精调阶段：基于多任务学习到的参数，分别在每个单独任务进行精调，其中单任务精调阶段为可选阶段。当多个任务的差异性比较大时，在每个单任务上继续优化参数可以进一步提升模型能力。\n\n假设有M个相关任务，其模型分别为$f_m(x,\\theta), 1\\leq m \\leq M$，多任务学习的联合目标函数为所有任务损失函数的线性加权：\n$$\nL(\\theta) = \\sum_{m=1}^{M}\\sum_{n=1}^{N_m} \\eta_m l_m(f_m(x^{(m,n)}, \\theta ), y_{(m,n)})\n$$\n\n其中$l_m$为第m个任务的损失函数，$\\eta_m$是第m个任务的权重，$\\theta$表示包含了共享模块和私有模块在内的所有参数。\n\n多任务学习中联合训练阶段的具体过程如下所示：\n![多任务学习中联合训练阶段的具体过程](https://img-blog.csdnimg.cn/20191112102456841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n多任务学习通常比单任务学习获得更好的泛化能力，主要由于以下几个原因：\n\n- 1.多任务学习在多个数据集上进行训练，训练集范围更大，且多任务之间具有一定的相关性，相当于是一种隐式的数据增强，可以提高模型的泛化能力。\n- 2.多任务学习中的共享模块需要兼顾所有任务，在一定程度上避免了模型过拟合到单个任务的训练集，可以看做是一种正则化。\n- 3.多任务学习比单任务学习可以获得更好的表示\n- 4.在多任务学习中，每个任务都可以“选择性”利用其他任务中学习到的隐藏特征，从而提高自身的能力。\n\n### 迁移学习\n标准机器学习的前提假设只训练数据和测试数据的分布是相同的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差。如何将相关任务的训练数据中学习到的可泛化知识迁移到目标任务中，就是迁移学习（Transfer Learning）要解决的问题。\n\n迁移学习根据不同的迁移方式又分为两个类型：归纳迁移学习（Inductive Transfer Learning）和推导迁移学习（Transductive Transfer Learning）。这两个类型分别对应两个机器学习的范式：归纳学习（Inductive Learning）和转导学习（Transductive Learning）。一般的机器学习任务都是指归纳学习，即希望再训练集上学习到使得期望风险最小的模型。而转导学习的目标是学习一种在给定测试集上错误率最小的模型，在训练阶段可以利用测试集的信息。\n\n#### 归纳迁移学习\n一般而言，归纳迁移学习要求源领域和目标领域时相关的，并且源领域$D_S$有大量的训练样本，这些样本可以是有标注的样本也可以时无标注的样本。\n- 当源领域只有大量无标注数据时，源任务可以转换为无监督学习任务，比如自编码和密度估计，通过无监督任务学习一种可迁移的表示，然后将这些表示迁移到目标任务上。\n- 当源领域有大量的标注数据时，可以直接将源领域上训练的模型迁移到目标领域上。\n归纳迁移学习一般有下面两种迁移方式：\n\n- 基于特征的方式：将预训练模型的输出或者中间隐藏层的输出作为特征直接加入到目标任务学习模型中。目标任务的学习模型可以时一般的浅层分类器（比如支持向量机等）或一个新的神经网络模型。\n- 精调的方式：在目标任务上复用预训练模型的部分参数，并对其参数进行精调。\n\n假设预训练的模型是一个深层神经网络，不同层的可迁移性也不尽相同。通常来说网络的低层学习一些通用的低层特征，中层或者高层学习抽象的高级语义特征，而最后几层一般学习和特定任务相关的特征。因此根据目标任务的自身特点以及和源任务的相关性，可以针对性的选择预训练模型的不同层来迁移到目标任务中。\n\n将预训练模型迁移到目标任务中通常会比从零开始学习的方式好，主要体现在以下三点：\n\n- （1）初始模型的性能一般比随机初始化的模型要好\n- （2）训练时模型的学习速度比从零开始学习要快，收敛性更好\n- （3）模型的最终性能更好，具有更好的泛化性\n\n归纳迁移学习和多任务学习也比较类似，但是有下面两点区别：\n\n- （1）多任务学习是同时学习多个不同任务，而归纳迁移学习通常分为两个阶段，即源任务上的学习阶段，和目标任务上的迁移学习阶段\n- （2）归纳迁移学习是单向的知识迁移，希望提高模型在目标任务上的性能，而多任务学习时希望提高所有任务的性能。\n\n#### 转导迁移学习\n转导迁移学习是一种从样本到样本的迁移，直接利用源领域和目标领域的样本进行迁移学习。转导迁移学习可以看作是一种特殊的转导学习。转导迁移学习通常假设源领域有大量的标注数据，而目标领域没有（或少量）的标注数据，但是有大量的无标注数据。目标领域的数据在训练阶段是可见的。\n\n转导迁移学习的一个常见子问题时领域适应（Domain Adaptation），在领域适应问题中，一般假设源领域和目标领域有相同的样本空间，但是数据分布不同$p_S(x,y) \\neq p_T(x,y)$。\n\n根据贝叶斯公式，$p(x,y)=p(x|y)p(y) = p(y|x)p(x)$，因此数据分布的不一致通常由三种情况造成。\n\n- （1）协变量偏移（Covariate Shift）：源领域和目标领域的输入边际分布不同$p_S(x) \\neq p_T(x)$，但后验分布相同$p_S(y|x) = p_T(y|x)$，即学习任务相同$T_s = T_T$\n- （2）概念偏移（Concept Shift）：输入边际分布相同$p_S(x) = p_T(x)$，但后验分布不同$p_S(y|x) \\ neq p_T(y|x)$，即学习任务不同$T_S \\neq T_T$\n- （3）先验偏移（Prior Shift）：源领域和目标领域中的输出$y$先验分布不同$p_S(y) \\neq p_T(y)$，条件分布相同$p_S(x|y) = p_T(x|y)$。在这样的情况下，目标领域必须提供一定数量的标注样本。\n\n### 终生学习\n\n终生学习（Lifelong Learning）也叫持续学习（Continuous Learning）是指像人类一样具有持续不断的学习能力，根据历史任务中学到的经验和知识来帮助学习不断出现的新任务，并且这些经验和知识是持续累积的，不会因为新的任务而忘记旧的知识。\n\n在终生学习中，假设一个终生学习算法已经在历史人任务$T_1, T_2, ...$上学习到一个模型，当出现一个新任务$T_{m+1}$时，这个算法可以根据过去在$m$个任务上学习到的知识来帮助第$m+1$个任务，同时积累所有的$m+1$个任务上的知识。\n\n在终生学习中，一个关键的问题是如何避免**灾难性遗忘（Catastrophic Forgetting）**，即按照一定顺序学习多个任务时，在学习新任务的同时不忘记先前学习到的历史知识。比如在神经网络模型中，一些参数对任务$T_A$非常重要，如果在学习任务$T_B$时被改变了，就可能给任务$T_A$造成不好的影响。\n\n解决灾难性遗忘的方法有很多，比如弹性权重巩固方法（Elastic Weight Coonsolidation）。\n\n### 元学习\n\n根据没有免费午餐定理，没有一种通用的学习算法在所有任务上都有效。因此当使用机器学习算法实现某个任务时，我们通常需要“就事论事”，根据任务的特定来选择合适的模型、损失函数、优化算法以及超参数。\n\n而这种动态调整学习方式的能力，称为元学习（Meta-Learning），也称为学习的学习（Learning to Learn）。\n\n元学习的目的时从已有的任务中学习一种学习方法或元知识，可以加速新任务的学习。从这个角度来说，元学习十分类似于归纳迁移学习，但元学习更侧重从多种不同的任务中归纳出一种学习方法。\n\n这里主要介绍两种典型的元学习方法：基于优化器的元学习和模型无关的元学习。\n\n#### 基于优化器的元学习\n目前神经网络的的学习方法主要是定义一个目标损失函数$L(\\theta)$，并通过梯度下降算法来最小化$L(\\theta)$\n$$\n\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\bigtriangledown L(\\theta_{t-1})\n$$\n其中$\\theta_t$为第$t$步时的模型参数，$\\bigtriangledown L(\\theta_{t-1})$为梯度，$\\alpha$为学习率。在不同的任务上，通常选择不同的学习绿以及不同的优化方法，比如动量法，Adam等。这些优化算法的本质区别在于更新参数的规则不同，因此一种很自然的元学习就是自动学习一种更新参数的规则，即通过另一个神经网络（比如循环神经网络）来建模梯度下降的过程。下图给出了基于优化器的元学习示例。\n\n![基于优化器的元学习示例](https://img-blog.csdnimg.cn/20191112151007633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n我们用函数$g_t(.)$来预测第$t$步时参数更新的差值$\\Delta \\theta_t = \\theta_t - \\theta_{t-1}$，函数$g_t(.)$称为优化器，输入是当前时刻的梯度值，输出时参数的更新差值$\\Delta \\theta_t$，这样第$t$步的更新规则可以写为：\n$$\n\\theta_{t+1} = \\theta_t + g_t(\\bigtriangledown L(\\theta_{t}), \\phi )\n$$\n其中$\\phi$为优化器$g_t(.)$的参数。\n\n学习优化器$g_t(.)$的过程可以看做是一种元学习过程，其目标是找到一个适用于多个不同任务的优化器。在标准的梯度下降中，每步迭代的目标是使得$L(\\theta)$下降。而在优化器的元学习中，我们希望在每步迭代的目标是$L(\\theta)$最小，具体的目标函数为：\n$$\nL(\\phi) = E_f [ \\sum_{t=1}^{T} w_t L(\\theta_t) ]\n\\\\\n\\theta_t = \\theta_{t-1} + g_t\n\\\\\n[g_t: h_t] = LSTM( \\bigtriangledown L(\\theta_{t-1}), h_{t-1}, \\phi)\n$$\n其中$T$为最大迭代次数，$w_t>0$为每一步的权重，一般可以设置$w_t=1,\\forall t$。由于LSTM网络可以记忆梯度的历史信息，学习到的优化器可以看做是一个高阶的优化方法。\n\n#### 模型无关的元学习\n\n模型无关的元学习（Model-Agnostic Meta-Learning， MAML）是一个简单的模型无关、任务无关的元学习算法。假设所有的任务都来自一个任务空间，其分布为$p(T)$，我们可以在这个任务空间的所有任务上学习一种通用的表示，这种表示可以经过梯度下降方法在一个特定的单任务上进行精调。假设一个模型为$f(\\theta)$，如果我们让这个模型适应到一个新任务$T_m$上，通过一步或多步的梯度下降更新，学习到的任务适配参数为：\n$$\n\\theta_m ' = \\theta- \\alpha \\bigtriangledown _\\theta L_{T_m}(f_\\theta)\n$$\n其中$\\alpha$为学习率，这里的$\\theta_m'$可以理解为关于$\\theta$的函数，而不是真正的参数更新。\n\nMAML的目标是学习一个参数$\\theta$使得其经过一个梯度迭代就可以在新任务上达到最好的性能。\n$$\n\\underset{ \\theta }{ min } \\sum_{T_m \\sim  p(T)} L_{T_m}(f(\\theta'_m)) = \\sum_{T_m \\sim  p(T)} L_{T_m} ( f(\\theta - \\alpha \\bigtriangledown _\\theta L_{T_m} (f_\\theta )) )\n$$\n即在所有任务上的元优化（Meta-Optimization）也采用梯度下降来进行优化，即：\n$$\n\\theta \\leftarrow \\theta - \\beta \\bigtriangledown _\\theta \\sum_{m=1}^{M} L_{T_m}(f_{\\theta_m'})\n$$\n其中$\\beta$为元学习率，这里为一个真正的参数更新步骤。需要计算关于$\\theta$的二阶梯度，但用一级近似通常也可以达到比较好的性能。\n\nMAML的具体过程算法如下：\n![MAML的具体过程算法](https://img-blog.csdnimg.cn/20191112155840241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n### 总结\n\n目前神经网路的学习机制主要是以监督学习为主，这种学习方式得到的模型往往是定向的，也是孤立的，每个任务的模型都是从零开始训练的，一切知识都需要从训练数据中得到，导致每个任务都需要大量的训练数据。本章主要介绍了一些和模型无关的学习方式，包括集成学习、自训练和协同训练、多任务学习、迁移学习、元学习，这些都是深度学习中研究的重点。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"【论文】文本相似度计算方法综述","url":"/2019/11/07/论文/【论文】文本相似度计算方法综述/","content":"\n\n# 概述\n在信息爆炸时代，人们迫切希望从海量信息中获取与自身需要和兴趣吻合度高的内容，为了满足此需求，出现了多种技术，如：搜索引擎、推荐系统、问答系统、文档分类与聚类、文献查重等，而这些应用场景的关键技术之一就是文本相似度计算技术。因此了解文本相似度的计算方法是很有必要的。\n\n# 文本相似度定义\n文本相似度在不同领域被广泛讨论，由于应用场景不同，其内涵有所差异，故没有统一、公认的定义。\n\nLin从信息论的角度阐明相似度与文本之间的共性和差异有关，共性越大、差异越小、则相似度越高；共性越小、差异越大、则相似度越低。相似度最大的情况是文本完全相同。同时提出基于假设推论出相似度定理，如下所示：\n$$\nSim(A,B) = \\frac{ log P(common(A,B)) } {log P(description(A,B))}\n$$\n其中，common(A,B)是A和B的共性信息，description(A,B)是描述A和B的全部信息，上述公式表达出相似度与文本共性成正相关。 由于没有限制领域，此定义被采用较多。\n\n> 相关度与相似度是容易混淆的概念，大量学者没有对此做过对比说明。相关度体现在文本共现或者以任何形式相互关联（包括上下位关系、同义关系、反义关系、部件-整体关系、值-属性关系等）反映出文本的组合特点。而相似度是相关度的一种特殊情况，包括上下位关系和同义关系。由此得出，文本的相似度越高，则相关度越大，但是相关度越大并不能说明相似度高。\n\n相似度一般用[0,1]表示，该实数可以通过语义距离计算获得。相似度与语义距离呈反比关系，语义距离越小，相似度越高；语义距离越大，相似度越低。通常用下面的公式表示相似度与语义距离的关系。\n$$\nSim(S_A,S_B) = \\frac {\\alpha} { Dis(S_A,S_B) + \\alpha }\n$$\n其中，$Dis(S_A,S_B)$表示文本$S_A,S_B$之间的非负语义距离，$\\alpha$为调节因子，保证了当语义距离为0时上述公式的意义。\n\n文本相似度计算中还有一个重要的概念是文本表示，代表对文本的基本处理，目的是将半结构化或非结构化的文本转化为计算机可读形式。**文本相似度计算方法的不同本质是文本表示方法不同**\n\n# 文本相似度计算方法\n![文本相似度计算方法](https://img-blog.csdnimg.cn/20191105152528460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n文本相似度计算方法可分为四大类：\n- 基于字符串的方法（String-Based）\n- 基于语料库的方法（Corpus-Based）\n- 基于世界知识的方法（Knowledge-Based）\n- 其他方法\n\n## 基于字符串的方法\n该方法从字符串匹配度出发，以字符串共现和重复程度为相似度的衡量标准。根据计算粒度不同，可以将该方法分为**基于字符的方法**和**基于词语的方法**。下图列出两种方法常见的算法以及思路\n\n![基于字符串的方法](https://img-blog.csdnimg.cn/20191105153106412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n基于字符串的方法是在字面层次上的文本比较，文本表示即为原始文本，该方法原理简单，易于实现，现已成为其他方法的计算基础。\n\n但不足的是将字符或词语作为独立的知识单元，并未考虑词语本身的含义和词语之间的关系。以同义词为例，尽管表达不同，但具有相同的含义，而这类词语的相似度依靠基于字符串的方法并不能准确计算。\n\n## 基于语料库的方法\n\n基于语料库的方法利用语料库中获取的信息计算文本相似度。基于语料库的方法可以划分为：\n- 基于词袋模型的方法\n- 基于神经网络的方法\n- 基于搜索引擎的方法\n\n### 基于词袋\n词袋模型（Bag of Words Model，BOW）建立在分布假说的基础上，即“词语所处的上下文语境相似，其语义则相似”。其基本思想是：不考虑词语在文档中出现的顺序，将文档表示成一系列词语的组合。\n\n根据考虑的语义成程度不同，基于词袋模型的方法主要包括：\n- 向量空间模型（Vector Space Model，VSM）\n- 潜在语义分析（Latent Semantic Analysis，LSA）\n- 概率潜在语义分析（Probabilistic Latent Semantic Analysis，PLSA）\n- 潜在狄利克雷分布（Latent Dirichlet Allocation，LDA）\n\n#### VSM\nVSM模型的基本思想是将每篇文档表示成一个基于词频或者词频-逆文档频率权重的实值向量，那么N篇文档则构成n维实值空间，其中空间的每一维都对用词项，每一篇文档表示该空间的一个点或者向量。两个文档之间的相似度就是两个向量的距离，一般采用余弦相似度方法计算。\n\nVSM有两个明显的缺点：一是该方法基于文本中的特征项进行相似度计算，当特征项较多时，产生的高维稀疏矩阵导致计算效率不高；二是向量空间模型算法的假设是文本中抽取的特征项没有关联，不符合文本语义表达。\n\n#### LSA，PLSA\n\nLSA算法的基本思想是将文本从稀疏的高维词汇空间映射到低维的潜在语义空间，在潜在的语义空间计算相似性。LSA是基于VSM提出的，两种方法都是采用空间向量表示文本，但LSA使用潜在语义空间，利用奇异值分解提高对高维的词条-文档矩阵进行处理，去除了原始向量空间的某些“噪音”，使数据不再稀疏。Hofmann在LSA的基础上引入主题层，采用期望最大化算法（EM）训练主题。\n\nLSA本质上是通过降维提高计算准确度，但该算法复杂度比较高，可移植性差，比较之下，PLSA具备统计基础，多义词和同义词在PLSA中分别被训练到不同的主题和相同的主题，从而避免了多义词，同义词的影响，使得计算结构更加准确，但不适用于大规模文本。\n\n#### LDA\nLDA主题模型是一个三层的贝叶斯概率网络，包含词、主题和文档三层结构。采用LDA计算文本相似性的基本思想是对文本进行主题建模，并在主题对应的词语分布中遍历抽取文本中的词语，得到文本的主题分布，通过此分布计算文本相似度。\n\n以上三类尽管都是采用词袋模型实现文本表示，但是不同方法考虑的语义程度有所不同。基于向量空间建模的方法语义程度居中，加入潜在语义空间概念，解决了向量空间模型方法的稀疏矩阵问题并降低了多义词，同义词的影响。基于LDA的主题模型的方法语义程度最高，基于相似词语可能属于统一主题的理论，主题经过训练得到，从而保证了文本的语义性。\n\n### 基于神经网络\n基于神经网络生成词向量计算文本相似度是近些年提的比较多的。不少产生词向量的模型和工具也被提出，比如Word2Vec和GloVe等。词向量的本质是从未标记的非结构文本中训练出一种低维实数向量，这种表达方式使得类似的词语在距离上更为接近，同时较好的解决了词袋模型由于词语独立带来的维数灾难和语义不足问题。\n\n基于神经网络方法与词袋模型方法的不同之处在于表达文本的方式。词向量是经过训练得到的低维实数向量，维数可以认为限制，实数值可根据文本距离调整，这种文本表示符合人理解文本的方式，所以基于词向量判断文本相似度的效果有进一步研究空间。\n\n### 基于搜索引擎\n基本原理是给定搜索关键词$x,y$，搜索引擎返回包含 $x,y$的网页数量$f(x),f(y)$以及同时包含$x,y$的网页数量$f(x,y)$，计算谷歌相似度距离如下所示:\n$$\nNGD(x,y) = \\frac { G(x,y) - min(G(x),G(y)) } { max(G(x),G(y)}\n\\\\\n= \\frac {max\\{ log \\,f(x), log\\,f(y) \\} - log \\, f(x,y)} { log \\, N - min{log \\,f(x), log \\, f(y)} }\n$$\n\n但是该方法最大的不足是计算结果完全取决于搜索引擎的查询效果, 相似度因搜索引擎而异\n\n## 基于世界知识的方法\n\n基于世界知识的方法是利用具有规范组织体系的知识库计算文本相似度，一般分为两种：基于本体知识和基于网络知识。\n### 基于本体知识\n\n文本相似度计算方法使用的本体不是严格的本体概念, 而指广泛的词典、叙词表、词汇表以及狭义的本体。由于本体能够准确地表示概念含义并能反映出概念之间的关系, 所以本体成为文本相似度的研究基础[7]。最常利用的本体是通用词典, 例如 WordNet、《知网》(HowNet)和《同义词词林》等, 除了词典还有一些领域本体, 例如医疗本体、电子商务本体、地理本体、农业本体等。\n\n结合Hliaoutaki、Batet等的研究，将基于本体的文本相似度算法概括为四种：\n- 基于距离\n- 基于内容\n- 基于属性\n- 混合式相似度\n\n下表列出了各种方法的基本原理、代表方法和特点\n![基于本体的文本相似度算法](https://img-blog.csdnimg.cn/20191105162644574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n### 基于网络知识\n\n由于本体中词语数量的限制，有些学者开始转向基于网络知识方法的研究，原因是后者覆盖范围广泛、富含丰富的语义信息、更新速度相对较快，使用最多的网络知识是维基百科、百度百科。网络知识一般包括两种结构，分别是词条页面之间的链接和词条之间的层次结构。\n\n基于网络知识的文本相似度计算方法大多利用页面链接或层次结构，能较好的反映出词条的语义关系。但其不足在于：词条与词条的信息完备程度差异较大，不能保证计算准确度，网络知识的生产方式是大众参与，导致文本缺少一定的专业性。\n\n\n## 其他方法 \n除了基于字符串、基于语料库和基于世界知识的方法, 文本相似度计算还有一些其他方法，比如：\n- 句法分析\n- 混合方法\n\n# 总结\n本文总结了文本相似度计算的四种方法，以及他们的优缺点。作者认为今后文本相似度的计算方法趋势有三个方向，分别是：\n- 基于神经网络的方法研究将更加丰富\n- 网络资源为文本相似度计算方法研究提供更多支持\n- 针对特定领域以及跨领域文本的相似度计算将成为今后发展的重点\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n---\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["论文"],"categories":["技术篇"]},{"title":"无监督学习中的无监督特征学习、聚类和密度估计","url":"/2019/11/05/深度学习/神经网络笔记/无监督学习中的无监督特征学习、聚类和密度估计/","content":"\n## 无监督学习概述\n无监督学习（Unsupervised Learning）是指从无标签的数据中学习出一些有用的模式，无监督学习一般直接从原始数据进行学习，不借助人工标签和反馈等信息。典型的无监督学习问题可以分为以下几类：\n- 无监督特征学习（Unsupervised Feature Learning）\n> 从无标签的训练数据中挖掘有效的特征表示，无监督特征学习一般用来进行降维，数据可视化或监督学习前期的特征预处理。\n\n- 密度估计（Density Estimation）\n> 是根据一组训练样本来估计样本空间的概率密度。密度估计可以分为：参数密度估计和非参数密度估计。参数密度估计是假设数据服从某个已知概率密度函数形式的分布，然后根据训练样本去估计该分布的参数。非参数密度估计是不假设服从某个概率分布，只利用训练样本对密度进行估计，可以进行任意形状的密度估计，非参数密度估计的方法包括：直方图、核密度估计等。\n\n- 聚类（Clustering）\n> 是将一组样本根据一定的准则划分到不同的组。一个通用的准则是组内的样本相似性要高于组间的样本相似性。常见的聚类方法包括：KMeans、谱聚类、层次聚类等。\n\n聚类大家已经非常熟悉了，下文主要介绍无监督特征学习和概率密度估计。\n\n## 无监督特征学习\n\n无监督特征学习是指从无标注的数据中自动学习有效的数据表示，从而能够帮助后续的机器学习模型达到更好的性能。无监督特征学习主要方法有：\n- 主成分分析\n- 稀疏编码\n- 自编码器\n\n### 主成分分析\n\n主成分分析（Principal Component Analysis，PCA）是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。以下部分摘自于 https://zhuanlan.zhihu.com/p/32412043\n\n#### PCA中的最大可分性思想\nPCA降维，用原始样本数据中最主要的方面代替原始数据，最简单的情况是从2维降到1维，如下图所示，我们希望找到某一个维度方向，可以代表两个维度的数据，图中列了两个方向 $u_1, u_2$，那么哪个方向可以更好的代表原始数据呢？\n\n![最大可分性示例](https://img-blog.csdnimg.cn/2019110418180564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n从直观上看，$u_1$比$u_2$好，这就是所说的最大可分性。\n\n#### 基变换\n![基变换](https://img-blog.csdnimg.cn/20191104182446889.jpg)\n\n其中$p_i \\in {p_1, p_2, ..., p_R}$，$p_i \\in R^{1*N}$是一个行向量，表示第i个基，$a_j \\in {a_1, a_2, ..., a_M}$，$a_i \\in R^{N*1}$是一个列向量，表示第$j$个原始数据记录，特别要注意的是，这里R可以小于N，而R决定了变维后数据的维数。\n\n从上图和文字解释我们可以得到一种矩阵相乘的物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学习矩阵相乘时，只是简单的记住了相乘的规则，但并不清楚其背后的物理意义。\n\n#### 方差\n\n如何考虑一个方向或者基是最优的，看下图：\n\n![周志华机器学习插图](https://img-blog.csdnimg.cn/20191104184311912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n我们将所有的点向两条直线做投影，基于前面PCA最大可分性思想，我们要找的是降维后损失最小，可以理解为投影后数据尽可能的分开，那么在数学中去表示数据的分散使用的是方差，我们都知道方差越大，数据越分散，方差的表达式如下：\n\n$$\nVar(a) = \\frac{1}{m} \\sum_{i=1}^{m} (a_i - \\mu)^2\n$$\n其中$\\mu$为样本均值，如果提前对样本做去中心化，则方差表达式为：\n$$\nVar(a) = \\frac{1}{m} \\sum_{i=1}^{m} (a_i)^2\n$$\n\n到现在，我们知道了以下几点：\n- 对原始数据进行（线性变换）基变换可以对原始样本给出不同的表示\n- 基的维度小于样本的维度可以起到降维的作用，\n- 对基变换后新的样本求其方差，选取使其方差最大的基\n\n那么再考虑另外一个问题？\n> 上面只是说明了优化目标，但并没有给出一个可行性的操作方案或者算法，因为只说明了要什么，但没说怎么做，所以继续进行探讨。\n\n#### 协方差\n从二维降到一维可以采用方差最大来选出能使基变换后数据分散最大的方向（基），但遇到高纬的基变换，当完成第一个方向（基）选择后，第二个投影方向应该和第一个“几乎重合在一起”，这样显然是没有用的，要有其他的约束，我们希望两个字段尽量表示更多的信息，使其不存在相关性。\n\n数学上使用协方差表示其相关性。\n$$\nCov(a,b)= \\frac{1}{m} \\sum_{i=1}^{m}a_i b_i\n$$\n当Cov(a,b)=0时表示两个字段完全独立，也是我们优化的目标。\n> 注意这里的 $a_i,b_i$是经过去中心化处理的。\n\n#### 协方差矩阵\n我们想要达到的目标与字段内方差及协方差有密切的关系，假如只有a、b两个字段，将他们按行组成矩阵X，表示如下：\n\n![矩阵X](https://img-blog.csdnimg.cn/20191104190715730.png)\n\n然后用X乘以X的转置矩阵，并乘以系数 $\\frac{1}{m}$得：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20191104190820539.png)\n\n可见，协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差，而其他元素是a 和 b的协方差，然后会发现两者被合并到了一个矩阵内。\n\n####  协方差矩阵对角化\n\n我们的目标是使$\\frac{1}{m}\\sum_{i=1}^{m}a_ib_i=0$，根据上述的推导，可以看出优化目标是$C=\\frac{1}{m}XX^T$等价于协方差矩阵对角化。即除对角线外的其他元素（如$\\frac{1}{m} \\sum_{i=1}^{m}a_i b_i$）化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达成了优化目的。\n\n这样说可能不是很明晰，我们进一步看下原矩阵和基变换后矩阵协方差矩阵的关系：\n\n设原始数据矩阵为X，对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：\n$$\nD=\\frac{1}{m}YY^T\n\\\\\n= \\frac{1}{m}(PX)(PX)^T\n\\\\\n= \\frac{1}{m} PXX^TP^T\n\\\\\n=P(\\frac{1}{m} XX^T)P^T\n\\\\\n= PCP^T\n\\\\\n=P \\begin{pmatrix}\n\\frac{1}{m} \\sum_{i=1}^{m} a_i^2 & \\frac{1}{m} \\sum_{i=1}^{m} a_i b_i \\\\ \n \\frac{1}{m} \\sum_{i=1}^{m} a_ib_i  & \\frac{1}{m} \\sum_{i=1}^{m} b_i^2 \n\\end{pmatrix} P^T\n$$\n\n可见我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。\n\n我们希望投影后的方差最大化，于是优化目标可以改写为：\n$$\n\\underset{P}{max} \\, tr(PCP^T)\n\\\\\ns.t. \\,PP^T=I\n$$\n利用拉格朗日函数可以得到：\n$$\nJ(P) = tr(PCP^T) + \\lambda(PP^T - I)\n$$\n对P求导有$CP^T + \\lambda P^T = 0$，整理得：\n$$\nCP^T = (- \\lambda) P^T\n$$\n于是，只需对协方差矩阵C进行特征分解，对求得的特征值进行排序，再对 $P^T = (P_1, P_2, ..., P_R)$取前K列组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。\n\n#### PCA算法流程\n从上边可以看出，求样本$x_i$的$n'$维的主成分，其实就是求样本集的协方差矩阵$\\frac{1}{m}XX^T$的前$n'$维个特征值对应特征向量矩阵P，然后对于每个样本$x_i$，做如下变换$y_i = P x_i$，即达到PCA降维的目的。\n\n具体的算法流程如下：\n- 输入：n维的样本集 $X=(x_i, x_2,...,x_m)$，要降维到的维数$n'$\n- 输出：降维后的维度Y\n\n1. 对所有的样本集去中心化 $x_i = x_i - \\frac{1}{m} \\sum_{j=1}^{m}x_j$\n2. 计算样本的协方差矩阵$C = \\frac{1}{m}XX^T$\n3. 求出协方差矩阵对应的特征值和对应的特征向量\n4. 将特征向量按照特征值从大到小，从上到下按行排列成矩阵，取前k行组成矩阵P\n5. $Y=PX$即为降维到K维之后的数据\n\n注意：有时候降维并不会指定维数，而是指定一个比例$t$，比如降维到原先的t比例。\n\n#### PCA算法总结\nPCA算法的主要优点：\n- 仅仅需要以方差衡量信息量，不受数据集意外因素的影响\n- 各主成分之间正交，可消除原始数据各成分间的相互影响的因素\n- 方法设计简单，主要运算是特征值分解，易于实现\n\nPCA算法的主要缺点：\n- 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的可解释性强\n- 方差小的非主成分也可能包含对样本差异的重要信息，因降维丢弃可能会对后续数据处理有影响\n- 当样本特征维度较大时，需要巨大的计算量（比如，10000*10000，这时候就需要SVD[奇异值分解]，SVD不仅可以得到PCA降维的结果，而且可以大大的减小计算量）\n\n### 稀疏编码\n#### 稀疏编码（Sparse Coding）介绍 \n\n在数学上，线性编码是指给定一组基向量$A=[a_1,a_2,...,a_p]$，将输入样本$x\\in R$表示为这些基向量的线性组合\n$$\nx = \\sum _{i=1}^{p} z_i a_i = Az\n$$\n其中基向量的系数$z=[z_1,...,z_p]$称为输入样本x的编码，基向量A也称为字典（dictionary）。\n\n编码是对d维空间中的样本x找到其在p维空间中的表示（或投影），其目标通常是编码的各个维度都是统计独立的，并且可以重构出输入样本。编码的关键是找到一组“完备”的基向量A，比如主成分分析等。但是是主成分分析得到的编码通常是稠密向量，没有稀疏性。\n\n> 如果p个基向量刚好可以支撑p维的欧式空间，则这p个基向量是完备的，如果p个基向量可以支撑d维的欧式空间，并且p>d，则这p个基向量是过完备，冗余的。\n<br>\n“过完备”基向量一般指的是基向量个数远大于其支撑空间维度，因此这些基向量一般是不具备独立，正交等性质。\n\n给定一组N个输入向量$x^1, ..., x^N$，其稀疏编码的目标函数定义为：\n$$\nL(A,Z)= \\sum _{n=1}^{N}( || x^n - Az^n || ^2 + \\eta \\rho (z^n))\n$$\n其中$\\rho(.)$是一个稀疏性衡量函数，$\\eta$是一个超参数，用来控制稀疏性的强度。\n\n对于一个向量$z \\in R$，其稀疏性定义为非零元素的比例。如果一个向量只有很少的几个非零元素，就说这个向量是稀疏的。稀疏性衡量函数$\\rho(z)$是给向量z一个标量分数。z越稀疏，$\\rho(z)$越小。\n\n稀疏性衡量函数有多种选择，最直接的衡量向量z稀疏性的函数是$l_0$范式\n$$\n\\rho(z) = \\sum _{i=1}^{p} I(|z_i| > 0)\n$$\n但$l_0$范数不满足连续可导，因此很难进行优化，在实际中，稀疏性衡量函数通常选用$l_1$范数\n$$\n\\rho(z) = \\sum _{i=1}^{p} |z_i|\n$$\n或对数函数\n$$\n\\rho(z) = \\sum _{i=1}^{p} log(1+z_i^2)\n$$\n或指数函数\n$$\n\\rho(z) = \\sum _{i=1}^{p} -exp(-z_i^2)\n$$\n\n#### 训练方法\n给定一组N个输入向量$x^1, ... , x^N$，需要同时学习基向量A以及每个输入样本对应的稀疏编码$z^1, ...,z^N$。\n\n稀疏编码的训练过程一般用交替优化的方法进行（这一点和ALS很相似）。\n\n（1）固定基向量A，对每个输入$x^n$ ，计算其对应的最优编码（原内容为减去稀疏性衡量函数，觉得不对）\n$$\n\\underset{x^n}{min} || x^n - Az^n ||^2 + \\eta \\rho (z^n), \\forall n \\in [1,N]\n$$\n（2）固定上一步得到的编码$z^1, ...,z^N$，计算其最优的基向量\n$$\n\\underset{A}{min} \\sum _{i=1}^{N} ( || x^n - Az^n ||^2 ) + \\lambda \\frac{1}{2} ||A||^2\n$$\n其中第二项为正则化项，$\\lambda$为正则化项系数。\n\n#### 稀疏编码优缺点\n\n稀疏编码的每一维都可以看作是一种特征，和基于稠密向量的分布式表示相比，稀疏编码具有更小的计算量和更好的可解释性等优点。\n\n**计算量** 稀疏性带来的最大好处就是可以极大的降低计算量\n\n**可解释性** 因为稀疏编码只有少数的非零元素，相当于将一个输入样本表示为少数几个相关的特征，这样我们可以更好的描述其特征，并易于理解\n\n**特征选择** 稀疏性带来的另一个好处是可以实现特征的自动选择，只选择和输入样本相关的最少特征，从而可以更好的表示输入样本，降低噪声并减轻过拟合\n\n### 自编码器\n自编码器（Auto-Encoder，AE）是通过无监督的方式来学习一组数据的有效编码。\n\n假设有一组d维的样本$x^n \\in R^d, 1 \\leq n \\leq N$，自编码器将这组数据映射到特征空间得到每个样本的编码$z^n \\in R^p, 1 \\leq n \\leq N$，并且希望这组编码可以重构出原来的样本。\n\n自编码器的结构可分为两部分：编码器（encoder）：$f: R^d -> R^p$和解码器（decoder）：$R^p -> R^d$\n\n自编码器的学习目标是最小化重构误差（reconstruction errors）\n\n$$\nL = \\sum_{n=1}^{N} || x^n -g(f(x^n)) ||^2 = \\sum || x^n -f \\cdot  g(x^n) ||^2\n$$\n如果特征空间的维度p小雨原始空间的维度d，自编码器相当于是一种降维或特征抽取方法。如果$p \\geq d$，一定可以找到一组或多组解使得$f \\cdot g$为单位函数（Identity Function），并使得重构错误为0。但是这样的解并没有太多的意义，但是如果再加上一些附加的约束，就可以得到一些有意义的解，比如编码的稀疏性、取值范围，f和g的具体形式等。如果我们让编码只能取k个不同的值（k<N），那么自编码器就可以转换为一个k类的聚类问题。\n\n最简单的自编码器如下图所示的两层神经网络，输入层到隐藏层用来编码，隐藏层到输出层用来解码，层与层之间互相全连接。\n\n![最简单的自编码器](https://img-blog.csdnimg.cn/20191104162315242.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n对于样本x，中间隐藏层为编码：\n$$\nz = s(W^1 x + b^l)\n$$\n输出为重构的数据\n$$\nx' = s(W^2 z + b^l)\n$$\n其中$W,b$为网格参数，$s(.)$为激活函数。如果令$W^2$等于$W^1$的转置，即$W^2=W^{(1)T}$，称为捆绑权重（tied weights）。\n\n给定一组样本 $x^n \\in [0,1]^d, 1 \\leq n \\leq N$，其重构错误为：\n$$\nL = \\sum_{n=1}^{N} || x^n -x^{'n} ||^2 + \\lambda ||W||_F^2\n$$\n其中$\\lambda$为正则化系数，通过最小化重构误差，可以有效的学习网格的参数。\n\n我们使用自编码器是为了得到有效的数据表示，因此在训练数据后，我们一般去掉解码器，只保留编码器，编码器的输出可以直接作为后续机器学习模型的输入。\n\n### 稀疏自编码器\n\n自编码器除了可以学习低维编码之外，也学习高维的稀疏编码。假设中间隐藏层z的维度为p，大于输入样本的维度，并让z尽量稀疏，这就是稀疏自编码器（Sparse Auto-Encoder）。和稀疏编码一样，稀疏自编码器的优点是有很高的模型可解释性，并同时进行了隐式的特征选择。\n\n通过给自编码器中隐藏单元z加上稀疏性限制，自编码器可以学习到数据中一些有用的结构。\n\n### 堆叠自编码器\n\n对于很多数据来说，仅使用两层神经网络的自编码器还不足以获取一种好的数据表示，为了获取更好的数据表示，我们可以使用更深层的神经网络。深层神经网络作为自编码器提取的数据表示一般会更加抽象，能够很好的捕捉到数据的语义信息。在实践中经常使用逐层堆叠的方式来训练一个深层的自编码器，称为堆叠自编码器（Stacked Auto-Encoder，SAE）。堆叠自编码一般可以采用逐层训练（layer-wise training）来学习网络参数。\n\n### 降噪自编码器\n\n降噪自编码器（Denoising Autoencoder）就是一种通过引入噪声来增加编码鲁棒性的自编码器。对于一个向量x，我们首先根据一个比例$\\mu$随机将x的一些维度的值设置为0，得到一个被损坏的向量$\\tilde x$。然后将被损坏的向量$\\tilde x$输入给自编码器得到编码z，并重构原始的无损输入x。\n\n下图给出了自编码器和降噪自编码器的对比，其中$f_{\\theta}$为编码器，$g_{\\theta^’}$为解码器，$L(x,x')$为重构错误。\n\n![自编码器和降噪自编码器的对比](https://img-blog.csdnimg.cn/20191104175727219.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n降噪自编码器的思想十分简单，通过引入噪声来学习更鲁棒性的数据编码，并提高模型的泛化能力。\n\n## 概率密度估计\n概率密度估计（Probabilistic Density Estimation）简称密度估计（Density Estimation），是基于一些观测样本来估计一个随机变量的概率密度函数。密度估计在机器学习和数学建模中应用十分广泛。\n\n概率密度估计分为：\n- 参数密度估计\n- 非参数密度估计\n\n### 参数密度估计\n\n参数密度估计（Parametric Density Estimation）是根据先验知识假设随机变量服从某种分布，然后通过训练样本来估计分布的参数。\n\n令 $D = {\\{x^n\\}}_{i=1}^{N}$为某个未知分布中独立抽取的N个训练样本，假设这些样本服从一个概率分布函数$p(x|\\theta)$，其对数似然函数为：\n$$\nlog\\,p(D|\\theta) = \\sum_{n=1}^{N}log\\,p(x^n|\\theta)\n$$\n\n要估计一个参数$\\theta ^{ML}$来使得：\n$$\n\\theta ^{ML} = \\underset{\\theta}{arg\\,max } \\sum_{n=1}^{N}log\\,p(x^n|\\theta)\n$$\n这样参数估计问题就转化为最优化问题。\n\n#### 正态分布中的参数密度估计\n假设样本$x \\in X$服从正态分布 $X \\sim N(\\mu,\\sigma^2)$，正态分布的表达式如下：\n$$\nX \\sim N(\\mu,\\sigma^2) = \\frac{1}{ \\sqrt{2\\pi} \\sigma^2} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n求 $\\mu,\\sigma^2$的最大似然估计量。\n\n$X$的概率密度为：\n$$\nf(x;\\mu,\\sigma^2) = \\frac{1}{ \\sqrt{2\\pi} \\sigma^2} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n似然函数为：\n$$\nL(\\mu,\\sigma^2) = \\prod_{i=1}^{N} \\frac{1}{ \\sqrt{2\\pi} \\sigma^2} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\\\\n= (2\\pi)^{-\\frac{N}{2}} (\\sigma^2)^{-\\frac{N}{2}} e^{(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (x_i - \\mu)^2)}\n$$\n对其求导可得对数似然函数为：\n$$\nLn\\, L =-\\frac{N}{2} ln(2\\pi)-\\frac{N}{2} ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N}(x_i - \\mu)^2\n$$\n令：\n$$\n\\left\\{\\begin{matrix}\n\\frac{\\partial }{\\partial \\mu }ln\\, L = \\frac{1}{\\sigma^2} (\\sum_{i=1}^{N} x_i -N\\mu ) =0 & \\\\ \n\\\\\n\\frac{\\partial }{\\partial \\sigma^2 }ln\\, L = - \\frac{N}{2\\sigma^2} + \\frac{1}{ (2\\sigma^2)^2} \\sum_{i=1}^{N}(x_i-\\mu)^2 =0& \n\\end{matrix}\\right.\n$$\n由前一式解得$\\tilde{\\mu}=\\frac{1}{N}\\sum_{i=1}^{N}x_i = \\bar{\\mu}$，代入后一式得$\\tilde{\\sigma^2}=\\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\bar{x})^2$，因此得$\\mu,\\sigma^2$的最大似然估计为：\n$$\n\\tilde{\\mu} = \\bar{X},\\tilde{\\sigma^2}=\\frac{1}{N}(x_i - \\bar{x})^2\n$$\n\n#### 多项分布中的参数密度估计\n假设样本服从K个状态的多态分布，令onehot向量$x\\in[0,1]^K$来表示第K个状态，即$x_k=1$，其余$x_{i,k \\neq k}=0$，则样本x的概率密度函数为：\n$$\np(x|\\mu) = \\prod_{k=1}^{K}\\mu_k ^{x_K}\n$$\n其中$\\mu_k$为第k个状态的概率，并且满足$\\sum_{k=1}^{K} \\mu_k =1$。\n\n数据集$D={\\{x^n\\}}_{n=1}^{N}$的对数似然函数为：\n$$\nlog(D|\\mu) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} x_n ^k log (\\mu _k)\n$$\n多项分布的参数估计为约束优化问题，引入拉格朗日乘子$\\lambda$，将原问题转化为无约束优化问题。\n\n$$\n\\underset{\\mu, \\lambda}{ max} \\sum_{n=1}^{N} \\sum_{k=1}^{K} x_k ^n log(\\mu_k) + \\lambda (\\sum_{k=1}^{K} \\mu_k -1)\n$$\n上式分别对$\\mu_k,\\lambda$求偏，并令其等于0，得到：\n$$\n\\mu_k ^{ML} = \\frac{m_k}{N}, 1 \\leq N \\leq K\n$$\n其中$m_k = \\sum_{n=1}^{N} x_k ^n$为数据集中取值为第k个状态的样本数量。\n\n在实际应用中，参数密度估计一般存在两个问题：\n- （1）模型选择问题，即如何选择数据分布的密度函数，实际的数据分布往往是非常复杂的，而不是简单的正态分布或者多项分布。\n- （2）不可观测变量问题，即我们用来训练数据的样本只包含部分的可观测变量，还有一些非常关键的变量是无法观测的，这导致我们很难估计数据的真实分布。\n- （3）维度灾难问题，即高维的参数估计十分困难。随着维度的增加，估计参数所需要的样本量呈指数增加。在样本不足时会出现过拟合。\n\n\n#### 非参数密度估计\n\n非参数密度估计（Nonparametric Density Estimation）是不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数。\n\n对于高纬空间中的一个随机向量x，假设其服从一个未知分布p(x)，则x落入空间中的小区域R的概率为：  $P=\\int_{R} p(x)dx$。\n\n给定N个训练样本$D=\\{x^n\\}_{n=1}^{N}$，落入区域R的样本数量K服从二项分布：\n$$\nP_K = \\binom{N}{K}P^K(1-P)^{1-K}\n$$\n其中$K/N$的期望为$E[K/N]=P$，方差为$var(K/N)=P(1-P)/N$。当N非常大时，我们可以近似认为：$P\\approx \\frac{K}{N}$，假设区域R足够小，其内部的概率密度是相同的，则有$P\\approx p(x)V$，其中V为区域R的提及，结合前边的两个公式，可得：$p(x)\\approx \\frac{K}{NV}$。\n\n根据上式，要准确的估计p(x)需要尽量使得样本数量N足够大，区域体积V尽可能的小。但在具体的应用中吗，样本数量一般有限，过小的区域导致落入该区域的样本比较少，这样估计的概率密度就不太准确。\n\n因此在实践中估计非参数密度通常使用两种方法：\n- （1）固定区域大小V，统计落入不同区域的数量，这种方式包括直方图和核方法两种\n- （2）改变区域大小，以使得落入每个区域的样本数量为K，这种方法成为K近邻方法\n\n##### 直方图方法\n直方图（Histogram Method）是一种非常直观的估计连续变量密度函数的方法，可以表示为一种柱状图。\n\n以一维随机变量为例，首先将其取值范围划分为M个连续的、不重叠的区间，每个区间的宽度为$\\Delta m$，给定$N$个训练样本，我们统计这些样本落入每个区间的数量$K_m$，然后将他们归一化为密度函数。\n\n$$\np_m = \\frac {K_m}{N\\Delta m},1 \\leq m \\leq  M\n$$\n直方图的关键问题是如何选择一个合适的$\\Delta m$，如果该值太小，那么落入每个区间的样本会特别少，其估计的区间密度也会有很大的随机性，如果该值过大，其估计的密度函数会变得十分平滑。下图给出了两个直方图的例子，其中蓝色表示真实的密度函数，红色表示直方图估计的密度函数。\n\n![直方图估计密度函数](https://img-blog.csdnimg.cn/20191104090336556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n直方图通常用来处理低维随机变量，可以非常快速的对数据的分布进行可视化，但其**缺点**是很难扩展到高维变量，假设一个d维的随机变量，如果每一维都划分为M个空间，那么整个空间的区域数量为$M^d$，直方图估计的方法会随着空间的增大而指数增长，从而形成**维度灾难（Curse Of Dimensionality）**\n\n##### 核方法\n\n核密度估计（Kernel Density Estimation），也叫Parzen窗方法，是一种直方图方法的改进。\n\n假设$R$为$d$维空间中的一个以点x为中心的“超立方体”，并定义核函数\n$$\n\\phi (\\frac{z-x}{h}) = \\left\\{\\begin{matrix}\n1 \\,\\,\\,\\,\\,\\, if \\, |z_i - x_i|< \\frac{h}{2},  1 \\leq i \\leq d & \\\\ \n0 \\,\\,\\,\\,\\,\\, else & \n\\end{matrix}\\right.\n$$\n\n来表示一个样本是否落入该超立方体中，其中$h$为超立方体的边长，也称为核函数的密宽度。\n\n给定$N$个训练样本$D$，落入区域$R$的样本数量$K$为：\n$$\nK = \\sum_{n=1}^{K} \\phi (\\frac {x^n - x}{h})\n$$\n则点$x$的密度估计为：\n$$\np(x) = \\frac{K}{Nh^d} =\\frac{1}{Nh^d} \\sum_{n=1}^{K} \\phi (\\frac {x^n - x}{h})\n$$\n其中$h^d$表示区域$R$的体积。\n\n除了超立方体的核函数意外之外，我们还可以选择更加平滑的核函数，比如高斯核函数：\n$$\n\\phi (\\frac {z-x}{h}) = \\frac {1}{ (2\\pi)^{\\frac{1}{2}} h} exp(- \\frac{||z-x||^2}{2h^2})\n$$\n其中$h^2$可以看做是高斯核函数的方差，这样点$x$的密度估计为：\n$$\np (x) = \\frac{1}{N} \\sum_{n=1}^{N}   \\frac {1}{ (2\\pi)^{\\frac{1}{2}} h} exp(- \\frac{||z-x||^2}{2h^2})\n$$\n\n##### K近邻方法\n\n核密度估计方法中的核宽度是固定的，因此同一个宽度可能对高密度的区域过大，而对低密度的区域过小。一种更加灵活的方式是设置一种可变宽度的区域，并使得落入每个区域中的样本数量固定为K。\n\n要估计点x的密度，首先找到一个以x为中心的球体，使得落入球体的样本数量为K，然后根据公式$p(x)\\approx \\frac{K}{NV}$就可以计算出点x的密度。因为落入球体的样本也是离x最近的K个样本，所以这种方法也称为K近邻（K-Nearest Neughbor）方法。\n\n在K近邻方法中，K值的选择十分重要，如果K太小，无法有效的估计密度函数，而K太大也会使局部的密度不准确，并且会增加计算开销。\n\nK近邻方法也经常用于分类问题，称为K近邻分类器。 当K=1时为最近邻分类器。\n\n最近邻分类器的一个性质是，当 $N \\rightarrow \\infty$，其分类错误率不超过最优分类器错误率的两倍。\n\n## 总结\n无监督学习是一种十分重要的机器学习方法，无监督学习问题主要可以分为聚类，特征学习，密度估计等几种类型。但是无监督学习并没有像有监督学习那样取得广泛的成功，主要原因在于其缺少有效客观评价的方法，导致很难衡量一个无监督学习方法的好坏。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"冷启动中的多避老虎机问题（Multi-Armed Bandit，MAB）","url":"/2019/10/15/RecSys/冷启动/冷启动中的多避老虎机问题（Multi-Armed Bandit，MAB）/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/102560272\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n---\n\n推荐系统中有两个很重要的问题：EE问题和冷启动。在实际的场景中很好的解决这两个问题又很难，比如冷启动，我们可以基于热门、用户、第三方等信息进行半个性化的推荐，但很难去获得用户的真实兴趣分布。那么有没有一种算法可以很好的解决这个问题呢？答案就是：Bandit。\n\n### Bandit算法与推荐系统\n在推荐系统领域里，有两个比较经典的问题常被人提起，一个是EE问题，另一个是用户冷启动问题。\n\nEE问题又叫Exploit-Explore。\n- Exploit表示的是对于用户已经确定的兴趣当然要迎合。\n- Explore表示的如果仅对用户进行兴趣投放，很快就会看厌，所以要不断的探索用户的新兴趣\n\n所以在进行物品推荐时，不仅要投其所好，还要进行适当的长尾物品挖掘。\n\n用户冷启动问题，也就是面对新用户时，如何能够通过若干次实验，猜出用户的大致兴趣。\n\n这两个问题本质上都是如何选择用户感兴趣的主题进行推荐，比较符合Bandit算法背后的MAB问题。\n\n比如，用Bandit算法解决冷启动的大致思路如下：用分类或者Topic来表示每个用户兴趣，也就是MAB问题中的臂（Arm），我们可以通过几次试验，来刻画出新用户心目中对每个Topic的感兴趣概率。这里，如果用户对某个Topic感兴趣（提供了显式反馈或隐式反馈），就表示我们得到了收益，如果推给了它不感兴趣的Topic，推荐系统就表示很遗憾（regret）了。如此经历“选择-观察-更新-选择”的循环，理论上是越来越逼近用户真正感兴趣的Topic的。\n\n### Bandit算法来源\nBandit算法来源于历史悠久的赌博学，它要解决的问题是这样的：\n\n一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, K-armed bandit problem, MAB）。\n\n怎么解决这个问题呢？最好的办法是去试一试，不是盲目地试，而是有策略地快速试一试，这些策略就是Bandit算法。\n\n这个多臂问题，推荐系统里很多问题都与它类似：\n\n- 假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。\n- 假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？\n- 我们的算法工程师又想出了新的模型，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？\n- 如果只是推荐已知的用户感兴趣的物品，如何才能科学地冒险给他推荐一些新鲜的物品？\n\nBandit算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？常见Bandit算法有哪些呢？往下看\n\n### Thompson sampling\n\n#### 1、Beta分布\nThompson Sampling是基于Beta分布进行的，所以首先看下什么是Beta分布？\n\nBeta分布可以看作是一个概率的概率分布，当你不知道一个东西的具体概率是多少时，他可以给出所有概率出现的可能性。Beta是一个非固定的公式，其表示的是一组分布（这一点和距离计算中的闵可夫斯基距离类似）。\n\n比如：\n\n二项分布（抛n次硬币，正面出现k次的概率）\n\n$$\nP(S=k)=\\binom{n}{k} p^k (1-p)^{n-k}\n$$\n\n几何分布（抛硬币，第一次抛出正面所需的次数的概率）\n\n$$\nP(T=t)= (1-p)^{t-1} p \n$$\n\n帕斯卡分布（抛硬币，第k次出现正面所需次数的概率）\n\n$$\nP(Y_k=t)=\\binom{t-1}{k-1} p^{k-1} (1-p)^{t-k}p\n$$\n\n去找一个统一的公式去描述这些分布，就是Beta分布：\n$$\nBeta(x| \\alpha,\\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^\\beta\n$$\n其中 $B(\\alpha, \\beta)$是标准化函数，他的作用是使总概率和为1，$\\alpha, \\beta$为形状参数，不同的参数对应的图像形状不同，他不但可以表示常见的二项分布、几何分布等，还有一个好处就是，不需要去关系某次实验结果服从什么分布，而是利用\n$\\alpha, \\beta$的值就可以计算出我们想要的统计量。\n\n常见的参数对应的图形为：\n\n![Beta分布](https://img-blog.csdnimg.cn/20191015085055652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n$Beta(\\alpha, \\beta)$的常见的统计量为：\n- 众数为：$\\frac {\\alpha-1}{\\alpha + \\beta -1}$\n- 期望为：$\\mu = E(x)= \\frac {\\alpha} {\\alpha + \\beta}$\n- 方差为：$Var(x) = E(x - \\mu)^2 = \\frac { \\alpha \\beta } { (\\alpha + \\beta)^2 (\\alpha + \\beta +1) }$\n\n#### 2、Beta分布的例子\n网上资料中一个很常见的例子是棒球运动员的，这里进行借鉴。\n\n棒球运动有一个指标是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。\n\n现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。\n\n对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。\n\n接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取$\\alpha$=81,$\\beta$=219\n\n![棒球运动员Beta分布例子](https://img-blog.csdnimg.cn/20191015090126646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n之所以取这两个参数是因为：\n- beta分布的均值是：$\\frac{81} {81 + 219}=0.27$\n- 从图中可以看到这个分布主要落在了(0.2,0.35)间，这是从经验中得出的合理的范围。\n\n> 在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率。也就是说beta分布可以看作一个概率的概率分布。\n\n有了这样的初始值，随着运动的进行，其表达式可以表示为：\n$$\nBeta(\\alpha_0 + hits , \\beta_0 + misses)\n$$\n其中 $\\alpha_0, \\beta_0$是一开始的参数，值为81，219。当击中一次球是 hits + 1，misses不变，当未击中时，hits不变，misses+1。这样就可以在每次击球后求其最近的平均水平了。\n\n#### 3、Thompson Smapling\n\nThompson sampling算法简单实用，简单介绍一下它的原理，要点如下：\n- 假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p。\n- 我们不断地试验，去估计出一个置信度较高的“概率p的概率分布”就能近似解决这个问题了。\n- 怎么能估计“概率p的概率分布”呢？ 答案是假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose。\n- 每个臂都维护一个beta分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。\n- 每次选择臂的方式是：计算每个臂现有的beta分布的平均水平，选择所有臂产生的随机数中最大的那个臂去摇。\n\n#### 4、TS的Python实现\n\n```\nimport numpy as np\nimport random\n\ndef ThompsonSampling(wins, trials):\n    pbeta = [0] * N\n    for i in range(0, len(trials)):\n        pbeta[i] = np.random.beta(wins[i] + 1, trials[i] - wins[i] + 1)\n    choice = np.argmax(pbeta)\n    trials[choice] += 1\n    if random.random() > 0.5:\n        wins[choice] += 1\n\nT = 10000  # 实验次数\nN = 10  # 类别个数\n# 臂的选择总次数\ntrials = np.array([0] * N )\n# 臂的收益\nwins = np.array([0] * N )\nfor i in range(0, T):\n    ThompsonSampling(wins, trials)\nprint(trials)\nprint(wins)\nprint(wins/trials)\n```\n\n### UCB\n\n#### 1、UCB的原理\n\nUCB（Upper Confidence Bound，置信区间上界）可以理解为不确定性的程度，区间越宽，越不确定，反之就越确定，其表达式如下：\n\n$$\nscore(i) = \\frac {N_i}{T} + \\sqrt{ \\frac{2 ln T}{N_i}}\n$$\n其中 Ni 表示第i个臂收益为 1 的次数，T表示选择的总次数\n\n公式分为左右两部分，左侧（+左侧部分）表示的是候选臂i到目前为止的平均收益，反应的是它的效果。右侧（+右侧部分）叫做Bonus，本质上是均值的标准差，反应的是候选臂效果的不确定性，就是置信区间的上边界。\n\n> 统计学中的一些统计量表达的含义\n\n如果一个臂的收益很少，即Ni很小，那么他的不确定性就越大，在最后排序输出时就会有优势，bouns越大，候选臂的平均收益置信区间越宽，越不稳定，就需要更多的机会进行选择。反之如果平均收益很大，即+号左侧户数很大，在选择时也会有被选择的机会。\n\n#### 2、UCB的Python实现\n\n```\nimport numpy as np\n\n# 定义 T = 1000 个用户，即总共进行1000次实现\nT = 1000\n# 定义 N = 10 个标签，即 N 个 物品\nN = 10\n\n# 保证结果可复现，设置随机数种子\nnp.random.seed(888)\n\n# 每个物品的累积点击率（理论概率）\ntrue_rewards = np.random.uniform(low=0, high=1, size= N)\n# true_rewards = np.array([0.5] * N)\n# 每个物品的当前点击率\nnow_rewards = np.zeros(N)\n# 每个物品的点击次数\nchosen_count = np.zeros(N)\n\ntotal_reward = 0\n\n# 计算ucb的置信区间宽度\ndef calculate_delta(T, item):\n    if chosen_count[item] == 0:\n        return 1\n    else:\n        return np.sqrt( 2 * np.log( T ) / chosen_count[item])\n\n# 计算UCB\ndef ucb(t, N):\n    # ucb得分\n    upper_bound_probs = [ now_rewards[item] + calculate_delta(t,item) for item in range(N) ]\n    item = np.argmax(upper_bound_probs)\n    # 模拟伯努利收益\n    # reward = sum(np.random.binomial(n =1, p = true_rewards[item], size=20000)==1 ) / 20000\n    reward = np.random.binomial(n =1, p = true_rewards[item])\n    return item, reward\n\nfor t in range(1,T+1):\n    # 为第 t个用户推荐一个物品\n    item, reward = ucb(t, N)\n    # print(\"item is %s, reward is %s\" % (item, reward))\n\n    # 一共有多少用户接受了推荐的物品\n    total_reward += reward\n    chosen_count[item] += 1\n\n    # 更新物品的当前点击率\n    now_rewards[item] =  ( now_rewards[item] * (t-1) + reward) /  t\n    # print(\"更新后的物品点击率为：%s\" % (now_rewards[item]))\n\n    # 输出当前点击率 / 累积点击率\n    # print(\"当前点击率为: %s\" % now_rewards)\n    # print(\"累积点击率为: %s\" % true_rewards)\n\n    diff =  np.subtract( true_rewards, now_rewards)\n    print(diff[0])\n    print(total_reward)\n```\n\n#### 3、UCB的推导\n\n观测 1：假设一个物品被推荐了k次，获取了k次反馈（点击 or 不点击），可以计算出物品被点击的平均概率\n\n当k 接近于正无穷时，p' 会接近于真实的物品被点击的概率\n$$\np' = \\frac {\\sum reward_i}{k}\n$$\n\n观测 2：现实中物品被点击的次数不可能达到无穷大，因此估计出的被点击的概率 p' 和真实的点击的概率 p 总会存在一个差值 d，即：\n$$\np'-d \\leqslant p \\leqslant p'+d\n$$\n最后只需要解决差值 d 到底是怎么计算的？\n\n首先介绍霍夫丁不等式（Chernoff-Hoeffding Bound），霍夫丁不等式假设reward_1, ... , reward_n 是在[0,1]之间取值的独立同分布随机变量，用p' 表示样本的均值，用p表示分布的均值，那么有：\n$$\nP\\{|p'-p| \\leqslant \\delta \\} \\geqslant 1 - 2e^{-2n\\delta ^2}\n$$\n\n当 $\\delta$ 取值为$\\sqrt { 2In T /n}$ （其中 T 表示有物品被推荐的次数，n表示有物品被点击的次数），可以得到：\n$$\nP\\{|p'-p| \\leqslant \\sqrt { \\frac{2In T }{n}} \\} \\geqslant 1 - \\frac{ 2 }{ T^4}\n$$\n\n也就是说：\n$$\np' - \\sqrt { \\frac{2In T }{n}} \\leqslant p \\leqslant p' + \\sqrt { \\frac{2In T }{n}}\n$$\n\n是以 1 - 2/T^4 的概率成立的，\n当T=2时，成立的概率为0.875\n当T=3时，成立的概率为0.975\n当T=4时，成立的概率为0.992\n可以看出 $d =  \\sqrt { \\frac{2In T }{n}}$ 是一个不错的选择。\n\n### Epsilon-Greedy\n\n#### 1、算法原理\n这是一个朴素的Bandit算法，有点类似模拟退火的思想：\n1. 选一个（0,1）之间较小的数作为epsilon；\n2. 每次以概率epsilon做一件事：所有臂中随机选一个；\n3. 每次以概率1-epsilon 选择截止到当前，平均收益最大的那个臂。\n\n是不是简单粗暴？epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只选择收益最大的。\n\n#### 2、Python实现\n\n```\nimport random\n\nclass EpsilonGreedy():\n    def __init__(self, epsilon, counts, values):\n        self.epsilon = epsilon\n        self.counts = counts\n        self.values = values\n\n    def initialize(self, n_arms):\n        self.counts = [0 for col in range(n_arms)]\n        self.values = [0.0 for col in range(n_arms)]\n\n    def select_arm(self):\n        if random.random() > self.epsilon:\n            return self.values.index( max(self.values) )\n        else:\n            # 随机返回 self.values 中个的一个\n            return random.randrange(len(self.values))\n\n    def reward(self):\n        return 1 if random.random() > 0.5 else 0\n\n    def update(self, chosen_arm, reward):\n        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n        n = self.counts[chosen_arm]\n\n        value = self.values[chosen_arm]\n        new_value = ((n - 1)  * value + reward ) / float(n)\n        self.values[chosen_arm] = round(new_value,4)\n\nn_arms = 10\n\nalgo = EpsilonGreedy(0.1, [], [])\n\nalgo.initialize(n_arms)\n\nfor t in range(100):\n    chosen_arm = algo.select_arm()\n    reward = algo.reward()\n    algo.update(chosen_arm, reward)\n\nprint(algo.counts)\nprint(algo.values)\n\n```\n\n### 朴素Bandit算法\n\n最朴素的Bandit算法就是：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。这个算法是人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。\n\nPython实现比较简单，这里就不做演示了。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["冷启动"],"categories":["技术篇"]},{"title":"神经网络中的网络优化和正则化（四）之正则化","url":"/2019/09/22/深度学习/神经网络中的网络化和正则化/神经网络中的网络优化和正则化（四）之正则化/","content":"\n### 引言\n\n神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍（如下所示），本篇为最后一篇主要介绍神经网络中的网络正则化。\n- 第一篇包括\n    - 网络优化和正则化概述\n    - 优化算法介绍\n- 第二篇包括\n    - 参数初始化 \n    - 数据预处理\n    - 逐层归一化\n- 第三篇包括\n    - 超参数优化\n- 第四篇包括\n    - 网络正则化\n\n机器学习模型中的关键是泛化问题，即样本在真实数据集上的期望风险最小化，而在训练集上的经验风险最小化和期望风险并不一致。由于神经网络的拟合能力很强，其在训练集上的训练误差会降的很小，从而导致过拟合。\n\n**正则化（Regularization）**是一类通过限制模型复杂度，从而避免过拟合，提高模型泛化能力的一种方法，包括引入一些约束规则，增加先验，提前终止等。\n\n在传统的机器学习模型中，提高模型泛化能力的主要方法是限制模型复杂度，比如$l_1,l_2$正则，但是在训练深层神经网络时，特别是在过度参数（OverParameterized）时，$l_1,l_2$正则化不如机器学习模型中效果明显，因此会引入其他的一些方法，比如：数据增强，提前终止，丢弃法，继承法等。\n\n### $l_1,l_2$正则\n$l_1,l_2$正则是机器学习中常用的正则化方法，通过约束参数的$l_1,l_2$范数来减少模型在训练数据上的过拟合现象。\n\n通过引入$l_1,l_2$正则，优化问题变为：\n$$\na\\theta ^* = \\underset{a}{ arg \\,  min } \\frac{1}{N} L ( y^n, f(x^n, \\theta))+\\lambda l_p(\\theta)\n$$\n$L$为损失函数，$N$为训练的样本数量，$f(.)$为待学习的神经网络，$\\theta$为参数，$l_p$为$l_1,l_2$正则中的一个，$\\lambda$为正则项系数。\n\n带正则化的优化问题等价于下面带约束条件的优化问题：\n$$\n\\theta ^* = \\underset{a}{ arg \\,  min } \\frac{1}{N} L ( y^n, f(x^n, \\theta))\n\\\\\nsubject \\, to \\, l_p(\\theta) \\leq 1\n$$\n\n下图给出了不同范数约束条件下的最优化问题示例：\n![不同范数约束条件下的最优化问题示例](https://img-blog.csdnimg.cn/20190926205404238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n上图中红线表示$l_p$范数，黑线表示$f(\\theta)$的等高线（简单起见，这里用直线表示）\n\n从上图最左侧图可以看出，$l_1$范数的约束条件往往会使最优解位于坐标轴上，从而使用最终的参数为稀疏向量，此外$l_1$范数在零点不可导，常用下式来代替：\n$$\nl_1(\\theta) = \\sum_{i} \\sqrt{\\theta_i ^2 + \\epsilon } \n$$\n其中$\\epsilon$为一个非常小的常数。\n\n一种折中的方法是**弹性网络正则化（Elastic Net Regularization）** ，同时加入$l_1, l_2$正则，如下：\n$$\na\\theta ^* = \\underset{a}{ arg \\,  min } \\frac{1}{N} L ( y^n, f(x^n, \\theta)_+\\lambda_1 l_1(\\theta) + \\lambda_2 l_2(\\theta)\n$$\n其中$\\lambda_1, \\lambda_2$分别是正则化项的参数。\n\n\n\n### 权重衰减\n**权重衰减（Weight Deacy）** 也是一种有效的正则化方法，在每次调参时，引入一个衰减系数，表示式为：\n$$\n\\theta_t \\leftarrow (1-w)\\theta_{t-1} - \\alpha g_t\n$$\n其中$g_t$为第t次更新时的梯度，$\\alpha$为学习率，$w$为权重衰减系数，一般取值比较小，比如0.0005。在标准的随机梯度下降中，权重衰减和$l_2$正则达到的效果相同，因此，权重衰减在一些深度学习框架中用$l_2$正则来代替。但是在较为复杂的优化方法中，两者并不等价。\n\n### 提前终止\n\n**提前终止（early stop）** 对于深层神经网络而言是一种简单有效的正则化方法，由于深层神经网络拟合能力很强，比较容易在训练集上过拟合，因此在实际操作时往往产出一个和训练集独立的验证集，并用在验证集上的错误来代表期望错误，当验证集上的错误不再下降时，停止迭代。\n\n然而在实际操作中，验证集上的错误率变化曲线并不是一条平衡的曲线，很可能是先升高再降低，因此提前停止的具体停止标准需要根据实际任务上进行优化。\n\n### 丢弃法\n\n当训练一个深层神经网络时，可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法称为 **丢弃法（Dropout Method）**。每次丢弃的神经元为随机的，对于每一个神经元都以一个概率p来判断要不要停留，对于每一个神经层 $y=f(Wx + b)$，我们可以引入一个丢弃函数$d(.)$使得$y=f(Wd(x)+b)$。丢弃函数的定义为：\n$$\nd(x) = \\left\\{\\begin{matrix}\nm \\odot x, When \\, Train\\\\ \npx, When \\,  Test\n\\end{matrix}\\right.\n$$\n其中$m \\in \\{0,1\\}^d$是丢弃掩码（dropout mask），通过以概率为p的贝努力分布随机生成，$p$可以通过一个验证集选取一个最优值，也可以设置为0.5， 这样对大部分网络和任务比较有效。在训练时，神经元的平均数量为原来的$p$倍，而在测试时，所有的神经元都可以是激活的，这会造成训练时和测试时的网络结构不一致，为了缓解这个问题，在测试时，需要将每一个神经元的输出乘以$p$，也相当于把不同的神经网络做了一个平均。\n\n下图给出了一个网络经过dropout的示例。\n![丢弃法示例](https://img-blog.csdnimg.cn/20190926170817842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n一般来讲，对于隐藏层的神经元，丢弃率$p=0.5$时最好，这样当训练时有一半的神经元是丢弃的，随机生成的网络结构具有多样性。对于输入层的神经元，其丢弃率往往设置为更接近于1的数，使得输入变化不会太大，对输入层的神经元进行丢弃时，相当于给数据增加噪声，提高网络的鲁棒性。\n\n丢弃法一般是针对神经元进行随机丢弃，但是也可以扩展到神经元之间的连接进行随机丢弃，或每一层进行随机丢弃。\n\n丢弃法有两种解释：\n\n（1）集成学习的解释\n每做一次丢弃，相当于从原始的网络中采样得到一个子网络，如果一个神经网络有n个神经元，那么可以采样出$2^n$个子网络，每次训练都相当于是训练一个不同的子网络，这些子网络都共享最开始的参数。那么最终的网络可以看成是集成了指数级个不同风格的组合模型。\n\n\n（2）贝叶斯学习的解释\n\n丢弃法也可以解释为一个贝叶斯学习的近似，用$y=f(x,\\theta)$表示一个要学习的网络，贝叶斯学习是假设参数$\\theta$为随机向量，并且先验分布为$q(\\theta)$，贝叶斯方法的预测为：\n$$\nE_{q(\\theta)}[y] = \\int_{q}f(x,\\theta)q(\\theta)d\\theta \\approx \\frac{1}{M}\\sum_{m=1}^{M}f(x, \\theta_m)\n$$\n其中$f(x, \\theta_m)$为第m次应用丢弃方法后的网络，其参数$\\theta_m$为全部参数$\\theta$的一次采样。\n\n### 数据增强\n深层神经网络的训练需要大量的样本才能取得不错的效果，因为在数据量有限的情况下，可以通过 **数据增强（Data Augmentation）**来增加数据量，提高模型鲁棒性，避免过拟合。目前数据增强主要应用在图像数据上，在文本等其他类型的数据还没有太好的方法。\n\n图像数据增强主要通过算法对图像进行转换，引入噪声方法增强数据的多样性，增强的方法主要有：\n- 转换（Rotation）：将图像按照顺时针或者逆时针方向随机旋转一定的角度；\n- 翻转（Flip）：将图像沿水平或者垂直方向随机翻转一定的角度；\n- 缩放（Zoom in/out）：将图像放大或者缩小一定的比例；\n- 平移（Shift）：将图像按照水平或者垂直的方法平移一定步长；\n- 加噪声（Noise）：加入随机噪声。\n\n### 标签平滑\n在数据增强中，可以通过给样本加入随机噪声来避免过拟合，同样也可以给样本的标签引入一定的噪声。假设在训练数据集中，有一些样本的标签是被错误标注的，那么最小化这些样本上的损失函数会导致过拟合。一种改善的正则化方法是**标签平滑（label smothing）**，即在输出标签中随机加入噪声，来避免模型过拟合。\n\n一个样本$x$的标签一般用onehot向量表示，如下：\n$$\ny = [0,...,0,1,.....,1]^T\n$$\n这种标签可以看作**硬目标（hard targets）**，如果使用softmax分类器并使用交叉熵损失函数，最小化损失函数会使得正确类和其他类权重差异很大。根据softmax函数的性质可以知道，如果要使得某一类的输出概率接近于1，其未归一化的得分要远大于其他类的得分，这样可能会导致其权重越来越大，并导致过拟合。i\n\n此外如果标签是错误的，会导致严重的过拟合现象，为了改善这种情况，我们可以引入一个噪声会标签进行平滑，即假设样本以$\\epsilon$的概率为其他类，平滑后的标签为：\n$$\n\\tilde{y} =[ \\frac{ \\epsilon }{K-1} ,...,\\frac{ \\epsilon }{K-1} ,1- \\epsilon,\\frac{ \\epsilon }{K-1},....,\\frac{ \\epsilon }{K-1}]^T\n$$\n其中$K$为标签数量，这种标签可以看作是**软目标（soft targets）**。标签平滑可以避免模型的输出过拟合到硬目标上，并且通常不会降低其分类能力。\n\n上边的标签平滑方法是给其他$K-1$个标签相同的概率$\\frac{\\epsilon}{K-1}$，没有考虑目标之间的相关性。一种更好的做法是按照类别相关性来赋予其他标签不同的概率，比如先训练另外一个更复杂的教师网络，并使用大网络的输出作为软目标进行训练学生网络，这种方法也称为知识精炼（Knowledge Distillation）。\n\n### 总结\n至此，神经网络中的网络优化和正则化（一）（二）（三）（四）篇已经完成，如下：\n- [神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向](https://thinkgamer.blog.csdn.net/article/details/100996744)\n- [神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化](https://thinkgamer.blog.csdn.net/article/details/101026786)\n- [神经网络中的网络优化和正则化（三）之超参数优化](https://thinkgamer.blog.csdn.net/article/details/101033047)\n- [神经网络中的网络优化和正则化（四）之正则化]()\n\n\n神经网络中的网络优化和正则化即是对立又统一的关系，一方面我们希望找到一个最优解使得模型误差最小，另一方面又不希望得到一个最优解，可能陷入过拟合。优化和正则化的目标是期望风险最小化。\n\n目前在深层神经网络中泛化能力还没有很好的理论支持，在传统的机器学习上比较有效的$l_1,l_2$正则化在深层神经网络中作用也比较有限，而一些经验性的做法，比如随机梯度下降和提前终止，会更加有效。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"神经网络中的网络优化和正则化（三）之超参数优化","url":"/2019/09/22/深度学习/神经网络中的网络化和正则化/神经网络中的网络优化和正则化（三）之超参数优化/","content":"\n> 公众号标题：神经网络中的优化方法之学习率衰减和动态梯度方向\n\n### 引言\n\n神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。\n- 第一篇包括\n    - 网络优化和正则化概述\n    - 优化算法介绍\n- 第二篇包括\n    - 参数初始化\n    - 数据预处理\n    - 逐层归一化\n- 第三篇包括\n    - 超参数优化\n- 第四篇包括\n    - 网络正则化 \n\n\n---\n\n无论是神经网络还是机器学习都会存在很多的超参数，在神经网络中，常见的超参数有：\n- 网络结构：包括神经元之间的连接关系，层数，每层的神经元数量，激活函数的类型等\n- 优化参数：包括优化方法，学习率，小批量样本数量\n- 正则化系数\n\n**超参数优化（Hyperparamter Optimization）** 主要存在两方面的困难：\n- 超参数优化是一个组合优化问题，无法像一般参数那样通过梯度优化的方法来求解，也没有一种通用的优化方法\n- 评估一组超参数配置时间代价很高，从而导致一些优化算法（比如时间演化算法）在超参数优化中难以应用\n\n对于超参数的设置，一般有三种比较简单的优化方法，人工搜索，网格搜素，随机搜索\n\n---\n### 网格搜索\n网格搜索（grid search）是一种通过尝试所有超参数的组合来寻找一组合适的超参数组合的方法。如果参数是连续，可以将其离散化。比如“学习率”，我们可以根据经验选取几个值:$\\alpha \\in {0.01, 0.1, 0.5, 1.0}$。\n\n一般而言，对于连续的超参数，不能采用等间隔的方式进行划分，需要根据超参数自身的特点进行离散化。\n\n网格搜索根据不同的参数组合在测试集上的表现，选择一组最优的参数作为结果。\n\n### 随机搜索\n不同超参数对模型的影响不同，有的超参数（比如正则项系数）对模型的影响有限，有的超参数（比如学习率）对模型的影响比较大，这时候采用网格搜索就会在影响不大的超参数上浪费时间。\n\n一种在实践中比较有效的方法是对超参数进行随机组合（比如不太重要的参数进行随机抽取，重要的参数可以按照网格搜索的方式选择），选择表现最好的参数作为结果,这就是**随机搜索（random search）**\n\n> 网格搜索和随机搜索没有利用超参数之间的相关性，即如果模型的超参数组合比较类似，其模型的性能表现也是比较接近的，这时候网格搜索和随机搜索就比较低效。下面介绍两种自适应的超参数优化方法：贝叶斯优化和动态资源分配。\n\n---\n### 动态资源分配\n在超参数优化中，每组超参数配置的评估代价很高，如果我们可以在较早的阶段就估计出该组超参数效果就比较差，然后提前终止该组参数的测试，从而将更多的资源留给其他。这个问题可以归结为**多臂赌博机问题**的一个泛化问题，即**最优臂问题（best-arm problem）**，即在给定有限次数的情况下，如何获取最大收益。\n\n动态资源分配的一种有效方法是**逐层减半（successive halving）**，将超参数优化看作是一种非随机的最优臂问题。该方法出自2015年的一篇论文，论文下载地址为：https://arxiv.org/pdf/1502.07943.pdf\n\n假设要尝试N组超参数配置，总共可利用的摇臂资源次数为B，我们可以通过$T= [log_2N]-1$轮逐次减半的方法来选取最优的配置，具体计算过程如下：\n\n![逐次减半](https://img-blog.csdnimg.cn/20190919191933874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n> 在逐次减半方法中，N的设置十分重要，如果N越大，得到最佳配置的机会也越大，但每组配置分配到的资源就越少，这样早期的评估结果可能不准确，反之，如果N越小，每组超参数配置的评估就会越准确，但也有可能无法得到最优的参数配置。因此如何设置N是评估“利用-探索”的一个关键因素，一种改进的方法是：HyperBrand方法，通过尝试不同的N来寻找最优的参数配置。对应的论文下载地址为：https://openreview.net/pdf?id=ry18Ww5ee\n\n---\n### 贝叶斯优化\n\n#### 贝叶斯优化背后的思想\n\n贝叶斯优化（Bayesian optimization）是一种自适应的超参数优化方法，根据当前已经试验的超参数组合，来预测下一个可能带来最大收益的组合。\n\n> 对于同一个算法来讲，不同的超参数组合其实是对应不同的模型，而贝叶斯优化可以帮助我们在众多模型中寻找性能最优的模型，虽然我们可以使用交叉验证的思想寻找更好的超参数组合，但是不知道需要多少样本才能从一系列候选模型中选择出最优的模型。这就是为什么贝叶斯优化能够减少计算任务加速优化过程的进程，同样贝叶斯优化不依赖于人为猜测需要样本量的多少，这种优化计算是基于随机性和概率分布得到的。\n<br>\n简单来说，当我们把第一条样本送到模型中的时候，模型会根据当前的样本点构建一条直接，当把第二天样本送到模型中的时候，模型将结合这两个点并从前面的线出发绘制一条修正的线，当输送第三个样本的时候，模型绘制的就是一条非线性曲线，当样本数据增加时，模型所结合的曲线就会变得更多，这就像统计学里的抽样定理，即我们从样本参数出发估计总体参数，且希望构建出的估计量与总体参数相合，无偏估计。\n<br>\n下图为非线性目标函数曲线图，对于给定的目标函数，在输送了所有的观察样本之后，它将搜寻到最大值，即寻找令目标函数最大的参数（arg max）。\n![非线性目标函数曲线图](https://img-blog.csdnimg.cn/20190920122716823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n我们的目标并不是使用尽可能多的样本去完全推断未知的目标函数，而是希望能求得使目标函数最大化的参数，所以我们将注意力从曲线上移开，当目标函数组合能提升曲线形成分布时，其就可以称为采集函数（Acquisition funtion），这就是**贝叶斯优化背后的思想**。（灰色区域部分参考：https://www.jiqizhixin.com/articles/2017-08-18-5）\n\n\n#### 时序模型优化\n\n一种常用的贝叶斯优化方法为时序模型优化（Sequential Model-Based Optimization，SMBD），假设超参数优化的函数f(x)服从高斯过程，则$p(f(x)|x)$为一个正态分布。贝叶斯优化过程是根据已有的N组实验结果$H={x_n,y_n}, n\\in(1,N)$（$y_n$为$f(x_n)$的观测值）来建模高斯过程，并计算$f(x)$的后验分布$p(f(x)|x,H)$。\n\n为了使得$p(f(x)|x,H)$接近其真实分布，就需要对样本空间进行足够多的采样，但是超参数优化中每一个样本的生成成本都很高，需要使用尽可能少的样本来使得$p_\\theta(f(x)|x,H)$接近于真实分布。因此需要定义一个收益函数（Acquisition funtion）$\\alpha (x, H)$来判断一个样本能否给建模$p_\\theta(f(x)|x,H)$提供更多的收益。收益越大，其修正的高斯过程会越接近目标函数的真实分布。\n\n收益函数的定义有很多方式，一个常用的是期望改善（Expected Improvement，EI）。假设$y^* = min \\left \\{  y_n, 1 \\leq n \\leq N \\right \\}$是当前已有样本中的最优值，期望改善函数为：\n$$\nEI(x, H) = \\int_{-\\infty }^{ +\\infty } max (y^* - y, 0) p(y|x, H) dy\n$$\n期望改善是定义一个样本$x$在当前模型$p(f(x)|x,H)$下，$f(x)$超过最好结果$y^*$的期望。除了期望改善函数之外，收益函数还有其他函数的定义，比如改善概率（Probability Of Improvement），高斯过程置信上界（GP Up Confidence Bound，GP-UCB）等。\n\n时序模型优化过程如下所示：\n![时序模型优化过程](https://img-blog.csdnimg.cn/20190920150457198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n贝叶斯优化的缺点是高斯建模过程需要计算矩阵的逆，时间复杂度为$O(n^3)$，因此不能很好的处理高维过程，深层神经网络的参数一般比较多，需要更加高效的高斯过程建模，也有一些方法将时间复杂度从$O(n^3)$降到了$O(n)$。\n\n> 至此，超参数优化部分已经介绍完成，这里并没有对超参数优化进行实现，有很多Python库已经对其进行了封装，感兴趣的可以关注下，另外贝叶斯优化在日常实践中用的比较多但是不太好理解，可以多看几遍，对比一些文章什么看下理解下。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化","url":"/2019/09/22/深度学习/神经网络中的网络化和正则化/神经网络中的网络优化和正则化（二）之参数初始化:数据预处理:逐层归一化/","content":"\n### 引言\n\n神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。\n- 第一篇包括\n    - 网络优化和正则化概述\n    - 优化算法介绍\n- 第二篇包括\n    - 参数初始化\n    - 数据预处理\n    - 逐层归一化\n- 第三篇包括\n    - 超参数优化\n- 第四篇包括\n    - 网络正则化 \n\n\n### 参数初始化\n#### 对称权重现象\n在上一篇文章中我们提到神经网络中的参数学习是基于梯度下降的，而梯度下降需要赋予一个初始的参数，所以这个参数的初始化就显得特别重要。\n\n在感知器和逻辑回归中，一般将参数初始化为0，但是在神经网络中如果把参数初始化为0，就会导致在第一次前向计算时，所有隐藏层神经元的激活值都相同，这样会导致深层神经元没有区分性，这种现象称为**对称权重现象**\n\n因此如果要高质量的训练一个网络，给参数选择一个合适的初始化区间是非常重要的，一般而言，参数初始化的区间应该根据神经元的性质进行差异化的设置，如果一个神经元的输入过多，权重就不要设置太大，以避免神经元的的输出过大（当激活函数为ReLU时）或者过饱和（激活函数为Sigmoid函数时）。\n关于神经网络中的激活函数介绍可参考：\n> https://blog.csdn.net/Gamer_gyt/article/details/89440152\n\n常见的参数初始化方法包括以下两种。\n\n#### Gaussian初始化\n\n高斯初始化是最简单的初始化方法，参数服从一个固定均值和固定方差的高斯分布进行随机初始化。\n\n初始化一个深度网络时，一个比较好的初始化方案是保持每个神经元输入的方差是一个常量，当一个神经元的输入连接数量为n时，可以考虑其输入连接权重以$N(0,\\sqrt{\\frac{1}{n}})$的高斯分布进行初始化，如果同时考虑神经元的输出连接数量为m时，可以按照$N(0,\\sqrt{\\frac{2}{m+n}})$进行高斯分布初始化。\n\n#### 均匀分布初始化\n> 均匀初始化是指在一个给定的区间[-r,r]内采用均匀分布来初始化参数，超参数r的设置也可以根据神经元的连接数量来进行自适应调整。\n\n**Xavier初始化方法**是一种自动计算超参数r的方法，参数可以在[-r,r]之间采用均匀分布进行初始化。\n\n如果神经元激活函数为logistic函数，对于第l-1层到第l层的权重参数区间可以设置为：\n$$\nr = \\sqrt{ \\frac{6}{ n^{l-1} + n^l}}\n$$\n$n^l$ 表示第l层神经元的个数，$n^{l-1}$表示l-1层神经元的个数。\n\n如果是tanh激活函数，权重参数区间可以设置为：\n$$\nr =4 \\sqrt{ \\frac{6}{ n^{l-1} + n^l}}\n$$\n> 在实际经验中，Xavier初始化方法用的比较多。\n\n### 数据预处理\n\n#### 为什么要进行数据预处理\n一般情况下，在原始数据中，数据的维度往往不一致，比如在电商数据中，某个商品点击的次数往往要远大于购买的次数，即**特征的分布范围差距很大**，这样在一些使用余弦相似度计算的算法中，较大的特征值就会起到绝对作用，显然这样做是极其不合理的。同样在深度神经网络中，虽然可以通过参数的调整来自适应不同范围的输入，但是这样训练的效率也是很低的。\n\n假设一个只有一层的网络 $y=tanh(w_1x_1 + w_2 x_2 +b)$，其中$x_1 \\in [0,10], x_2 \\in [0,1]$。因为激活函数 tanh的导数在[-2,2]之间是敏感的，其余的值域导数接近0，因此$w_1x_1 + w_2 x_2 +b$过大或者过小都会影响训练，为了提高训练效率，我们需要把$w_1x_1 + w_2 x_2 +b$限定在[-2,2]之间，因为$x_1,x_2$的取值范围，需要把$w_1$设置的小一些，比如在[-0.1, 0.1]之间，可以想象，如果数据维度比较多的话，我们需要精心的去设置每一个参数，**但是如果把特征限定在一个范围内，比如[0,1]**，我们就不需要太区别对待每一个参数。\n\n除了参数初始化之外，不同特征取值范围差异比较大时也会影响梯度下降法的搜索效率，下图（图1-1）给出了数据归一化对梯度的影响，对比等高线图可以看出，归一化后，梯度的位置方向更加接近于最优梯度的方向。\n\n![数据归一化对梯度的影响](https://img-blog.csdnimg.cn/20190919111801183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n#### 数据预处理的方法\n\n关于原始数据归一化的方法有很多，可以参考《推荐系统开发实战》中第四章节部分内容，写的很全面，而且有对应的代码实现。该书的购买链接：\n\n> [点击查看详情-京东链接](https://item.jd.com/12671716.html)\n\n### 逐层归一化\n\n#### 深层神经网络中为什么要做逐层归一化\n\n在深层神经网络中，当前层的输入是上一层的输出，因此之前层参数的变化对后续层的影响比较大，就像一栋高楼，低层很小的变化就会影响到高层。\n\n从机器学习的角度去看，如果某个神经网络层的输入参数发生了变化，那么其参数需要重新学习，这种现象叫做**内部协变量偏移（Internal Covariate Shift）**。\n\n> 这里补充下机器学习中的协变量偏移（Covariate Shift）。协变量是一个统计学的概念，是影响预测结果的统计变量。在机器学习中，协变量可以看作是输入，一般的机器学习都要求输入在训练集和测试集上的分布是相似的，如果不满足这个假设，在训练集上得到的模型在测试集上表现就会比较差。\n![协变量偏移](https://img-blog.csdnimg.cn/20190919130030578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n\n\n为了解决内部协变量偏移问题，需要对神经网络层的每一层输入做归一化，下面介绍几种常见的方法：\n- 批量归一化\n- 层归一化\n- 其他方法\n\n#### 批量归一化\n为了减少内部协变量偏移的影响，需要对神经网络每一层的净输入$z^l$进行归一化，相当于每一层都要做一次数据预处理，从而加快收敛速度，但是因为对每一层都进行操作，所以要求归一化的效率要很高，一般使用标准归一化，将净输入$z^l$的每一维都归一到标准正态分布，其公式如下：\n$$\n\\hat{z}^l = \\frac{ z^l - E[z^l] }{ \\sqrt{var(z^l) + \\epsilon } }\n$$\n$E[z^l]，var(z^l)$表示在当前参数下$z^l$的每一维在整个训练集上的期望和方差，因为深度神经网络采用的是下批量的梯度下降优化方法，基于全部样本计算期望和方差是不可能的，因为通常采用小批量进行估计。给定一个包含K个样本的集合，第$l$层神经元的净输入$z^{(1,l)},....z^{(K,l)}$的均值和方差为：\n$$\n\\mu _\\beta = \\frac{1 }{ K } \\sum_{k=1}^{K } z^{(k,l)}\n\\\\\n\\sigma^2 _\\beta = \\frac{1 }{ K }\\sum_{ k=1}^{K} (z^{(k,l)} - \\mu _\\beta)^2\n$$\n对净输入$z^l$的标准归一化会使其取值集中在0附近，这样当使用sigmoid激活函数时，这个取值空间刚好接近线性变换的空间，减弱了神经网络的非线性性质。因此为了不使归一化对网络产生影响，需要对其进行缩放和平移处理，公式如下：\n$$\n\\hat{z}^l = \\frac{ z^l - \\mu _\\beta }{ \\sqrt{\\sigma^2 _\\beta+ \\epsilon } } \\odot \\gamma  + \\beta \n$$\n其中$\\gamma  , \\beta$分别代表缩放和平移的向量。\n\n> 这里需要注意的是每次小批量样本的均值和方差是净输入$z^l$的函数，而不是常量因此在计算梯度时要考虑到均值和方差产生的影响，当训练完成时，用整个数据集上的均值和方差来代替每次小样本计算得到的均值和方差。在实际实践经验中，小批量样本的均值和方差也可以使用移动平均来计算。\n\n#### 层归一化\n\n批量归一化的操作对象是单一神经元，因此要求选择样本批量的时候，不能太小，否则难以计算单个神经元的统计信息，另外一个神经元的输入是动态变化的，比如循环神经网络，那么就无法应用批量归一化操作。\n\n**层归一化（Layer Normalization）** 是和批量归一化非常类似的方法，但层归一化的操作对象是某层全部神经元。\n\n对于深层神经网络，第$l$层神经元的净输入为$z^l$，其均值和方差为：\n$$\nu^l = \\frac{1}{n^l} \\sum_{i=1}^{ n^l} z_i^l\n\\\\\n\\sigma ^2_l = \\frac{1}{n^l} \\sum_{i=1}^{ n^l} (z_i^l - u^l )\n$$\n其中$n^l$为第$l$层神经元的数量。则层归一化定义为：\n$$\n\\hat{z^l} = \\frac{z^l - u^l }{ \\sqrt {\\sigma^2 _l + \\epsilon } } \\odot \\gamma  + \\beta \n$$\n其中$\\gamma ,\\beta$分别代表缩放和平移的向量，和$z^l$的维度相同。\n\n**循环神经网络中的层归一化**为：\n$$\nz_t = U h_{t-1} + W x_t\n\\\\\nh_t = f(\\hat{z^l})\n$$\n其中隐藏层为$h_t$，$x_t$为第$t$时刻的净输入，$U,W$为参数。\n> 在标准循环网络中，循环神经层的输入一般就随着时间慢慢变大或者变小，从而引起梯度爆炸或者梯度消失，而层归一化的神经网络可以有效的缓解这种状况。\n\n#### 其他方法\n除了上面介绍的两种归一化方法之外，还有一些其他的一些归一化方法，感兴趣的可以自行搜索查看。\n- 权重归一化(Weight Normalization)\n- 局部响应归一化\n\n> 至此，神经网络中的优化方法第二部分介绍完成，主要包好了三部分内容：参数初始化，数据预处理和逐层归一化。再下一篇将会重点介绍超参数优化的方法不仅适用于深度神经网络，也适用于一般的机器学习任务。如果你觉得不错，分享一下吧！\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向","url":"/2019/09/22/深度学习/神经网络中的网络化和正则化/神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向/","content":"\n### 引言\n\n神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。\n- 第一篇包括\n    - 网络优化和正则化概述\n    - 优化算法介绍\n- 第二篇包括\n    - 参数初始化\n    - 数据预处理\n    - 逐层归一化\n- 第三篇包括\n    - 超参数优化\n- 第四篇包括\n    - 网络正则化 \n\n---\n\n### 概述\n虽然神经网络有比较强的表达能力，但是应用神经网络到机器学习任务时仍存在一些问题，主要分为：\n1.  网络优化\n> 神经网络模型是一个非凸函数，再加上神经网络中的梯度消失和梯度爆炸，很难进行优化，另外网络的参数比较多，且数据量比较大导致训练效率比较低。\n2.  正则化\n> 神经网络拟合能力强，容易在训练集上产生过拟合，需要一些正则化的方法来提高网络的泛化能力。\n\n从大量的实践经验看主要是从网络优化和正则化两个方面提高学习效率并得到一个好的网络模型。\n\n在低维空间的非凸优化问题中主要是存在一些局部最优点，基于梯度下降优化算法会陷入局部最优点，因此低维空间的非凸优化的难点在于如何选择合适的参数和逃离局部最优点。\n\n深层神经网络中参数较多，其是在高维空间的非凸优化问题中，和低维空间的非凸优化有些不同，其主要难点在于如何逃离鞍点（Saddle Point），鞍点的梯度为0，但是在一些维度上是最高点，在另一些维度上是最低点，如下图所示（图1-1）：\n![鞍点示例](https://img-blog.csdnimg.cn/2019091814412421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n在高维空间中，局部最优点要求在每一维度上都是最低点，这种概率很低，假设网络有1000\n个参数，每一维上取得局部最优点的最小概率为p，则在整个参数空间中取得局部最优点的最小概率为$p^{1000}$，这种概率很小，也就是说在整个参数空间中，大部分梯度为0的点都是鞍点。\n\n\n---\n\n### 优化算法介绍\n\n深层神经网络的参数学习主要是通过梯度下降算法寻找一组最小结构的风险参数，梯度下降分为：\n- 批量梯度下降\n- 随机梯度下降\n- 小批量梯度下降\n\n根据不同的数据量和参数量，可以选择一种合适的梯度下降优化算法，除了在收敛效果和效率上的区别，这三种梯度下降优化算法还存在一些共同问题（**具体会在下一篇进行详细介绍**）：\n- 如何初始化参数\n- 预处理数据\n- 如何选择合适的学习率，避免陷入局部最优\n\n在训练深层神经网络时，通常采用小批量梯度下降算法。令$f(x,\\theta)$为一个深层神经网络，$\\theta$为网络参数，使用小批量梯度优化算法时，每次选择K个训练样本$I_t =\\left \\{ (x^t,y^t)  \\right \\} , t \\in (1,T)$，第t次迭代时损失函数关于$\\theta$的偏导数为（公式1-1）：\n$$\ng_t(\\theta ) = \\frac{ 1 }{ K } \\sum_{ (x^t,y^t) \\in I_t} \\frac{ \\partial L(y^t,f(x^t, \\theta)) }{ \\partial \\theta }\n$$\n\n第t次更新的梯度$g'_t$定义为（公式1-2）：\n$$\ng_t'(\\theta)= g_t(\\theta_{t-1})\n$$\n使用梯度下降来更新参数（公式1-3）：\n$$\n\\theta_t = \\theta_{t-1} - \\alpha g'(\\theta)\n$$\n一般批量较小时，需要选择较小的学习率，否则模型不会收敛。下图（图1-2）给出了在Mnist数据集上批量大小对梯度的影响。从图1-2(a)可以看出，批量大小设置的越大，下降的越明显，并且下降的比较平滑，当选择批量的大小为1时，整体损失呈下降趋势，但是局部比较震荡。从图1-2(b)可以看出，如果按整个数据集上的迭代次数（Epoch）来看损失变化情况，则是批量样本数越小，下降效果越明显。\n\n![批量的大小对梯度的影响](https://img-blog.csdnimg.cn/20190918153354525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n为了更加有效的训练深层神经网络，在标准的小批量梯度下降算法中，经常使用一些改进方法加快优化速度，常见的改进方法有两种：\n- 学习率衰减\n- 梯度方向优化\n\n> 这些改进的优化方法也同样可以应用在批量梯度下降算法和随机梯度下降算法。\n\n---\n### 学习率衰减\n\n在梯度下降中，学习率的设置很重要，设置过大，则不会收敛，设置过小，则收敛太慢。从经验上看，学习率在一开始要设置的大些来保证收敛速度，在收敛到局部最优点附近时要小些来避免震荡，因此比较简单的学习率调整可以通过学习率衰减（Learning Rate Decay）的方式来实现。假设初始学习率为$\\alpha_0$，第t次迭代的学习率为$a_t$，常用的衰减方式为按照迭代次数进行衰减，例如\n- 逆时衰减（公式1-4）\n\n$$\na_t = a_0 \\frac{1 }{ 1 + \\beta t}\n$$\n- 指数衰减（公式1-5）\n\n$$\na_t = a_0\\beta^t\n$$\n\n- 自然指数衰减（公式1-6）\n\n$$\na_t = a_0 exp(-\\beta * t)\n$$\n其中$\\beta$为衰减率，一般为0.96\n\n#### AdaGrad\nAdaGrad（Adaptive Gradient）算法是借鉴L2正则化的思想，每次迭代时自适应的调整每个参数的学习率。AdaGrad的参数更新公式为（公式1-7）：\n$$\nG_t = \\sum_{t=1}^{T} g_t \\odot g_t\n\\\\\n\\bigtriangleup \\theta_t = - \\frac{\\alpha }{ \\sqrt{G_t + \\epsilon  } } \\odot g_t\n\\\\\ng'_t(\\theta) = g_t(\\theta_{t-1}) + \\bigtriangleup \\theta_t\n$$\n其中$\\alpha$为学习率，$\\epsilon$是为了保证数据稳定性而设置的非常小的常数，一般取值是 $e^{-7}$到$e^{-10}$，这里的开平方，加，除运算都是按照元素进行的操作。\n\n在AdaGrad算法中，如果某个参数的偏导数累积比较大，其学习率相对较小，相反，如果其偏导数累积比较大，其学习率相对较大。但是整体上随着迭代次数的增加，学习率逐渐减小。\n\nAdaGrad算法的缺点是在经过一定次数的迭代后依然没有找到最优点，由于这时候的学习率已经很小了，就很难找到最优点。\n\n\n#### RMSProp\n\n\nRMSProp是Geoff Hinton提出的一种自适应学习率的方法，可以在有些情况下避免AdaGrad的学习率单调递减以至于过早衰减的缺点。\n\nRMSProp算法首先计算的是每次迭代速度$g_t$平方的指数衰减移动平均，如下所示（公式1-8）：\n$$\nG_t = \\beta G_{t-1}  + (1-\\beta) g_t \\odot g_t = (1- \\beta) \\sum_{t=1}^{T} \\beta ^{T-t} g_t \\odot g_t\n$$\n其中$\\beta$为衰减率，一般取值为0.9，RMSProp算法参数更新公式为（公式1-9）：\n$$\n\\bigtriangleup \\theta_t = - \\frac{\\alpha }{ \\sqrt{G_t + \\epsilon  } } \\odot g_t\n\\\\\ng'_t(\\theta) = g_t(\\theta_{t-1}) + \\bigtriangleup \\theta_t\n$$\n其中$\\alpha$为学习率，通常为0.001。\n\n> 从公式1-8 可以看出，RMSProp和AdaGrad的区别在于$G_t$的计算由累积方式变成了指数衰减移动平均，在迭代过程中，每个参数的学习率并不是呈衰减趋势，即可以变大，也可以变小。\n\n\n#### AdaDelta\nAdaDelta算法也是AdaGrad算法的一个改进，和RMSProp算法类似，AdaDelta算法通过梯度平方的指数衰减移动平均来调整学习率，除此之外，AdaDelta算法还引入了每次参数更新差$\\bigtriangleup \\theta$的平方的指数衰减移动平均。\n\n第t次迭代时，每次参数更新差$\\bigtriangleup \\theta_t , 1<t<T-1$的指数衰减移动平均为（公式1-10）：\n$$\n\\bigtriangleup X^2_{t-1} =\\beta _1 \\bigtriangleup X^2_{t-2} + (1 - \\beta) \\bigtriangleup \\theta_{t-1} \\odot \\bigtriangleup \\theta_{t-1}\n$$\n其中$\\beta_1$为衰减率，AdaDelta算法的参数更新差值为（公式1-11）：\n$$\n\\bigtriangleup \\theta_t = - \\frac{\\sqrt {\\bigtriangleup X^2_{t-1} + \\epsilon }}{ \\sqrt {G_t + \\epsilon}} g_t\n$$\n\n> 其中$G_t$的计算方式和RMSProp算法一样。从公式1-11可以看出，AdaDelta算法将RMSProp算法中的初始学习率$\\alpha$改为动态计算的$\\sqrt {\\bigtriangleup X^2_{t-1} + \\epsilon }$，在一定程度上减缓了学习旅率的波动。\n\n\n---\n### 梯度方向优化\n除了调整学习率外，还可以使用最近一段时间内的平均梯度来代替当前时刻的梯度来作为参数的更新方向，从图1-2中可以看出，在小批量梯度下降中，如果每次选取样本数量比较小，损失就会呈现震荡的方式下降，有效的缓解梯度下降中的震荡的方式是通过用梯度的移动平均来代替每次的实际梯度。并提高优化速度，这就是**动量法**。\n\n#### 动量法\n动量法（Momentum Method）是用之前积累的动量来替代真正的梯度，每次替代的梯度可以看作是加速度。\n\n在第t次迭代时，计算负梯度的“加权移动平均”作为参数的更新方向，如下所示（公式1-12）：\n$$\n\\bigtriangleup \\theta_t = \\rho \\bigtriangleup \\theta_{t-1}-\\alpha g_t\n$$\n其中$\\rho$为动量因子，通常设置为0.9，$\\alpha$为学习率。\n\n> 参数的实际更新值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内梯度方向不一致时，参数更新的幅度变小，相反，参数更新的幅度变大，起到加速的作用。\n\n> 一般而言，在迭代初期，梯度的更新方向比较一致，动量法会起到加速作用，可以更快的起到加速的作用，可以更快的到达最优点，在迭代后期，梯度的更新方向不一致，在收敛时比较动荡，动量法会起到减速作用，增加稳定性。从某种程度来讲，当前梯度叠加上部分的上次梯度，一定程度上可以看作二次梯度。\n\n\n#### Nesterov加速梯度\n\nNesterov加速梯度（Nesterov Accelerated Gradient， NAG）也叫Nesterov动量法（Nesterov Momentum），是一种对动量法的改进。\n\n在动量法中，实际的参数更新方向$\\bigtriangleup \\theta_t$为上一步的参数更新方向$\\bigtriangleup \\theta_{t-1}$和当前的梯度 $-g_t$的叠加，这样，$\\bigtriangleup \\theta_t$可以拆分为两步进行，先根据$\\bigtriangleup \\theta_{t-1}$更新一次得到参数$\\tilde{\\theta }$，再用$g_t$进行更新，如下所示（公式1-13）：\n$$\n\\tilde{\\theta } = \\theta_{t-1} + \\rho \\bigtriangleup \\theta_{t-1} \n\\\\\n\\theta_t = \\tilde{\\theta } - \\alpha g_t\n$$\n\n其中$g_t$为点$\\theta_{t-1}$上的梯度，所以第二步不太合理，更合理的更新方向为$\\tilde{\\theta }$上的梯度，这样合并后的更新方向为（公式1-14）：\n$$\n \\bigtriangleup \\theta_t =  \\rho \\bigtriangleup \\theta_{t-1} -\\alpha g_t(\\theta_{t-1} + \\rho \\bigtriangleup \\theta_{t-1} )\n$$\n其中$g_t(\\theta_{t-1} + \\rho \\bigtriangleup \\theta_{t-1} )$表示损失函数在$\\tilde{\\theta } = \\theta_{t-1} + \\rho \\bigtriangleup \\theta_{t-1}$上的偏导数。\n\n下图（图1-3）给出了动量法和 Nesterov 加速梯度在参数更新时的比较：\n\n![动量法和 Nesterov 加速梯度在参数更新时的比较](https://img-blog.csdnimg.cn/20190918192321930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n#### AdaM算法\n自适应动量估计算法（Adaptive Moment Estimation，Adam）可以看作是动量法和RMSprop的结合，不但使用动量作为参数更新，而且可以自适应调整学习率（公式1-15）。\n$$\nM_t = \\beta _1M_{t-1} + (1-\\beta _1)g_t\n\\\\ \nG_t = \\beta _2 G_{t-1} + (1-\\beta _2)g_t \\odot g_t\n$$\n其中$\\beta_1 ，\\beta_2$分别为两个移动平均的衰减率，通常取值：$\\beta_1=0.9,\\beta_2=0.99$。\n\n$M_t$可以看作是梯度的均值（一阶矩），$G_t$可以看作是梯度的未减去均值的方差（二阶矩）。\n\n假设$M_t =0,G_t=0$，那么在迭代初期，$M_t，G_t$的值会比真实的均值和方差要小，特别是当$\\beta_1 ，\\beta_2$都接近1时，偏差会很大，因此需要对偏差进行修正，如下所示（公式1-16）：\n$$\n\\tilde{M_t} = \\frac{M_t}{ 1 - \\beta^t _1}\n\\\\\n\\tilde{G_t} = \\frac{G_t}{ 1 - \\beta^t _2}\n$$\nAdam算法的更新差值为（公式1-17）：\n$$\n\\bigtriangleup \\theta_t = - \\frac{\\alpha }{\\sqrt{ \\tilde{G_t} + \\varepsilon  }} \\tilde{M_t}\n$$\n其中学习率$\\alpha$通常设置为0.001，并且也可以进行衰减，比如$a_t = \\frac{a_0} { \\sqrt{t}}$。\n\n> Adam算法是RMSprop与动量法的结合，因此一种自然的Adam改进方法是引入Nesterov加速梯度，称为Nadam算法。\n\n\n#### 梯度截断\n\n在深层神经网络或者循环网络中，除了梯度消失之外，梯度爆炸是影响学习效率的主要隐私，在基于梯度下降的优化过程中，如果梯度突然增大，用较大的梯度更新参数，反而会使结果远离最优点，为了避免这种情况，当梯度达到一定值的时候，要进行梯度截断（gradient clipping）。\n\n梯度截断是一种比较简单的启发式方法，把梯度的模限定在一个范围内，当梯度的模大于或者小于某个区间时，就进行截断，一般截断的方式有以下几种：\n- 按值截断\n\n在第t次迭代时，梯度为$g_t$，给的一个区间[a,b]，如果梯度小于a时，令其为a，大于b时，令其为b。\n\n- 按模截断\n将梯度的模截断到一个给定的截断阈值b。如果$||g_t||^2 \\leq b$保持梯度不变，如果$||g_t||^2 > b$，则$g_t= \\frac{ b}{||g_t||} g_t$。\n\n截断阈值 b 是一个超参数,也可以根据一段时间内的平均梯度来自动调整。实验中发现,训练过程对阈值 b 并不十分敏感,通常一个小的阈值就可以得到很好的结果。\n\n> 在训练循环神经网络时，按模截断是避免梯度爆炸的有效方法。\n\n----\n\n### 优化算法总结\n\n本文介绍了神经网络中的网络优化和正则化概述，以及网络优化中的加快网络优化的两种方法，这些方法大体分为两类：\n- 调整学习率，使得优化更稳定\n> 比如：AdaGrad，RMSprop，AdaDelta\n- 调整梯度方向，优化训练速度\n> 比如：动量法，Nesterov加速梯度，梯度截断\n\nAdam则是RMSprop 和 动量法的结合。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"【论文】RecSys18-序列推荐模型TransFM(Translation-based Factorization Machines for Sequential Recommendation)","url":"/2019/08/31/论文/【论文】RecSys18-序列推荐模型TransFM(Translation-based Factorization Machines for Sequential Recommendation)/","content":"\n序列推荐模型 Translation-based Recommendation，参考：[点击阅读](https://thinkgamer.blog.csdn.net/article/details/100129827)\n\n<!--More-->\n\n# 概述\n论文是由Rajiv Pasricha和Julian McAuley两位大佬提出的发表在RecSys18 上的，是TransRec和FM的结合版本（论文下载地址：https://cseweb.ucsd.edu/~jmcauley/pdfs/recsys18a.pdf）。在下面会简单介绍TransRec和FM。\n\n\n\n对于电商网站（如亚马逊），媒体网站（如Netflix，Youtube）等而言，推荐系统是其中至关重要的一环。传统的推荐方法尝试对用户和物品的全局交互进行建模。例如矩阵分解和其派生模型，虽然能够有效的捕获到用户的偏好，但是未考虑到时序特征，其忽略了用户的最近交互行为，提供了一个静态的推荐列表。\n\n序列推荐的目的是基于用户的历史行为序列去预测用户将来的行为。Julian McAuley作为主要作者的另一篇论文（Translation-based Recommendation）提出了“翻译”空间的概念，将物品作为一个点嵌入到“翻译”空间内，用户的序列行为则作为一个翻译向量存在于该空间，然后通过距离计算便根据用户u的当前行为物品i，预测其接下来可能有行为的物品，具体可参考：https://mp.weixin.qq.com/s/YovZKGd2BDqnpW5BBGLA-A。TransRec的主要思路如下图所示：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831085905828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n\n本论文中提出了TransFM，其结合了FM和TransRec的思想，将其应用在序列推荐中，这样做的好处是使用简单的模型对复杂的交互之间进行建模并能取得不错的效果。\n\n> FM能够对任意的实值特征向量进行操作，并通过参数分解对特征之间的高阶交互进行建模。他可以应用在一般的预测任务里，并可以通过特征替换，取代常见的推荐算法模型。\n\nTransFM的主要思路如下图所示：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831085920392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n\nTransFM是对所有观察到的行为之间可能的交互进行建模，对于每一个特征i，模型学习到两部分：一个低维的embedding向量$\\overrightarrow{v_i}$和一个翻译向量$\\overrightarrow{v_i'}$\n\n特征之间的交互强度使用平方欧几里德距离来进行计算，在上图中，展示了user，item，time的embedding特征和翻译向量，交互行为之间的权重由起始点和结束点之间的平方欧几里德距离进行计算。与FM一样，TransFM可以在参数和特征纬度的线性时间内进行计算，从而有效的实现大规模数据集的计算。\n# 相关研究\n## 序列推荐\n已经存在了许多基于MC（马尔可夫链，Markov Chains）的序列推荐模型，比如FPMC（Factorized Personalized Markov Chains），使用独立分解矩阵对三阶交互行为进行分解，继而来模拟成对的相互作用。PRME使用欧几里德距离替换内积对用户-物品之间的交互行为进行建模。TransRec同样也是一个序列推荐模型，通过共享物品的embedding向量空间，将用户行为转化为翻译向量，其计算公式如下：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831085937428.png\" width=200px></center>\n这些对于给定的用户历史行为序列十分有效，但是在不改变模型结构的前提下，并不能捕获时间，地理和其他的上下文特征。\n\n## 因子分解机\nFM对于任意的机器学习任务来讲是一个通用的学习框架，他模型任意任意特征之间的二阶交互，并很容易扩招到更高阶，每个特征的交互通过参数之间的内积来权衡。其公式如下（这里讨论的是FM的二阶形式）：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831085947201.png\" width=200px></center>\n通过选择合适的损失函数，FM可以应用在任意的分类，回归或者排序任务中，在这篇文章里主要是针对隐式反馈结合BPR算法框架去优化预测的结果。\n\n## 混合推荐\n混合推荐结合了协同和conetnt-based，目的在于提升效果并且为行为很较少的用户提供有效的选择，在一定程度上缓解了用户冷启动。这里可以利用的潜在的信息包括：时间特征，地理特征，社交特征等。最近的一些关于混合推荐的工作结合了图像特征，或者是使用深度学习自动生成有用的内容特征。\n\n虽然这些方法都取得了不错的表现，但依赖于专门的模型和技术。相比之间，论文里提出的TransFM是一种更广义的办法，可以对任意的特征向量和预测任务进行操作，通过适当的特征工程，TransFM模型可以结合时间，地理，人口统计和其他内容特征，而无需更改模型本身结构。\n\n# TransFM模型\n## 问题定义\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831085958369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n\nTransFM使用平方欧几里德距离替换FM中的内积计算，并用embedding 向量和翻译向量之和表示特征v_i的向量，其公式如下：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090026658.png\" width=300px></center>\n其中距离计算方式为：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090035517.png\" width=230px></center>\n\n使用平方欧几里德距离替换内积的好处是：提高模型的泛化能力，更有效的捕获embedding之间的传递性。比如(a,b)，(b,c)之间有很高的交互权重，那么(a,c)之间的相关性也会更强。\n\n下图展示了TransFM和其他几种算法的预测方法，从中可以看出PRME学习的是两个用户的embedding向量之间的距离，FM学习的是任意特征与相应参数之间的内积，TransRec学习的是物品的embedding向量和用户行为的翻译序列，TransFM学习的是每个特征的embedding向量和翻译向量，使用平方欧几里德距离去度量特征之间的交互。\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090058665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n## 模型计算\nFM是可以将计算复杂度降低到nk的，同样TransFM也可以降低其计算负责度。首先：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090139230.png\" width=300px></center>\n\n其次进行化简得：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090151717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n上面的第一个总和可以分成六个单独项，每一项又可以继续进行化简：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090204314.png\" width=300px></center>\n\n假设输入的特征是n维，隐向量长度为k，那么时间复杂度就是O(nk)，而不是O(n^2k)。\n## 参数优化\n模型使用S-BPR（Sequential Bayesian Personalized Ranking）进行优化，其优化方式如下：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090239744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=350px></center>\n其中Ω(Θ)为L2正则。\n\n## 实践和推断\n作者等人在TensorFlow中对TransFM进行了实现，用的是mini-batch gradient descent 和 Adam进行模型的训练（adam对于有大量参数且稀疏的数据集上表现良好）。\n作者这里也罢代码进行了开源，包括数据集，已经不同算法实现实现对比，其地址为：https://github.com/rpasricha/TransFM\n\n# 实验\n作者结合了一些算法在亚马逊和谷歌数据集上进行测试，其中评价的指标是AUC，效果如下：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090454914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090503108.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=350px></center>\n\n上边的Table 3是指从Amazon选取top5 品类 ，从Google Local 中选取6个城市作为实验依据。\n# FM模型和其他模型的融合\nPRME（Personalized Ranking Metric Embedding）\n<center><img  src=\"https://img-blog.csdnimg.cn/2019083109063261.png\" width=300px></center>\n\n> 和TransFM对比的不同在于TransFM中i的向量是embedding向量和translation向量和，而这里没有translation向量。实时证明TransFM效果要好很多。\n\nHRM（Hierarchical Representation Model ）\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090640757.png\" width=300px></center>\n\n对比的实验结果如下：\n<center><img  src=\"https://img-blog.csdnimg.cn/20190831090647827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n\n# 我的总结\n1. TransFM结合了TransRec 和 FM和优势，在大量，稀疏的数据集上取得了不错的效果。\n2. 在参数和特征纬度下，计算时间线性增大（nk）\n3. 改变FM中的内积计算方式，使用平方欧几里德距离，提高了模型的泛化能力，和样本特征之间的传递性\n4. 在不改变模型结构的前提下，可以轻易将时间，地域或者其他内容特征加入到模型中\n5. 数据集拆分时避免了从整体数据集中的随机拆分，而是按照时间先后的顺序进行拆分。保证了一定的时间连续性，很多论文中划分训练集和测试集时都是这样做的，在工业界中模型的训练和评估大部分也是这样做的。\n6. 根据经验将参数限定在一个范围内，根据网格搜索法寻找最佳参数\n7. 实验对比的丰富性，使结论更具有说服力\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["论文"],"categories":["技术篇"]},{"title":"【论文】RecSys17-序列推荐模型 Translation-based Recommendation","url":"/2019/08/29/论文/【论文】RecSys17-序列推荐模型 Translation-based Recommendation/","content":"\n> 序列推荐模型 TransFM（Translation-based Factorization Machines for Sequential Recommendation）参考：[点击阅读](https://thinkgamer.blog.csdn.net/article/details/100168818)\n\n<!--More-->\n\n# 背景\n这篇论文是由 Ruining He，Wang-Cheng Kang和Julian McAuley三位大佬提出的，在2017年的ACM推荐系统会议（RecSys'17）上获得了最佳论文奖（在大佬主页可以下载该论文中涉及的代码和数据集，可惜代码是C++写的，不懂C++的童鞋挑战性很大～）\n- 第一作者的Ruining He主页为https://sites.google.com/view/ruining-he/\n- RecSys历届最佳论文地址：https://recsys.acm.org/best-papers/\n- 本论文下载地址：https://arxiv.org/pdf/1707.02410.pdf\n\n论文的两个研究点：\n- 用户的序列推荐（用户在浏览了一些items之后给他推荐物品j）\n- 物品到物品的推荐（用户购买了一个牛仔裤，给他推荐一个衬衫）\n\n---\n# 概述\n对用户和物品以及物品和物品之间的关系进行建模是设计一个成功推荐系统的核心。一种经典的做法是预测用户行为序列（或者是下一个物品的推荐），其挑战在于对用户，用户历史行为物品和用户接下来有行为的物品之间的三阶交互关系进行建模。现有的方法是对这些高阶的交互分解为成对的关系组合，通过不同的模型去对用户的偏好（用户和物品的交互）和序列匹配（物品和物品的交互）进行建模。\n\n比如MF（Matrix Factorization，矩阵分解）只对用户和物品之间的交互行为进行建模；MC（Markov Chain，马尔可夫链）只对用交互过程中的物品对进行建模，通常通过对转移矩阵进行分解提高其泛化能力。\n\n对于序列推荐，研究者提出了可扩展的张量分解方法，比如FPMC（Factorized Personalized Markov Chains），FPMC通过两个成对的交互关系来模拟u，i，j之间的三阶交互关系，其实这就是MF和MC的结合。对于提升FPMC有两个方向的研究思路，一个思路是在个性化度量嵌入方法用欧几里德距离替换FPMC中的内积，其中度量假设尤其是三角不等式使模型的泛化性更好，然而，这些研究工作采用的仍然是对用户偏好和序列的连续性分别建模的框架，由于这两部分本身存在关系，因此这样做是存在一定的问题的。另一个思路是利用平均/最大池化等操作去聚合用户u和前一项i的向量表示，然后再测量它们与下一个项j的相似度，这种思路虽然部分解决了两个组件之间的相互依赖问题，但很难解释而且不能从度量embedding向量中获得收益。\n\n为了解决上述存在的问题，提出了Translation-based Recommendation模型，具体解释往下看。\n\n----\n\n# 模型介绍\n## 问题定义\n涉及的相关字符含义如下图所示：\n<center><img src=\"https://img-blog.csdnimg.cn/20190829080844419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px>\n</center>\n\n## 模型结构\nTransRec的主要思路如下图：\n<center><img src=\"https://img-blog.csdnimg.cn/20190829080909516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px>\n</center>\n\n物品作为一个点被嵌入到翻译空间内，用户的序列行为则作为一个翻译向量存在于该空间，然后，通过个性化翻译操作捕获前面提到的三阶交互，其基本思路就是用户的翻译向量和上一个行为物品的翻译向量之和，确定下一个有行为的物品j，如下所示\n$$\n\\underset{ \\gamma _i }{\\rightarrow} + \\underset{t_u}{\\rightarrow}  \\approx  \\underset{ \\gamma _j }{\\rightarrow} \n$$\n\n其中距离的计算可以采用L1-distance或者L2-distance。\n由于生产环境中数据的稀疏性，很难为每个用户学习到一个向量，因此添加了一个全局翻译向量来初始化所有的用户，这样也能够有效的缓解用户冷启动。如下所示\n$$\n\\underset{ T_u }{\\rightarrow} = \\underset{t}{\\rightarrow}  + \\underset{ t_u }{\\rightarrow} \n$$\n在一个生产系统中，往往会存在头部物品（即热门物品），这些物品的流行度很高，那么如果仅仅采用简单的距离计算的话，那些头部物品就很可能出现在每个用户的推荐结果中。因此在计算时增加了一个偏置项beta_j来对热门物品进行降权。最终对于给定的用户u和之前的行为物品i可以由以下的计算公式为其推荐物品j。\n<center><img src=\"https://img-blog.csdnimg.cn/20190829080953218.png\" width=200px>\n</center>\n物品j的热度越高，则beta_j越小，这样当两个物品计算出来的距离一致时，倾向于推荐那些热度小的物品，这样也能够在一定程度上提高推荐物品的多样性。\n\n***这里有一点需要注意的是：为了避免“维度诅咒”问题，将r_j限定在整个翻译空间的一个子集上，例如一个单位球体的空间范围。***\n\n对于给定过的用户和历史行为序列，模型的目标是对集合中的物品进行排序，这里采用的是pairwise方法的S-BPR（Sequential Bayesian Personalized Ranking）。其优化的公式如下：\n<center><img src=\"https://img-blog.csdnimg.cn/20190829081100913.png\" width=300px>\n</center>\n其中j是真实的下一个交互的物品，j'是除j之外的集合中的任意一个物品。omega为L2正则项。\n\n## 参数学习\n物品i和用户对应的全局翻译向量随机初始化为单元向量，每个物品的偏置向量和每个用户的翻译向量初始化为0。\n\n目标函数通过随机梯度上升进行优化，随机从集合中抽取用户u，正例j和负例j'，通过下面的计算公式进行迭代：\n<center><img src=\"https://img-blog.csdnimg.cn/20190829081139397.png\" width=300px></center>\n其中ε为学习率，λ为正则项参数。重复该公式，直到收敛或者效果达到最优或者达到最大迭代次数。\n\n## 最近邻查找\n在测试时，可以通过最近邻搜索进行推荐，一个小的挑战就是物品的偏差。这里分为两部分去解决这个问题。\n第一使用：\n$$\\beta _j ' \\leftarrow  \\beta _j - max_{k\\in I}\\beta _k$$ 表示$\\beta _j$\n\n对偏置项进行转换，不会改变计算结果的排序。\n\n\n第二使用L2范数计算\n$$\\overrightarrow{\\gamma _j}' = (\\overrightarrow{\\gamma _j}';\\sqrt{-\\beta _j'})$$\n或者使用L1范数计算\n$$\\overrightarrow{\\gamma _j}' = (\\overrightarrow{\\gamma _j}';-\\beta _j')$$\n实验证明L2范数效果更好。对于给定的用户u和物品i，在整个向量空间内为其计算寻找最近的$\\overrightarrow{\\gamma _j}'$\n\n---\n# 实验\n为了充分验证TransRec的优势，使用了大量的公开数据集，如下图所示：\n<center><img src=\"https://img-blog.csdnimg.cn/20190829081637879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n在算法选择上以PopRec为baseline，BPR-MF，FMC，FPMC，PRME，HRM作为对比算法模型，在上表中的数据集上对比实验如下：\n<center><img src=\"https://img-blog.csdnimg.cn/2019082908164614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n在进行实验寻找最佳参数时，使用的是网格搜索法，使用随机提督上升优化模型的学习率为0.05，正则项参数的测试范围是：{0, 0.001, 0.01, 0.1, 1}。在TransRec中尝试了L2-ball 和 L2-sphere计算距离，L2-ball的效果更好一些。\n\n在最开始我们提到本论文主要有两点\n用户的序列推荐（用户在浏览了一些items之后给他推荐物品j）\n物品到物品的推荐（用户购买了一个牛仔裤，给他推荐一个衬衫）\n\n在TransRec中，通过删除个性化向量部分，TransRec可以直接进行物品到物品的推荐，这和知识图谱中的推荐有点相似，因为需要对不同项之间的关系进行建模。其中实现的结果类似于下图这样。\n<center><img src=\"https://img-blog.csdnimg.cn/20190829081659946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n同样也对该部分进行了单独的实验，采用的数据集，对比的算法和实验的结论如下：\n<center><img src=\"https://img-blog.csdnimg.cn/20190829081707975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n----\n# 我的总结\n1. 文章中介绍了TransRec的优势\n（1）只用一个模型来模拟用户，物品之间的三阶交互\n（2）可以从隐式假设度量中获益\n（3）轻易的解决数据量大的问题\n2. 论文中不仅提出了使用一个模型来对用户物品之间的三阶关系进行建模，还借鉴知识图谱中的思想提出了物品道物品之间的推荐。\n3. 文中实验时将数据集拆分成了三部分，训练集，验证集，和测试集。其比例为8:1:1。\n4. 数据集拆分时避免了从整体数据集中的随机拆分，而是按照时间先后的顺序进行拆分。保证了一定的时间连续性，这一点值得借鉴。\n5. 在寻找最佳参数时使用的是网格搜索法。\n6. 用户的翻译向量采用了全局翻译向量和个性化的翻译向量之和。一定程度上解决了用户的冷启动。\n7. 样本偏置项，减小物品本身热度对模型的影响。\n8. 为了避免“维度诅咒”问题，将样本限定在整体样本空间的一个子集上。\n9. 基于TransRec的思路，作者又提出了和FM的结合，其论文是Translation-based Factorization Machines for Sequential Recommendation，接下来会对其进行介绍。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["论文"],"categories":["技术篇"]},{"title":"Spark排序算法系列之ALS模型实现","url":"/2019/08/13/RecSys/Spark与推荐系统/Spark排序算法系列之ALS模型实现/","content":"\n> 在上一篇文章中介绍了ALS算法的原理（[点击阅读](https://blog.csdn.net/Gamer_gyt/article/details/98897829)），在这篇文章中主要介绍一下ALS算法在Spark中的实现。\n\n<!--More-->\n\n### 概述\n协同过滤(Collaborative Filtering)在推荐系统中应用的非常广，该算法的目标是去填充用户-物品评分矩阵中的缺失值，即未评分。该算法的Spark的ML包和MLlib包中均有实现。\n\n其中涉及的参数如下：\n- numBlocks：数据分区的数目，默认为10\n- rank：隐向量的长度，默认是10（m * n => m * k - k * n）\n- maxIter：最大迭代次数，默认为10\n- regParam：正则化参数系数，默认为1.0\n- implicitPrefs：控制使用显式反馈还是隐式反馈，默认是false即显式反馈。\n- alpha：隐式反馈时的置信度参数，默认为1.0\n- nonnegative：是否对最小二乘使用非负约束，默认为false\n\n### 隐式反馈与显式反馈\n基于矩阵分解的协同过滤标准方法将用户-物品矩阵中的rate视为用户对项目给出的显式偏好，例如：用户对电影进行评分。\n\n在许多实际的用例中，通常只能获取隐式反馈数据（例如：观看，点击，购买，喜欢，分享等）。spark.ml中用于处理此类数据的方法取自Collaborative Filtering for Implicit Feedback Datasets。本质上，这种方法不是试图直接对评级矩阵进行建模，而是将数据视为表示用户操作观察强度的数字（例如点击次数或某人花在观看电影上的累积持续时间）。然后，这些数字与观察到的用户偏好的置信水平相关，而不是与项目的明确评级相关。然后，该模型试图找到可用于预测用户对项目的预期偏好的潜在因素。\n\n### 正则化参数\n通过用户-物品的评分矩阵中用户的评分物品数和物品收到的评分个数来作为正则项，解决最小二乘更新过程中的问题。 这种方法被命名为“ALS-WR”，可以参考论文： Collaborative Filtering for Implicit Feedback Datasets。它减小来regParam对数据集规模的依赖，因此我们可以将从采样子集中学习的最佳参数应用于完整数据集，并获得较好的结果。\n\n\n### 应用场景\n\nSpark ALS算法支持输出item 或者user的隐向量，据此我们可以计算出用户或者物品的相似度，继而进行排序得到用户或者item的top N相似user或者item。这样在数据进行召回时便可以进行召回了。\n\n比如根据用户用行为的物品召回，当用户浏览了若干了item时，便将这些item相似的item加入到召回池中，进行rank排序。\n\n### ML中的ALS实现\n\n```\nobject ALSML {\n\n    def main(args: Array[String]): Unit = {\n        val spark = SparkSession.builder().master(\"local[5]\").appName(\"ALSML\").enableHiveSupport().getOrCreate()\n        Logger.getRootLogger.setLevel(Level.WARN)\n\n        val input = \"data/sample_movielens_ratings.txt\"\n        val model_param = \"maxIters:10,rank:5,numBlocks:10,regParam:0.01,alpha:0.618,userCol:userId,itemCol:movieId,rateCol:rating,implicitPrefs:true\"\n        val output_model = \"model/als_ml\"\n        // 训练模型 找到合适的参数\n        runBasedML(spark,input,model_param,output_model)\n    }\n\n    def runBasedML(spark: SparkSession, input: String, param: String,output_model_path: String) = {\n        import spark.sqlContext.implicits._\n        val ratings = spark.read.textFile(input).map(parseRating).toDF()\n        val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))\n\n        println(\"创建并训练ALS模型 ...\")\n        val als = ALSMLUtil.createModel(param)\n        val model = als.fit(training)\n        println(\"模型的效果评估 ...\")\n        ALSMLUtil.evaluateModel(model, test)\n\n        println(\"为用户进行item推荐 ...\")\n        model.recommendForAllUsers(10).show(10)\n\n        println(\"为指定用户进行top N item推荐 ...\")\n        val users = ratings.select(als.getUserCol).distinct().limit(3)\n        model.recommendForUserSubset(users,10).show(10)\n\n        println(\"为item进行用户推荐 ...\")\n        model.recommendForAllItems(10).show(10)\n        println(\"为指定的item进行top N 用户推荐 ...\")\n        val movies = ratings.select(als.getItemCol).distinct().limit(3)\n        model.recommendForItemSubset(movies, 10).show(10)\n\n        println(\"输出隐向量 ...\")\n        model.itemFactors.rdd.map(f => (f.get(0), f.getList(1).toArray.mkString(\",\"))).take(10).foreach(println)\n\n        println(\"保存与加载模型 ...\")\n        model.write.overwrite().save(output_model_path)\n        val newModel = ALSModel.load(output_model_path)\n        newModel.itemFactors.rdd.map(f => (f.get(0), f.getList(1).toArray.mkString(\",\"))).take(10).foreach(println)\n    }\n\n    def parseRating(str: String): Rating = {\n        val fields = str.split(\"::\")\n        assert(fields.size == 4)\n        Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)\n    }\n\n    case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\n}\n```\n\n### MLlib中的ALS实现\n```\nobject ALSMLlib {\n\n    def main(args: Array[String]): Unit = {\n        val spark = SparkSession.builder().master(\"local[5]\").appName(\"ALS\").enableHiveSupport().getOrCreate()\n        Logger.getRootLogger.setLevel(Level.WARN)\n\n        val input = \"data/sample_movielens_ratings.txt\"\n        val model_param = \"maxIters:10,rank:5,numBlocks:10,regParam:0.01,alpha:0.618,implicitPrefs:true\"\n        val output_model_path = \"model/als_ml\"\n\n        run(spark, input, model_param, output_model_path)\n    }\n\n    def run(spark: SparkSession, input: String, model_param: String, output_model_path: String): Unit = {\n        println(\"加载数据 ...\")\n        val ratings = spark.sparkContext.textFile(input)\n            .map(_.split(\"::\").slice(0,3) match { case Array(userId, movieId, rating)  =>\n                Rating(userId.toString.toInt, movieId.toString.toInt, rating.toString.toDouble)\n            })\n        println(\"训练模型 ...\")\n        val param = new ALSMLlibParam()\n        param.parseString(model_param)\n        val model = ALS.train(ratings,param.getRank, param.getMaxIters,param.getAlpha,param.getNumBlocks)\n\n        println(\"评估模型 ...\")\n        val usersProducts = ratings.map { case Rating(user, product, rate) => (user, product) }\n        val predictions = model.predict(usersProducts).map{ case Rating(user, product, rate) => ((user,product),rate)}\n        val rateAndPre = ratings.map { case Rating(user, product, rate) => ((user, product), rate) }.join(predictions)\n        val MSE = rateAndPre.map { case ((user, product), (r1, r2)) =>\n            val err = (r1 - r2)\n            err * err\n        }.mean()\n        println(\"Mean Squared Error = \" + MSE)\n\n\n        println(s\"用户（2）对 物品（2）的预测评分为：${model.predict(2,2)}\")\n        println(\"用户纬度的特征向量为：\")\n        model.userFeatures.map(f => (f._1,f._2.mkString(\",\"))).take(10).foreach(println)\n        println(\"物品纬度的特征向量为：\")\n        model.productFeatures.map(f => (f._1,f._2.mkString(\",\"))).take(10).foreach(println)\n    }\n}\n```\n\n\n\n### 问题\n```\norg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat com.kk.recommend.tools.model.ALSBasedMLUtil$.evaluateModel(ALSBasedMLUtil.scala:51)\n\tat com.kk.recommend.topic.follow.ItemCFV2$.testBasedML(ItemCFV2.scala:104)\n\tat com.kk.recommend.topic.follow.ItemCFV2$.main(ItemCFV2.scala:40)\n\tat com.kk.recommend.topic.follow.ItemCFV2.main(ItemCFV2.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:688)\nCaused by: java.lang.ArrayStoreException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:90)\n\tat scala.collection.IndexedSeqOptimized$class.copyToArray(IndexedSeqOptimized.scala:180)\n\tat scala.collection.mutable.WrappedArray.copyToArray(WrappedArray.scala:35)\n\tat scala.collection.TraversableOnce$class.copyToArray(TraversableOnce.scala:278)\n\tat scala.collection.AbstractTraversable.copyToArray(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:286)\n\tat scala.collection.mutable.WrappedArray.toArray(WrappedArray.scala:73)\n\tat com.kk.recommend.tools.model.ALSBasedMLUtil$$anonfun$2.apply(ALSBasedMLUtil.scala:48)\n\tat com.kk.recommend.tools.model.ALSBasedMLUtil$$anonfun$2.apply(ALSBasedMLUtil.scala:46)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1364)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n```\n\n结局办法：\n\n解决办法：[点击阅读](https://stackoverflow.com/questions/32727518/genericrowwithschema-exception-in-casting-arraybuffer-to-hashset-in-dataframe-to)\n\n打印的Schema信息：\n```\nroot\n |-- userId: integer (nullable = false)\n |-- recommendations: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- topicId: integer (nullable = true)\n |    |    |-- rating: float (nullable = true)\n```\n\n> 在row 中的这些列取得时候，要根据类型取，简单的像String，Seq[Double] 这种类型就可以直接取出来，但是像 Seq[(Double,Double)] 这种类型直接取得花就会丢失schema信息，虽然值能取到，但是schema信息丢了，在dataFrame中操作的时候就会抛错\n\nALS测试结果数据的格式如下：\n\n|userId|     recommendations|\n------|----------------------\n  148 |  [[1972, 0.0334868...\n\n\n原始的写法是：\n\n```\nresult.select(\"userId\", \"recommendations\")\n        .filter(row => !(row.isNullAt(0) || row.isNullAt(1)))\n        .rdd.flatMap( l=>{\n            val uid = l.get(0).toString\n            val itemList = l.getAs[mutable.WrappedArray[(Int,Double)]](\"recommendations\")\n            for(item<- itemList) yield (uid, item._1.toString)\n        })\n    \n```\n修改后为：\n```\nresult.select(\"userId\", \"recommendations\")\n        .filter(row => !(row.isNullAt(0) || row.isNullAt(1)))\n        .rdd.flatMap( l=>{\n            val uid = l.get(0).toString\n            val itemList= l.getAs[Seq[Row]](1).map(x=>{(x.getInt(0),x.getFloat(1))})\n            for(item<- itemList) yield (uid, item._1.toString)\n            })\n```\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"基于协同的ALS算法原理介绍与实现","url":"/2019/08/08/RecSys/Spark与推荐系统/基于协同的ALS算法原理介绍与实现/","content":"> ALS也是一种协同算法，其全称是交替最小二乘法（Alternating Least Squares），由于简单高效，已被广泛应用在推荐场景中，目前已经被集成到Spark MLlib和ML库中，在下一篇文章会对其使用方式进行详细介绍，本篇文章主要介绍ALS的底层算法原理。\n\n<!--More-->\n\n### 最小二乘法（Least Squares）\n在介绍ALS算法之前，先来了解LS，即最小二乘法。LS算法是ALS的基础，是一种数学优化技术，也是一种常用的机器学习算法，他通过最小化误差平方和寻找数据的最佳匹配，利用最小二乘法寻找最优的未知数据，保证求的数据与已知的数据误差最小。\n\nLS也被用于拟合曲线，比如所熟悉的线性模型。下面以简单的线性一元线性回归模型说明最小二乘法。\n假设我们有一组数据{(x1,y1),(x2,y2),(x3,y3)...}其符合线性回归，假设其符合的函数为如下：\n$$\ny = w_0 + w_1 x\n$$\n我们使用一个平方差函数来表达参数的好坏，平方差函数如下：\n$$\nL_n = (y_n - f(x;w_0,w_1))^2\n$$\n其中f(.) 表示我们假设的线性回归函数。显然Ln越小越好，Ln越小表示误差越小。假设有N个样本，则N个样本的平均平方差为：\n$$\nL = \\frac{1}{N} \\sum_{n=1}^{N} (y_n - f(x;w_0,w_1))^2\n$$\nL越小表示参数w越精确，而这里最关键的就是寻找到最合适的w0和w1，则此时的数学表达式为：\n$$\n\\underset{w_0,w_1}{arg \\ min}  \\frac{1}{N} \\sum_{n=1}^{N} (y_n - f(x;w_0,w_1))^2\n$$\n将先行回归函数代入到最小二乘损失函数中，得到的结果为：\n$$\nL = \\frac{1}{N} \\sum_{n=1}^{N} (y_n - w_0 - w_1 x_n)^2\n\\\\\n\\frac{1}{N} \\sum_{n=1}^{N} (w_1 ^2x_n^2 + 2w_1x_n(w_0 - y_n) + w_0^2 - 2w_0y_n + y_n^2)\n$$\nL函数取得最小值时，w0和w1的一阶偏导数一定是0（因为误差平方和是一个大于等于0的数，是没有最大值的，所以取得最小值时，一阶偏导数一定为0）。因为对L函数分别求偏导，使其等于0，并对w0和w1求解，即可。\n\n### 交替最小二乘法（Alternating Least Squares）\nALS算法本质上是基于物品的协同，近年来，基于模型的推荐算法ALS(交替最小二乘)在Netflix成功应用并取得显著效果提升，ALS使用机器学习算法建立用户和物品间的相互作用模型，进而去预测新项。\n\n#### 基本原理\n用户对物品的打分行为可以用一个矩阵（R）来表示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190808233244547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n矩阵中的打分值 r_ij表示用户 u_i 对物品 v_j 的打分，其中”?”表示用户没有打分，这也就是要通过机器学习的方法去预测这个打分值，从而达到推荐的目的。\n\n#### 模型抽象\n根绝协同过滤的思想，R矩阵的行向量对应每个用户U，列向量对应每个物品V。ALS的核心思想是：将用户和物品都投射到k维空间，也就是说假设有k个隐向量特征，至于这个k个隐向量是什么不用关系（可能是标签，年龄，性别等），将每个用户和每个物品都用k维的向量来表示，把他们的内积近似为打分值，这样便可以得到近似的评分。\n$$\nR \\approx UV^T\n$$\n其中：\n- R为打分矩阵（m*n，m表示用户个数，n表示物品个数）\n- U表示用户对隐含特征的偏好矩阵（m*k）\n- V表示物品对隐含特征的归属矩阵（n*K）\n\n上述模型的参数就是U和V，求得U和V之后，就可以近似的得到用户对未评分物品的评分。\n\n#### 代价函数\n求上述公式中的U和V，就需要一个代价函数来衡量参数的拟合程度。用户对物品的行为分为显式行为和隐式行为，两种不同类型的行为下，对应的代价函数也是不一样的。\n> 关于显式行为和隐式行为的介绍可以餐考 我的《推荐系统开发实战》一书。\n\n**显式反馈代价函数**\n如果用户对物品有明确的评分行为，那么可以对比重构出来的评分矩阵和实际的评分矩阵，便可得到误差。由于用户对物品的评分却失很多，仅以有评分行为的物品去计算误差。下面是显式反馈的代价函数。\n$$\nJ(U,V) = \\sum_{i}^{m} \\sum_{j}^{n}[(r_{ij} - u_iv_j^T) ^2 + \\lambda ( ||u_i||^2 + ||v_j||^2 ) ]\n$$\n其中：λ 为正则项系数\n#### 隐式反馈代价函数\n***隐式反馈对应的ALS算法即：ALS-WR（Alternating Least Squares With Weighted-λ -regularization）***\n\n很多情况下，用户并没有明确反馈对物品的偏好，需要通过用户的相关行为去推测其对物品的偏好，比如在电商网站中，用户是否点击物品，点击的话在一定程度上表示喜欢，未点击的话可能是不喜欢，也可能是没有看到该物品。这种形式下的反馈就被称为隐式反馈。即矩阵R为隐式反馈矩阵，引入变量p_ij表示用户u_i对物品v_j的置信度，如果隐式反馈大于0，置信度为，反之置信度为0。\n$$\np_{ij} = \\left\\{\\begin{matrix}\n1  & r_{ij} >0 \\\\ \n0 & r_{ij} =0\n\\end{matrix}\\right.\n$$\n上文也提到了，隐式反馈为0，不代表用户完全不喜欢，也可能是用户没有看到该物品。另外用户点击一个物品，也不代表是喜欢他，可能是误点，所以需要一个信任等级来显示用户喜欢某个物品，一般情况下，r_ij越大(用户行为的次数)，越能暗示用户喜欢某个物品，因此引入变量c_ij，来衡量p_ij的信任度。\n$$\nc_{ij} = 1 + \\alpha r_{ij}\n$$\nα 为置信度系数，那么代价函数变为如下形式：\n$$\nJ(U,V) = \\sum_{i}^{m} \\sum_{j}^{n}[c_{ij}(p_{ij} - u_iv_j^T) ^2 + \\lambda ( ||u_i||^2 + ||v_j||^2 ) ]\n$$\n#### 算法求解\n无论是隐式代价函数求解还是显式代价函数求解，他们都是凸函数，而且变量耦合在一起，常规的梯度下降算法不能求解。但是先固定U求V，再固定V求U，如此迭代下去，问题就可以解决了。\n$$\nU^0 \\rightarrow V^1 \\rightarrow U^1 \\rightarrow V^2 \\rightarrow U^2 ....\n$$\n固定一个变量，求另外一个变量，用什么方法求解呢？梯度下降？可以，但是比较麻烦。这其实是一个最小二乘的问题，由于一般隐含的特征k不会太大，可以直接当做是正规方程去解决。如此的交替的使用最小二乘去求解，所以名字就叫做交替最小二乘法。\n\n**显氏反馈求解**\n固定V求解U，对公式进行求导化简，可得：\n$$\nU ^T = \\left( V^T V + \\lambda I \\right)^{-1} V^T R^T\n$$\n同理，固定U求解V，对公式进行求导化简，可得：\n$$\nV ^T = \\left( U^T U + \\lambda I \\right)^{-1} U^T R\n$$\n**隐式反馈求解**\n固定V求解U，对公式进行求导化简，可得：\n$$\nU ^T = \\left( V^T C_v V + \\lambda I \\right)^{-1} V^T C_v R^T\n$$\n同理，固定U求解V，对公式进行求导化简，可得：\n$$\nV ^T = \\left( U^T C_u U + \\lambda I \\right)^{-1} U^T C_u R\n$$\n\n---\n### 面试点\n1. 最小二乘法英文名字是什么？解释及其对应的数学原理\n```\nLeast Squares\n参考上文\n```\n2. ALS全称是什么？为什么叫交替最小二乘法？\n```\nAlternaing Least Squares\n参考上文\n```\n3. 隐式反馈和显氏反馈的区别？两种形式下ALS的代价函数\n```\n《推荐系统开发实战》中有对其的介绍\n 两种形式下的代价函数参考上文\n```\n4. 代价函数中的正则项及其含义？\n```\n参考上文\n```\n5. 过拟合和欠拟合的含义？在ALS中什么情况会出现过拟合和欠拟合？对应的解决办法？\n```\n欠拟合定义：拟合的函数与训练集误差较大，\n过拟合定义：拟合的函数与训练集完美匹配（误差很小）\n合适拟合定义：拟合的函数与训练集误差较小\n\n欠拟合出现原因：数据规模太小，特征太多，正则化项系数较小\n过拟合出现原因：数据特征太少，正则化项系数较大\n\n欠拟合解决办法：增大数据规模、减小数据特征数（维数）、增大正则化系数λ\n过拟合解决办法：增多数据特征数、添加高次多项式特征、减小正则化系数λ\n```\n6. Spark实现ALS可调节的参数有哪些？分别表示什么含义？\n```\nmaxIters // 最大迭代次数，默认10 \nrank // 隐向量的长度，默认是10，一般远小于m，n\nnumBlocks // 数据分区的个数，默认是10\nregParam // ALS中的正则化参数，默认是1.0\nalpha // ALS隐氏反馈变量的参数，置信度系数，默认是1.0\nuserCol // 用户列名\nitemCol // item列名\nrateCol // 评分列名\nimplicitPrefs // 显氏反馈 还是 隐氏反馈，默认false，意味显氏反馈\n```\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"【技术分享】2019全球人工智能技术峰会PDF资料拿走不谢","url":"/2019/08/08/Share/【技术分享】2019全球人工智能技术峰会PDF资料拿走不谢/","content":"\n> 2019 全球人工智能技术峰会PDF资料免费分享，资料内容涵盖各个方面，全部都是一线互联网公司的产业实践。\n\n<!--More-->\n\n### 工业实践\n- 「百度」源于产业实践的开源深度学习平台飞浆（PaddlePaddle）\n- 「易观」如何建设大数据中台（从0到1建设大数据中台）\n- 「华为」云边协同，重新定义AI\n\n### 机器学习\n- 「网易云」AI算法在音乐推荐中的应用\n- 「VIPKID」在线教育行业中视频理解的应用\n- 「美团点评」美团外卖商业变现实践\n### 搜索推荐\n- 「荔枝」荔枝UGC推荐探索与实践\n- 「金山」推荐系统在剑网3推栏项目中的落地\n\n### 知识图谱\n- 「瑞士再保险」知识图谱构建（数据，算法与架构）\n- 「美团点评」基于知识图谱的问答在O2O智能交 互场景中的应用和演进\n- 「中科院」基于知识图谱的问答关键技术—从答案到自然答案\n\n### NLP \n- 「阿里巴巴」语音对话机器人在阿里小蜜中的相关技术探索\n- 「追一科技」深度学习在企业智能交互中的应用\n- 「贝壳找房」4D看房：写稿机器人与VR的美丽邂逅\n\n### 智能安防\n- 「扇贝」A Short Intro: 无处不在的对抗样本攻防\n- 「友邦安达」AI边缘计算的崛起\n\n### 智能金融\n- 「旷视」旷视AI 在金融风控领域的运用\n- 「MinTech」MinTech的金融科技 实践与探索\n- 「百信银行」机器阅读在智能银行中的应用深度剖析与实践\n\n### 智慧零售\n- 「WakeData」唤醒沉睡的数据\n- 「MobTech」基于创新算法的半监督的 lookalike效果营销\n- 「京东」新零售时代的智慧中台\n\n### 智慧城市\n- 「哈罗出行」科技推动出行进化-密密织就的出行智能\n\n### 智能商业\n- 「苏宁」苏宁物流 智能决策系统建设与应用\n- 「贝锐科技」三生万物理论的实践进化\n- 「科大讯飞」一场客服与AI的融合之旅\n\n### IT架构\n- 「威佩网络」人工智能和大数据系统在电子竞技数据处理平台中的应用\n- 「快狗打车」快狗打车智能调度系统架构演进\n- 「国美」国美Redis集群-国美千亿级Redis集群架构变迁的思考\n\n### AIOps\n- 「宜信」分布式主动感知在智能运维中的实践\n- 「F5」无探针实时应用大数据采集引擎最佳实践和AIOps实现\n- 「日志易」海量日志分析与智能运维\n\n### 智能企业赋能\n- 「百度」企业赋能AI 服务生活 DuerOS的技能服务开发\n- 「蘑菇街」基于图像技术构建蘑菇街时尚目的地\n- 「51Talk」人工智能赋能教育-人工智能如何助力K12在线英语\n\n---\n\n以上资料，私聊微信获取\n<center><img src=\"https://img-blog.csdnimg.cn/20190808125509913.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=200px></center>\n\n---\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" width=200px>\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n\n<center>\n<font color=red>注：《推荐系统开发实战》已经在京东上线，感兴趣的朋友可以进行关注！</font>\n</center>\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=40% />\n</center>","tags":["技术分享"],"categories":["技术篇"]},{"title":"重庆保时捷女司机揭露了丑恶世道，但终要相信有善有正义有张小敬这样的长安不良帅","url":"/2019/08/04/随手记/重庆保时捷女司机揭露了丑恶世道，但终要相信有善有正义有张小敬这样的长安不良帅/","content":"\n世道很好，只是被藏的很好，只有走在那些青石板上，我才能看到石砖墙瓦的流芳百年\n\n世道很坏，只是很少的存在，只有遇见那些孤立无援，我才能看到人心冷暖的肆无忌惮\n\n<!--More-->\n\n> 这是我第一次写对社会事件的看法，当然不是为了蹭热点，而是这件事真的是触动了我。因为最近沉迷于关于西藏的记录片，真心感受到了藏民的质朴无华和亲切友善，然而这个开着保时捷的重庆女司机却再次刷新了我的三观。我以为那些不被人看到的丑恶会被岁月淡化，时光只会记录到存于这世界的美好，然而我失望了...\n\n> PS：最近看的两部藏区纪录片是《冈仁波齐》《阿拉姜色》\n\n\n7月30日上午，重庆一位驾驶保时捷的女子在斑马线违章掉头时与另一辆车的男司机发生口角。这件事本身没什么，同样作为一名司机，我觉得双方互相道个歉，然后故事就大结局了！不会像《长安十二时辰》中龙波能不能把花萼楼炸掉那样牵动着我的心，时时不能大结局。\n\n<video src=\"https://v.qq.com/x/page/y090600y5uh.html\" controls=\"controls\"></video>\n\n然而，女司机的一巴掌直接把她扇到了微博热搜，扇成了人民茶余饭后的谈资，扇出了她的往往种种违章，扇醒了沉迷于和平安乐的普罗大众，扇惊了她身后的派出所所长丈夫。\n\n再来看下女司机，16年购入保时捷至今，三年中共有29条交通违法记录，违法行为包括闯红灯、乱停车、违反禁止标线、驾驶时拨打接听手持电话等。曾经在郊区无意间闯了一个红灯，被扣了3分+200人民币！说实话对于这钱，这分我挺悔恨的，毕竟我们的驾驶证只有12分，我们也都是平凡人！但是她却不一样，29条违章记录之后依旧在道路上横行霸道？难道人家的本本跟我的不太一样！\n\n随着事件的发酵，这个江湖人称“月姐”的保时捷女司机被挖出了更多的“不良形象”，头批大波浪，戴白帽，身着喇叭裤，吊带小背心，黑色墨镜，嘴刁中南海，黑道感十足，简直就像黑道中的大嫂。再加上那个标志性的用食指指人的动作，说实话，遇见这种人，我会离她方圆十里开外！\n\n关于这件事的处理结果还没有公布，不过她的几句霸语倒惊醒了我：\n- “我出了名的飙车！红灯从来都是闯，没人敢把我怎么样。”\n- “我一个电话，所有记录都可以删除。”\n- “信不信，我分分钟找人弄你全家。”\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190804105026263.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\"></center>\n## 世道是怎样的败落，竟没人敢把你怎么样？\n\n“这世间的种种繁荣，都只不过是展现在你眼前的假象”，这句话说的有点过了，但是这世道上的一些黑暗是超乎我们想象的。\n\n某些拿着国家津贴的管员不作为，贪污受贿，包养小三，当被抓时的那种悔恨懊恼是你触碰法律底线最后的救赎吗\n\n某些身穿白大褂原本该济世救人的医生，却也受不了“灰色收入”的诱惑，而丢掉了该有的职业操守\n\n某些卖房时说的比唱的都好听的开发商，在收完定金和首付款之后，迟迟不能交房，最后变成烂尾楼\n\n这种事情在社会上屡见不鲜，但有法律这条约束线，还能起到警戒和惩罚的作用，我们能做的就是不管他人，约束好自己，不触碰法律的底线，要始终相信那些触犯法律的人终会得到制裁！\n\n## 这世上谁可以命令除他之外的所有人，是大唐时高高在上的圣人吗？\n\n权利大概是这个世界上最让人崇尚的东西，也是最让人畏惧的东西，没钱的怕有钱的，有钱的怕有权的，有权的天不怕地不怕，就怕权利更大的。\n\n不晓得这女司机是何方神圣，但这种通过“权利”可以办成一些平凡人 办不成的事的现象是显然存在的，这就像一个潜规则，只不过我们都是受害者！\n\n只要你背靠大树，那你就可以享受枝繁叶茂带来的阴凉。《长安十二时辰》中背靠永王的熊火帮不是烧杀抢掠，无恶不作吗！\n\n同样的一些其他潜规则，比如，谁有钱有权更霸道，谁就更有理，这是强者的逻辑。\n\n再比如，忍一时风平浪静，退一步海阔天空，这是弱者的借口。\n\n有句话说得好：人善被人欺，马善被人骑，柿子专挑软的捏。\n\n有些人之所以肆无忌惮、横行霸道，就是看准了老实人心地善良，不敢反抗。\n\n\n## 你的那句“弄死你全家”让我心有余悸。\n\n生老病死是人之常情，但欲加之罪何患无词。弱者生存的阶级里最害怕的就是强者的践踏。\n\n是你的错，你先动的手，他还了一下，就放话要弄死他全家！说实话真的怕了，能说出这句话的人，肯定处于金字塔的上层，这样她才有这样的底气，如此肆无忌惮。\n\n如今的法制社会，我们应该捍卫的是法律，在遇到事情时，拿起法律的武器来保护自己。而那个女司机则是靠着“法律执行人”的丈夫“弄死你全家”。\n\n想想也挺可笑的，士兵手里的剑是刺杀敌人的，医生手里的刀是济世救人的，官员手里的权利是维护治安的，而你则是来刷新我的三观的！\n\n## 那个扇了女司机的男司机以后会怎么样？\n在这个世界上，有人打心底里认为，只要有钱，便可以为所欲为，同样因为没钱，这世上的很多弱者在自己受了委屈之后，都会选择默不作声，任人宰割。\n\n即使是事件中的男司机，一时忍无可忍的，动手还击。可事后冷静下来，还是第一时间道歉了事。尽管错不在他，但我们也能体会到他内心的无奈。\n\n弱小的力量怎么能去和金钱，势力抗衡！\n\n《长安十二时辰》中有这样一个情节交代，永王（右相想要扶持的王子）因为为西域的小勃律使修建一个驿馆，在花萼楼上领功。被太子揭了老底，原来搞的暴力拆迁。而闻染的父亲闻无忌，虽为烽燧堡战役的幸存者，但依旧不能抵抗这场所谓的暴力拆迁，他的协商和争论只是让他白白送了姓名！\n\n这件事件的男司机代表的不正是我们这些平凡的人，堂堂的七尺男儿，为了妻儿家人，在事发之后也不得不低下头，进行了道歉！\n\n谁叫对方有钱优有势，而自己却是个普通之人呢！\n\n\n## 那个开保时捷的女司机后来怎么样了？\n\n重庆市公安局已经成立了调查组，彻查相关情况，依法处理，重视群众反应。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190805175924660.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\"></center>\n\n政府的态度已经很明显了，但那只是政府的官方态度，具体后事如何，暂时是未知的。不过这让我想起来了最近热播的《长安十二时辰》。\n\n其中发生的事件是：龙波携众人和阙乐霍多，对花萼楼麒麟臂进行偷梁换柱，意图送圣人西去，而“大案牍术”选择的张小敬对其穷追不舍，彻查此案。\n\n事件的起因是：安西铁军第八团在烽燧堡巡查之际，遭遇敌兵，于是死守城墙，但是援军迟迟不来，最后得知援军早已撤离，而援军不至的缘由是当时的兵部尚书林九郎并未下令进行支援，于是敌意升级到了大唐的圣人。\n\n不过圣人在这里是做了个冤大头为什么这么说呢？因为当时的底层官员上报的是大唐边境平静，并无敌军来犯，如果作为林九郎下令增兵烽燧堡，这无疑是在打圣人的脸，是拆穿大唐边境一片平和的谎言。那么圣人必会迁怒于他，他的右相之路恐怕就没那么顺利了吧...\n\n曾经表面“繁荣”的大唐不和现在的我们一样，看似和平安定，岁月静好，实则波涛汹涌，多少不为人知的“黑暗料理”一个个在显露出来，接下来我们就静等重庆女司机的处理结果吧！\n\n## 该有的光明会来，因为有张小敬这样的长安不良帅\n<center><img src=\"https://img-blog.csdnimg.cn/20190804105136566.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\n\"></center>\n一个好人，什么时候开始变坏？从他觉得孤立无援的那一刻起；\n\n一个老实人，什么时候开始冷漠，从他看透人心冷暖的那一秒始。\n\n可如果人人都如此，那社会上的好人就越来越少，坏人就越来越多。最后必然陷入整体崩溃中。\n\n所以，不要把这个世界让给那些坏人，也不要让好人在黑暗里孤独的抗争。\n\n十年饮冰难凉热血，面对丑恶，忍耐不是美德，愤怒才是。而愤怒，绝不是一两个人的事情，而是所有有正义感的人共同的事业。\n\n说到底，这世界并不完美，犹如表象繁荣的大唐，但却依旧值得我们去奋斗，就像张小敬一样，不退！！！\n\n你的善良，你的坚持，终会发光！\n\n---\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" width=200px>\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n\n<center>\n<font color=red>注：《推荐系统开发实战》已经在京东上线，感兴趣的朋友可以进行关注！</font>\n</center>\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=40% />\n</center>","tags":["随手记"],"categories":["随手记"]},{"title":"基于协同的Slope One算法原理介绍和实现","url":"/2019/08/02/RecSys/Spark与推荐系统/基于协同的Slope One算法原理介绍和实现/","content":"\n> 该篇文章主要介绍Slope One算法。Slope One 算法是由 Daniel Lemire 教授在 2005 年提出的一个 Item-Based 的协同过滤推荐算法。和其它类似算法相比, 它的最大优点在于算法很简单, 易于实现, 执行效率高, 同时推荐的准确性相对较高。 \n\n<!--More-->\n\n- [协同过滤算法理解和Python实现](https://blog.csdn.net/Gamer_gyt/article/details/51346159)\n- [基于标签的推荐算法](https://thinkgamer.blog.csdn.net/article/details/51684716)\n- [基于图的推荐算法](https://thinkgamer.blog.csdn.net/article/details/51694250)\n\n# 经典的ItemCF的问题\n\n经典的基于物品推荐，相似度矩阵计算无法实时更新，整个过程都是离线计算的，而且还有另一个问题，相似度计算时没有考虑相似度的置信问题。例如，两个物品，他们都被同一个用户喜欢了，且只被这一个用户喜欢了，那么余弦相似度计算的结果是 1，这个 1 在最后汇总计算推荐分数时，对结果的影响却最大。\n\nSlope One 算法针对这些问题有很好的改进。不过 Slope One 算法专门针对评分矩阵，不适用于行为矩阵。\n\n# Slope One算法过程\n\nSlope One 算法是基于不同物品之间的评分差的线性算法，预测用户对物品评分的个性化算法。\n\nSlope算法主要分为3步\n\n1. 计算物品之间的评分差的均值，记为物品间的评分偏差 (两物品同时被评分)\n$$\nR(i,j) = \\frac{ \\sum_{ u \\in N(i)\\bigcap N(j) } (r_{ui} - r_{uj}) }{ | N(i) \\bigcap N(j) | }\n$$\n> ( r_ui - r_uj ) 表示评分的差,这里需要注意的是j相对i的评分偏差是 r_ui - r_uj ，如果是i相对j的评分偏差则是 r_uj - r _ui,两 者是互为相反数的关系。\n\n其中：\n- r_ui ：用户u对物品i的评分\n- r_uj ：用户u对物品j的评分\n- N(i) ：物品i评过分的用户\n- N(j) ：物品j评过分的用户\n- N(i) 交 N(j) ：表示同时对物品i 和物品j评过分的用户数。\n\n2. 根据物品间的评分偏差和用户的历史评分，预测用户对未评分的物品的评分。\n$$\np_{uj} = \\frac{ \\sum_{i \\in N(u)} |N(i) \\bigcap N(j) |(r_{ui} - R(i,j))  }{ \\sum_{i \\in N(u)}|N(i) \\bigcap N(j)| }\n$$\n其中：\n- N(u) ：用户u评过分的物品\n\n\n3. 将预测评分进行排序，取Top N对应的物品推荐给用户\n\n\n# 实例说明\n例如现在有一份评分数据，表示用户对电影的评分：\n\n - | a | b | c | d | e\n --- | --- | --- | ----|--- | ---\nU1 | 2 | 3 | 3 | 4 | \nU2 |  | 4 | 2 | 3  | 3\nU3 | 4 | 2 | 3 |  | 2\nU4 | 3 |  | 5 | 4 | 3\n\n现在我们来预测预测每个用户对未评分电影的评分。\n\nStep1: 计算物品之间的评分偏差，以U1为例：\n$$\nR(a,b) = \\frac{ (2-3) + (4-2) }{ 2 } = 0.5\n$$\n$$\nR(a,c) = \\frac{ (2-3) + (4-3) +(3-5) }{ 3 } = -0.67\n$$\n$$\nR(a,d) = \\frac{ (2-4) + (3-4) }{ 2 } = -1.5\n$$\n$$\nR(a,e) = \\frac{ (4-2) + (3-3) }{ 2 } = 1\n$$\n\n同理可以计算出电影b，c，d，e与其他电影的评分偏差。\n\nStep2: 计算用户对未评分物品的可能评分（为了方便计算，这里以U2为例）\n\n由上表可知，用户U2 对电影a没有评分，这里计算用户U2对电影a的评分。\n\n$$\np_{u_2,a} = \\frac{2 *  (4-0.5) +3 * (2-(-0.67)) + 2 * (3-(-1.5) ) + 2 * (3-1))    }{ 2+3+2+2}  = 3.11\n$$\n\nStep3: 评分排序\n\n由于给定样例中，U2只对a没有评过分，所以这里不需要进行排序，正常的话，按分数进行倒排就行。\n\n# 代码实现\n\n这里采用Python实现，在实现过程中并没有考虑算法的复杂度问题。\n\n加载数据\n```\n    def loadData(self):\n        user_rate = {\n            \"U1\": {\"a\": 2, \"b\": 3, \"c\": 3, \"d\": 4},\n            \"U2\": {\"b\": 4, \"c\": 2, \"d\": 3, \"e\": 3},\n            \"U3\": {\"a\": 4, \"b\": 2, \"c\": 3, \"e\": 2},\n            \"U4\": {\"a\": 3, \"c\": 5, \"d\": 4, \"e\": 3}\n        }\n        item_rate = {\n            \"a\": {\"U1\": 2, \"U3\": 4, \"U4\": 3},\n            \"b\": {\"U1\": 3, \"U2\": 4, \"U3\": 2},\n            \"c\": {\"U1\": 3, \"U2\": 2, \"U3\": 3, \"U4\": 5},\n            \"d\": {\"U1\": 4, \"U2\": 3, \"U4\": 4},\n            \"e\": {\"U2\": 3, \"U3\": 2, \"U4\": 3}\n        }\n        return user_rate,item_rate\n```\n\n计算物品之间的评分偏差\n```\n    def cal_item_avg_diff(self):\n        avgs_dict = {}\n        for item1 in self.item_rate.keys():\n            for item2 in self.item_rate.keys():\n                avg = 0.0\n                user_count = 0\n                if item1 != item2:\n                    for user in self.user_rate.keys():\n                        user_rate = self.user_rate[user]\n                        if item1 in user_rate.keys() and item2 in user_rate.keys():\n                            user_count += 1\n                            avg += user_rate[item1] - user_rate[item2]\n                    avg = avg / user_count\n                avgs_dict.setdefault(item1,{})\n                avgs_dict[item1][item2] = avg\n        return avgs_dict\n```\n计算预估评分\n```\n    def item_both_rate_user(self, item1, item2):\n        count = 0\n        for user in self.user_rate.keys():\n            if item1 in self.user_rate[user].keys() and item2 in self.user_rate[user].keys():\n                count += 1\n        return count\n\n    def predict(self, user, item, avgs_dict):\n        total = 0.0 # 分子\n        count = 0   # 分母\n        for item1 in self.user_rate[user].keys():\n            num = self.item_both_rate_user(item, item1)\n            count += num\n            total += num * (self.user_rate[user][item1] - avgs_dict[item][item1])\n        return total/count\n```\n主函数调用\n```\nif __name__ == \"__main__\":\n    slope = SlopeOne()\n    avgs_dict = slope.cal_item_avg_diff()\n    result = slope.predict(\"U2\", \"a\", avgs_dict)\n    print(\"U2 对 a的预测评分为: %s\" % result)\n```\n\n打印结果为：\n```\nU2 对 a的预测评分为: 3.111111111111111\n```\n和上边我们计算的结果一致。\n\n完整代码在：https://github.com/Thinkgamer/Machine-Learning-With-Python/tree/master/Recommend\n\n# Slope One的应用场景\n\n该算法适用于物品更新不频繁，数量相对较稳定并且物品数目明显小于用户数的场景。比较依赖用户的用户行为日志和物品偏好的相关内容。 \n\n其优点： \n\n- 算法简单，易于实现，执行效率高； \n- 可以发现用户潜在的兴趣爱好； \n\n其缺点： \n- 依赖用户行为，存在冷启动问题和稀疏性问题。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n---\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n----\n\n<center>\n<font color=red>注：《推荐系统开发实战》已经在京东上线，感兴趣的朋友可以进行关注！</font>\n</center>\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=40% />\n</center>","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"【技术分享】你想知道的网易云音乐推荐架构解析，都在这里！","url":"/2019/08/01/Share/【技术分享】你想知道的网易云音乐推荐架构解析，都在这里！/","content":"\n> 本文选自网易云音乐推荐算法负责人-肖强前辈在全球人工智能峰会上的分享，主要介绍了三方面：关于网易云音乐，AI算法在音乐推荐中的应用和AI场景下的音乐思考。这里拿来分享给大家，并加上自己的理解，希望对大家有所帮助。\n\n<!--More-->\n\n> 首先说明我是网易云音乐的深度用户，目前级别LV9，每天都会去听日推。喜欢网易云音乐的原因不仅是友好的用户交互设计，而且还是因为在网易云音乐中能看到一个个陌生的故事。虽不知这些故事是真是假，但总会找到一些共鸣，在这里像是一个心灵的寄托一样。我相信大多数人和我一样，喜欢在敲代码的时候戴上耳机，来一个燥一点的音乐，所有的事情与我无关，我只想专心写代码，哈哈，开篇小小的题外话，下面进入正题。\n\n \n\n关于本文的PDF，可关注公号 合作=>私信 获取小编微信，私聊获取！\n\n本文将从三个方面介绍AI算法在网易云音乐推荐中的应用：\n- 关于网易云音乐\n- AI算法在音乐推荐中的应用\n- 音乐场景下的AI思考\n\n---\n\n# 关于网易云音乐\n> 关于网易云音乐的介绍就不用多说了，相信大家都知道这个产品。但是这个产品里边也有很多其他的业务，如下这些。当然推荐也会在各个业务线进行应用，最大化的提高用户体验。\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjlOWUpEU3laT0hGUkZERGJ1WGdWajdLdnBtWkNXaGY3S3Rvem5kZlQ1T3ZyT0tQMUdpY0tBalEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnBqYjhlUFhSakVEekV3SHFSUXBjSnB6NkROVnBFc01ZYzNSYWFHMlhYblNMaWIybDB4TjB3UFEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjdYNnlYdlQySjk0OGROcXAzbUZNTHo1S3Bnd0d2ZUhobWsxaWFMZ0pGMWJLb3VQbW5FQkloMncvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n# AI算法在音乐推荐中的应用\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjR5ZTFNaWFxckt2d1lBVGp0SUQ2ZlJKREFRQWljeGpFV3h3VHZkSGljUkk1WFFic2xpY3h1Y2xYZ1EvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n> 除了上图中介绍的三个场景以外，推荐在云音乐还有其他很多的应用，比如推荐的MV/视频，推荐的电台，推荐的Look直播等。\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjVMemc4MFg5RERWa053RVZpY01lUWNkc3dXRzJFT25QOXl2WXRBZldJbXlJdjNQUW9Rbk1WTVEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkt2U1htcGJ3c3JzWlE1SVNpYUZnUWhpY3hGNFg0bjB0amhBaFpMZXlpYm5EbnhpYWo3SGlicHRmcEtnLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n> 几乎所有的推荐系统或者业务系统的分析和底层数据支持都离不开用户的行为日志，在对基础的日志进行ETL处理之后，进行数仓的存储，画像模型，特征工程等均基于这个日志来进行构建。\n\n<br/>\n> 在此之上这是推荐系统的召回模块，然后进行粗排，精排和展示。这是所有推荐系统的通用架构，然后各个公司，各个部门会在此基础上进行适合各自业务的开发和精细化应用，以适应具体的需求，继而最大化的发挥推荐系统的价值。\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbll5WnBPaWFoU1JqWWhLRHVXdkYzVVNpY0pSbGlieUxIQ1E2TnJtSEdpYWhpYW41YVdvWGlhcWFybWt6US82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblpFS3IzMUJ3TnVpYzhrd0hOS0FHUzNscnFVaWNhREtLS3ZiQVB4WmxWQld3SEJMWnZ5eWgyek1nLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnhKaWFXbmw3V3hkWXRHOWt4aWNJMk85RzMyQ0NmVzk1SzQyQXFaOTN4dWt2aWNJMHdaSlJGWXVzQS82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnRSaGY4a2lhcjZqSXVYNmFjTWxCWG5tMUlUMEVFYXhzTlFNRE1WWUgwV2Fub29OOG80NjUyU0EvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjc1dXNibmJFeUJJdlZZZkdodE41aWFzbnYwN05IVGI1YnRUNjRwWThpY29xRFFiV1NJN3hIbzNRLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n> 不同的业务场景下，推荐的侧重点和实现方法是不一样的。音乐推荐，视频推荐，商品推荐，新闻推荐等，这些各自有各自的特点，音乐本身的复杂性，就要要求系统能够更好的理解音乐，网易云音乐则主要从NLP和视频，图像层面去理解音乐。\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkhPZWV6dVRaMmRxb2Uwa0swbTdweGE1VWRqdTE5VkJsUGljaWI3R2ljQ2ljUWdkeFQ0TFB4a21pYlFBLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjZ4UFNHY25qemVUdURhSzlyTGlhY2hKc2FicE5mTXRKbTBtRXd2MkFaZHYzTjV6SndKYzU3T3cvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n> 传统的CF算法应用的是余弦相似度或者Jaccard距离进行计算，云音乐这里则对其相似度计算方法进行了优化，其计算公式如上图所示，与传统相比，效果提升显著。PS：CF即协同过滤，虽然传统，但是在推荐中扮演的角色至关重要。\n\n> 接下来主要介绍下云音乐中的排序模型，其经历了线性模型，树模型，FTRL，深度学习模型，深度时序模型。PS：不过在这里小编要提醒的，做排序模型要一步一个脚印，一步一步来，我们不可能在建设推荐系统之时，直接上深度学习，因为只有经历过每个阶段的排序模型，我们才能更好的去理解业务，去提升业务。\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbmZSMjdmYXg0dThRMkFtcjNLTk9OMVc3QWREdHQ5UmhpYlh4QjZFZU1ya1pjemlhSjBoNXNMWEh3LzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkgwMU9vQmpVWUtwNHl4NkZWd200djBOS1NWN1c2U0FNMWNXdGlhODVBbGMxRFlpYmVTTGlhTU9aQS82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbmZOT1FzTEgxR0poTUE1NWliN2xpYzNKWlZaSjlxV2J6bUtWN3Q2NEk3VFdTbzVEbXJMUFEwZ1l3LzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbk1rVWR5cHRHV201b0x6NjJHckhFbldFY2liM2F2VEVSN1d1SjQzYU12bXdsNXNFdjBUVUdVbHcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblpqSXdNeWo0TDRHbVVnZkpDcURrODFLaWJmd2ljVUtyNVFkaWJBVUtqc21nQTlIVGtpYlJONXVQMXcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnhpY2tDaWF3ODM2c3pLZ3IzVFJrU2VxZFU2VkhYQnk4aG9vZ3M3ZFpZaWF4R0w2eTQ1c25obzZpYUEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbm11aWNRS3d3ZU96UnV4aWNONGd0OWtudHRUZE5BSmtsbVVtZlJQbE1oMXZxSFI4OTRVaDhyeGJBLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnpvUE1oamxxcW9XTXpMVmtiTXk1NGljR1VobFQzcWptVHI4cmpjYkk3ZUluaFRxbVpsamtzc3cvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnZBdTh6S1dpYzBiTmZGZWRmeGx3cUNpY0hwSkFQalpnUmpIU2ZsNUpCRmFnTXEwdVdTODVpY3ZmQS82NDA_d3hfZm10PXBuZw\"> </center>\n\n# 音乐场景下的AI思考\n> 当某种算法在推荐中发挥的价值很难再进行提升时，我们需要进行的是业务的深层次的思考和对更加有效或者先进的算法进行探索。\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbmlhbE1WdFhZMzgzUkNqUkx0MTZGSGJHOWgzaldtTEtHWXJQY3hQZ1Z4QWw5QzJnMzRtRWhZd1EvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnhJb2pQSmgyWFhKWWh1TVBVemNuRjM3dDlmQnNnWGszaG5VaHlNeGt5UEhSS3c0ZnhDRlhnZy82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblp4alJLWkN0aGFxc0dKQnEyb1BJSnFTUmNtMzB5RndIMXpvN3FyZWh3czFtWFFEWDNXRkRLQS82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkdYaWJkQjlUVm9KaWE3OEdoSWxTdGlhSkpxeFBVOXFjcDNscko2VHc5UzNOQ2JpY2liU1pCUkJ1S1d3LzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbklQaWNpYWZLWnVtemVBcVFma0o4dnZwcVpiTzVNOGp1Q2w5YXdlUzJ0UnRyTHNQNzhEbEptaWM1US82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblRpY2xTZWt0ejNKY29pYUdLWkhPRGliRzFJaWNIbnEyQlBHWDYzSTFSYXZZd2FoaWFaZzJKZHJtSzNBLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnZGSlBCUDhSME1SWGhydjUya3FtYUk3NEZLWHRKcUYzZEVKeFFhMm12d0lRUmdmc3BQaGVlUS82NDA_d3hfZm10PXBuZw\"> </center>\n\n---\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n<font color=red>注：《推荐系统开发实战》是小编的最新出版的技术图书，已经在京东，当当上线，感兴趣的朋友可以进行购买阅读！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["技术分享"],"categories":["技术篇"]},{"title":"【技术分享】美团外卖的商业变现的技术思考和实践","url":"/2019/08/01/Share/【技术分享】美团外卖的商业变现的技术思考和实践/","content":"> 本文选自美团-王永康前辈在全球人工智能峰会上的分享，主要介绍了四方面：业务介绍，平台侧收入优化，商家侧转化优化和用户侧体验优化。这里拿来分享给大家，并加上自己的理解，希望对大家有所帮助。\n\n<!--More-->\n\n本文将从四个方面介绍美团外卖商业变现实践：\n- 业务介绍\n- 平台侧：收入优化\n- 商家侧：转化优化\n- 用户侧：体验优化\n\n# 业务介绍\n首先介绍了美团外卖的业务情况，其包含了外卖商家360w，用户数3亿+，日活跃骑手数60w，覆盖城市2500+。其次介绍了外卖的业务形态，包含：\n- 展示广告\n- 搜索广告\n- feed流广告\n- 消息push广告\n\n其广告的转化形式由曝光到点击再到下单，则为一条有效的转化。其中涉及的名次含义为：\n- CPT：Cost Per Time（成本/时间），即按时长计费广告。按时长计费是包时段包位置投放广告的一种形式。\n- GD：Guarentee Delivery（保证交货），保证递送的广告，即保量广告，按展示量定价。\n- CPM：Cost Per Mille（成本/千次），千次展示成本，即按展示付费。\n- CPC：Cost Per Click（每次点击成本），每次点击成本，按点击付费，如关键词广告。\n- CPA：Cost Per Action（每次行为成本），CPA计价方式是指按广告投放实际效果，即按回应的有效问卷或定单来计费，而不限广告投放量。\n\n\n\n另外补充两个名次含义：\n- oCPC：optimization Cost Per Click（每次点击优化成本），目标转化成本，仍按点击付费。\n- 刷次：用户在APP信息流界面，手指每次下滑刷新，叫做一次刷次。\n\n同时介绍了计算广告的核心——最佳匹配。​\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmVaTEZwVGI1aWFMc1NPaWJYU25ITTMxMHdFY21YbmJpY1F2VTRkZTc5eHZZZlFlZUxSd3NuMDJwZy82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlh2MFl5NTZWUXhVZHFoN0c5c0o1WTVOZjB1Ynh4TEhYUE1MNjVITWRVMVNpYmlhVnVsQWpPcHFnLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlJJN1lHeTJnaWN2bUxmbUo2bjBPUDJKTDBjVFdjV1JYWkpRb1JRc3E1ZWh0eVhHNkxZb2ljcnlRLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n\n\n# 平台侧：收入优化\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlc2V3hpYldBbDVGWE13Yk9KUFBlSWU3ajNsTlJvcnNDRFhtcTJPckFESnhCZkFDa0tydzVmcFEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlhYM1IxY203U1FxOG54RDI2ME9xV2lhaDJCak93R3pONjQyM3hqTXdCaWJVcDdVdWd3NUtPTVBRLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlZZM2ljVjZpYWxuTGV5eDFYR0xQMXA2Y3ZpYWx5YXZrWVM0U1NXSW1QVUduVEV6aWJBcGFjMGhLdHcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRjFkbWN5akM0MFdySjI3MjlITTFPSkQyZ1p5d1prWW5PQlo5ZVRUdUhuOWlhRzl2bGpaMmEzYXcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmdEa210OW9lMzY1WktaRjI2RUd2Q0VwVDhBNUxrV0VweGlhdkk5ODg1SkVTeVZsc3ZadWgyNHcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnB0elhHMUdqcWlicGliaGZhejQ5SUl1YmE1aWNlcGVIdHl6cWFjY1NsbHlWZjczMFZJM0tVTkdNQS82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmhEeUFTZVpFWVRERFlpY2VFR3BlaHJpYnpvbDhqUG9UcEFPaWF0OGljVFc4RUxIQjc0Yng4YVpSblEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n> 需要注意的是这里提到了一个使用Side Information框架解决冷启动的方法，感兴趣的读者可以自己研究下什么是side information\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRjNTc2tmTjFiV21jeUNlZXV6NGZQTHVQQkk2Nm82aWM1OGNxczZnV0xINUhhRmdQOGEzWjVxdUEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n> 这里基于用户的行为序列数据，可以使用wode2vec，LSTM，VGG，Inception进行Embedding序列的生成\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRk93NFRUNkROWEZLVjZoVVlRY3IxcldFWHVxeTNpYmM2VlR6UUFhc0VSa1RDR2liWlVnbmJkS3VnLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlB5d1VtazA3YXlBTlhaa2N3NVBFbjNJUnp3c0drYXo0TVFiRWZGRlJRcWh3T2lja085UlFXVHcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRkhyVUNRaEtxcWREbDV1cXAzcFhxRHNaaGRQUGlhM2ZzSklrV04wa2lhTXUwb3JuRUtMYXhsTzFBLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRkRYMmh1S1JBNVFvdnBUMkJlbnByZGFpY2ZxYVJFRUk2aWJ6bzVJSjFhNW1naWNiT0xuTEVhSEZnUS82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnBISFU1Z3dyWGF6cG1DbXRwVHJja3Q5TTlXbk1Ub0JXaWFtQVJ2MHcxamJmdEtOR0t2c2lhczFRLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRkZjeG5RU1liOWxCUTVnOWJpYmZ2eHJmTVh0aWFTYXR3ZnFITHpLUkVZcThHMjh5OHdCR0gzaEhnLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n# 商家侧：转化优化\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnI2QWNiT1ZpYlo2VUxNQk1FeE15TzVkNmhndWliRWxUYlBLMFQzNkxqMFFIbTgzOVNQNVJxaWJLUS82NDA_d3hfZm10PXBuZw\"> </center>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRklGTmJoS0dQVTBnOER2dmNsbVdEZTVIMEVpYThHRmN0eVJsNzRUQ3lwdVI1N0ljQXN3aWFZRFVBLzY0MD93eF9mbXQ9cG5n\"> </center>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnZMSlBGWFNpYnB6bkQ5aWNkUHhEbGpQSVhVUmliMGRyTzl6a0xrVkVQSXd1V1NncEhTU0tVaWJqNUEvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmFSNEVJSUpVZFFoVHozY05ZaWNTdHNRcWY4VThLY0xZMVIwZ29iZFhqWHFmTno1NkhpY0ZOdHlRLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmRDVEhJbXEzb3JqeGljb2REbnBFd2liUVNadU5TcjZ1bWZYWU9kaDNrTjlpYW5JVmpsWmhrWnpIUS82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnE4emlhRUloV0Q4aWJhWThibE5pYlh2YUNPWjhQUTNra09ZUk5UUHROaWNnZjJvWkNWMW5xRVpGVHcvNjQwP3d4X2ZtdD1wbmc\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmIxTkFzZUswV29udVFqc1JYWFZpYmpveVBuelBkdk1lMXBxT3ZyTXhWUVNlc28xZmU2aGJ1MGcvNjQwP3d4X2ZtdD1qcGVn\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRjRENTBCUWZNYzRmTjV0R2FtSGZMWU0yVXhVdGlhYVgyNXFXY1pwcWljeUFBRVQ0TElmbkZqN3N3LzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRktpYWdNdWUxUzlkOHlpYUF3YnZRaEgxaWFTTzNyZFRjdVRhbndIM1BhT0I2YUZ3OThwbXNLU3c3US82NDA_d3hfZm10PXBuZw\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRno4bURaOWw0Rk5BeVdWeExldWxCYk50SDJQbURnT0dyNzJjc2x2TExCMWlia2lidUJ6VERyUkRRLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n# 用户侧：体验优化\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmljVVU1bW1vaFhzVEhCZDFHWG1BZ055NGtpYWlhbHdpYllkamJJM2d6Ulg0VTRQWXdmN2diaWFPWlJ3LzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRklORzJTM2t3R0w1UUV1aWFMbno5dk5sdklhY0J5bmNMVENGZ3NEbVp5N2lhclA1RDBnQ284YXZnLzY0MD93eF9mbXQ9cG5n\"> </center>\n\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlh4SjZwWFhkR2taS2E3ZFN6cWdJRnhCMHR6M1cyakhOUU1WT2VoTGxaSWZpYmlib2txYTF6dTN3LzY0MD93eF9mbXQ9cG5n\"> </center>\n\n---\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n<font color=red>注：《推荐系统开发实战》是小编的最新出版的技术图书，已经在京东，当当上线，感兴趣的朋友可以进行购买阅读！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>","tags":["技术分享"],"categories":["技术篇"]},{"title":"Spark MLlib 之 数据类型与大规模数据集的相似度计算原理探索","url":"/2019/07/29/Spark/Spark MLlib 之 数据类型与大规模数据集的相似度计算原理探索/","content":"\n> 本文出自「xingoo」在原文的基础上加以小编自己的理解形成的学习笔记，希望对读者有帮助。原文出自：[Spark MLlib 之 大规模数据集的相似度计算原理探索](https://www.cnblogs.com/xing901022/p/9296882.html)\n\n<!--More-->\n\n\n# 背景\n\n最近小编在做的是计算两两用户的粉丝重合度，根据粉丝重合度去评估两个用户之间的相似度，根据条件进行过滤之后大概有3000个用户，但每个用户的粉丝量参差不齐，有上百万的，有几千的，这样在去构建笛卡尔积的时候，进行粉丝数据关联，得到的用户集就会特别大，spark运行的时候就会很慢，而且会出现很严重的数据倾斜。这个时候了解到了spark支持的数据类型，看到了CoordinateMatrix，然后深究其原理，便看到了这篇文章，经过整理形成了此文。\n\n# Spark支持的数据类型\n\n官方文档地址：https://spark.apache.org/docs/latest/mllib-data-types.html\n\n## 1.Local Vector（本地向量）\n\n本地向量是从0开始的下标和double类型的数据组成，存储在本地机器上，所以称为Local Vector。它支持两种形式：\n- Dense （密集的向量）\n- Sparse （稀疏的向量）\n\n比如一个向量[1.0,0.0,3.0]，用Dense表示为：[1.0,0.0,3.0]，用Sparse表示为：(3,[0,2],[1.0,3.0])，其中3为向量的长度，[0,2]表示元素[1.0,3.0]的位置，可见sparse形式下0.0是不存储的。\n\n\n```\nimport org.apache.spark.mllib.linalg.Vectors\n\nval denseVector = Vectors.dense(1.0,0.0,3.0)\nval sparseVector1 = Vectors.sparse(3,Array(0,2),Array(1.0,3.0))\nval sparseVector2 = Vectors.sparse(3,Seq((0,1.0),(2,3.0)))\n\nprintln(s\"DenseVector is : $denseVector\")\nprintln(s\"DenseVector to Sparse is : ${denseVector.toSparse}\")\n\nprintln(s\"sparseVector1 is : $sparseVector1\")\nprintln(s\"sparseVector1 to Dense is : ${sparseVector1.toDense}\")\n\nprintln(s\"sparseVector2 is : $sparseVector2\")\nprintln(s\"sparseVector2 to Dense is : ${sparseVector2.toDense}\")\n```\n输出为：\n```\nDenseVector is : [1.0,0.0,3.0]\nDenseVector to Sparse is : (3,[0,2],[1.0,3.0])\n\nsparseVector1 is : (3,[0,2],[1.0,3.0])\nsparseVector1 to Dense is : [1.0,0.0,3.0]\n\nsparseVector2 is : (3,[0,2],[1.0,3.0])\nsparseVector2 to Dense is : [1.0,0.0,3.0]\n```\n## 2. Labeled point(带标签的点)\n\nlabeled point由本地向量组成，既可以是dense向量，也可以是sparse向量。在mllib中常用于监督类算法，使用double类型来保存该类型的数据，因为也可以用于回归和分类算法。例如二分类，label可以是0（负例）或1（正例），对于多分类，label可以是0，1，2...\n\n\n```\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nval pos = LabeledPoint(1.0, Vectors.dense(1.0,0.0,3.0))\nval neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))\n\n```\n\n**sparse data**\n\n稀疏数据存储是非常普遍的现象，mllib支持读取libsvm格式的数据，其数据格式如下：\n```\nlabel index1:value1,index2:value2 ...\n```\n其读取方式包括：\n```\nimport org.apache.spark.mllib.util.MLUtils\n\n// method 1\nspark.read.format(\"libsvm\") .load(\"libsvm data path\")\n\n// method 2\nMLUtils.loadLibSVMFile(spark.sparkContext, \"libsvm data path\")\n```\n\n## 3. Local Matrix（本地矩阵）\n\nlocal matrix由行下标，列索引和double类型的值组成，存储在本地机器上，mllib支持密集矩阵和稀疏矩阵，其存储是按照列进行存储的。\n\n例如下面的为密集矩阵:\n\n<center><img  src=\"https://img-blog.csdnimg.cn/20190722205304310.gif\" width=100px></center>\n\n通过数组存储的形式为： [1.0, 3.0, 5.0, 2.0, 4.0, 6.0]，矩阵大小为[3，2]\n\n```\n// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\nval denseMatrix = Matrices.dense(3,2, Array(1.0,3.0,5.0,2.0,4.0,6.0))\nprintln(s\"denseMatrix is : $denseMatrix\")\n\n// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nval sparseMatrix = Matrices.sparse(3,2, Array(0,1,3),Array(0,2,1),Array(9,6,8))\nprintln(s\"sparseMatrix is : $sparseMatrix\")\n```\n\n注：稀疏矩阵解释，首先指定矩阵是3行2列，Array(0, 1, 3)是指，第0个非零元素在第一列，第一第二个非零元素在第二列。\n\nArray(0, 2, 1)是指，第一个非零元素在第0行，第二个非零元素在第2行，第三个非零元素在第1行。\n\n此处设计比较好，假设100个元素分两列，不需要把每个元素所在列都标出来，只需要记录3个数字即可。Array(9, 6, 8)表示按顺序存储非零元素.\n\n--- \n\nArray(0,1,3)比较难理解，可以参考以下文章：\n- https://www.cnblogs.com/lyy-blog/p/9288701.html\n- https://www.tuicool.com/articles/A3emmqi\n\n## 4. Distributed Matrix（分布式矩阵）\n一个分布式矩阵由下标和double类型的数据组成，不过分布式的矩阵的下标不是int类型，而是long类型，数据保存在一个或多个rdd中，选择一个正确的格式去存储分布式矩阵是非常重要的。分布式矩阵转换成不同的格式需要一个全局的shuffle(global shuffle)，而全局shuffle的代价会非常高。到目前为止，Spark MLlib中已经实现了三种分布式矩阵。\n\n最基本的分布式矩阵是RowMatrix，它是一个行式的分布式矩阵，没有行索引。比如一系列特征向量的集合。RowMatrix由一个RDD代表所有的行，每一行是一个本地向量。假设一个RowMatrix的列数不是特别巨大，那么一个简单的本地向量能够与driver进行联系，并且数据可以在单个节点上保存或使用。IndexedRowMatrix与RowMatrix类似但是有行索引，行索引可以用来区分行并且进行连接等操作。CoordinateMatrix是一个以协同列表（coordinate list)格式存储数据的分布式矩阵，数据以RDD形式存储。\n\n注意：因为我们需要缓存矩阵的大小，所以分布式矩阵的RDDs格式是需要确定的，使用非确定RDDs的话会报错。\n\n### Row Matrix\n\nRowMatrix它是一个行式的分布式矩阵，没有行索引。比如一系列特征向量的集合。RowMatrix由一个RDD代表所有的行，每一行是一个本地向量。因为每一行代表一个本地向量，所以它的列数被限制在Integer.max的范围内，在实际应用中不会太大。\n\n一个RowMatrix可以由一个RDD[Vector]的实例创建。因此我们可以计算统计信息或者进行分解。QR分解（QR decomposition）是A=QR，其中Q是一个矩阵，R是一个上三角矩阵。对sigular value decomposition(SVD和principal component analysis（PCA）,可以去参考降维的部分。 \n　　\n```\n// Row Matrix\nprintln(\"Row Matrix ...\")\nval arr = Array(Vectors.dense(1,0),Vectors.dense(0,1))\nval rows = spark.sparkContext.parallelize(arr)\nval mat: RowMatrix = new RowMatrix(rows)\nval m = mat.numRows()\nval n = mat.numCols()\nval qrResult = mat.tallSkinnyQR(true)\nprintln(s\"m is: $m，n is $n，\\nqrResult is :\")\nqrResult.Q.rows.foreach(println)\nprintln()\nqrResult.R.rowIter.foreach(println)\n\n```\n输出为：\n```\nRow Matrix ...\nm is: 2，n is 2，\nqrResult is :\n[1.0,0.0]\n[0.0,1.0]\n\n[1.0,0.0]\n[0.0,1.0]\n```\n### IndexedRowMatrix\n\nIndexedRowMatrix与RowMatrix类似，但是它有行索引。由一个行索引RDD表示，索引每一行由一个long型行索引和一个本地向量组成。 \n\n一个IndexedRowMatrix可以由RDD[IndexedRow]的实例来生成，IndexedRow是一个（Long, Vector)的封装。去掉行索引，IndexedRowMatrix能够转换成RowMatrix。\n\n\n```\n// IndexedRowMatrix\nprintln(\"Indexed Row Matrix ...\")\nval arr2 = Array(\n        IndexedRow(0,Vectors.dense(1,0)),\n        IndexedRow(1,Vectors.dense(0,1))\n    )\nval rows2: RDD[IndexedRow] = spark.sparkContext.parallelize(arr2)\nval mat2 = new IndexedRowMatrix(rows2)\nval m2 = mat2.numRows()\nval n2 = mat2.numCols()\n// 去掉行索引，转换成RowMatrix\nval qrResult2 = mat2.toRowMatrix().tallSkinnyQR(true)\nprintln(s\"m2 is: $m2，n2 is $n2，\\nqrResult2 is :\")\nqrResult2.Q.rows.foreach(println)\nprintln()\nqrResult2.R.rowIter.foreach(println)\n```\n\n输出为：\n\n```\nIndexed Row Matrix ...\nm2 is: 2，n2 is 2，\nqrResult2 is :\n[1.0,0.0]\n[0.0,1.0]\n\n[1.0,0.0]\n[0.0,1.0]\n```\n\n### CoordinateMatrix\nCoordinateMatrix是一个分布式矩阵，其实体集合是一个RDD，每一个是一个三元组(i:Long, j:Long, value:Double）。其中i是行索引，j是列索引，value是实体的值。当矩阵的维度很大并且是稀疏矩阵时，才使用CoordinateMatrix。 \n\n一个CoordinateMatrix可以通过一个RDD[MatrixEntry]的实例来创建，MatrixEntry是一个(Long, Long, Double)的封装。CoordinateMatrix可以通过调用toIndexedRowMatrix转换成一个IndexedRowMatrix。CoordinateMatrix的其他降维方法暂时还不支持（Spark-1.6.2)。 \n　　\n\n```\n// CoordinateMatrix\nprintln(\"Coordinate Matrix ...\")\nval arr3 = Array(\n    MatrixEntry(0,0,1),\n    MatrixEntry(1,1,1)\n)\nval entries = spark.sparkContext.parallelize(arr3)\nval mat3 = new CoordinateMatrix(entries)\nval m3 = mat.numRows()\nval n3 = mat.numCols()\nval qrResult3 = mat3.toIndexedRowMatrix().toRowMatrix().tallSkinnyQR(true)\nprintln(s\"m3 is: $m3，n3 is $n3，\\nqrResult3 is :\")\nqrResult3.Q.rows.foreach(println)\nprintln()\nqrResult3.R.rowIter.foreach(println)\n```\n输出为：\n\n```\nCoordinate Matrix ...\nm3 is: 2，n3 is 2，\nrowMat3 is :\n[1.0,0.0]\n[0.0,1.0]\n\n[1.0,0.0]\n[0.0,1.0]\n```\n### BlockMatrix\n\n一个BlockMatrix是一个分布式的矩阵，由一个MatrixBlocks的RDD组成。MatrixBlock是一个三元组((Int, Int), Matrix),其中(Int, Int)是block的索引，Matrix是一个在指定位置上的维度为rowsPerBlock * colsPerBlock的子矩阵。BlockMatrix支持与另一个BlockMatrix对象的add和multiply操作。BlockMatrix提供了一个帮助方法validate，这个方法可以用于检测该`BlockMatrix·是否正确。\n\n可以通过IndexedRowMatrix或者CoordinateMatrix调用toBlockMatrix快速得到BlockMatrix对象。默认情况下toBlockMatrix方法会得到一个1024 x 1024的BlockMatrix。使用时可以通过手动传递维度值来设置维度，toBlockMatrix(rowsPerBlock, colsPerBlock)。\n\n\n```\n// BlockMatrix\nprintln(\"Block Matrix ...\")\nval arr4 = Array(\n    MatrixEntry(0,0,1),\n    MatrixEntry(1,1,1)\n)\nval entries4: RDD[MatrixEntry] = spark.sparkContext.parallelize(arr4)\nval coordMat: CoordinateMatrix = new CoordinateMatrix(entries4)\nval matA: BlockMatrix = coordMat.toBlockMatrix().cache()\n// 检测BlockMatrix格式是否正确，错误的话会抛出异常，正确的话无其他影响\nmatA.validate()\nmatA.blocks.foreach(println)\nval m4 = matA.numRowBlocks\nval n4 = matA.numColBlocks\nprintln(s\"m4 is: $m4，n4 is $n4\")\n\n// 计算A^T * A.\nval ata = matA.transpose.multiply(matA)\nata.blocks.foreach(println)\n\n```\n输出为：\n\n```\nBlock Matrix ...\n((0,0),2 x 2 CSCMatrix\n(0,0) 1.0\n(1,1) 1.0)\nm4 is: 1，n4 is 1\n((0,0),1.0  0.0  \n0.0  1.0  )\n```\n\n# 相似度计算原理探索\n> 无论是ICF基于物品的协同过滤、UCF基于用户的协同过滤、基于内容的推荐，最基本的环节都是计算相似度。如果样本特征维度很高或者<user, item, score>的维度很大，都会导致无法直接计算。设想一下100w*100w的二维矩阵，计算相似度怎么算？\n\n> 在spark中RowMatrix提供了一种并行计算相似度的思路，下面就来看看其中的奥妙吧！\n\n## 相似度计算\n相似度有很多种，每一种适合的场景都不太一样。比如：\n- 欧氏距离，在几何中最简单的计算方法\n- 夹角余弦，通过方向计算相似度，通常在用户对商品评分、NLP等场景使用\n- 杰卡德距离，在不考虑每一样的具体值时使用\n- 皮尔森系数，与夹角余弦类似，但是可以去中心化。比如评分时，有人倾向于打高分，有人倾向于打低分，他们的最后效果在皮尔森中是一样的\n- 曼哈顿距离，一般在路径规划、地图类中常用，比如A*算法中使用曼哈顿来作为每一步代价值的一部分（F=G+H, G是从当前点移动到下一个点的距离，H是距离目标点的距离，这个H就可以用曼哈顿距离表示）\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190722195328417.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\"> </center>\n\n上面两个向量(x1,y1)和(x2,y2)计算夹角的余弦值就是两个向量方向的相似度，其公式为：\n$$\ncos(\\theta )=\\frac { a\\cdot b }{ ||a||\\ast ||b|| } \\\\ =\\quad \\frac { { x }_{ 1 }\\ast { x }_{ 2 }\\quad +\\quad { y }_{ 1 }\\ast y_{ 2 } }{ \\sqrt { { x }_{ 1 }^{ 2 }+{ y }_{ 1 }^{ 2 } } \\ast \\sqrt { { x }_{ 2 }^{ 2 }+{ y }_{ 2 }^{ 2 } }  }\n$$\n\n其中，||a||表示a的模，即每一项的平方和再开方。\n\n## 公式拆解\n那么如果向量不只是两维，而是n维呢？比如有两个向量：\n$$\n第一个向量：({x}_{1}, {x}_{2}, {x}_{3}, ..., {x}_{n})\\\\\n第二个向量：({y}_{1}, {y}_{2}, {y}_{3}, ..., {y}_{n})\n$$\n他们的相似度计算方法套用上面的公式为：\n$$\ncos(\\theta )\\quad =\\quad \\frac { \\sum _{ i=1 }^{ n }{ ({ x }_{ i }\\ast { y }_{ i }) }  }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \\ast \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } \\\\ =\\quad \\frac { { x }_{ 1 }\\ast { y }_{ 1 }+{ x }_{ 2 }\\ast { y }_{ 2 }+...+{ x }_{ n }\\ast { y }_{ n } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \\ast \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } \\\\ =\\quad \\frac { { x }_{ 1 }\\ast { y }_{ 1 } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \\ast \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +\\frac { { x }_{ 2 }\\ast { y }_{ 2 } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \\ast \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +...+\\frac { { x }_{ n }\\ast { y }_{ n } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \\ast \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } \\\\ =\\quad \\frac { { x }_{ 1 } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }  } \\ast \\frac { { y }_{ 1 } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +\\frac { { x }_{ 2 } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }  } \\ast \\frac { { y }_{ 2 } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +...+\\frac { { x }_{ n } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }  } \\ast \\frac { { y }_{ n } }{ \\sqrt { \\sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  }\n$$\n\n通过上面的公式就可以发现，夹角余弦可以拆解成每一项与另一项对应位置的乘积x1∗y1，再除以每个向量自己的\n$$\n\\sqrt { \\sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }\n$$\n就可以了。\n\n## 矩阵并行\n\n画个图看看，首先创建下面的矩阵：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190722195942186.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\"></center>\n\n注意，矩阵里面都是一列代表一个向量....上面是创建矩阵时的三元组，如果在spark中想要创建matrix，可以这样：\n\n```\nval df = spark.createDataFrame(Seq(\n      (0, 0, 1.0),\n      (1, 0, 1.0),\n      (2, 0, 1.0),\n      (3, 0, 1.0),\n      (0, 1, 2.0),\n      (1, 1, 2.0),\n      (2, 1, 1.0),\n      (3, 1, 1.0),\n      (0, 2, 3.0),\n      (1, 2, 3.0),\n      (2, 2, 3.0),\n      (0, 3, 1.0),\n      (1, 3, 1.0),\n      (3, 3, 4.0)\n    ))\n\nval matrix = new CoordinateMatrix(df.map(row => MatrixEntry(row.getAs[Integer](0).toLong, row.getAs[Integer](1).toLong, row.getAs[Double](2))).toJavaRDD)\n```\n然后计算每一个向量的normL2，即平方和开根号。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190722200134362.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\"></center>\n以第一个和第二个向量计算为例，第一个向量为(1,1,1,1)，第二个向量为(2,2,1,1)，每一项除以对应的normL2，得到后面的两个向量：\n$$\n0.5*0.63+0.5*0.63+0.5*0.31+0.5*0.31 \\approx 0.94\n$$\n两个向量最终的相似度为0.94。\n\n那么在Spark如何快速并行处理呢？通过上面的例子，可以看到两个向量的相似度，需要把每一维度乘积后相加，但是一个向量一般都是跨RDD保存的，所以可以先计算所有向量的第一维，得出结果\n$$\n(向量1的第1维，向量2的第1维，value)\\\\\n(向量1的第2维，向量2的第2维，value)\\\\\n...\\\\\n(向量1的第n维，向量2的第n维，value)\\\\\n(向量1的第1维，向量3的第1维，value)\\\\\n..\\\\\n(向量1的第n维，向量3的第n维，value)\\\\\n$$\n最后对做一次reduceByKey累加结果即可.....\n\n## 阅读源码\n首先创建dataframe形成matrix：\n\n\n```\nimport org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\nimport org.apache.spark.sql.SparkSession\n\nobject MatrixSimTest {\n  def main(args: Array[String]): Unit = {\n    // 创建dataframe，转换成matrix\n    val spark = SparkSession.builder().master(\"local[*]\").appName(\"sim\").getOrCreate()\n    spark.sparkContext.setLogLevel(\"WARN\")\n\n    import spark.implicits._\n\n    val df = spark.createDataFrame(Seq(\n      (0, 0, 1.0),\n      (1, 0, 1.0),\n      (2, 0, 1.0),\n      (3, 0, 1.0),\n      (0, 1, 2.0),\n      (1, 1, 2.0),\n      (2, 1, 1.0),\n      (3, 1, 1.0),\n      (0, 2, 3.0),\n      (1, 2, 3.0),\n      (2, 2, 3.0),\n      (0, 3, 1.0),\n      (1, 3, 1.0),\n      (3, 3, 4.0)\n    ))\n\n    val matrix = new CoordinateMatrix(df.map(row => MatrixEntry(row.getAs[Integer](0).toLong, row.getAs[Integer](1).toLong, row.getAs[Double](2))).toJavaRDD)\n    // 调用sim方法\n    val x = matrix.toRowMatrix().columnSimilarities()\n    // 得到相似度结果\n    x.entries.collect().foreach(println)\n  }\n}\n```\n得到的结果为：\n\n```\nMatrixEntry(0,3,0.7071067811865476)\nMatrixEntry(0,2,0.8660254037844386)\nMatrixEntry(2,3,0.2721655269759087)\nMatrixEntry(0,1,0.9486832980505139)\nMatrixEntry(1,2,0.9128709291752768)\nMatrixEntry(1,3,0.596284793999944)\n\n```\n直接进入columnSimilarities方法看看是怎么个流程吧！\n\n\n```\ndef columnSimilarities(): CoordinateMatrix = {\n  columnSimilarities(0.0)\n}\n```\n内部调用了带阈值的相似度方法，这里的阈值是指相似度小于该值时，输出结果时，会自动过滤掉。\n\n\n```\ndef columnSimilarities(threshold: Double): CoordinateMatrix = {\n  //检查参数...\n\n  val gamma = if (threshold < 1e-6) {\n    Double.PositiveInfinity\n  } else {\n    10 * math.log(numCols()) / threshold\n  }\n\n columnSimilaritiesDIMSUM(computeColumnSummaryStatistics().normL2.toArray, gamma)\n}\n```\n这里的gamma用于采样，具体的做法咱们来继续看源码。然后看一下computeColumnSummaryStatistics().normL2.toArray这个方法：\n\n```\ndef computeColumnSummaryStatistics(): MultivariateStatisticalSummary = {\n  val summary = rows.treeAggregate(new MultivariateOnlineSummarizer)(\n    (aggregator, data) => aggregator.add(data),\n    (aggregator1, aggregator2) => aggregator1.merge(aggregator2))\n  updateNumRows(summary.count)\n  summary\n}\n```\n之前有介绍这个treeAggregate是一种带“预reduce”的map-reduce，返回的summary，里面帮我们统计了每一个向量的很多指标，比如\n\n```\ncurrMean    为 每一个向量的平均值\ncurrM2      为 每个向量的每一维的平方和\ncurrL1      为 每个向量的绝对值的和\ncurrMax     为 每个向量的最大值\ncurrMin     为 每个向量的最小值\nnnz         为 每个向量的非0个数\n```\n这里我们只需要currM2，它是每个向量的平方和。summary调用的normL2方法：\n\n\n```\noverride def normL2: Vector = {\n  require(totalWeightSum > 0, s\"Nothing has been added to this summarizer.\")\n\n  val realMagnitude = Array.ofDim[Double](n)\n\n  var i = 0\n  val len = currM2.length\n  while (i < len) {\n    realMagnitude(i) = math.sqrt(currM2(i))\n    i += 1\n  }\n  Vectors.dense(realMagnitude)\n}\n```\n上面这步就是对平方和开个根号，这样就求出来了每个向量的分母部分。\n下面就是最关键的地方了：\n\n\n```\nprivate[mllib] def columnSimilaritiesDIMSUM(\n      colMags: Array[Double],\n      gamma: Double): CoordinateMatrix = {\n    // 一些参数校验\n\n    // 对gamma进行开方\n    val sg = math.sqrt(gamma) // sqrt(gamma) used many times\n\n    // 这里把前面算的平方根的值设置一个默认值，因为如果为0，除0会报异常，所以设置为1\n    val colMagsCorrected = colMags.map(x => if (x == 0) 1.0 else x)\n\n    // 把抽样概率数组 和 平方根数组进行广播\n    val sc = rows.context\n    val pBV = sc.broadcast(colMagsCorrected.map(c => sg / c))\n    val qBV = sc.broadcast(colMagsCorrected.map(c => math.min(sg, c)))\n\n    // 遍历每一行，计算每个向量该维的乘积，形成三元组\n    val sims = rows.mapPartitionsWithIndex { (indx, iter) =>\n      val p = pBV.value\n      val q = qBV.value\n      // 获得随机值\n      val rand = new XORShiftRandom(indx)\n      val scaled = new Array[Double](p.size)\n      iter.flatMap { row =>\n        row match {\n          case SparseVector(size, indices, values) =>\n            // 如果是稀疏向量，遍历向量的每一维，除以平方根\n            val nnz = indices.size\n            var k = 0\n            while (k < nnz) {\n              scaled(k) = values(k) / q(indices(k))\n              k += 1\n            }\n\n            // 遍历向量数组，计算每一个数值与其他数值的乘机。\n            // 比如向量(1, 2, 0 ,1)\n            // 得到的结果为 (0,1,value)(0,3,value)(2,3,value)\n            Iterator.tabulate (nnz) { k =>\n              val buf = new ListBuffer[((Int, Int), Double)]()\n              val i = indices(k)\n              val iVal = scaled(k)\n              // 判断当前列是否符合采样范围，如果小于采样值，就忽略\n              if (iVal != 0 && rand.nextDouble() < p(i)) {\n                var l = k + 1\n                while (l < nnz) {\n                  val j = indices(l)\n                  val jVal = scaled(l)\n                  if (jVal != 0 && rand.nextDouble() < p(j)) {\n                    // 计算每一维与其他维的值\n                    buf += (((i, j), iVal * jVal))\n                  }\n                  l += 1\n                }\n              }\n              buf\n            }.flatten\n          case DenseVector(values) =>\n            // 跟稀疏同理\n            val n = values.size\n            var i = 0\n            while (i < n) {\n              scaled(i) = values(i) / q(i)\n              i += 1\n            }\n            Iterator.tabulate (n) { i =>\n              val buf = new ListBuffer[((Int, Int), Double)]()\n              val iVal = scaled(i)\n              if (iVal != 0 && rand.nextDouble() < p(i)) {\n                var j = i + 1\n                while (j < n) {\n                  val jVal = scaled(j)\n                  if (jVal != 0 && rand.nextDouble() < p(j)) {\n                    buf += (((i, j), iVal * jVal))\n                  }\n                  j += 1\n                }\n              }\n              buf\n            }.flatten\n        }\n      }\n    // 最后再执行一个reduceBykey，累加所有的值，就是i和j的相似度\n    }.reduceByKey(_ + _).map { case ((i, j), sim) =>\n      MatrixEntry(i.toLong, j.toLong, sim)\n    }\n    new CoordinateMatrix(sims, numCols(), numCols())\n  }\n```\n这样把所有向量的平方和广播后，每一行都可以在不同的节点并行处理了。\n\n总结来说，Spark提供的这个计算相似度的方法有两点优势：\n\n- 通过拆解公式，使得每一行独立计算，加快速度\n- 提供采样方案，以采样方式抽样固定的特征维度计算相似度\n\n不过杰卡德目前并不能使用这种方法来计算，因为杰卡德中间有一项需要对向量求dot，这种方式就不适合了；如果杰卡德想要快速计算，可以去参考LSH局部敏感哈希算法，这里就不详细说明了。\n\n---\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n<font color=red>注：《推荐系统开发实战》是小编的最新出版的技术图书，已经在京东，当当上线，感兴趣的朋友可以进行购买阅读！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之11、三大案例带你从0到1自己实现一个推荐系统","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之11、三大案例带你从0到1自己实现一个推荐系统/","content":"\n> 俗话说的好：“眼看千遍,不如手写一遍”，前几篇介绍了推荐系统的来源、发展、数据预处理、常见的推荐算法，以及冷启动和推荐系统效果评估。本篇文章将会通过实例介绍推荐系统的完整开发过程，以便大家有更加直观、系统的理解。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/96211201\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n\n在学习了《推荐系统开发实战》的一些基础知识之后，如何才能将这些知识应用到实际的案例中呢，本文介绍了以下三个案例，来带领大家温习和应用学到的知识。三大案例包含：\n- 新闻推荐系统\n- 音乐推荐系统\n- 图书推荐系统\n\n## 新闻推荐系统\n实现的第一个案例是新闻推荐系统，这里使用的数据集为：某新闻网站指定日期前的部分新闻数据，节选的几个主题为：国际要闻、互联网、经济要闻、社会公益、书评、影视综艺。爬取的每条数据包含三个字段：标题、时间、正文，如图所示。\n<center><img src=\"https://img-blog.csdnimg.cn/20190717002449843.png\" width=700px></center>\n在得到基础数据之后就是对数据的预处理，其主要包括：\n- 对原始数据的加工\n- 新闻相似度计算\n- 新闻热度值计算\n- 指定标签下的新闻统计\n\n其系统的整体架构如下所示：\n<center><img src=\"https://img-blog.csdnimg.cn/20190717002709673.png\" width=500px></center>\n\n其中各个模块介绍如下。\n- MySQL数据存储：这里使用MySQL存储系统所使用的数据。系统实现选用的是Python的Django框架，在框架中有对数据对象的封装，在第11.4节中将会说明所创建的数据对象。\n\n- 用户选择：系统指定了三个用户（张三、李四、王五）作为演示用户，只是为了区分不同用户、不同行为，“为你推荐”的内容也不同。\n\n- 选择标签：用户与系统的交互过程，解决系统的冷启动。当然用户也可以不选择相应的标签，此时“为你推荐”模块显示的是热度数据。\n\n- 用户点击浏览文章：即用户在系统中产生了相关行为，每篇文章的详细页都会推荐该篇文章的相似文章。\n\n- 热度榜：按照第11.2.2节中计算的热度值进行排序，显示热度值较大的新闻。\n\n- 为你推荐：如果用户是初次登录，则根据用户选择的标签返回“为你推荐”的内容；若用户没有选择标签，则返回热度值较高的新闻作为“为你推荐”的内容；如果用户是在点击\u001f\u001f浏览过新闻之后返回“为你推荐”模块，则返回用户有行为文章的相似文章，作为“为你推荐”的内容。\n\n系统整体架构设计比较简单，和线上真正应用的复杂的推荐系统是有很大差距的，但实现思路是一致的。\n\n接下来就是系统实现了，本实例采用前后端分离的形式进行实现，后端采用Python的Django框架进行开发，前端采用Vue.js框架开发。\n\n后端开发依赖于Python 3.6版本，其中使用的包为：Django==2.1，PyMySQL==0.9.2，jieba==0.39，xlrd==1.1.0，gensim==3.6.0。\n\n前端开发依赖于node.js环境，使用的是Vue.js框架，node.js对应的版本是10.13。\n\n其最终实现的效果图如下：\n<center><img src=\"https://img-blog.csdnimg.cn/20190717003718141.jpg\" width=700px></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190717003727282.jpg\" width=700px></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20190717003738407.png\" width=500px></center>\n\n\n通过这样一个实例，推荐系统变得不那么抽象，更加直观地展示在读者眼前。当然，每一个推荐系统背后都要付出很多，而不仅是算法层面，在从事相关工作时，更要拥有全局意识，要明白一个好的推荐系统是数据、算法、架构和展示等共同决定的，而不是靠“一己之力”。\n\n## 音乐推荐系统\n过多的内容这里不展开介绍，大家看下效果图，感兴趣的化可以关注《推荐系统开发实战》\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004024151.png\" width=700px></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004034658.png\" width=700px></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004044810.png\" width=700px></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004053533.png\" width=700px></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004101890.png\" width=700px></center>\n\n---\n## 图书推荐系统\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004217623.png\" width=700px></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190717004226269.png\" width=500px></center>\n<center><img src=\"https://img-blog.csdnimg.cn/2019071700423590.png\" width=700px></center>\n\n---\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之10、业内推荐系统架构介绍","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之10、业内推荐系统架构介绍/","content":"> 不管是电商网站，还是新闻资讯类网站，推荐系统都扮演着十分重要的角色。一个优秀的推荐系统能够推荐出让人满意的物品，但这不仅是推荐算法的功劳，整个推荐架构所扮演的角色也举足轻重。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/96265282\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n\n学术界往往更加关注推荐算法的各项评估指标。从基本的协同过滤到点击率预估算法，从深度学习到强化学习，学术界都始终走在最前列。一个推荐算法从出现到在业界得到广泛应用是一个长期的过程，因为在实际的生产系统中，首先需要保证的是稳定、实时地向用户提供推荐服务，在这个前提下才能追求推荐系统的效果。\n\n在生产系统中，不管是用户维度、物品维度还是用户和物品的交互维度，数据都是极其丰富的，学术界对算法的使用方法不能照搬到工业界。当一个用户访问推荐模块时，系统不可能针对该用户对所有的物品进行排序，那么推荐系统是怎么解决的呢？对应的商品众多，如何决定将哪些商品展示给用户？对于排序好的商品，如何合理地展示给用户？\n\n# 架构介绍\n图14-1所示是业界推荐系统通用架构图，主要包括：底层基础数据、数据加工存储、召回内容、计算排序、过滤和展示、业务应用。\n底层基础数据是推荐系统的基石，只有数据量足够多，才能从中挖掘出更多有价值的信息，进而更好地为推荐系统服务。底层基础数据包括用户和物品本身数据、用户行为数据、用户系统上报数据等。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190717080622698.png\" width=500px></center>\n\n图14-2～图14-4所示为用户本身数据、物品本身数据和用户行为数据。\n<center><img src=\"https://img-blog.csdnimg.cn/20190717080738525.png\" width=300px></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190717080746725.png\" width=300px></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190717080753359.png\" width=300px></center>\n\n得到底层基础数据之后，就要对数据进行加工处理和分析了，如结合用户属性信息和行为信息构建用户画像，结合物品属性信息和用户对物品的行为信息构建物品画像。基于用户对物品的行为数据构建特征工程，同时进行相关的数据分析。\n数据在处理之后存储到相应的位置（业务推荐系统使用的数据一般存储在redis中），供推荐系统实时调用。\n\n\n# 召回内容\n电商网站、内容网站、视频网站中数据量很大，并不能直接把所有的物品数据全部输送到推荐系统进行排序，那么如何对物品进行筛选就成了很关键的问题。\n第4章中介绍了一些常用的数据挖掘算法和应用场景，在进行物品召回时可以基于一些常用的机器学习算法构建用户偏好模型、用户兴趣模型、物品相似模型、物品互补模型等。在进行内容召回时，只召回和用户有偏好关系、和用户有直接关联、和用户有直接关系的相关物品，输入排序模型，进行打分排序。\n例如，在某新闻类网站中，根据用户对新闻的相关行为信息构建用户对新闻标签的兴趣模型，在为用户推荐时就可以推荐用户偏好标签下的新闻数据，如图14-5所示。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190717080944181.png\" width=500px></center>\n\n在物品召回过程中，重点是如何构建合适的用户偏好模型，只有保证偏好模型的准确性才能确保用户召回物品的准确性。\n\n# 计算排序\n## 特征工程\n“数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。\n\n特征工程在机器学习中起着非常重要的作用，一般认为包括特征构建、特征提取、特征选择三部分。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是：\n- 特征提取强调通过特征转换的方式得到一组具有明显物理意义或统计意义的特征；\n- 特征选择是从特征集合中挑选一组具有明显物理意义或统计意义的特征子集。\n\n两者都能帮助减少特征维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征对于模型构建的重要性。\n特征工程的标准化流程主要分为以下几步：\n（1）基于业务理解，找到对因变量有影响的所有自变量，即特征。\n（2）评估特征的可用性、覆盖率、准确率等。\n（3）特征处理：包括特征清洗、特征预处理（特征预处理可参考第4章“数据预处理”部分）、特征选择。\n（4）特征监控：特征对算法模型的影响很大，微小的浮动都会带来模型效果的很大波动，因此做好重要特征的监控可防止特征异常变动带来线上事故。\n## 特征分类\n在工业界的推荐系统中，典型的特征主要分为以下四类。\n- 相关性特征：评估内容的属性与用户是否匹配。显性的匹配包括关键词匹配、分类匹配、来源匹配、主题匹配等。\n- 环境特征：包括地理位置、时间。这些既是偏差特征，又能以此构建一些匹配特征。\n- 热度特征：包括全局热度、分类热度、主题热度及关键词热度等。内容热度信息在大的推荐系统中特别是在用户冷启动时非常有效。\n- 协同特征：可以在一定程度上帮助解决所谓算法越推越窄的问题。协同特征并不考虑用户已有历史，而是通过用户行为分析不同用户间的相似性，如点击相似、兴趣分类相似、主题相似、兴趣词相似，甚至向量相似，从而扩展模型的探索能力。\n## 排序算法\n在得到召回的物品之后，就要考虑如何对这些物品进行正确的排序。目前业界在机器学习领域最普遍的做法是将排序推荐模型作为二分类模型来训练，即在构造样本集的过程中对应的标签为0或1（未点击或点击）。常用的排序算法包括但不局限于GBDT、LR、XGBoost等，当然也有很多把GBDT和LR结合起来使用的，但是模型融合后的效果在不同的业务场景中带来的提升并不是很大。\n\n# 物品过滤和展示\n> 过滤和展示直接影响用户体验，因此在做推荐系统时一定要注意相关的过滤和展示规则。\n\n## 物品过滤\n经常会听到人们说“电商网站经常给我推荐我已经买过的东西”。其实在做推荐系统的过程中会有相关的过滤规则，在电商推荐系统中，最常用的过滤规则是：用户购买过滤，即在进行商品召回时过滤掉用户过去一段时间内已经购买过的商品和相似商品。例如，用户昨天买了一个机械键盘，今天的推荐系统就不会再给该用户推荐机械键盘了。\n同时也会有一些其他过滤规则如：\n- 项目指定的一些敏感词汇或敏感商品等过滤。\n- 刷单商品过滤。\n- 曝光商品过滤（有时会认为那些曝光过的商品是用户不感兴趣的，即看到了没有进行点击）。\n- 无货商品过滤。\n\n至于为什么推荐系统会给用户推荐已经购买过的商品，是因为在用户购买该商品之后，又对该类型的商品产生了新的行为，所以推荐系统会再次进行推荐。\n## 物品展示\n展示即用户看到的推荐结果。不同类型的推荐系统中展示的规则不一样，但基本原则是：品类隔离展示，即同类型的商品不能出现在相邻的位置。例如推荐系统返回的推荐结果集中有两个手机，这两个手机就不能在相邻的位置展示。\n\n有的推荐系统会要求第一屏内不能出现同类型的商品，如推荐系统给用户的第一屏展示了8个商品，那么这8个商品中就不能出现同类型的商品（如不能出现两个手机）。\n\n# 效果评估\n无论是推荐架构最开始的召回内容、计算排序，还是最后的过滤和展示，每次新上一个方案之后都要进行效果统计，生产系统中最常用的效果评估方法就是ABTest，更多关于ABTest的使用介绍可以参考第10章。\n在生产系统中，进行ABtest之后，往往会将不好的方案下线，保留效果更好的一方，同时也会不断上线新的召回、排序特征等，迭代优化模型，提升线上效果。\n\n---\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之9、效果评估","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之9、效果评估/","content":"\n> 推荐系统的评估方法分为用户调研、在线评估和离线评估。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/96207006\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n\n# 用户调研\n推荐系统的离线实验指标和实际商业指标之间存在差异。例如，预测准确率和用户满意度之间就存在很大的差异，高预测准确率不等于高用户满意度。因此，要准确评估一个算法，需要相对真实的环境，最好的方法就是将算法直接上线测试。但如果对算法是否会降低用户满意度不太有把握，那么直接上线往往有较大的风险，所以在上线测试前一般需要做一次用户调研。\n在进行用户调研的过程中，需要保证测试用户的分布和真实用户分布相同。例如，男女各一半，年龄、活跃度的分布都和真实用户分布尽量相同。此外，用户调查要尽量保证是双盲实验，即不要让实验人员和用户事先知道测试的目标，以免用户的回答和实验人员的测试受主观成分的影响。\n\n用户调研的优缺点也很明显。\n- 优点：可以获得很多体现用户主观感受的指标，比在线实验风险低，出现错误后很容易弥补。\n- 缺点：招募测试用户代价较大；很难组织大规模的测试用户，因此测试结果的统计意义不足。\n\n此外，在很多时候设计双盲实验非常困难，而且用户在测试环境下的行为和真实环境下的行为可能有所不同，因而，在测试环境下收集到的测试指标可能与真实环境下的不太相同。所以，在实际推荐系统评估过程中，基本不会采用该方式进行评估，相反，会通过线上的一些行为统计得出结果。例如，豆瓣FM频道的点赞和删除，界面如图10-1所示。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716234700573.png\" width=500px></center>\n\n# 在线评估\n在线评估：设计一个在线实验，然后根据用户的在线反馈结果来衡量推荐系统的表现。在线评估中，比较重要的两个选择点是——在线实验方式和在线评估指标，10.3和10.4节将分别介绍这两个知识点。\n\n## ABTest介绍\nABTest就是为了实现同一个目标制定两个方案，让一部分用户使用A方案，另一部分用户使用B方案，记录下两部分用户的反馈情况，然后根据相应的评估指标确认哪种方案更好。\n\n互联网行业里，在软件快速上线的过程中，ABTest是一个帮助我们快速试错的实验方法。在统计学上，ABTest其实是假设检验的一种形式。它能帮助开发者了解推荐系统的改动是否有效、能够带来多大的KPI提升。\n\n在推荐系统中，为了对比不同算法、不同数据集对最终结果的影响，通过一定的规则将用户随机分成几组，并对不同组采取不同的召回或推荐算法，最终通过不同组用户的各种评估指标来进行对比分析。\n\n一个典型得到ABTest架构图如下所示：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716234907436.png\" width=500px></center>\n\n这里需要注意的是，在对用户进行分桶并召回商品之后，需要重新将用户打散并分桶，这样能确保不同桶之间的用户没有相关性，召回池中的ABTest和排序部分的ABTest没有关联，互不影响。\n\n## ABTest注意事项\nABTest是一种在线上测试算法好坏的方法，由于其简单、直接、便于实施，被广泛应用在公司中。但在使用过程中仍要注意以下几个问题。\n（1）证实偏差。\n证实偏差是指：遇到一个命题时，人们倾向于寻找支持这个命题的证据，而忽略否定这个命题的证据。\n在ABTest中，算法工程师在调优的过程中，会自然地将测试假设和设计建立在他们自己的态度和观点上，而忽略了一些互相矛盾的信息，不去测试和设计与自己意见不一致的想法。这就会导致：一旦推荐系统出现符合预期的结果，他们会认为此前的想法是对的，也就不再进行实验了。这样就会导致很大的误差，不同的业务场景会受到营销和外界其他活动的影响。因此，在进行ABTest时要注意外界因素对系统的影响，应适当拉长测试周期。\n（2）幸存偏差。\n幸存偏差是一种认知偏差，其逻辑谬误为：推荐系统倾向于关注经常来访用户，而忽略他们在访问推荐系统的过程中已经被影响。\n例如“乔布斯勇于挑战体制取得了成功，所以大家都应该都去尝试挑战体制”，在这句话中没有提到那些挑战体制失败的人，仅以一名成功者为例说明，没有说明失败者的下场。\n在推荐系统中也一样，不能只关注来访者的行为特征和偏好，更要注意那些没有来访的用户的行为特征和偏好，这样才能保证推荐系统的泛化能力。\n（3）辛普森悖论。\n开始进行ABTest后，就不要更改设置、变量或对照的设计，并且不要在实验过程中更改已经分配到变量的流量。\n在测试期间调整变量的流量分配，可能会影响测试结果。当两组数据合并时，不同数据组中的趋势消失，便会产生“辛普森悖论”现象。\n（4）均值回归。\n在进行ABTest几天后，如果发现KPI指标有大幅提升，请不要立即下结论。因为这种早期的显著提升往往会在接下来的几天或几周的测试中逐渐消失，此时看到的不过是均值回归。也就是说，如果某一指标在第一次评估时出现极端结果，在后续的观察中，该指标会逐渐趋向于平均值。小样本尤其容易生产极端结果，因此不要在刚开始生成数据时就将所得到的结果解读成转化率，要适当增加测试时间，至少保证一个时间周期（如一周）。\n## 在线评估指标\n在线评估指标是指在实际的业务场景中去评估推荐系统好坏的指标。常见的在线评估指标包括点击率、转化率、GMV等。\n> 这里不展开介绍，欢迎阅读《推荐系统开发实战》\n\n# 离线评估\n离线评估：根据待评估推荐系统在实验数据集上的表现，基于一些离线评估指标来衡量推荐系统的效果。相比于在线评估，离线评估更方便、更经济，一旦数据集选定，只需将待评估的推荐系统在此数据集上运行即可。离线评估最主要的环节有两个：拆分数据集、选择离线评估指标。\n## 数据集拆分\n在机器学习中，通常将数据集拆分为训练数据集、验证数据集和测试数据集。它们的功能分别如下。\n- 训练数据集（Train Dataset）：用来构建机器学习模型。\n- 验证数据集（Validation Dataset）：辅助构建模型，用于在构建过程中评估模型，为模型提供无偏估计，进而调整模型的超参数。\n- 测试数据集（Test Dataset）：评估训练完成的最终模型的性能。\n三类数据集在模型训练和评估过程中的使用顺序如图所示。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190716235204796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n数据集拆分的方式有：\n- 留出法\n- K-折交叉验证法\n- 自助法\n\n> 具体每种方法是什么意思，以及他们的实现，这里不展开介绍，欢迎阅读《推荐系统开发实战》\n\n## 离线评估指标\n离线评估指标用于预估模型上线前在整个推荐系统中能达到的效果。常见的离线评估指标可以分为两大类：\n- 准确度指标：评估推荐系统的最基本的指标，衡量的是指标推荐算法在多大程度上能够准确预测用户对推荐商品的偏好程度，可以分为分类准确度指标、预测评分准确度指标、预测评分指标关联。\n- 非准确度指标：在推荐系统达到一定的准确度之后，衡量推荐系统丰富度和多样性等的指标。\n\n其中预测分类准确度指标包含：\n- AUC\n- 准确率（Accuracy）\n- 精确率（Precision）\n- 召回率（Recall）\n- F-measure值。\n\n预测评分准确度指标包含：\n- 平均绝对误差（MAE）\n- 均方误差（MSE）\n- 均方根误差（RMSE）\n\n预测评分关联指标包含：\n- 皮尔逊积距相关系统\n- 斯皮尔曼等级相关系数\n- 肯德尔等级相关系数\n\n预测排序准确度指标包含：\n- 评价排序分\n\n非准确度的指标包含：\n- 多样性\n- 新颖性\n- 惊喜度\n- 覆盖率\n- 信任度\n- 实时性\n- 健壮性\n- 商业目标\n\n> 具体指标的含义这里不过多介绍，欢迎阅读《推荐系统开发实战》了解具体含义和实现以及应用场景。\n\n---\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之8、冷启动介绍与解决","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之8、冷启动介绍与解决/","content":"> 推荐系统基于用户大量的历史行为做出事物呈现，因此用户的历史行为数据是构建一个优质推荐系统的先决条件，但在实际场景中并非所有的用户都拥有丰富的历史数据，如首次进入电商网站的用户。如何在没有丰富历史数据的情况下为用户推荐个性化的商品，这就是冷启动问题。 \n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/96203152\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n\n# 冷启动介绍\n冷启动主要分为三类：用户冷启动、物品冷启动、系统冷启动。\n- 用户冷启动：解决的是如何给新用户进行个性化推荐的问题。当一个新用户进入网站或APP时，由于系统之前没有任何关于该用户的历史行为数据，导致无法对用户进行兴趣建模，从而无法为该用户进行个性化推荐。\n- 物品冷启动：解决的是如何将新加入系统的物品推荐给用户。由于新物品没有任何被动行为，在系统中所占的权重几乎为0，这会导致在对商品排序或进行协同过滤推荐时该物品无法出现在推荐列表中。\n- 系统冷启动：解决的是在一个新系统中没有用户，也没有用户行为，只有物品信息，如何给用户进行个性化推荐的问题。\n\n针对推荐系统的冷启动，主要有以下几种实现方法：\n- 基于热门数据推荐；\n- 利用用户注册信息；\n- 利用用户上下文信息；\n- 利用第三方数据；\n- 利用用户和系统之间的交互；\n- 利用物品内容属性；\n- 利用专家标注数据。\n下面将对如何解决推荐系统中的冷启动问题进行解答。\n\n# 基于热门数据推荐实现冷启动\n热门数据是指（某类）物品按照一定规则进行排序得到的排名靠前的数据。热门数据反映的是大众的偏好，但受外界影响因素较大。例如某电商网站上的一个商品推广广告，可能会导致该商品在很短的时间内热度飙升；某新闻网站中的一条娱乐新闻，热度容易受舆论和明星效应的影响。\n虽然热门数据不能够准确地传达出用户偏好，但在某种程度上也是用户群体中大部分人的短期兴趣点。将热门数据作为解决用户冷启动的推荐数据，“个性化”地展示给用户，用户在这些数据中产生行为之后，再进行个性化推荐。\n\n热门数据排行榜在实际场景中应用十分广泛。例如，当用户新到达一个地方，打开某生活服务APP的美食频道后，附近的商家就会默认以热度排序展示给用户，如图9-1所示。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716232241310.png\"  width=400px/></center>\n\n> 在工业界的推荐系统中，召回的类型是多种多样的，但往往也会召回性别热门、地域热门或群体热门等数据，进而扩展推荐系统的丰富性。同时，在用户行为较少或偏好较少的情况下，也能准确地为用户进行商品推荐。\n\n# 利用用户注册信息实现冷启动\n用户注册信息是指用户在新注册一个系统时所填写的信息。这些信息是联系新用户和系统的关键，也是系统获取的用户直接信息。\n\n当一个新用户注册某个网站时，系统并不知道该用户喜欢什么物品，系统可以基于热门数据推荐为用户进行商品推荐。但如果系统能在用户进行注册时获取一些信息，则可以根据这些信息为用户进行商品推荐。例如，系统知道该用户来自东北，就可以给他推荐一些东北区域的热门物品；若知道该用户是来自东北的女性朋友，那么就会在地域的约束条件内，再给她推荐一些适合女性使用的物品，或者在性别的约束条件内，给她推荐一些区域内的热门商品。\n\n用户在进行注册时，所填写的注册信息可以分为以下三类。\n- 人口统计学信息：包括用户的年龄、身高、体重、居住地等。\n- 用户兴趣的描述：某些网站或APP会让用户填写自己的兴趣爱好。\n- 其他网站的导入数据：如用户通过微信、微博等登录第三方网站。\n\n网站获取这些数据之后，就可以对用户进行粗粒度的个性化推荐了。其推荐的大致流程如下：\n- 获取用户注册信息；\n- 根据用户注册信息对用户进行分类（可以是多分类，即一个用户被分到多个类别中）；\n- 给用户推荐其所在分类的用户最喜欢的物品，对不同类别下的物品进行加权求和。\n# 利用用户上下文信息实现冷启动\n前边介绍了用户的时间和地域等上下文信息在推荐系统中的应用。在实际的业务场景中，用户的上下文信息所构造的特征维度更加丰富，如用户使用的设备信息、用户所处的时间地域信息、用户看到商品的展示信息。\n> 上下文特征是代表用户当前时空状态、最近一段时间的行为抽象的特征。\n## 设备信息特征\n设备信息主要是用户进行浏览的载体（手机、平板电脑、电脑等）的信息。不同设备所携带的信息是不一样的。例如，手机或平板的操作系统分为iOS、Android、PC等；计算机的操作系统分为Windows、MacOS、UNIX等，手机和计算机品牌更是多种多样，不同设备的分辨率、屏幕尺寸、价格也是不一样的。\n不同设备下用户的偏好也是不一样的。例如，iOS系统的用户可能是个“苹果粉”，那么在冷启动时就可以推荐一些苹果相关的产品，为了提升推荐系统的丰富度，也可以推荐一些手机数码类别的商品。再如，用户是用UNIX系统进行商品浏览的，那么该用户有可能是IT工作者，可以给该用户推荐一些数码、技术书籍等商品。\n当然，除了在冷启动中使用这些特征，在正常训练推荐系统模型时也可以对设备的相关信息进行特征构造，进而作为特征来训练模型。\n## 时间地域信息特征\n时间和地域是推荐系统中比较重要的信息，在第7章中介绍了相关内容。那么针对冷启动，时间和地域是怎样发挥它们作用的呢？\n时间可以是节假日、季节、周末等。地域可以是省市区、经纬度等，也可以是逻辑上的区域划分（如中关村软件园、商务中心区、海滨城市等）。当一个新用户来访时，通过对其建立时间和地域上的映射来为用户召回相关的商品并进行推荐。\n在构建特征时，时间和地域也是非常重要的。例如，对于时间，可以构造如下特征：是否是工作日、是否是休息日、访问时间所属的时间段（早上、上午、中午、下午、晚上、凌晨）等。\n## 实现原理\n针对用户的上下文信息，可以根据用户的历史数据分析出用户在相应属性下的行为偏好，为相应的商品打上对应的时间和地域信息。在新用户来访时，系统通过获取时间和地域信息，召回对应属性下的数据，并按照一定的规则进行排序，然后返回前 K条数据给用户。\n例如，服装、食物类商品有着明显的季节属性，特产、海鲜类商品有着明显的节日属性，每个商品都有产地属性和品牌属性，这样通过相应的标签就可以将用户的上下文信息和已经构建好的标签进行关联了。\n同样，不仅可以将用户的上下文信息作为召回的标签数据，也可以对信息属性进行One-Hot编码，作为训练模型的特征使用。\n# 利用第三方数据实现冷启动\n目前很多APP支持第三方账户登录，通过第三方的授权登录，系统可以获取到用户在第三方平台上的相关信息（包括用户本身的属性信息和朋友关系信息），从而可以使用协同过滤算法计算出用户可能感兴趣的商品，进而解决用户的冷启动问题，为用户推荐个性化的内容。\n\n# 利用用户和系统之间的交互实现冷启动\n交互，即用户对系统的推荐结果做出反馈，或者系统通过一定的方式向用户进行兴趣征集。交互不仅在冷启动方面有着比较重要的作用，在推荐系统的结果反馈中也有着很重要的作用。\n\n> 交互实现冷启动的原理和推荐系统中实时交互的应用有那些呢？\n\n# 利用物品的内容属性实现冷启动\n前边介绍的是如何解决推荐系统中的用户冷启动问题，对于物品冷启动问题该怎么解决呢？物品冷启动要解决的问题是，如何将新加入系统的物品推荐给对它感兴趣的用户。\n\n物品的冷启动在新闻、娱乐、资讯类网站中格外重要。由于新闻的生命周期较短，如果无法在短期内将其曝光给更多的用户，那么其本身的价值将大大减小。\n\n对于物品的冷启动，可以利用物品的内容属性将物品展示给尽可能符合其偏好的用户。\n\n## 物品内容属性的分类\n物品的内容属性多种多样，不同类型的物品有不同的内容属性，这里将物品的内容属性概括为三大类：\n- 物品本身的属性：用来描述物品本身的属性，具有宏观上的唯一性，如物品编码、标题名字、产出时间等。\n- 物品的归纳属性：用来形容物品的类别信息属性，具有宏观概括性，如类别、品牌、标签、风格等。\n- 物品的被动属性：用户表示物品的被动行为属性，具有客观概括性，如物品的浏览量、点击率、评论等。\n\n> 物品内容属性分析？ 物品内容属性应用？\n\n# 利用专家标注数据实现冷启动\n很多推荐系统在刚开始建立时，既没有用户行为数据，也没有能用来准确计算物品相似度的物品信息。那么为了在刚开始就让用户获得良好的体验，很多系统都会采用专家标数据。这方面的代表是Pandora音乐电台。\n\nPandora是一个给用户播放音乐的个性化网络电台。计算音乐、视频之间的相似度是非常难的，音乐属于流媒体，如果从音频分析入手计算音乐的相似度，技术门槛很高，而且结果也难以令人满意。另外，仅仅利用音乐的专辑、歌手等信息也难以得到令人满意的结果。\n\n为了解决这个问题，Pandora启动了一项称为“音乐基因组”的项目。这个名为“音乐基因组”的项目开始于1999年，该项目雇用一批懂计算机的音乐人，听了几万名歌手的歌，并对这些歌从各个维度进行标注，最终他们提供了450多维的特征来区分不同的音乐，这些标签可以细化到一首歌是否有吉他的和弦、是否有架子鼓、主唱的年龄等。在得到这些维度的特征之后就可以利用基于内容的推荐算法进行相似度计算了。这里的内容指的是音乐本身所表现出来的内容。\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之7、基于点击率预估的推荐算法介绍和案例开发实战","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之7、基于点击率预估的推荐算法介绍和案例开发实战/","content":"> 本系列之前介绍的都是一些基本的推荐算法，将这些算法真正应用到工业界（即应用推荐系统的地方，如电商网站、广告推广等）其实是很难的。并不是说这些算法没有用武之地，而是要根据具体的场景来判断是否能使用推荐系统。本篇会先对传统的推荐算法进行总结和说明，然后对目前业界用得最广的GBDT算法和LR算法进行介绍。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/95519780\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n# 传统推荐算法的局限和应用\n## 1. 海量数据\n例如，协同过滤算法能够容易地为“千万”级的用户提供推荐，但是对于电子商务网站（其用户数和物品数往往以“亿”来计量），协同过滤算法就很难提供服务了。\n在协同过滤算法中，能利用最新的信息及时为用户产生相对准确的用户兴趣度预测，或者进行推荐。但是面对日益增多的用户，数据量急剧增加，算法的扩展性问题（即适应系统规模不断扩大的问题）成为制约推荐系统实施的重要因素。\n与基于模型的算法相比，全局数值算法虽然节约了为建立模型而花费的训练时间，但是其用于识别“最近邻居”算法的计算量会随着用户和物品的增加而急剧增大。\n对于以“亿”来计量的用户和物品，通常的算法会遇到严重的扩展性瓶颈问题。对于采用了协同过滤技术的推荐系统，该问题解决不好，直接会影响其实时性。推荐系统的实时性越好、精确度越高，该系统才越会被用户所接受。\n## 2. 稀疏性\n伴随着海量数据的一个问题便是数据的稀疏性。\n在电子商务网站中，活跃用户所占的比例很小，大部分用户都是非活跃用户，非活跃用户购买或点击的商品数目也很少。因此，在使用协同过滤算法构建矩阵时，矩阵会非常稀疏；使用基于内容的推荐算法为用户构建的偏好矩阵也是非常稀疏的。这样，一方面难以找到最近邻的用户集，或者难以准确地得到用户行为偏好；另一方面，在计算的过程中会消耗大量的资源。\n## 3. 实时性\n实时性是评判一个推荐系统能否及时捕捉用户兴趣变化的重要指标。推荐系统的实时性主要包括两方面：\n- 推荐系统能实时地更新推荐列表来满足用户新的行为变化；\n- 推荐系统能把新加入系统的物品推荐给用户。\n而传统的协同过滤算法每次都需要计算所有用户和物品的数据，难以在“秒”级内捕捉到用户的实时兴趣变化。\n\n# 点击率预估在推荐系统中的应用\n点击率预估（CTR）最早应用于搜索广告中。时至今日，点击率预估的应用场景不仅从最开始的搜索广告扩展到展示广告、信息流广告等各种各样的广告，而且在推荐系统的场景中也得到了广泛应用。\n\n从用户的点击行为来分析，“点击率预估”在广告或推荐场景中的应用是一致的。广告的“点击率预估”计算的是用户点击广告的可能性；而在推荐系统中，推荐商品也被预测用户的兴趣，如果用户对一个商品感兴趣便会去点击。这也是近些年CTR在推荐系统中被广泛应用的原因。\n目前在CTR领域应用较多的算法包含LR、GBDT、XGBoost、FM、FFM、神经网络算法等，这些算法也被应用到推荐系统中。其中，GBDT是一种非线性算法，基于集成学习中的Boosting（提升方法）思想，每次迭代都在减少残差的梯度方向新建立一棵决策树，迭代多少次就会生成多少棵决策树。\n\nGBDT算法的思想使其具有天然优势：可以发现多种有区分性的特征和特征组合；决策树的路径可以直接作为LR输入特征使用；省去了人工寻找特征、特征组合的步骤。\n\n# 点击率预估算法的基础\n## 集成学习\n机器学习算法分为有监督学习算法和无监督学习算法。在有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面都表现较好的模型。但实际情况往往不理想，有时只能得到多个在某些方面表现比较好的“弱监督模型”。集成学习就是组合多个“弱监督模型”以得到一个更好、更全面的“强监督模型”。\n集成学习本身不是一个单独的机器学习算法，而是通过构建并组合多个弱学习器来完成学习任务，如图所示\n<center><img src=\"https://img-blog.csdnimg.cn/20190716083854202.png\" width=400px /></center>\n集成学习包括Boosting算法（提升法）、Bagging算法（自助法）和Stacking算法（融合法）三种算法。\n> 那么三种集成学习方法的具体含义是什么呢？\n\n## 导数、偏导数、方向导数、梯度\n了解这些概念是学习点击率预估算法的基础，很多算法都是基于梯度下降进行求解的，但要了解梯度下降就必须要明白导数，偏导数，方向导数的概念。\n\n这里不展开介绍，大家可以从《推荐系统开发实战》中获取内容。\n\n# GBDT算法\nGBDT算法（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法。\n\n该算法中构建多棵决策树组成，所有决策树的结论累加起来作为最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。\n## GBDT的算法原理\nGBDT算法可以看成是T棵树组成的加法模型，其对应的公式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716084320362.png\" width=300px /></center>\n\n式中：\n- x：输入样本；\n- w：模型参数；\n- h：分类回归树；\n- α：每棵树的权重。\nGBDT算法的实现过程如下。\n（1）初始化函数F0常量（其中L为损失函数）：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716084400344.png\" width=300px /></center>\n\n（2）循环执行M次，建立M棵分类回归树。创建第m（m=1,2,…,M）棵树的过程见步骤（3）~步骤（6）。\n（3）计算第m棵树对应的响应值（伪残差），计算公式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716084426566.png\" width=300px /></center>\n\n\n（4）使用CART回归树拟合数据得到第m棵树的叶子节点区域Rj,m，其中j=1,2,… ,Jm。\n（5）对于j=1,2, … ,Jm，计算：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716084452561.png\" width=300px /></center>\n\n（6）更新Fm为:\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716084508847.png\" width=300px /></center>\n\n（7）输出Fm(x)\n\n> 具体的GBDT算法实例，这里不展开介绍。\n\n\n# 回归分析\n回归分析算法（Regression Analysis Algorithm）是机器学习算法中最常见的一类机器学习算法。就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。例如有一组随机变量X（x1，x2，x3，…）和另外一组随机变量Y（y1，y2，y3，…），那么研究变量X与Y之间关系的统计学方法就叫作回归分析。因为这里X和Y是单一对应的，所以这里是一元线性回归。\n\n回归分析算法分为线性回归算法和非线性回归算法。\n- 线性回归\n\n线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以用一个二维平面、三维曲面等。\n一元线性回归：只有一个自变量的回归。例如房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增加。这里的自变量只有面积，所以是一元线性回归。 \n多元线性回归：自变量大于或等于两个的回归。例如房子面积（Area）、楼层（floor）和房屋价格（Money）的关系，这里自变量有两个，所以是二元线性回归。\n典型的线性回归方程如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085329978.png\" width=300px /></center>\n\n在统计意义上，如果一个回归等式是线性的，那么它相对于参数就必须是线性的。如果相对于参数是线性的，那么即使相对于样本变量的特征是二次方或多次方的，这个回归模型也是线性的。例如下面的式子：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085345548.png\"  width=300px /></center>\n\n甚至可以使用对数或指数去形式化特征，如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085402694.png\" width=300px /></center>\n\n- 非线形回归\n\n有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫作一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫作多元回归分析。\n\t例如下面的两个回归方程：\n\n<center><img src=\"https://img-blog.csdnimg.cn/2019071608541966.png\" width=300px /></center>\n\n与线性回归模型不一样的是，这些非线性回归模型的特征因子对应的参数不止一个。\n- 广义线性回归\n\n有些非线性回归也可以用线性回归的方法来进行分析，这样的非线性回归叫作广义线性回归。 典型的代表是Logistic回归。\n# LR算法\n逻辑回归与线性回归本质上是一样的，都是通过误差函数求解最优系数，在形式上只不过是在线性回归上增加了一个逻辑函数。与线性回归相比，逻辑回归（Logistic Regression，LR）更适用于因变量为二分变量的模型，Logistic 回归系数可用于估计模型中每个自变量的权重比。\n\n> 我们都知道LR算法使用的是Sigmoid函数作为结果值的区分函数，那么LR为什么要使用Sigmoid呢？\n\n## LR的算法原理\n机器学习模型实际上把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，还希望这组限定条件简单而合理。\n逻辑回归模型所做的假设是：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085713775.png\" width=300px /></center>\n\n这里的g(h)就是Sigmoid函数，相应的决策函数为：\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085734391.png\" width=300px /></center>\n\n选择0.5作为阈值是一般的做法，实际应用时，特定的情况下可以选择不同的阈值。如果对正例的判别准确性要求高，可以使阈值大一些；如果对正例的召回要求高，则可以使阈值小一些。\n在函数的数学形式确定之后，就要求解模型中的参数了。统计学中常用的一种数学方法是最大似然估计，即找到一组参数，使得在这组参数条件下数据的似然度（概率）更大。在逻辑回归算法中，似然函数可以表示为：\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085751784.png\" width=300px /></center>\n\n取对数，可以得到对数形式的似然函数：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085808618.png\"  width=300px /></center>\n\n同样这里也使用损失函数来衡量模型预测结果准确的程度，这里采用lg损失函数，其在单条数据上的定义为：\n<center><img src=\"https://img-blog.csdnimg.cn/2019071608583169.png\" width=300px /></center>\n\n如果取整个数据集上的平均lg损失，可以得到：\n<center><img src=\"https://img-blog.csdnimg.cn/20190716085842515.png\" width=150px /></center>\n\n在逻辑回归模型中，最大化似然函数和最小化lg损失函数实际上是等价的。对于该优化问题，存在多种求解方法，这里以梯度下降的情况为例说明。基本步骤如下：\n<img src=\"https://img-blog.csdnimg.cn/20190716085925776.png\" width=420px>\n沿梯度负方向选择一个较小的步长可以保证损失函数的值是减小的，另外，逻辑回归模型的损失函数是凸函数（加入正则项后是严格凸函数），可以保证找到的局部最优值是全局最优值。\n## 正则化\n当模型中参数过多时，容易产生过拟合，这时就要控制模型的复杂度，其中最常见的做法是在目标中加入正则项，通过惩罚过大的参数来防止过拟合。常见的正则化方法包括L1 正则化和L2 正则化。其分别对应如下两个公式：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716090002490.png\" width=200px /></center>\n\n- L1 正则化是指权值向量w 中各个元素的绝对值之和，通常表示为||w||1。\n- L2 正则化是指权值向量w 中各个元素的平方和然后再求平方根（可以看到Ridge 回归\n的L2 正则化项有平方符号），通常表示为||w||2。\n\n## 模型融合\n## 背景介绍\n在CTR 预估问题发展初期，使用最多的方法就是逻辑回归（LR），LR 使用了Sigmoid 变换将函数值映射到0~1 区间，映射后的函数值就是CTR 的预估值。LR 属于线性模型，容易并行化，可以轻松处理上亿条数据，但是学习能力十分有限，需要大量的特征工程来增强模型的学习能力。\nGBDT 是一种常用的非线性模型，它基于集成学习中的Boosting 思想，每次迭代都在减少残差的梯度方向新建立一棵决策树，迭代多少次就会生成多少棵决策树。GBDT 的思想使其具有天然优势，可以发现多种有区分性的特征及特征组合。决策树的路径可以直接作为LR 输入特征使用，省去了人工寻找特征、特征组合的步骤。这种通过GBDT 生成LR 特征的方式（GBDT+LR），业界已有实践（Facebook、Kaggle 等），且取得了不错的效果。\n## 为什么使用GBDT和LR进行模型融合\n在介绍模型融合之前，需要先了解下面两个问题。\n- 为什么使用集成的决策树\n\n一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT 中，每棵树都在学习前面的树存在的不足，迭代多少次就会生成多少棵树。按Facebook的论文及Kaggle 竞赛中的GBDT+LR 融合方式，多棵树正好满足LR 每条训练样本可以通过GBDT 映射成多个特征的需求。\n\n- 为什么使用GBDT 构建决策树而不是RandomForest（RF）\n\nRF（随机森林）也是多棵树组成的，但从效果上有实践证明不如GBDT。对于GBDT 前面的树，特征分裂主要体现对多数样本有区分度的特征；对于后面的树，主要体现的是经过前N棵树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，这样的思路更加合理，这也是用GBDT 的原因。\n\n# GBDT+LR 模型融合的原理\nGBDT+LR 模型融合思想来源于Facebook 公开的论文Practical Lessons from Predicting\nClicks on Ads at Facebook。其主要思想是：GBDT 每棵树的路径直接作为LR 的输入特征使用。\n即用已有特征训练GBDT 模型，然后利用GBDT 模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1 的，向量的每个元素对应于GBDT 模型中树的叶子节点。若一个样本点通过某棵树最终落在这棵树的一个叶子节点上，那么在新特征向量中这个叶子节点对应的元素值为1，而这棵树的其他叶子节点对应的元素值为0。新特征向量的长度等于GBDT 模型里所有树包含的叶子节点数之和。在Facebook 的公开论文中，有一个例子，如图8-10 所示。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190716090158539.png\" width=70% /></center>\n\n图8-10 中共有两棵树，x 为一条输入样本，遍历两棵树后，x 样本分别落到两棵树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树就得到了该样本对应的所有LR特征。构造的新特征向量是取值0/1 的。举例来说：图8-10 中有两棵子树，左子树有三个叶子节点，右子树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设x 落在左子树第一个节点时，编码[1,0,0]，落在右子树第二个节点时编码[0,1]，则整体的编码为[1,0,0,0,1]，这类编码作为特征，输入到LR 中进行分类。\n\n# 电信客户流失案例\n这里将会介绍使用GBDT，LR和模型融合三种方式实现电信客户流失，在三种情况的对比下，模型融合的方法效果更好，具体不展开介绍，可以参考《推荐系统开发实战》\n\n-----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之6、基于上下文的推荐算法介绍和案例实战开发","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之6、基于上下文的推荐算法介绍和案例实战开发/","content":"> 在本系列之前介绍的推荐算法主要是为了联系用户的偏好和物品，将符合用户偏好的物品推荐给用户。例如“基于标签的推荐算法”中介绍的偏好是“用户的主观意见表达”，物品本身所传达的含义和用户所处的客观环境对推荐系统而言也是极其重要的，如冬天推荐短袖、中秋节推荐粽子等都是不合理的，不能说用户在冬天搜索了短袖或在中秋节搜索了粽子就给用户推荐不合时节的物品。因此，准确地了解用户的上下文信息（包括用户访问推荐系统的时间、地点、访问时的心情等），并将这些信息应用到推荐系统中，是实现一个好的推荐系统的关键。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/95445593\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n# 基于时间特征的推荐\n## 什么是时间效应？\n时间效应在日常生活中随处可见。例如，随着年龄的增长，人们的穿衣风格会改变，钟爱的课外读物也会改变；季节不同，人们的穿着会改变，果蔬供给也会改变等。\n\n在推荐系统中，时间效应可以定义为：用户的偏好兴趣、物品的生命周期等随着时间的变化而发生变化。\n\n时间效应对推荐系统的效果有着直接的影响，其对用户兴趣的影响主要表现在以下几方面。\n\n- 偏好迁移。\n由于用户自身原因，随着时间的变化其偏好、兴趣发生了改变。例如，人们在不同年龄所热爱的事物不一样，用户A小时候喜欢吃糖果，长大了却不再吃糖果了；用户B在上高中时喜欢读一些小说之类的读物，可在念了大学之后，便开始阅读一些和专业课相关的读物。\n\n\t用户的偏好直接影响着推荐的结果集，所以，推荐系统需要实时关注用户的实时兴趣变化。例如，用户在某个时刻点击或关注了某个商品，那么在下一刻，用户已经点击或者关注的相关商品就应该出现在推荐结果集中。但是，推荐系统还要注意挖掘用户的短期偏好和长期偏好（即挖掘用户兴趣中的长尾商品），这时就需要根据用户过去一段时间内的行为习惯进行兴趣建模。\n- 生命周期。\n\t生命周期即事物合理存在的时间周期。例如某个热门新闻，在新闻刚发布时，受关注的程度很高，各大媒体网站都会进行报道，但随着时间的推移，该新闻的热度在逐渐减小，最后慢慢被人遗忘，这就是该热门新闻的生命周期。\n\n\t推荐系统在进行事物推荐时，要注意该事物的有效性。一个合理的推荐系统不会在2018年推荐2004年雅典奥运会刘翔打破奥运纪录的新闻，也不会推荐某种过时的食物。不同场景下的推荐系统中，推荐事物的生命周期长度也不尽相同。新闻的生命周期较短，一般在几天之内就会褪去热度，而食物的生命周期就较长。\n- 季节效应。\n\t季节效应：事物的流行度与季节是强相关的，反映的是时间本身对用户偏好兴趣的影响。例如，人们在夏天穿短袖，在冬天穿羽绒服，在夏天喝啤酒，在冬天吃火锅等。\n\n\t在不同季节，人们的衣食住行选择都会发生变化。在推荐系统中要实时捕捉到季节的变化，进而给用户推荐符合时节的物品。\n- 节日选择。\n\t节日选择：不同的节日对用户的选择会产生影响，也是时间效应中的一种。例如，端午节人们会选购一些粽子送给亲朋好友，而在中秋节则会选购一些月饼、螃蟹。又如，美国的感恩节，人们会购买火鸡作为餐桌上的主菜。\n\t\n\t在不同的节日适当地给用户推送一些节日主打物品，不仅可以提高用户点击率，而且可以在一定程度上发掘用户的隐含兴趣\n## 时间效应分析\n推荐系统中引入时间信息后，就从一个静态的推荐系统变成了一个动态的推荐系统。时间信息对推荐系统的影响主要表现在以下三方面：\n- 个人兴趣度会随时间发生变化；\n- 物品流行度会随时间发生变化；\n- 社会群体兴趣度会随时间发生变化。\n> 那么个人兴趣，物品流行度，社会群体兴趣度是如何随时间变化呢？\n\n## 推荐系统的实时性\n用户的兴趣是不断发生变化的，其变化体现在用户不断增加的行为中。例如电商网站中的点击、加购、分享、收藏等，或者新闻网站中的点击、评论、停留时长等。 一个实时的推荐系统应实时响应用户的新行为，让推荐结果不断发生变化，从而满足用户实时兴趣需求。\n\n现在几乎所有的电商网站中都引入了实时推荐，而且响应时间在“秒”之内。例如当当，京东，淘宝等电商，内容，娱乐平台。\n> 那么他们是如何做到实时推荐呢？实时推荐的具体表现是什么？\n\n## 协同过滤中的实时推荐\n### 1.UserCF中的时间特征\n使用UserCF为用户推荐物品时，先找到与目标用户兴趣相近的用户集合，然后根据这些用户的购买行为为用户进行物品推荐，故该算法的关键是“找到相似用户集合”（即计算用户之间的相似度）。两个用户产生过行为的物品集合交集越大，则两个用户越相似。用户相似度计算公式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711090120314.png\" width=200px></center>\n \n 式中，N(x)表示用户x产生过行为的物品集合，分子表示的是用户u和用户v有交集的物品的个数，分母表示的是用户u和用户v产生行为的并集物品的个数。\n但是由于热门物品被很多用户有过行为，但相对于热门共有物品而言，冷门物品更能说明两个用户之间的相似性，所以在计算两个用户之间的相似度时，对热门物品加一个惩罚项，所以这里的用户相似度计算式可以修改为：\n<center><img src=\"https://img-blog.csdnimg.cn/20190711090134644.png\" width=200px></center>\n\n式（7.2）中，N(i)表示对物品i产生过行为的所有用户的个数。\n但由于最近的行为最能表达用户当前的兴趣，所以在计算两个用户相似度时要增加时间衰减函数，可得到以下公式：\n<center><img src=\"https://img-blog.csdnimg.cn/20190711090151739.png\" width=250px></center>\n\n式中，f(|tui-tvi|)为时间衰减函数，其形式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711090206776.png\" width=250px></center>\n\n式中，为时间衰减因子，tui表示用户u对物品i产生行为的时间，tvi表示用户v对物品i产生行为的时间。\n用户当前的评分受相似用户集合最近评分的影响比较大，所以在计算用户对物品的评分时还要加上时间衰减函数f(|t0-tvi|)，所以最终得到的用户u对物品i的偏好程度为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711090228501.png\" width=300px></center>\n\n其中f(|t0-tvi|)的表达式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711090242554.png\" width=300px></center>\n\n式中，t0表示当前时间，tvi表示用户v对物品i产生行为的时间。\n> 那么ItemCF中的时间特征如何理解呢？我们如何使用代码来实现CF中的时间衰减呢？\n\n# 基于地域和热度特征的推荐\n除了时间，地域特征在推荐系统中也十分重要，不同地区的用户喜欢的事物不一样，用户到了不同的地方喜欢的事物也会发生变化。例如南方人喜欢吃米，北方人喜欢吃面；又如，一个国内的人去韩国可能是为了购物和旅游等。\n而另外一种经常和地域一同出现的是热度特征，最常见的如排行榜，就是基于地域和统计的一种排序。\n> 为什么要把时间和地域特征放在一起讨论呢？\n\n明尼苏达大学的研究人员提出过一个称为LARS（Location Aware Recommender System，位置感知推荐系统）的推荐系统。该系统首先将物品分为两类：\n- 有空间属性（如餐厅、商店、景点等）物品；\n- 无空间属性（如图书、电影、音乐）物品。\n同时，也将用户分为两类：\n- 有空间属性（如用户包含相应的空间属性信息）用户。\n- 无空间属性（如用户没有相应的空间属性信息）用户。\n\n基于地域和热度的推荐算法的基本原理是：按照地域对事物进行划分，然后根据热度对事物进行排序，进而推荐给用户。下面以新闻为例说明热度算法的基本原理。\n\n在一则新闻录入数据库后，初始化一个热度分（S0），此时该新闻就进入了新闻推荐的候选池。\n- 随着新闻不断被用户点击（click）、转发（share）、关注（follow）、评论（comment）、点赞（up）等，对应的和用户交互维度的热度（S1）不断增加。\n-  另外，新闻要求具有时效性，因此在新闻发布后，热度（S2）会随着时间衰减。\n随着时间的后移，新闻的热度不断发生变化，对应的推荐抽选池排序也在不断地发生变化。最终新闻热度对应的计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711204214130.png\" width=150px></center>\n\n但这里需要考虑三个因素：\n- 新闻的初始热度应该不一致\n- 用户的行为规则应该发生变化\n- 热度随时间衰减的趋势非线性\n\n# 其他上下文特征信息\n上述介绍了时间、地域和热度信息在推荐系统中的应用。但在实际的应用环境中，上下文信息更加丰富（如用户使用的客户端、用户的性别、天气、用户调用接口的次数、推荐商品的位置等），这些信息在很大程度上影响用户浏览物品时的心情和兴趣。\n\n在实际的建模应用过程中，如果能够正确地使用这些信息，可提高推荐系统的效率。\n\n> 文中的相关疑问和更详细的介绍都可以在《推荐系统开发实战》一书中找到答案。\n\n-----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之5、基于标签的推荐算法介绍和案例实战开发","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之5、基于标签的推荐算法介绍和案例实战开发/","content":"\n> 标签系统的传统用法是，在一些网站中，用户会为自己感兴趣的对象打上一些标签，如豆瓣、网易云音乐、Last.fm等。这些社会化标签即资源的分类工具，也是用户个人偏好的反映，因此社会化标签为推荐系统获得用户偏好提供了一个新的数据来源。之所以说“传统”，是因为这些标签是用户主观意愿的表达，是主动行为。但是，有些电商网站也会对用户或商品进行一些客观的打标，如对一个经常网购数码产品的用户打上一个“数码达人”的标签，以便后继给该用户推荐数码类商品。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/95403294\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n# 标签系统的应用\n推荐系统的目的是联系用户和物品，这种联系需要不同的“媒介”。例如：\n- 相似用户（给用户推荐相似用户喜欢的物品），媒介是用户。\n- 相似物品（给用户推荐他喜欢物品的相似物品），媒介是物品。\n- 隐含的特征（根据用户的历史行为构造特征，进而预测对新物品的偏好程度），媒介是行为特征。\n\n本章将介绍一种新的联系媒介——标签。工业界中的主流标签系统包含：\n- Last.fm\n- Delicious\n- 豆瓣\n- 网易云音乐\n\n# 数据标注与关键词提取\n关键词是指能够反映文本语料主题的词语或短语。在不同的业务场景中，词语和短语具有不同的意义。例如：\n- 从电商网站商品标题中提取标签时，词语所传达的意义就比较突出。\n- 从新闻类网站中生成新闻摘要时，短语所传达的意义就比较突出。\n\n## 数据标注\n数据标注即利用人工或AI（人工智能）技术对数据（文本、图像、用户或物品）进行标注。\n标注有许多类型，如:\n- 分类标注：即打标签，常用在图像、文本中。一般是指，从既定的标签中选择数据对应的标签，得到的结果是一个封闭的集合。\n- 框框标注：常用在图像识别中，如有一张环路上的行车照片，从中框出所有的车辆。\n- 区域标注：常见于自动驾驶中。例如从一张图片中标出公路对应的区域。\n- 其他标注：除了上述常见的标注类型外，还有许多个性化需求。例如，自动摘要、用户或商品的标签（因为其中总有一些未知标签，当然也可以看成是多分类）。\n\n数据标注的一般步骤为：\n（1）确定标注标准：设置标注样例和模板（如标注颜色时对应的比色卡等）。对于模棱两可的数据，制定统一的处理方式。\n（2）确定标注形式：标注形式一般由算法人员确定。例如，在垃圾问题识别中，垃圾问题标注为1，正常问题标注为0。\n（3）确定标注方法：可以使用人工标注，也可以针对不同的标注类型采用相应的工具进行标注。\n\n>那么，数据标注与标签的对应关系是什么呢？\n## 数据标注在推荐系统中的应用\n关键词提取在推荐系统中的应用也十分广泛，主要用于用户物品召回（根据用户对关键词的行为偏好，召回相应关键词下的物品）和特征属性构造（对物品的属性进行补充）。\n> 具体的案例这里不过多做介绍。\n## 推荐系统中的关键词提取\n关键词是指能够反映文本语料主题的词语或短语。在不同的业务场景中，词语和短语具有不同的意义。例如：\n- 从电商网站商品标题中提取标签时，词语所传达的意义就比较突出。\n- 从新闻类网站中生成新闻摘要时，短语所传达的意义就比较突出。\n\n这里所介绍的关键提取和数据标注同样都是一个动作，都是为了得到一些标签或属性特征。\n关键词提取从最终的结果反馈上来看可以分为两类：\n- 关键词分配：给定一个指定的词库，选取和文本关联度最大的几个词作为该文本的关键词。\n- 关键词提取：没有指定的词库，从文本中抽取代表性词作为该文本的关键词。\n不管通过哪种方式生成，关键词都是对短文本所传达含义的抽取概述，都直接反映了短文本的所传达的属性或特征\n\n# 标签的分类\n在推荐系统中，不管是数据标注还是关键词提取，其目的都是得到用户或物品的标签。但是在不同场景下，标签的具体内容是不定的。例如，同样是分类标注，新闻的类别里可以有军事、科技等，但音乐的类别里就很少会涉及军事或科技了。\n对于社会化标签在标识项目方面的功能，Golder和Huberman将其归纳为以下7种：\n- 标识对象的内容。此类标签一般为名词，如“IBM”“音乐”“房产销售”等。\n- 标识对象的类别。例如标识对象为“文章”“日志”“书籍”等。\n- 标识对象的创建者或所有者。例如博客文章的作者署名、论文的作者署名等。\n\n标识对象的品质和特征。例如“有趣”“幽默”等。\n- 用户参考用到的标签。例如“myPhoto”“myFavorite”等。\n- 分类提炼用的标签。用数字化标签对现有分类进一步细化，如一个人收藏的技术博客，按照难度等级分为“1”“2”“3”“4”等。\n- 用于任务组织的标签。例如“to read”“IT blog”等。\n当然以上7种类别标签是一个通用框架，在每一个具体的场景下会有不同的划分。\n\n# 基于TF-IDF提取标题中的关键词\nTF-IDF（Term Frequency–Inverse Document Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF算法的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或短语具有很好的类别区分能力，适合用来分类。TF-IDF实际是TF*IDF。\n> 那么TF-IDF的具体算法原理是什么？以及我们如何从商品标题中提取关键词呢？\n# 基于标签的推荐系统\n标签是用户描述、整理、分享网络内容的一种新的形式，同时也反映出用户自身的兴趣和态度。标签为创建用户兴趣模型提供了一种全新的途径。\n本节将展开介绍基于标签的用户如何进行兴趣建模。\n## 标签评分算法\n用户对标签的认同度可以使用二元关系表示，如“喜欢”或“不喜欢”；也可以使用“连续数值”表示喜好程度。\n\n二元表示方法简单明了，但精确度不够，在对标签喜好程度进行排序时，也无法进行区分。所以，这里选用“连续数值”来表达用户对标签的喜好程度。\n\n为了计算用户对标签的喜好程度，需要将用户对物品的评分传递给这个物品所拥有的标签，传递的分值为物品与标签的相关度。\n### 1.用户对标签的依赖程度\n如图6-13所示，用户u对艺术家A的评分为5星，对艺术家B的评分为3星，对艺术家C的评分为4星。\n艺术家A与标签1、2、3的相关度分别为：0.6，0.8，0.4；\n艺术家B与标签1、2、3的相关度分别为：0.3，0.6，0.9；\n艺术家C与标签1、2、3的相关度分别为：0.5，0.7，0.6。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711081522439.png\" width=70%></center>\n\n对应的用户（u）对标签（t）的喜好程度计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/2019071108154257.png\" width=50%></center>\n\n式中：\n- rate(u,t)表示用户u对标签t的喜好程度。\n- rate(u,i)表示用户u对艺术家i的评分。\n- rel(i,t)表示艺术家i与标签t的相关度。\n\n根据式（6.4）计算出用户u对标签1的喜好程度为：\n（50.6+30.3+40.5）/（0.6+0.3+0.5）=4.21\n\n同理可以计算出用户u对标签2的喜好程度为 4.10，对标签3的喜好程度为3.74。\n### 2.优化用户对标签的喜好程度\n> 如果一个用户的评分行为较少，就会导致预测结果存在误差。那么该如何改进呢？\n## 标签评分算法改进\n\n这里使用TF-IDF算法来计算每个标签的权重，用该权重来表达用户对标签的依赖程度。\nTF-IDF算法在6.3.1节中进行了介绍，这里不再赘述。每个用户标记的标签对应的TF值的计算公式为：\n<center><img src=\"https://img-blog.csdnimg.cn/20190711005210591.png\" width=50%></center>\n\n式中：\n- n(u,ti)表示用户u使用标签ti标记的次数。\n- 分母部分表示用户u使用所有标签标记的次数和。\nTF(u,t)表示用户u使用标签t标记的频率，即用户u对标签t的依赖程度。\n\n## 优化用户对标签的依赖程度\n在社会化标签的使用网站中存在“马太效应”，即热门标签由于被展示的次数较多而变得越来越热门，而冷门标签也会越来越冷门。大多数用户标注的标签都集中在一个很小的集合内，而大量长尾标签则较少有用户使用。\n事实上，较冷门的标签才能更好地体现用户的个性和特点。为了抑制这种现象，更好地体现用户的个性化，这里使用逆向文件频率（IDF）来对那些热门标签进行数值惩罚。\n\n每个用户标记的标签对应的IDF值的计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711005414747.png\" width=50%></center>\n\n- 分子表示所有用户对所有标签的标记计数和。\n- 分母表示所有用户对标签t的标记计数和。\n- IDF( u, t)表示t的热门程度，即一个标签被不同用户使用的概率。\n对于一个标签而言，如果使用过它的用户数量很少，但某一个用户经常使用它，说明这个用户与这个标签的关系很紧密。\n\n## 用户对标签的兴趣读\n综合式（6.6）和式（6.7），用户对标签的依赖度为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/2019071100570952.png\" width=50%></center>\n\n在6之前分析了用户对标签的主观喜好程度，本节分析了用户对标签的依赖程度，综合可以得到用户u对标签的兴趣度为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711005730699.png\" width=50%></center>\n\n## 标签基因\n标签基因是GroupLens研究组的一个项目。\n在社会化标签系统中，每个物品都可以被看作与其相关的标签的集合，rel(i, t)以从0（完全不相关）到1（完全正相关）的连续值衡量一个标签与一个物品的符合程度。\n例如图6-13中：\n- rel(艺术家A，标签1)=0.6；\n- rel(艺术家A，标签2)=0.8；\n- rel(艺术家A，标签3)=0.4。\n采用标签基因可以为每个艺术家i计算出一个标签向量rel(i)，其元素是i与T中所有标签的相关度。这里，rel( i)相当于以标签为基因描绘出了不同物品的基因图谱。形式化的表达如下：\n<center><img src=\"https://img-blog.csdnimg.cn/20190711083629938.png\" width=50%></center>\n\n例如，图6-13中，艺术家A的标签基因为：rel(艺术家A)=[0.6,0.8,0.4]。\n选用标签基因来表示标签与物品的关系有以下三个原因：\n（1）它提供了从0到1的连续数值；\n（2）关系矩阵是稠密的，它定义了每个标签t∈ T与每个物品 i∈ I的相关度；\n（3）它是基于真实数据构建的。\n## 用户兴趣建模\n根据训练数据，可以构建所有商品的标签基因矩阵Ti和用户最终对标签的兴趣度Tu，则用户对商品的可能喜好程度为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190711083701613.png\" width=50%></center>\n\n式中：\n- Tu：用户u对所有标签的兴趣度矩阵（1行m列，m为标签个数）。\n- Ti^T：所有商品的标签基因矩阵Ti的转置矩阵（m行n列，m为标签个数，n为商品个数）。\n- T(u,i)：用户u对所有商品的喜好程度矩阵（1行n列，n为商品个数）。\n最终从计算结果中选取前K个推荐给用户。\n# 基于标签推荐算法实现艺术家推荐\n利用标签推荐算法实现一个艺术家推荐系统，即，根据用户已经标记过的标签进行标签兴趣建模，进而为用户推荐喜好标签下最相关的艺术家。\n\n这里使用Last.fm数据集中的数据作为基础数据，该数据集在3.3节有相关的介绍。该实例的具体实现思路如下：\n（1）加载并准备数据；\n（2）计算每个用户对应的标签基因；\n（3）计算用户最终对每个标签的兴趣度；\n（4）进行艺术家推荐和效果评估。\n\n\n\n---\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之4、基于用户行为特征的推荐算法介绍和案例实战开发","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之4、基于用户行为特征的推荐算法介绍和案例实战开发/","content":"\n> 推荐系统的受众对象为用户，只有明白用户的意图，才能给用户推荐更好的内容。基于用户行为特征的推荐，其实在真正的“个性化推荐系统”诞生之前就已经存在了。最简单的就是各种排行榜，它们基于简单的用户统计，又对其他选择提供一定的指引。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/95302470\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n# 用户行为分类\n用户行为分为两种——显性反馈行为和隐性反馈行为。\n- 显性反馈行为\n> 显性反馈行为是指，用户很明显地表达出自己的喜好，如对内容评分、表示喜欢/不喜欢等。例如，豆瓣电影中的评分机制和YouTube中的“点赞”功能都是典型的显性反馈。\n- 隐性反馈行为\n> 隐性反馈行为是指，用户不明确表达出自己的喜好信息。例如，用户在京东APP中的商品浏览日志、在网易云上听歌的日志等，实际上京东和网易已经得到了一定的用户行为数据，但没有以显性方式直接反馈，而是在其他地方间接地反馈出来。\n\n> <font color=red>疑问：显性反馈行为 和 隐性反馈行为的区别是什么？\n   \n# 基于内容的推荐算法\n> 基于内容的推荐算法，根据用户过去一段时间内喜欢的物品，以及由此推算出来用户偏好，为用户推荐相似物品。其中的“内容”指的便是：用户过去一段时间内喜欢的物品，以及由此推算出来的用户偏好。\n\n图5-2所示是一个基于内容推荐的例子，用户A和用户C喜欢爱情、浪漫类型的电影，用户B喜欢恐怖、惊悚的电影，因此将类型为爱情、浪漫且用户A没有行为的电影C推荐给用户A。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190710233722482.png\" width=70% /></center>\n\n## 原理分析\n基于内容（Content Based，CB）的推荐原理非常简单：向用户推荐所喜欢的Item的相似Item。其中包含了三步：\n（1）构造Item的特征；\n\t\n在真实应用场景中，往往会用一些属性描述Item的特征，这些属性通常分为以下两种：\n- 结构化属性：意义比较明确，其取值固定在某个范围内。\n- 非结构化属性：特性意义相对不太明确，取值没有什么限制，不可以直接使用\n\n（2）计算Item之间的相似度；\n\n在确定好Item的特征和用户的偏好模型后，需要计算两个Item间的相似度。根据具体场景，往往需要使用不同的相似度计算方法。\n>  <font color=red>疑问：相似度计算方法有哪些？\n\n（3）评判用户是否喜欢某个Item。\n\n在推荐算法中评判用户是否喜欢某个Item就是：利用监督学习或非监督学习的方法，来评判用户喜欢哪些Item，不喜欢哪些Item，从而根据用户的喜好，为他生成一个偏好模型，进而对未知的Item进行喜好评判。\n\n在基于内容的推荐算法中，使用的则是监督学习，利用用户对Item的已知评分和Item所属的类别，学习得到用户对每种类型的偏好程度，然后结合Item的类别特征计算用户对Item的偏好程度。\n\n>  <font color=red>疑问：监督学习和非监督学习的区别？\n## 实际案例\n利用基于内容的推荐算法编写一个电影推荐系统，当用户在浏览某部电影时，为其推荐所浏览电影的相似电影。其实现步骤如下：\n- 构建电影的特征信息矩阵\n- 构建用户的偏好信息\n- 计算用户与每部电影的距离\n\n> 介于篇幅原理这里的具体实现步骤和代码不再赘述，欢迎关注《推荐系统开发实战》一书。\n# 基于近邻的推荐算法\n基于近邻的推荐算法是比较基础的推荐算法，在学术界和工业界应用十分广泛。这里讨论的基于近邻的推荐算法指的是协同过滤（Collaborative Filtering）算法。\n基于近邻的协同过滤推荐算法分为：\n- 基于用户的协同过滤（User-CF-Based）算法\n- 基于物品的协同过滤（Item-CF-Based）算法\n\n关于协同过滤，一个最经典的例子就是看电影：有时不知道哪一部电影是我们喜欢的或评分比较高的，通常的做法就是问问周围的朋友，看看最近有什么好的电影推荐。在询问时，都习惯于问与自己品味差不多的朋友，这就是协同过滤的核心思想。\n## UserCF算法\n### 原理分析\n基于用户的协同过滤通过用户的历史行为数据发现用户喜欢的物品，并对这些偏好进行度量和打分，然后根据不同用户对相同物品的评分或偏好程度来评测用户之间的相似性，对有相同偏好的用户进行物品推荐。\n\n简单地讲，基于用户的协同过滤就是给用户推荐“和他兴趣相投的其他用户”喜欢的物品。\n\n图5-5所示是一个基于用户的协同过滤推荐的例子，用户A和用户C同时喜欢电影A和电影C，用户C还喜欢电影D，因此将用户A没有表达喜好的电影D推荐给用户A。\n\n又如，现在有A、B、C、D四个用户，分别对a、b、c、d、e五个物品表达了自己喜好程度（通过评分的高低来表现自己的偏好程度高低），现在要为C用户推荐物品：\n（1）计算得到C用户的相似用户；\n（2）找到这些相似用户喜欢的但C没有进行过评分的物品并推荐给C。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190710091525298.png\" width=70% /></center>\n\n### 实际案例\n利用UserCF编写一个电影推荐系统，根据被推荐用户的相似用户的喜好，为被推荐用户推荐电影。\n\n> 介于篇幅原理这里的具体实现步骤和代码不再赘述，欢迎关注《推荐系统开发实战》一书。\n \n## ItemCF算法\n### 原理分析\n基于物品的协同过滤推荐则通过不同对item的评分来评测item之间的相似性，从而基于item的相似性做推荐。\n\n简单地讲就是，给用户推荐他之前喜欢物品的相似物品。 \n\n从原理上理解可以得知：基于item的协同过滤推荐和被推荐用户的偏好没有直接关系。例如，用户A买了一本书a，那么会给用户A推荐一些和书a相似的书。这里要考虑的是如何衡量两本书的相似度。\n\n图5-8所示是一个基于物品的协同过滤推荐的例子，用户C喜欢电影A，电影C和电影A相似，那么便把用户C没有表达喜好的电影C推荐给用户C。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190710091806909.png\" width=70% ></center>\n\n### 实际案例\n利用ItemCF算法编写一个推荐系统，当用户进行电影浏览时，向用户推荐和该部电影相似的电影。\n\n> 介于篇幅原理这里的具体实现步骤和代码不再赘述，欢迎关注《推荐系统开发实战》一书。\n# 对比分析\n## UserCF与ItemCF\n从用适用场景、推荐系统多样性、用户特点对推荐的影响三方面来分析下两者的对比。\n### 在适用场景上的比较\nItemCF算法利用物品间的相似性来推荐，所以当用户数量远远超过物品数量时，可以考虑使用ItemCF算法。例如，购物网站和技术博客网站的商品或文章数据相对稳定，因此计算物品相似度时不但计算量小，而且不必频繁进行更新。\n\nUserCF算法利用用户间的相似性来推荐，所以当物品数量远远超过用户数量时，可以考虑UserCF算法。UserCF算法更适合新闻类和短视频类等快消素材网站。例如，在社交网站中，UserCF是一个不错的选择，而且可解释性也更强。因为这类网站的内容更新比较频繁，且用户更加注重社会化热点。\n\n> <font color=red>举例：略～\n\n### 在推荐系统多样性上的比较\n单用户的多样性方面：ItemCF算法不如UserCF算法多样性丰富。因为，ItemCF算法推荐的是和之前有行为物品的相似物品，物品覆盖面比较小，丰富度低。\n\n系统的多样性方面：ItemCF算法的多样性要远远好于UserCF算法。因为，UserCF算法更加注重推荐热门物品。\n\nItemCF算法的推荐有很好的新颖性，容易发现并推荐长尾里的物品。所以大多数情况下，ItemCF算法的精度稍微小于UserCF算法。如果考虑多样性，ItemCF算法比UserCF算法好很多。而ItemCF算法只推荐A领域给用户，这样有限的推荐列表中就可能包含了一定数量的不热门的长尾物品。\n\n由于UserCF算法经常推荐热门的物品，所以它在推荐长尾里的物品方面能力不足。\n### 在用户特点上的比较\n（1）UserCF算法推荐的原则是“假设用户喜欢那些和他有相同喜好的用户喜欢的东西”。但是，如果用户暂时找不到兴趣相同的邻居，那么基于用户的推荐效果就打了大打折扣了。\n因此，用户是否适应UserCF，与“他有多少邻居”是成正比的。\n（2） 基于物品的协同过滤算法的前提是“用户喜欢和他以前购买过的物品类型相同的物品”，可以计算一个用户喜欢的物品的自相似度。\n- 一个用户喜欢物品的自相似度大，即用户喜欢的物品的相关度大，则说明他喜欢的东西是比较相似的。即，这个用户比较符合ItemCF算法的基本假设，他对ItemCF算法的适应度比较好。\n- 反之，如果自相似度小，即用户喜欢物品的相关度小，就说明这个用户的喜好习惯并不满足ItemCF算法的基本假设，那么用ItemCF算法所做出的推荐对于这种用户来说效果可能不是很好。\n## 基于内容与基于近邻\n基于内容（Content-Based，CB）的推荐算法和基于物品（Item-Based，IB）的协同过滤算法十分相似，因为两种算法都在Item的基础上进行相似度计算。\n但是两者基于的Item特征是不一样的：\n- 基于内容的推荐算法中，计算用户相似度用的是Item本身的特征。\n- 基于物品的协同过滤算法中，则用“用户对Item的行为”来构造Item的特征。\n\n> <font color=red>举例：略～\n\n# 基于隐语义模型的推荐算法\n## 原理分析\n从矩阵的角度来理解，LFM原理矩阵表示图如图5-12所示。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190710232811778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n- R矩阵表示用户对物品的偏好信息。其中，R_ij代表User i对Item j的兴趣度。\n- P矩阵表示用户对各物品类别的一个偏好信息。其中，P_ij代表User i对Class j的兴趣度。\n- Q矩阵表示各个物品归属到各个类别的信息。其中，Q_ij代表Item j在Class i中的权重或概率。\n隐语义模型就是要将矩阵R分解为P和Q的乘积，即通过矩阵中的物品类别（Class）将用户User和物品Item联系起来。实际上需要根据用户当前的物品偏好信息R进行计算，从而得到对应的矩阵P和矩阵Q。\n从上述矩阵转换关系中可以得到隐语义模型计算用户对物品兴趣度的公式：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190710232939793.png)\n其中参数说明如下。\n- Pu,k：用户u兴趣和第k个隐类的关系。\n- Qi,k：第k个隐类和物品i的关系。\n- k：隐类的数量。\n- R：用户对物品的兴趣度。\n\n> <font color=red>疑问：LFM解决的问题是什么？LFM的样本集问题？LFM的推导？LFM的优缺点？请持续关注《推荐系统开发实战》\n\n## 实际案例\n编写一个基于隐语义模型的电影推荐系统。当用户在浏览电影并表达自己的兴趣后，系统向用户推荐用户可能喜欢的电影。\n其实现思路如下：\n（1）初始化用户对每个隐分类的兴趣度矩阵P，以及每个物品与每个隐分类的匹配程度矩阵Q；\n（2）根据训练数据集指定损失函数，迭代更新矩阵P和Q；\n（3）使用测试机对模型结果进行测试。\n\n> 由于篇幅原因，具体的代码实现和过程，这里不过多介绍，请参考《推荐系统开发实战》\n\n-----\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之3、推荐系统的灵魂伴侣-数据挖掘","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之3、推荐系统的灵魂伴侣-数据挖掘/","content":"\n> 个性化推荐是数据挖掘（Data Mining）中的一个目的明确的应用场景，所以，可以利用数据挖掘技术为推荐系统做一些基本工作，如了解数据、异常值处理、对用户群进行分类、对物品的价格进行聚类、构建用户的价格段偏好等，从而让推荐系统能够“千人千面”。本篇文章主要介绍一下常见的数据挖掘算法应用案例。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/95162286\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n# 数据预处理\n数据预处理（Data Preprocessing）是指：在使用数据进行建模或分析之前，对其进行一定的处理。真实环境中产生的数据往往都是不完整、不一致的“脏数据”，无法直接用来建模或进行数据分析。为了提高数据挖掘质量，需要先对数据进行一定的处理。\n\n常见的数据预处理包括：标准化、离散化、抽样、降维、去噪等。\n## 数据标准化\n数据标准化（Normalization）是指：将数据按照一定的比例进行缩放，使其落入一个特定的小区间。其中，最典型的就是数据的归一化处理，即将数据统一映射到0～1之间。\n> 数据标准化的好处有：加快模型的收敛速度，提高模型的精度。\n\n常见的数据标准化方法包括：\n- Min-Max标准化\nMin-Max标准化是指对原始数据进行线性变换，将值映射到[0,1]之间。其计算公式为：\n<center><img src=\"https://img-blog.csdnimg.cn/20190709083829394.png\" ></center>\n式中，x为原始数据中的一个数据，x_min表示原始数据中的最小值，x_max表示原始数据中的最大值，x'为Min-Max标准化后的数据。\n\n- Z-Score标准化\nZ-Score（也叫Standard Score，标准分数）标准化是指：基于原始数据的均值（mean）和标准差（standard deviation）来进行数据的标准化。Z-Score标准化的计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709083909697.png\" ></center>\n式中，x为原始数据中的一个数据，μ表示原始数据的均值，σ表示原始数据的标准差，x'为Z-Score标准化后的数据。\n\n- 小数定标（Decimal scaling）标准化\n小数定标标准化是指：通过移动小数点的位置来进行数据的标准化。小数点移动的位数取决于原始数据中的最大绝对值。小数定标标准化的计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709084036701.png\" ></center>\n\n例如，一组数据为[-309, -10, -43,87,344,970]，其中绝对值最大的是970。为使用小数定标标准化，用1000（即j=3）除以每个值。这样，-309被标准化为-0.309，970被标准化为0.97。\n- 均值归一化法\n均值归一化是指：通过原始数据中的均值、最大值和最小值来进行数据的标准化。均值归一化法的计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709084107869.png\" ></center>\n\n- 向量归一化\n向量归一化是指：通过用原始数据中的每个值除以所有数据之和来进行数据的标准化。向量归一化的计算公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709084122912.png\" ></center>\n\n- 指数转换\n指数转换是指：通过对原始数据的值进行相应的指数函数变换来进行数据的标准化。常见的函数方法有lg函数、Softmax函数和Sigmoid函数。\n\n<font color=red>由于篇幅原因，数据标准化的具体实现不贴代码了，可在《推荐系统开发实战》一书中找到找到！</font>\n## 数据离散化\n数据离散化（也叫数据分组）是指将连续的数据进行分组，使其变为一段段离散化的区间。\n根据离散化过程中是否考虑类别属性，可以将离散化算法分为有监督算法和无监督算法两类。由于有监督算法（如基于熵进行数据的离散化）充分利用了类别属性的信息，所以在分类中能获得较高的正确率。\n常见的数据离散化方法有以下几种：\n- 等宽分组\n- 等频分组\n- 单变量分组\n- 基于信息熵分组\n\n<font color=red>由于篇幅原因，基于信息熵的数据离散化实例这里贴代码了，可在《推荐系统开发实战》一书中找到!</font>\n## 数据抽样\n数据抽样也叫数据采样。数据抽样是选择数据子集对象的一种常用方法。\n- 在统计学中，抽样的目的是实现数据的调查和分析。\n- 在数据挖掘中，抽样的目的是压缩数据量，减小数据挖掘算法的资源开销。\n\n在数据挖掘中，抽样主要是从海量数据中产生训练集（Train Set）、测试集（Test Set）和验证集（Validation Set）。训练集、测试集和验证集三者的区别如下：\n- 训练集用来进行模型训练。\n- 测试集用来衡量模型的一些统计指标，如准确率、召回率等。在训练模型的过程中不允许使用测试集，否则会导致模型过拟合。\n验证集用来验证模型、辅助构建模型。在使用机器学习算法时，验证集是可选的。\n> “过拟合”表示：模型学习特征过于彻底时，噪声数据也会进入模型，导致后期测试时不能很好地识别数据，泛化能力太差。“欠拟合”表示：没有很好地捕捉到数据特征，不能很好地拟合数据。\n\n常见的数据抽样方法包括：\n- 随机抽样\n- 分层抽样\n- 系统抽样\n- 渐进抽样\n\n## 数据降维\n在构建机器学习模型时，有时特征是极其复杂的，当特征的维度达到几千维时，模型训练将会耗费大量的时间。另外，如果特征较多，还会出现多重共线性、稀疏性的问题。\n因此，需要简化属性、去噪、去冗余，以求取更典型的属性，但同时又希望不损失数据本身的意义，这时就需要对特征进行降维。\n\n数据降维分为线性降维和非线性降维。\n- 线性降维：分为主成分分析（PCA）、线性判断分析（LDA）。\n- 非线性降维：分为基于核函数的KPCA、KICA、KDA和基于特征值的ISOMAP、LLE、LE、LPP、LTSA、MVU等。\n## 数据清理\n“脏数据”对算法模型的直接影响是不能被使用，间接影响是降低模型的精度。这种情况下就需要对数据进行清理，包括（但不局限于）：不合格数据修正、缺失值填充、噪声值处理、离群点处理。\n\n- 不合格数据修正\n- 缺失值填充\n-  噪声值处理\n- 离群点处理\n## 相似度计算\n相似度计算在数据挖掘和推荐系统中有着广泛的应用场景。例如：\n- 在协同过滤算法中，利用相似度计算用户之间或物品之间的相似度。\n- 在利用k-means进行聚类时，利用相似度计算公式计算个体到簇类中心的距离，进而判断个体所属的类别。\n- 利用KNN进行分类时，利用相似度计算个体与已知类别之间的相似性，从而判断个体所属的类别等。\n下面将依次介绍常见的相似度计算方法。\n- 欧式距离\n- 曼哈顿距离\n- 切比雪夫距离\n- 马氏距离\n- 夹交余弦距离\n- 杰卡德相似系数与杰卡德距离\n- 相关系数与相关距离\n# 数据分类\n分类算法是数据挖掘中常用的基本算法之一，属于有监督学习算法（Supervised Learning）。\n在实际应用场景中，往往利用分类算法对基础数据进行处理，或者做一些基础模型供推荐系统使用。\n## KNN算法\nKNN算法的原理和具体的代码实现，这里不过多介绍，可参考《推荐系统开发实战》\n\nKNN是一个分类算法，但可以使用KNN的原始算法思路进行推荐，即，为每个内容或物品寻找K个与其最相似的内容或物品，然后推荐给用户。\n例如，在一个简单的电商网站中，用户浏览了一本图书，则推荐系统会依据图书的一些性质特征为用户推荐前 K个与该图书最相似的图书。\n\n对性别进行预测在电商网站中也常用到。某些用户在填写注册信息时并没有注明性别，或者填写的数据不正确。如果在性别未知的情况下进行商品推荐，则容易将男性商品推荐给女性，或者将女性商品推荐给男性。这种情况下就需要对用户性别进行判定。这时候KNN算法就可以被派上用场了。\n## 决策树\n在点击率预估场景中我们经常使用的GBDT/XGBoost，其底层也是一棵棵决策树。了解决策树的算法和原因对我们后续学习GBDT也是很重要的。\n决策树（Decision Tree）是根据一系列规则对数据进行分类的过程。分为回归决策树和分类决策树。\n- 回归决策树是对连续变量构建决策树。\n- 分类决策树是对离散变量构建决策树。\n其中分类决策树的代表为ID3算法，C4.5和CART既可构建分类决策树也可以构建回归决策树。\n\n## 朴素贝叶斯算法\n在电商网站中，往往会存在一些异常用户，包括恶意刷单用户、爬虫爬取数据的用户等。这些异常用户产生的数据信息在推荐场景中往往是没有用的，即所说的“脏数据”。那么在准备推荐算法相关数据时，应过滤掉这些异常用户所产生的数据。\n\n这个时候就可以使用贝叶斯算法了，贝叶斯算法是一类算法的总称，这些算法均以贝叶斯定理为基础。\n\n贝叶斯理论是以18世纪的一位神学家托马斯•贝叶斯（Thomas Bayes）的名字命名的。通常，在事件B已经发生的前提下事件A发生的概率，与事件A已经发生的前提下事件B发生的概率是不一样的，然而这两者是有关系的。贝叶斯定理就是针对这种关系所做的陈述。\n\nP(A|B)表示在事件B已经发生的前提下事件A发生的概率。其基本求解公式为：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709085708314.png\"></center>\n\n贝叶斯定理便是基于条件概率的等式定理，其计算公式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709085717276.png\"></center>\n\n特征独立性假设是指，假设每个特征之间是没有联系的，朴素贝叶斯算法则是建立在这样的基础之上的。\n\n朴素贝叶斯算法有三种常见模型：\n- 多项式模型\n- 高斯模型\n- 伯努利模型。\n# 数据聚类\n聚类算法也是数据挖掘中常用的基本算法之一，属于无监督学习算法（Unsupervised Learning）。在实际应用场景中，会利用聚类算法对基础数据进行处理，或者做一些基础模型供推荐系统使用。\n## K-means算法\n在电商网站中，商品的数目很多，对应的商品价格也很多。但对于用户来讲，并不是对所有价格的商品都感兴趣。例如，一个经常网购1000元左右手机的用户，通常没必要向他推荐价格超过5000元的手机。\n\n所以，需要对商品的价格进行聚类，进而求出用户感兴趣的价格段，从而提高推荐系统的准确度和可信赖度。\n\n这时候基于kMeans算法进行商品价格聚类算法就随之产生了。\n\nkMeans（K均值聚类算法）的基本原理是：\n（1）随机初始化K个初始簇类中心，对应K个初始簇类，按照“距离最近”原则，将每条数据都划分到最近的簇类；\n（2）第一次迭代之后，更新各个簇类中心，然后进行第二次迭代，依旧按照“距离最近”原则进行数据归类；\n（3）直到簇类中心不再改变，或者前后变化小于给定的误差值，或者达到迭代次数，才停止迭代。\n\n具体的执行步骤如下：\n（1）在数据集中初始K个簇类中心，对应K个初始簇类；\n（2）计算给定数据集中每条数据到K个簇类中心的距离；\n（3）按照“距离最近”原则，将每条数据都划分到最近的簇类中；\n（4）更新每个簇类的中心；\n（5）迭代执行步骤（2）～步骤（4），直至簇类中心不再改变，或者变化小于给定的误差区间，或者达到迭代次数；\n（6）结束算法，输出最后的簇类中心和对应的簇类。\n\n## 二分-Kmeans算法\n二分-kMeans算法（二分-K均值聚类算法）是分层聚类（Hierarchical Clustering）的一种，是基于kMeans算法（K-均值聚类算法）实现的。\n\n在二分-kMeans算法中，调用kMeans（k =2）把一个簇类分成两个，迭代此过程，直至分成k个。其实现的具体思路为：\n（1）初始化簇类表，使之包含所有的数据；\n（2）对每一个簇类应用k均值聚类算法（k = 2）；\n（3）计算划分后的误差，选择所有被划分的聚簇中总误差最小的并保存；\n（4）迭代步骤（2）和步骤（3），簇类数目达到K后停止。\n\n相对于kMeans算法，二分-kMeans的改进点有以下两点：\n- 加速了kMeans的执行速度，减少了相似度的计算次数；\n- 能够克服“kMeans收敛于局部最优”的缺点。\n\n# 关联分析\n关联分析的目的是，找到具有某种相关性的物品。这种分析在推荐系统中也有很大的作用，如经常出现在一个购物篮中的商品就可以相互推荐。关联算法最常见的就是Apriori。\n\n该算法中涉及的一些概念解释为：\n- 关联分析（Association Analysis）：从大规模数据集中寻找商品的隐含关系。\n- 项集（Item Set）：包含0个或多个项的集合。\n- 频繁项集：那些经常一起出现的物品集合。\n- 支持度计数（Support Count）：一个项集出现的次数（即，整个交易数据集中包含该项集的事物数）。\n- 项集支持度：一个项集出现的次数与数据集所有事物数的百分比，计算公式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/2019070909025024.png\"></center>\n\n- 项集置信度（confidence）：数据集中同时包含A、B的百分比，计算公式如下：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709090259813.png\"></center>\n\nApriori算法使用一种称为逐层搜索的迭代方法，其中k项集用于探索（k+1）项集（如使用频繁1项集找到频繁2项集），其实现过程如下：\n- （1）通过扫描数据库，累计每个项的计数，并收集满足最小支持度的项，找出频繁1项集的集合。该集合记作L1；\n- （2）使用L1找出频繁2项集的集合L2，使用L2找出L3；\n- （3）如此下去，直至不能再找到频繁k项集，每找出一个Lk需要一次完整的数据库扫描。\n\n----\n\n数据挖掘在推荐系统中的应用非常广泛，无论是数据预处理还是基础模型开发，他都发挥着举足轻重的作用，因此在理解推荐算法的同时，对数据挖掘也要进行一定程度的掌握。关于文中的更多详细内容请持续关注<font color=red>《推荐系统开发实战》\n\n-----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之2、从搭建一个电影推荐系统开始学推荐系统开发实战","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之2、从搭建一个电影推荐系统开始学推荐系统开发实战/","content":"\n> 推荐系统在我们的生活中无处不在，比如购物网站，视频音乐网站，新闻网站等，那么推荐系统是如何工作的，他是基于什么方式实现的？可以在《推荐系统开发实战》这本书中进行系统的了解和学习，本篇文章是该系列文章的开篇之作，带领大家认识一下基于最近相似用户的推荐。以下内容摘自于《推荐系统开发实战》\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/93927941\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n- 嗨，Susan，最近有什么好看的电影吗？\n- Thinkgamer，我觉得《芳华》不错，推荐你可以去看下。\n\n这样的场景相信我们会经常遇到，当我们不知道要看哪部电影时，会咨询一下身边的朋友，从他们那里得到一些意见。当我们在咨询别人时，往往会有自己的判断，Thinkgamer喜欢文艺片，他不会去征求喜欢动画片的Jake，但是他会去咨询同样喜欢文艺片的Susan。\n\n基于上边的描述，我们可以总结出UserCF的算法过程：\n1. 计算用户相似度\n2. 寻找给定用户最相近的K个用户\n3. 将K个用户喜欢的且给定用户没有行为的物品推荐给给定用户\n\n简单的讲就是：给用户推荐“和他兴趣相投的其他用户”喜欢的物品。\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190627234358613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\"  width=70% height= /></center>\n\n上图所示是一个基于用户的协同过滤推荐的例子，用户A和用户C同时喜欢电影A和电影C，用户C还喜欢电影D，因此将用户A没有表达喜好的电影D推荐给用户A。\n\n针对上述过程，可以分为以下步骤：\n## 构建用户物品评分表\n假设用户与物品所表达的喜好程度（即评分），如下表所示：\n\n用户|\t物品a|\t物品b|\t物品c\t|物品d\t|物品e\n---|--|--|--|--|--|--\nA\t|3.0|\t4.0\t|0\t|3.5|\t0\nB\t|4.0|\t0\t|4.5|\t0|\t3.5\nC\t|0|\t3.5\t|0\t|0|\t3\nD|\t0|\t4|\t0\t|3.5|\t3\n\n## 相似度计算\n计算用户之间相似度的方法有很多，这里选用的是余弦相似度，如下：\n\n<center>\n<img  src=\"https://img-blog.csdnimg.cn/20190707101722627.png\"   />\n</center>\n\n针对用户u和v，上述公式中的参数如下。\n\n - N(u)：用户u有过评分的物品集合；\n - N(v)：用户v有过评分的物品集合；\n - Wuv：用户u和用户v的余弦相似度。\n\n结合上表，可以分别求得用户C和其他三个用户的相似度，见下面三公式：\n<center>\n<img  src=\"https://img-blog.csdnimg.cn/20190707101854595.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% />\n</center>\n从计算结果来看，D用户与C用户相似度最大。\n从表中也可以直接看出，用户D和C都在b和e物品上进行了评分，用户A、B和C也都在b物品上进行了评分。\n\n## 计算推荐结果\n用户C进行评分的物品是b和e，接下来计算用户C对物品a、c、d的偏好程度，见下面三公式：\n<center>\n<img src=\"https://img-blog.csdnimg.cn/2019070710202317.png\" width=60% />\n</center>\n从上面的计算可以得到，在用户C没有进行评分的物品中倒序排列为a→c→e。这样就可以根据需要取前 K个物品推荐给C用户。\n\n<font color=red>完整代码可参考《推荐系统开发实战》</font>\n\n## 算法复杂度优化\n但是上面的计算存在一个问题——需要计算每一对用户的相似度。代码实现对应的时间复杂度为O(|U|*|U|)，U为用户个数。\n\n在实际生产环境中，很多用户之间并没有交集，也就是并没有对同一样物品产生过行为，所以很多情况下分子为0，这样的稀疏数据就没有计算的必要。\n\n上面的代码实现将时间浪费在计算这种用户之间的相似度上，所以这里可以进行优化：\n（1）计算出的用户对（u，v）；\n（2）对其除以分母得到u和v的相似度。\n\n针对以上优化思路，需要两步：\n（1）建立物品到用户的倒排表T，表示该物品被哪些用户产生过行为；\n（2）根据倒查表T，建立用户相似度矩阵W：\n\n- 在T中，对于每个物品i，设其对应的用户为j、k，\n- 在W中，更新对应位置的元素值，W[j][k]=W[j][k]+1，W[k][j]=W[k][j]+1。\n\n以此类推，这样在扫描完倒查表T之后，就能得到一个完整的用户相似度矩阵W了。\n这里的W对应的是前面介绍的余弦相似度中的分子部分，然后用W除以分母，便能最终得到两个用户的兴趣相似度。\n以上表为例，总共有4个用户，那么要建一个4行4列的倒排表，具体建立过程如下：\n（1）由用户的评分数据得到每个物品被哪些用户评价过，如图5-6所示。\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190707144648916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=70% />\n</center>\n\n（2）建立用户相似度矩阵W，如图5-7所示。\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190707144715866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=70% />\n</center>\n\n得到的相似度矩阵W对应的是计算两两用户相似度的分子部分，然后除以分母得到的便是两两用户的相似度。\n\n还是以C用户为例。从图5-7可知，A、B用户与C用户相似度计算的分子都为1，D用户与C用户相似度计算的分子部分为2。其他用户与C用户的相似度计算如下：\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190707152109180.png\"  width=50% />\n</center>\n得到用户的相似度之后，就可以计算用户对未评分物品的可能评分了。采用的计算方式依旧是：\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190707152129605.png\"  width=50% />\n</center>\n\n其中各参数说明如下。\n- P(u,i)：用户u对物品i的感兴趣程度；\n- S(u,K)：和用户u兴趣最接近的K个用户；\n- N(i)：对物品i有过行为的用户集合；\n- Wuv：用户u和用户v的兴趣相似度；\n- rvi：用户v对物品i的兴趣，即用户对物品的评分。\n依据上式，分别计算用户C对物品a、c、d的可能评分：\n<center><img src=\"https://img-blog.csdnimg.cn/20190707152205766.png\" width=50% /></center>\n同样，对比优化前后的计算可知，结果是一致的。\n\n<font color=red>具体的代码实现可参考：《推荐系统开发实战》一书。</font>\n\n## 惩罚热门物品\n如果两个用户都买过《新华字典》，这并不能说明他们兴趣相同，因为绝大多数中国人都买过《新华字典》。\n但如果两个用户都买过《机器学习实战》，那可以认为他们的兴趣比较相似，因为只有研究机器学习的人才可能买这本书。\n因此，John S. Breese在论文中提出了式（5.4），根据用户行为计算用户的兴趣相似度：\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190707152309805.png\" width=50% /></center>\n\n- 分子中的倒数部分，惩罚了用户u和用户v共同兴趣列表中热门物品，减小了热门物品对用户相似度的影响。\n- N(i)是对物品i有过行为的用户集合。物品i越热门，N(i)越大。\n\n对此，修改用户相似度的计算方式，具体的代码实现如函数userSimilarityBest()所示。\n\n<font color=red>具体的代码实现可参考：《推荐系统开发实战》一书。</font>\n\n## 案例实战\n在了解完UserCF的算法原理之后，来开发一个电影推荐系统。这里我们选用的MovieLens数据集，该数据集在《实战》一书中的第三章有详细介绍。\n\n搭建一个推荐系统的步骤包括：\n- 准备数据\n- 选择算法\n- 模型训练\n- 效果评估\n\n这里不过多进行介绍，欢迎关注小编图书。\n\n-----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：文中多次提到的《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>\n","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"《推荐系统开发实战》系列之1、推荐系统的前世今生与古往今来","url":"/2019/07/10/RecSys/推荐系统开发实战/《推荐系统开发实战》系列之1、推荐系统的前世今生与古往今来/","content":"\n> 接下来将会用10篇文章介绍下推荐系统的起源，应用，算法和案例，并带领大家从0到1实现属于自己的推荐系统。本篇文章为该系列文章的第一篇，不会涉及具体的技术，用科普的形式让大家对推荐系统有一个感性的认识。\n\n<!--More-->\n\n推荐系统在我们的生活中应用的非常广泛，无论是生活，娱乐，工作都会或多或少的涉及一些，接下来将会整理一个系列的文章，介绍一下推荐系统，并从0到1的去实现一个推荐系统，本篇内容主要涉及的是推荐系统的背景发展和开发一个自己的推荐系统需要掌握的技能和知识。\n\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/95116871\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n\n# 前世\n“啤酒”与“尿布”的故事对推荐系统的学习有着积极的影响。从该故事出发，我们依旧能看到从20世纪90年代到现在个性化推荐系统的演进和发展。\n\n“啤酒与尿布”的故事相信很多人都知道。它讲述的是20世纪90年代，在美国沃尔玛超市中，管理人员分析数据时，发现了一个奇怪的现象：在某些特定的情况下，“啤酒”与“尿布”两件看上去毫无关系的商品出现在了同一个购物篮中。这种独特的销售现象引起了管理人员的注意，经过后续调查发现，一些年轻的爸爸常到超市去购买婴儿尿布，有30％～40％的新爸爸，会顺便买点啤酒犒劳自己。随后，沃尔玛对啤酒和尿布进行了捆绑销售，不出意料，销售量双双增加。\n\n该案例出自于涂子沛先生的《数据之巅》一书。在这个案例中，数据和情节让这个故事不容置疑，然而据吴甘沙先生透漏，该案例是TeraData公司一位经理编出来的“故事”。目的是让数据分析看起来更有力，更有趣。在历史上该案例并没有出在美国的任何一个沃尔玛超市中。\n虽然这个“故事”是杜撰的，但其中涉及的“捆绑销售”却不失为一种购物推荐。物品A与物品B经常出现在一个购物篮中，那么向购买物品A（物品B）的人推荐物品B（物品A）是有数据依据和理论基础的。依据该理论，后来衍生出了关联规则算法，很多电商类或者视频类的网站都使用到该类算法进行商品或者视频的推荐。\n\n# 今生\n在19世纪和20世纪，推荐系统在互联网这片肥沃的土壤中得到了充分的发展，如今已经在各个公司开发结果。以下列了一些有代表性的发展点：\n- Xerox公司在1992年设计的应用协同过滤算法的邮件系统——Tapestry。同年，Goldberg提出了“推荐系统”这个概念。\n- 1994年，明尼苏达大学的GroupLens研究组使用基于主动协同过滤的推荐算法，开发了第一个自动化推荐系统 GroupLens，并将其应用在Usenet新闻组中。\n- 1996年，卡内基梅隆大学的Dunja Mladenic在Web Watcher的基础上进行了改进，提出了个性化推荐系统Personal Wgb Watchero。1996年，著名的网络公司Yahoo也注意到了个性化服务的巨大优势和潜在商机，推出个性化入口MyYahoo。\n- 1997年，Resnick和Varian首次在学术届正式提出推荐系统的定义。他们认为：推荐系统可以帮助电子商务网站向用户提供商品和建议，促成用户的产品购买行为，模拟销售人员协助客户完成购买过程。\n- 1998年，亚马逊（Amazon.com）上线了基于物品的协同过滤算法，将推荐系统的规模扩大至服务千万级用户和处理百万级商品，并带来了良好的推荐效果。\n- 2003年，亚马逊（Amazon.com）的Linden等人发表论文，公布了“基于物品的协同过滤算法”。\n- 2005年，Adomavicius等人发表综述论文，将推荐系统分为3类——基于内容的推荐、基于协同过滤的推荐和混合推荐，并提出了未来可能的主要研究方向。\n- 2006年10月，北美在线视频服务提供商 Netflix 宣布了一项竞赛，任何人只要能够将它现有电影推荐算法 Cinematch 的预测准确度提高10%，就能获得100万美元的奖金。\n- 2007年，第一届ACM推荐系统大会在美国举行，到2017年已经是第11届。这是推荐系统领域的顶级会议，展示了推荐系统在不同领域的最近研究成果、系统和方法。\n- 2016年，YouTube发表论文，将深度神经网络应用推荐系统中，实现了从“大规模可选的推荐内容”中找到“最有可能的推荐结果”。\n- 2018年，阿里巴巴提出论文《基于注意力机制的用户行为建模框架以及在推荐领域的应用（ATRank）》，被AAAI录用。\n\n# 用途\n推荐系统已经广泛存在于我们的生活之中，其主要应用场景如下：\n- 电商类（京东/淘宝/拼多多...）\n<center><img src=\"https://img-blog.csdnimg.cn/2019070900060147.png\"  width=70%/></center>\n\n- 社交类（微博/微信/豆瓣...）\n<center><img src=\"https://img-blog.csdnimg.cn/20190709000507866.png\"  width=70%/></center>\n\n- 音乐类（网易云音乐/QQ音乐/虾米...）\n<center><img src=\"https://img-blog.csdnimg.cn/20190709000039604.png\"  width=70%/></center>\n\n- 视频类（YouTube/爱奇艺/腾讯视频...）\n<center><img src=\"https://img-blog.csdnimg.cn/20190709000122366.png\"  width=70%/></center>\n\n- 阅读类（头条/知乎/书旗...）\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190709000247410.png\"  width=70%/></center>\n\n- 服务类（美团/饿了么/口碑...）\n<center><img src=\"https://img-blog.csdnimg.cn/20190709000402258.png\"  width=70%/></center>\n\n# 技术栈\n目前工业界所使用的推荐系统整个框架是比较大的，并非一个人所能完成，那么作为初学者，我们如何能够充分理解推荐算法的知识，如何自己手动开发一个推荐系统呢？\n\n首先是选定开发语言，Python作为使用最广泛的编程语言，当之无愧，而且py也已经成为了算法工程师的必备知识技能。同时在开发Python版本推荐系统过程中，不能缺失的是数据结构思想，不能缺失的是工程能力！\n\n那如何去定义一个人工程能力的好坏呢？主要从以下几方面去判断：\n- 编码能力\n> 强悍的编码能力，并不是说在很短的时间内完成一个业务开发，而是说在高效完成的同时能够追求代码的质量和可读性。只有同时具备高效、质量和可读性，才能说是工程能力很强。\n- 解决问题能力\n> 工程能力的核心在于发现问题、分析问题和解决问题。\n能够发现问题，说明拥有创新能力和自觉性的态度。\n而对于已经发生的问题，能够慎密地进行分析、定位，继而快速解决，这样才真正到了“知其然，知其所以然”的境界。\n- 快速学习能力\n> 计算机行业中，知识的迭代是非常快的。就好比，2014年左右Hadoop分布式计算框架是主流的，但到2017年，基于内存计算的Spark框架成了分布式处理的新宠。如果一个从业者没有很强的学习、适应和突破能力，是不行的。\n好的学习能力前提是：拥有一个好的学习技巧，在学习之初就明白学习的目的是什么。然后围绕该目的，列出一系列的问题，然后在学习的过程中进行解决，同时进行手动实践，总结和积累经验。\n- 文档梳理能力\n> 工程能力的强弱很多时候体现在文档上。是否完成过一个项目，对项目的理解是不一样的。对项目是否有深入的思考，写出的文档深浅也是不一样的。\n一个优秀的文档，除了对自己，对于团队、他人来说也是十分重要的。\n- 产品质量责任心\n> 好的工程能力还要拥有一个负责任的心。试想一下，上级交给你一个任务，你如果敷衍了事，那么你在这个岗位上是走不远的。\n- 沟通协作\n> 在实际的工作环境中，往往每个任务都需要多人协作完成。一个从业者，如果不懂得沟通和协作，所完成的任务必然是不能满足需求的。\n\n-----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n\n\n---\n<font color=red>注：以上内容摘自于《推荐系统开发实战》，该书是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font>\n<center><img src=\"https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=50% /></center>","tags":["推荐系统开发实战"],"categories":["技术篇"]},{"title":"Thinkgamer's 简历","url":"/2019/06/21/随缘/关于我/","content":"\n---\n# 个人信息\nI&nbsp;&nbsp;&nbsp;D：&nbsp;&nbsp;&nbsp;&nbsp;**Thinkgamer**\n邮箱：&nbsp;&nbsp;&nbsp;&nbsp;**thinkgamer@163.com**\n毕业：&nbsp;&nbsp;&nbsp;&nbsp;**沈阳航空航天大学-计算机学院-软件工程**\n就职：&nbsp;&nbsp;&nbsp;&nbsp;**北京 | 某厂 | 算法工程师**\n\n<!--More-->\n\n---\n\n# 技术园地：\n\n- CSDN：http://blog.csdn.net/gamer_gyt\n- Github：https://github.com/Thinkgamer\n- 知乎: https://www.zhihu.com/people/thinkgamer/activities\n- 公众号：搜索与推荐Wiki\n\n\n\n<img src=\"/assets/img/gongzhonghao.jpg\" width=\"250px\" height=\"250px\">\n\n---\n\n# 工作经历\n## 2017-12～2019-06 | 京东商城\n- 个性化消息Push\n\n> 简称“种草”。针对京东用户进行消息的个性化Push，增强用户黏性和交互。\n> 负责种草整个联动方案，组织会议进行讨论和需求下发，资源安排等。\n> 基于机器学习的个性化消息push模型开发，训练，调优，上线等。\n> 深度学习模型调研，基于公司内部平台，上线基于tensorflow-serving的深度学习模型，效果较ML模型提升显著。\n\n- Plus会员个性化推荐\n\n> 理解内部推荐架构原理，负责开发方案，资源安排。\n> 基于机器学习的个性化消息push模型开发，训练，调优，上线等。\n> 相关CTR预估模型研究与在plus业务数据集上的离线测试。\n\n- 商品价格段模型\n\n> 基于KMeans构建商品价格段模型，实现了基于MR和Spark两个版本的代码。\n\n- 商品质量分模型\n\n> 基于线性模型构建商品质量分模型，用户推荐架构中的召回粗排。 \n\n- 特征监控模型\n\n> 数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限。特征对于模型来说极其重要，因为对于特征的监控十分有必要，该模型支持使用者自定义监控指标和监控字段，能够有效的减少出现问题时的排查时间提高效果，并进行预警。\n\n- 基础数据开发\n\t- 业务内特征开发\n\t- 全站特征开发\n\t- 召回数据开发\n\n## 2016-10～2017-12 | 北京万维星辰科技有限公司\n- 搭建基于 Hadoop 和 ELK 技术栈的日志分析系统\n\n> 参与设计了基于 ELK 的日志分析系统，提出并搭建了 Hadoop 数据备份系统，研究了 ELK 周边的\n开源产品 ，学习并使用 rails 实现 es 数据的快照备份。\n\n- 异常检测算法研究与实现\n\n> 1：根据合作方提供的 wlan 上网数据，对用户进行肖像刻画，从而对后入数据进行异常值估计。\n\n> 2：研究基于指数平滑和线性回归的异常值检测，并使用 python 的 elasticsearch 进行实现。\n\n- 中彩/德州银行日志审计项目\n\n> 利用公司的日志分析系统对中国福利彩票和德州银行的日志进行分析，并形成安全事件，提出相应的整改和解决意见，形成月度报告。\n\n## 2016-07～2016-09 | 北京广联达软件有限公司\n> 实习以课题形式（课题为：基于质量数据的数据分析平台搭建）展开，利用 Hadoop 等开源组件搭建了 5 台分布式系统，包含 Hadoop，Hive，Spark，Zookeeper，Sqoop 和 Hbase，在该平台上完成了豆瓣影评数据分析 Demo\n\n\n-----\n# 技能掌握\n- 熟练掌握基于机器学习和深度学习的CTR预估算法，包括GBDT/LR/FM/FFM/FTRL/XGBoost/Wide&Deep/DeepFM/DNN/FNN等。\n- 熟悉推荐系统的数据流和过滤，召回，排序，展示等架构。\n- 熟练掌握相关机器学习算法并用来构建基本模型。\n- 熟练用户画像/物品画像/特征工程。\n- 熟练Spark/MR/Hive/Python开发，了解相关大数据产品。\n- 了解爬虫/Web后端开发，曾开发多个基于Django的网站后端。\n- 了解强化学习/迁移学习/NLP。\n- 熟悉ELK技术栈/Linux/Docker。\n\n----\n\n# 大学经历\n\n## 项目经历\n- 基于 Hadoop 和机器学习的博客统计分析平台\n\n> 采用 Django 作为 Web 开发基础，Python 爬取了 CSDN 博客的部分数据，存储到 hdfs 上，利用 MapReduce 对数据进行了离线计算，将解析好的字段存储到 Hive 中，利用 python 开发实现了协同过滤算法和 PangRank 算法。最终此项目在辽宁省计算机作品大赛中获得二等奖，中国大学生计算机作品大赛中获得三等奖。\n\n- 图书推荐系统\n\n> python 爬取了豆瓣图书数据，对数据进行清洗之后，使用基于 Item 和 User 的协同过滤算法对登录用户产生图书推荐，此项目为大三期间为一个网友做的毕业设计。\n\n## 荣誉奖励\n\n- 单项一等奖学金  * 2\n- 综合二等奖学金  * 2\n- 单项支援服务标兵\n- 优秀团干 * 2\n- 辽宁省ACM优秀志愿者\n- 校ACM三等奖\n- 沈阳航空航天大学计算机作品大赛二等奖【网站】\n- 辽宁省计算机作品大赛二等奖【博客统计分析系统】\n- 中国大学生作品大赛三等奖【博客统计分析系统】\n\n## 工作经历\n\n- 助理辅导员 | 2014.09-2015.07\n\n> 计算机学院 2014 级新生助理辅导员，协助辅导员进行大一班级的日常管理\n\n- 活动部部长 | 2014.09-2015.07\n\n> 爱心联合会活动部部长，负责相关活动的宣传推广与执行\n\n- 班级团支书 | 2013.09-2017.06\n\n> 协助辅导员进行班级日常的管理和相关共青团工作的开展\n\n\n-----\n\n# 自我评价\n- 不服输，爱钻研，具有自学能力和解决问题能力。\n- 喜欢看书，看文章，整理笔记。\n- 文艺Coder。\n\n---\n\n# 联系我：\n\n- <img src=\"/assets/img/myweixin.png\" height=\"300\" width=\"220\" />\n \nPS：加我微信，拉你进数据与算法交流群，进行头脑风暴\n","tags":["随手记"],"categories":["随手记"]},{"title":"陆月，我想说的和我要改名了！","url":"/2019/06/21/随手记/陆月，我想说的和我要改名了！/","content":"> 当你看到这篇文章的时候不要惊讶，因为我已经两个月没有发技术文章了！突然又接着更新了，不要害怕，因为这就是我，我都不确定我什么时候会发出一篇文章来。 首先要明确的是这不是一篇技术文章，只是这一段时间内我的一些胡思乱想和东拼西凑。其中不免包含了近段时间我的经历和思考，也会包含一些接下来做事的计划和思考。\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n<a href=\"https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=2247484950&idx=1&sn=0ac8fb054e7fc2f547672dfd27228d80&chksm=ea66ce8cdd11479ab31558baf6c650b3e80515742605b06dc9f1ff81a01a2fc5c73c0b44b7a0&token=1671615317&lang=zh_CN#rd\">点击阅读原文</a>\n\n---\n# Chapter 1\n\n5月和6月对我来讲其实挺难熬的，五一回家开始打算装修房子，回家了7天，每天都在奔波，终于在回京前一天把相关事情敲定。回京之后，我突然萌生了一个念头——辞职，最终这个念头在我心里生了根，发了芽，长成了参天大树。这其中有很多原因，并不是因为某一件事而促就的，不是京东不好，也不是我有多么优秀，而是纵横交错的心结最终都指向了那一个方向。\n\n\n过了今天，我若再踏进这座大楼，恐怕就是以一个“访客”的身份进来了，一度秋冬，两番春夏，感恩京东带给我的成长和经历，让我自己有了一个质的改变，也很团队和老大们的帮助，鼓励，让我能够在工作和生活中得心应手。但天下没有不散的宴席，相遇是缘，再见是份，不管身处何方，心中的那份情不会忘。今日挥手，再见已是江湖。祝福彼此，逆风翻盘，涅槃重生！\n\n我不确定现在的我是一个怎么样的水平，我也不知道我接下来的工作会是一场怎样的经历，但是万变不离其宗，不管在哪，在接下来几年内，我要做的就是好好沉淀自己。无论是为人处事，还是工作生活，都要静下来，好好吸收一下！有时候走的太快真的不一定是好事。\n\n转眼间已经六月了，岁月真的是一场有去无回的游戏，想想曾经自己立的那些flag，现在真的是啪啪打脸了，但无论如何，有梦固然是好的，因为有梦才有方向，没有梦你就没有方向。\n\n过完了在这的最后一个618，就要告辞了，原谅在这个时候我做了一个“逃兵”，但无风不起浪，空穴不来风，真心希望京东能好，能够在未来的时代中站起来，做一些能够改变世界的事情，愿你不固步自封，愿你能任人唯贤，愿你能斗志昂扬，愿你能让勇士不流浪，愿你能让智者挺脊梁。愿我们都能被时光善待，都能未来可期。\n\n# Chapter 2\n\n这个公众号最早叫《码农故事多》，因为我本是一个“码农”，我喜欢旅行，喜欢摄影，喜欢写作，还喜欢敲代码，我觉得我曾经的经历颇多，如果围炉小坐，我能滔滔诉说。后来不知怎么的我又觉得这个名字很Low，没有定位，没有方向，更像是一个大杂烩，于是我思前想后，起了个《数据与算法联盟》的名字，其寓意是把大数据和算法结合起来，重塑世界！愿景很大，但其实没什么卵用，我太渺小了，渺小的像是沧海一粟，像是浩瀚宇宙中的日月星辰，我觉得我配不上这个名字。\n\n我的新书《推荐系统开发实战》即将上市，趁着这个机会，我想把我的公众号定位到推荐系统相关的，但我们都知道，推荐，搜索，广告其实都差不太多，无非就是CTR/CVR。至于用什么技术去解决，与预估，这都可以。所以呢，我打算给我的公众号重新赋予一个名字《搜索与推荐Wiki》生效时间2019.06.22，目前小编从事的是推荐算法相关的工作，后序也会涉及搜索/文本相关的工作，后序在更新文章上也会有所选材和注重文章的连续性，之前更新文章算是随意自由的更新，但今后我会在一定程度上约束自己，希望我的读者能够和我一起进步，一起用技术改变世界！\n\n最后我想说的是：公号还是那个公号，只是名字变了，小编还是那个小编，只是规划更清晰了！那么你还愿意做我的读者吗？\n\n愿你雨天有伞，黑天有灯，余生有良人！\n\n感恩，祝福！\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["随手记"],"categories":["随手记"]},{"title":"Spark排序算法系列之（MLLib、ML）LR使用方式介绍","url":"/2019/05/07/RecSys/Spark与推荐系统/Spark排序算法系列之（MLLib、ML）LR使用方式介绍/","content":"\n>【Spark排序算法系列】主要介绍的是目前推荐系统或者广告点击方面用的比较广的几种算法，和他们在Spark中的应用实现，本篇文章主要介绍LR算法。\n\n<!--More-->\n\n本系列还包括（持续更新）：\n\n- [Spark排序算法系列之GBDT（梯度提升决策树）](https://blog.csdn.net/Gamer_gyt/article/details/86695837)\n- Spark排序算法系列之模型融合（GBDT+LR）\n- Spark排序算法系列之XGBoost\n- Spark排序算法系列之FTRL（Follow-the-regularized-Leader）\n- Spark排序算法系列之FM与FFM\n\n\n\n\n# 背景\n逻辑回归（Logistic Regression，LR）是较早应用在推荐排序上的，其属于线性模型，模型简单，可以引入海量离散特征，这样的好处就是模型可以考虑更加细节或者说针对具体个体的因素。如果想要引入非线性因素需要做特征交叉，这样很容易产生百亿特征，在很早之前ctr就主要靠堆人力搞特征工程工作来持续优化效果。\n\n虽然目前在工业界LR应用的并不多，但是对于初学者,一些中小企业或者应用场景不需要负责排序模型的时候，LR扔不失为一个不错的选择。\n\n关于LR的算法原理，这里不做过多说明，可参考：\n- [回归分析之逻辑回归-Logistic Regression](https://blog.csdn.net/Gamer_gyt/article/details/80115252)\n- [线性模型篇之Logistic Regression数学公式推导](https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=100001113&idx=1&sn=711c3af661c54aa2757ca41d867cef6a#rd)\n\n\n# LR介绍\nLR的数学表达式可以简写为：\n\n$$\nL(w,x,y)=log(1+exp(-yw^Tx))\n$$\n对于二分类模型，LR是一个分类算法，模型计算得到预测值后会通过以下函数进转化。\n$$\nf(z) = \\frac{1}{1+e^{-zx}}\n$$\n如果L(w,x,y) > 0.5 则是1 否则为0。当然在实际应用过程中，并不是一定取0.5作为界限值，而是根据实际情况进行调整。\n\n二进制回归可以转化为多分类回归问题。关于多分类介绍和基于Spark实现多分类可参考[多分类实现方式介绍和在Spark上实现多分类逻辑回归（Multinomial Logistic Regression）](https://blog.csdn.net/Gamer_gyt/article/details/86378882)\n\n在Spark.mllib包中提供了两种LR分类模型，分别是：\n- mini-batch gradient descent（LogisticRegressionWithLBFGS）\n- L-BFGS（LogisticRegressionWithSGD）\n\n但官方给出的建议是：推荐使用LBFGS，因为基于LBFGS的LR比基于SGD的能更快的收敛。其原话如下：\n\n> We implemented two algorithms to solve logistic regression: mini-batch gradient descent and L-BFGS. We recommend L-BFGS over mini-batch gradient descent for faster convergence.\n\n而且LRWithLBFGS不仅支持二分类还支持多分类，但LRWithSGD只支持二分类。所以后续只介绍下Spark mllib中的LogisticRegressionWithLBFGS相关操作。\n\n# mllib中的LRWithLBFGS\n\n设置变量和创建spark对象\n```\nval file = \"data/sample_libsvm_data.txt\"\nval model_path = \"model/lr/\"\nval model_param = \"numInterations:5,regParam:0.1,updater:SquaredL2Updater,gradient:LogisticGradient\"\n\nval spark = SparkSession.builder()\n\t.master(\"local[5]\")\n    .appName(\"LogisticRegression_Model_Train\")\n\t.getOrCreate()\nLogger.getRootLogger.setLevel(Level.WARN)\n```\n\n拆分数据集\n```\n// 记载数据集 并拆分成训练集和测试集\nval data = MLUtils.loadLibSVMFile(spark.sparkContext,file).randomSplit(Array(0.7,0.3))\nval (train, test) = (data(0), data(1))\n```\n\nLRWithLBFGS模型设置参数\n```\n// 定义分类的数目，默认为2，是logisticregression的参数\nprivate var numClass: Int = 2\n// 定义是否添加截距,默认值为false，是logisticregression的参数\nprivate var isAddIntercept: Option[Boolean] = None\n// 定义是否在训练模型前进行验证，是logisticregression的参数\nprivate var isValidateData: Option[Boolean] = None\n\n// 定义迭代的次数，默认值是100，LBFGS的参数\nprivate var numInterations: Option[Int] = None\n// 定义正则化系数值，默认值是0.0，LBFGS的参数\nprivate var regParam: Option[Double] = None\n// 定义正则化参数，支持：L1Updater[L1]、SquaredL2Updater[L2]、SimpleUpdater[没有正则项]，LBFGS的参数\nprivate var updater: Option[String] = None\n// 定义计算梯度的方式，支持：LogisticGradient、LeastSquaresGradient、HingeGradient ，LBFGS的参数\nprivate var gradient: Option[String] = None\n// 人工定义的收敛阈值\nprivate var threshold:Option[Double]=None\n// 定义模型收敛阈值，默认为 10^-6\nprivate var convergenceTol: Double= 1.0e-6\n```\n\n创建模型\n```\ndef createLRModel(model_param: String): LogisticRegressionWithLBFGS={\n\t// 设置模型参数\n\tval optimizer = new LROptimizer()\n\toptimizer.parseString(model_param)\n\tprintln(s\"模型训练参数为：${optimizer.toString}\")\n\n\t// 创建模型并指定相关参数\n\tval LRModel = new LogisticRegressionWithLBFGS()\n\t// 设置分类数目\n\tLRModel.setNumClasses(optimizer.getNumClass)\n\t// 设置是否添加截距\n\tif(optimizer.getIsAddIntercept.nonEmpty) {LRModel.setIntercept(optimizer.getIsAddIntercept.get)}\n\t// 设置是否进行验证模型\n\tif(optimizer.getIsValidateData.nonEmpty){LRModel.setValidateData(optimizer.getIsValidateData.get)}\n\t// 设置迭代次数\n\tif(optimizer.getNumInterations.nonEmpty){LRModel.optimizer.setNumIterations((optimizer.getNumInterations.get))}\n\t// 设置正则项参数\n\tif(optimizer.getRegParam.nonEmpty) { LRModel.optimizer.setRegParam(optimizer.getRegParam.get) }\n\t// 设置正则化参数\n\tif(optimizer.getUpdater.nonEmpty){\n\t\toptimizer.getUpdater match {\n\t\t\tcase Some(\"L1Updater\") => LRModel.optimizer.setUpdater( new L1Updater())\n\t\t\tcase Some(\"SquaredL2Updater\") => LRModel.optimizer.setUpdater(new SquaredL2Updater())\n\t\t\tcase Some(\"SimpleUpdater\") => LRModel.optimizer.setUpdater(new SimpleUpdater())\n\t\t\tcase _ => LRModel.optimizer.setUpdater(new SquaredL2Updater())\n\t\t}\n\t}\n\t// 设置梯度计算方式\n\tif(optimizer.getGradient.nonEmpty){\n\t\toptimizer.getGradient match {\n\t\t\tcase Some(\"LogisticGradient\") => LRModel.optimizer.setGradient(new LogisticGradient())\n\t\t\tcase Some(\"LeastSquaresGradient\") => LRModel.optimizer.setGradient(new LeastSquaresGradient())\n\t\t\tcase Some(\"HingeGradient\") => LRModel.optimizer.setGradient(new HingeGradient())\n\t\t\tcase _ => LRModel.optimizer.setGradient(new LogisticGradient())\n\t\t}\n\t}\n\t// 设置收敛阈值\n\tif(optimizer.getThreshold.nonEmpty){ LRModel.optimizer.setConvergenceTol(optimizer.getThreshold.get)}\n\telse {LRModel.optimizer.setConvergenceTol(optimizer.getConvergenceTol)}\n\n\tLRModel\n}\n```\n\n模型效果评估\n```\n\tdef evaluteResult(result: RDD[(Double,Double,Double)]) :Unit = {\n\t\t// MSE\n\t\tval testMSE = result.map{ case(real, pre, _) => math.pow((real - pre), 2)}.mean()\n\t\tprintln(s\"Test Mean Squared Error = $testMSE\")\n\t\t// AUC\n\t\tval metrics = new BinaryClassificationMetrics(result.map(x => (x._2,x._1)).sortByKey(ascending = true),numBins = 2)\n\t\tprintln(s\"0-1 label AUC is = ${metrics.areaUnderROC}\")\n\t\tval metrics1 = new BinaryClassificationMetrics(result.map(x => (x._3,x._1)).sortByKey(ascending = true),numBins = 2)\n\t\tprintln(s\"score-label AUC is = ${metrics1.areaUnderROC}\")\n\t\t// 错误率\n\t\tval error = result.filter(x => x._1!=x._2).count().toDouble / result.count()\n\t\tprintln(s\"error is = $error\")\n\t\t// 准确率\n\t\tval accuracy = result.filter(x => x._1==x._2).count().toDouble / result.count()\n\t\tprintln(s\"accuracy is = $accuracy\")\n\t}\n```\n\n保存模型\n```\n\tdef saveModel(model: LogisticRegressionModel, model_path: String): Unit = {\n\t\t// 保存模型文件 obj\n\t\tval out_obj = new ObjectOutputStream(new FileOutputStream(model_path+\"model.obj\"))\n\t\tout_obj.writeObject(model)\n\n\t\t// 保存模型信息\n\t\tval model_info=new BufferedWriter(new FileWriter(model_path+\"model_info.txt\"))\n\t\tmodel_info.write(model.toString())\n\t\tmodel_info.flush()\n\t\tmodel_info.close()\n\n\t\t// 保存模型权重\n\t\tval model_weights=new BufferedWriter(new FileWriter(model_path+\"model_weights.txt\"))\n\t\tmodel_weights.write(model.weights.toString)\n\t\tmodel_weights.flush()\n\t\tmodel_weights.close()\n\n\t\tprintln(s\"模型信息写入文件完成，路径为：$model_path\")\n\t}\n\n```\n加载模型\n```\n\tdef loadModel(model_path: String): Option[LogisticRegressionModel] = {\n\t\ttry{\n\t\t\tval in = new ObjectInputStream( new FileInputStream(model_path) )\n\t\t\tval model = Option( in.readObject().asInstanceOf[LogisticRegressionModel] )\n\t\t\tin.close()\n\t\t\tprintln(\"Model Load Success\")\n\t\t\tmodel\n\t\t}\n\t\tcatch {\n\t\t\tcase ex: ClassNotFoundException => {\n\t\t\t\tprintln(ex.printStackTrace())\n\t\t\t\tNone\n\t\t\t}\n\t\t\tcase ex: IOException => {\n\t\t\t\tprintln(ex.printStackTrace())\n\t\t\t\tprintln(ex)\n\t\t\t\tNone\n\t\t\t}\n\t\t\tcase _: Throwable => throw new Exception\n\t\t}\n\t}\n\n```\n\n使用加载的模型进行分值计算\n```\n\t// 加载obj文件进行预测\n\tval model_new = loadModel(s\"$model_path/model.obj\")\n\t// 使用加载的模型进行样例预测\n\tval result_new = test.map(line =>{\n\t\tval pre_label = model_new.get.predict(line.features)\n\t\t// blas.ddot(x.length, x,1,y,1) (向量x的长度，向量x，向量x的索引递增间隔，向量y，向量y的索引递增间隔)\n\t\tval pre_score = blas.ddot(model.numFeatures, line.features.toArray, 1, model.weights.toArray, 1)\n\t\tval score = Math.pow(1+Math.pow(Math.E, -2 * pre_score), -1)\n\t\t(line.label, pre_label,score)\n\t} )\n\tresult_new.take(2).foreach(println)\n```\n\n# ml中的二分类LR\n\nml包中的LR既可以用来做二分类，也可以用来做多分类。\n- 二分类对应：Binomial logistic regression\n- 多分类对应：multinomial logistic regression\n\n其中二分类可以通过Binomial logistic regression 和 multinomial logistic regression实现。\n\n基于Binomial logistic regression的LR实现：\n```\ndef BinaryModel(train: Dataset[Row], model_path: String, spark: SparkSession) = {\n\t// 创建模型\n\tval LRModel = new LogisticRegression()\n\t\t.setMaxIter(20)\n\t\t.setRegParam(0.3)\n\t\t.setElasticNetParam(0.8)\n\t// 训练评估模型\n\tval model = LRModel.fit(train)\n\tevalute(model, train, spark)\n}\n\ndef evalute(model: LogisticRegressionModel, train: Dataset[Row], spark: SparkSession):Unit = {\n\t\t// 打印模型参数\n\t\tprintln(s\"模型参数信息如下：\\n ${model.parent.explainParams()} \\n\")\n\t\tprintln(s\"Coefficients（系数）: ${model.coefficients}\")\n\t\tprintln(s\"Intercept（截距）: ${model.intercept}\")\n\t\t// 查看训练集的预测结果 rawPrediction：row 计算的分值，probability：经过sigmoid转换后的概率\n\t\tval result = model.evaluate(train)\n\t\tresult.predictions.show(10)\n\t\t// 将 label，0 值概率，predict label提取出来\n\t\tresult.predictions.select(\"label\",\"probability\",\"prediction\").rdd\n\t\t\t.map(row => (row.getDouble(0),row.get(1).asInstanceOf[DenseVector].toArray(0),row.getDouble(2)))\n\t\t\t.take(10).foreach(println)\n\t\t// 模型评估\n\t\tval trainSummary = model.summary\n\t\tval objectiveHistory = trainSummary.objectiveHistory\n\t\tprintln(\"objectiveHistoryLoss:\")\n\t\tobjectiveHistory.foreach(loss => println(loss))\n\n\t\tval binarySummary = trainSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n\n\t\tval roc = binarySummary.roc\n\t\troc.show()\n\t\tprintln(s\"areaUnderROC: ${binarySummary.areaUnderROC}\")\n\n\t\t// Set the model threshold to maximize F-Measure\n\t\tval fMeasure = binarySummary.fMeasureByThreshold\n\t\tfMeasure.show(10)\n\t\tval maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n\t\timport spark.implicits ._\n\t\tval bestThreshold = fMeasure.where($\"F-Measure\"===maxFMeasure).select(\"threshold\").head().getDouble(0)\n\t\tmodel.setThreshold(bestThreshold)\n\t}\n```\n\n基于Multimial logistic regression的LR实现：\n```\ndef BinaryModelWithMulti(train: Dataset[Row], model_path: String, spark: SparkSession) = {\n\t// 创建模型\n\tval LRModel = new LogisticRegression()\n    \t.setMaxIter(10)\n    \t.setRegParam(0.3)\n    \t.setElasticNetParam(0.8)\n    \t.setFamily(\"multinomial\")\n\t// 训练模型\n\tval model = LRModel.fit(train)\n\t// 打印模型参数\n\tprintln(s\"模型参数信息如下：\\n ${model.parent.explainParams()} \\n\")\n\tprintln(s\"Coefficients（系数）: ${model.coefficientMatrix}\")\n\tprintln(s\"Intercept（截距）: ${model.interceptVector}\")\n}\n```\n\n# ml中的多分类LR\n\n某条样本属于类别k的概率计算为：\n$$\nP(Y=k | X,\\beta_k,\\beta_{0k} ) = \\frac { e^{ \\beta _k \\cdot X + \\beta_{0k} }}\n{ \\sum_{k^j=0}^{K-1} e^{ \\beta _{k^j} \\cdot X + \\beta_{0k^j}} } \n$$\n\n其中K表示类别，J表示特征个数\n\n权重最小化使用的是最大似然函数，其更新公式如下：\n$$\n\\underset{ \\beta , \\beta_0 }{min} - [\\sum_{i=1}^{L}w_i \\cdot logP(Y=y_i|X_i)  ] + \\lambda [ \\frac{1}{2}(1-\\alpha )||\\beta||_2^2 + \\alpha ||\\beta||_1]\n$$\n\n使用的数据集形式为：\n```\n1 1:-0.222222 2:0.5 3:-0.762712 4:-0.833333\n1 1:-0.555556 2:0.25 3:-0.864407 4:-0.916667\n1 1:-0.722222 2:-0.166667 3:-0.864407 4:-0.833333\n1 1:-0.722222 2:0.166667 3:-0.694915 4:-0.916667\n0 1:0.166667 2:-0.416667 3:0.457627 4:0.5\n1 1:-0.833333 3:-0.864407 4:-0.916667\n2 1:-1.32455e-07 2:-0.166667 3:0.220339 4:0.0833333\n2 1:-1.32455e-07 2:-0.333333 3:0.0169491 4:-4.03573e-08\n```\n\n多分类LR模型实现为：\n```\ndef MultiModel(file_multi: String, spark: SparkSession, model_path: String): Unit = {\n\tval training = spark.read.format(\"libsvm\").load(file_multi)\n\tval lr = new LogisticRegression()\n\t\t.setMaxIter(10)\n\t\t.setRegParam(0.3)\n\t\t.setElasticNetParam(0.8)\n\n\t// Fit the model\n\tval lrModel = lr.fit(training)\n\n\t// Print the coefficients and intercept for multinomial logistic regression\n\tprintln(s\"Coefficients: \\n${lrModel.coefficientMatrix}\")\n\tprintln(s\"Intercepts: ${lrModel.interceptVector}\")\n}\n```\n\n----------------\n\n# 参考资料\n\nhttps://spark.apache.org/docs/2.1.0/mllib-linear-methods.html#classification\n\nhttps://spark.apache.org/docs/2.1.0/ml-classification-regression.html#logistic-regression\n\nhttps://blog.csdn.net/pupilxmk/article/details/80735599\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"常见的五种神经网络(5)-生成对抗网络（下）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(5)-生成对抗网络（下）篇/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/103231385\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n> 在上一篇文章中介绍了[生成模型的基本结构、功能和变分自动编码器](https://blog.csdn.net/Gamer_gyt/article/details/103754752)，在本篇文章中主要介绍一下生成对抗网络（Generative Adversaarial Networks，GAN）\n# KL散度、JS散度、Wassertein距离\n## KL散度\nKL散度又称相对熵，信息散度，信息增益。KL散度是两个概率分布P和Q差别的非对称性的度量。在经典境况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布。\n\n$$\nD_{KL}(P \\parallel Q)= \\sum_{i=1}^{n}P_i log(\\frac{P_i}{Q_i})\n$$\n\n## JS散度\nJS散度是度量两个概率分布的相似度，是基于KL散度的变体，解决了KL散度非对称的问题。\n$$\nD_{JS}(P \\parallel Q)=\\frac{1}{2} D_{KL}(P \\parallel \\frac{P+Q}{2}) + \\frac{1}{2} D_{KL}(Q \\parallel \\frac{P+Q}{2})\n$$\nKL散度和JS散度度量的时候都有一个问题：如果两个分布P,Q距离较远，完全没有重叠的时候，KL散度是没有意义的，在学习的时候，这就意味着在这一点的梯度为0，即梯度消失了。\n\n## Wassertrin距离\nWasserstein距离度量的是两个管理分布之间的距离。\n$$\nW(P,Q) = \\underset{\\gamma  \\sim \\prod (P, Q) }{inf} E_{(x,y) \\sim \\gamma} \\left [ ||x-y|| \\right ]\n$$\n其中$\\prod (P, Q)$是边际分布为$P$和$Q$的所有可能的联合分布集合。\n\n\n# 显式和隐式密度模型\n\n在上一篇文章中介绍的变分自动编码器，之前介绍的深度信念网络都是显式的构建样本的密度函数$p(x|\\theta)$，并通过最大似然估计来求解参数，称之为**显式密度模型（Explicit Density Model）**。\n\n如果只是希望有一个模型能生成符合数据分布$p_r(x)$的样本，那么可以不显式地估计出数据分布的密度函数。假设在低维空间$Z$中有一个简单容易采样的分布$p(z)$，$p(z)$通常为标准多元正态分布$N(0,1)$。我们使用神经网络构建一个映射函数$G: Z \\rightarrow X$称为生成网络。利用神经网络强大的拟合能力，使得$G(z)$服从数据分布$p_r(x)$。这种模型就称为**隐式密度模型（Implicit Density Model）**。所谓隐式模型就是指并不显示地建模$p_r(x)$，而是建模生成过程。\n\n# 网络分解与训练\n\n## 判别网络\n隐式密度模型的一个关键是如何确保生成网络产生的样本一定是服从真实的数据分布。既然我们不构建显式密度模型，就无法通过最大似然估计等方法来训练。\n\n生成对抗网络（Generative Adversarial Networks，GAN）是通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布。在生成对抗网络中，有两个网络进行对抗训练。一个是**判别网络**，目标是尽量准确地判断一个样本是来自于真实数据还是生成网络产生的；另一个是**生成网络**，目标是尽量生成判别网络无法区分来源的样本。这两个目标相反的网络不断地进行交替训练。当最后收敛时，如果判别网络再也无法判断出一个样本的来源，那么也就等价于生成网络可以生成符合真实数据分布的样本。生成对抗网络的流程图如下图所示：\n\n![生成对抗网络的流程图](https://img-blog.csdnimg.cn/20191230233737148.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n判别网络（Discriminator Network）$D(x,\\phi)$ 的目标是区分出一个样本$x$是来自于真实分布$p_r(x)$还是来自于生成模型$p_{\\theta}(x)$，因此判别网络实际上一个两类分类器。用标签$y=1$来表示样本来自于真实分布，$y=0$表示样本来自模型，判别网络的$D(x,\\phi)$的输出为$x$属于真实数据分布的概率，即：\n$$\np(y=1|x)=D(x, \\phi)\n$$\n则样本来自模型生成的概率位$p(y=0|x)=1-D(x, \\phi)$\n\n给定一个样本$(x,y),y=\\{1,0\\}$表示其是来自于$p_r(x)$还是$p_{\\theta}(x)$，判别网络的目标函数为最小化交叉熵，即最大化对数函数（公式1-1）。\n$$\n\\underset{\\phi}{min} = \\left ( E_x[y log \\,\\, p(y=1 | x) + (1-y) log \\,\\, p(y=0|x) ]  \\right )\n\\\\\n= \\underset{\\phi}{max}  \\left (   E_{x\\sim p_r(x)} [log \\,\\, D(x, \\phi)] + E_{z\\sim p(z)}[ log (1-D(G(z,\\theta), \\phi))]   \\right )\n$$\n\n其中$\\theta, \\phi$分别是生成网络和判别网络的参数。\n## 生成网络\n**生成网络（Generator Network）** 的目标刚好和判别网络相反，即让判别网络将自己生成的样本判别为真实样本。其目标函数如下（公式1-2）：\n$$\n \\underset{\\theta}{max} \\left (   E_{z\\sim p(z)} [log \\,\\, D(G(z , \\theta), \\phi)]   \\right )\n\\\\\n=  \\underset{\\theta}{min} \\left (   E_{z\\sim p(z)} [log \\,\\, (1-D(G(z , \\theta), \\phi))]   \\right )\n$$\n上面的这两个目标函数是等价的。但是在实际训练时，一般使用前者，因为其梯度性质更好。我们知道，函数$log(x), x\\in (0,1)$在$x$接近1时的梯度要比接近0时的梯度小很多，接近饱和区间。这样，当判别网络$D$以很高的概率认为生成网络$G$产生的样本是“假”样本，即$(1-D(G(z, \\theta), \\phi)) \\rightarrow 1$。这时目标函数关于$\\theta$的梯度反而很小，从而不利于优化。\n\n## 网络训练\n在生成对抗网络的训练过程中，需要平衡两个网络的能力。对于判别网络来说，一开始的判别能力不能太强，否则难以提升生成网络的能力。然后也不能太弱，否则针对他训练的生产网络也不会太好。在训练时需要使用一些技巧，使得在每次迭代中，判别网络臂生成网络的能力强一些，但又不能强太多。\n\n生成对抗网络的训练流程如下所示，每次迭代时，判别网络更新$K$次而生成网络更新一次，即首先要保证判别网络足够强才能开始训练生成网络。在实践中$K$是一个超参数，其取值一般取决于具体任务。\n\n![生成对抗网络的训练流程](https://img-blog.csdnimg.cn/20191231090237628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n# DCGAN\n\n## DCGAN介绍\n生成对抗网络是指一类采用对抗训练方式来进行学习的深度生成模型，其包含的判别网络和生成网络都可以根据不同的生成任务使用不同的网络结构。\n\n在深度卷积生成对抗网络（Dee Convolutional Generative Adversarial Neteorks，DCGAN）中，判别网络是一个传统的深度卷积网络，但使用了带步长的卷积来实现下采样操作，不用最大汇聚（pooling）操作。生成网络使用一个特殊的深度卷积网络来实现，如下图所示，使用微步卷积来生成64x63大小的图像。\n\n![DCGAN中的生成网络](https://img-blog.csdnimg.cn/20191231091304687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n上图中，第一层是全连接层，输入是从均匀分布中随机采样的100维向量$z$，输出是4x4x1024的向量，重塑为4x4x1024的张量，然后是四层的微步卷积，没有汇聚层。\n\nDCGAN的主要优点是通过一些经验性的网络结构设计使得对抗训练更加稳定。比如：\n- （1）使用代步长的卷积（在判别网络中）和微步卷积（在生成网络中）来代替汇聚操作，以免损失信息\n- （2）使用批量归一化\n- （3）去除卷积层之后的全连接层\n- （4）在生成网络中，除了最后一层使用Tanh激活函数外，其余层都使用ReLU函数\n- （5）在判别网络中，都适用LeakyReLU激活函数\n\n## 模型分析\n将判别网络和生成网络合并，整个生成对抗网络得整个目标函数看作最小最大化游戏（Minimax Game），表达式如下（1-3）：\n$$\n\\underset{\\theta}{min} \\, \\underset{\\phi}{max}\\left (  E_{x \\sim p_r{(x)}} \\left [  log\\,\\, D(x, \\phi) \\right ] + E_{x \\sim p_{\\theta}(x)} \\left [  log\\,\\,(1- D(x, \\phi)) \\right ]  \\right )\n$$\n因为之前提到的生成网络梯度问题，这个最小化最大化形式的目标函数一般用来进行理论分析，并不是实际训练时的目标函数。\n\n假设$p_r(x)$和$p_{\\theta}(x)$已知，则最优得判别器为：\n$$\nD^*(x) =  \\frac{p_r(x)}{p_r(x) + p_{\\theta}(x)}\n$$\n将最优得判别器$D^*(x)$代入公式1-3，则目标函数变为（公式1-4）：\n$$\nL(G|D^*) =  E_{x \\sim p_r{(x)}} \\left [  log\\,\\, D^*(x) \\right ] + E_{x \\sim p_r{(\\theta})} \\left [  log\\,\\,(1- D^*(x)) \\right ] \n\\\\\n= 2D_{JS}(p_r||p_{\\theta}) - 2log2\n$$\n其中$D_{JS}$为JS散度。\n\n\n在生成对抗网络中，当判别网络为最优时，生成网络的优化目标是最小化真实分布$p_r$和模型分布$p_{\\theta}$之间得JS散度。当两个分布相同时，JS散度为0，最优生成网络$G^*$对应得损失为$L(G^*|D^*)=-2log2$。\n\n然而JS散度的一个问题是：当两个分布没有重叠时，他们之间得JS散度恒等于常数log2。对生成网络来说，目标函数关于参数的梯度为0。\n$$\n\\frac{ \\partial L(G|D^*) }{ \\partial \\theta} =0\n$$\n下图给出了生成对抗网络中的梯度消失问题的示例。当真实分布$p_r$和模型分布$p_{\\theta}$没有重叠，最优的判断网对对所有生成数据得输出都为0，$D^*(G(z, \\theta))=0, \\forall  z$。因此生成网络得梯度消失。\n\n![生成网络中得梯度消失问题](https://img-blog.csdnimg.cn/20191231134841397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n在实际训练生成对抗网络时，我们一般不会将判别网络训练到最优，只进行一步或多步梯度下降，使得生成网络的梯度依然存在。然而，判别网络也不能太差，否则生成网络的梯度为错误的梯度。如何使得判别网络在梯度消失和梯度错误之间取得平衡并不是一件容易的事。\n\n## 模型坍塌\n\n如果使用公式1-2作为生成网络的目标函数，将最优判断网络$D^*$代入，得到：\n$$\nL'(G|D^*) = E_{x \\sim p_{\\theta}(x)} \\left [ log \\, D^*(x) \\right ]\n\\\\\n= E_{x \\sim p_{\\theta}(x)} \\left [ log \\, \\frac {p_r(x)}{p_r(x) + p_{\\theta}(x) } \\cdot  \\frac{p_{\\theta}(x) }{p_{\\theta}(x) } \\right ]\n\\\\\n= - E_{x \\sim p_{\\theta}(x)}\\left [ log \\, \\frac{p_{\\theta}(x)}{p_r(x)} \\right ] + E_{x \\sim p_{\\theta}(x)} \\left [  log \\, \\frac {p_r(x)}{p_r(x) + p_{\\theta}(x) }  \\right ]\n\\\\\n= -D_{KL}(p_{\\theta} || p_r) +  E_{x \\sim p_{\\theta}(x)} \\left [ log (1-D^*(x)) \\right ]\n\\\\ \n = -D_{KL}(p_{\\theta} || p_r) +  2D_{JS}(p_r || p_{\\theta})-2log2 - E_{x \\sim p_r(x)} \\left [ log \\, D^* (x) \\right ]\n$$\n\n其中后两项和生成网络无关，因此：\n$$\n\\underset{ \\theta }{ max } L'(G | D^*) =\\underset{ \\theta }{ min } D_{KL} (p_{\\theta} || p_r) - 2D_{JS} (p_r || p_{\\theta} )\n$$\n其中JS散度和$D_{JS}(p_{\\theta} || p_r) \\in [0, log 2]$为有界函数，因此生成网络的目标是为更多的受逆向KL散度$D_{KL}(p_{\\theta} || p_r)$影响，使得生成网络更倾向于生成一些更安全的样本，从而造成**模型坍塌（Model Collapse）**问题\n\n## 前向和逆向KL散度\nKL散度是一种非对称的散度，在计算真实分布$p_r$和模型分布$p_{\\theta}$之间得KL散度时，按照顺序不同，有两种KL散度：前向KL散度（Forward KL divergence）$D_{KL}(p_r || p_{\\theta})$ 和逆向KL散度（Reverse KL divergence）$D_{KL}(p_{\\theta} || p_r)$\n\n前向和逆向KL散度分别定义为：\n$$\nD_{KL}(p_r || p_{\\theta}) = \\int p_r(x) log\\, \\frac{p_r(x)}{p_{\\theta}(x)}dx\n\\\\\nD_{KL}(p_{\\theta} || p_r) = \\int p_{\\theta}(x) log\\, \\frac{p_{\\theta}(x)}{p_r(x)}dx\n$$\n在前向KL散度中：\n- （1）当$p_r(x) \\rightarrow 0$而 $p_{\\theta}(x)> 0$时，$p_r(x) log\\, \\frac{p_r(x)}{p_{\\theta}(x)} \\rightarrow  0$。不管$p_{\\theta}(x)$如何取值，都对前向KL散度的计算没有贡献。\n- （2）当$p_r(x) > 0$而 $p_{\\theta}(x) \\rightarrow 0$时，$p_r(x) log\\, \\frac{p_r(x)}{p_{\\theta}(x)} \\rightarrow  \\infty$。前向KL散度会变得非常大。\n\n因此，前向KL散度会鼓励模型分布$p_{\\theta}(x)$尽可能的覆盖所有真实分布$p_r(x)>0$的点，而不用回避$p_r(x)\\approx 0$的点。\n\n在逆向KL散度中：\n- （1）当$p_r(x) \\rightarrow 0$而 $p_{\\theta}(x)> 0$时，$p_{\\theta}(x) log\\, \\frac{p_{\\theta}(x)}{p_r(x)} \\rightarrow  \\infty$。即当$p_{\\theta}(x)$接近于0，而$p_{\\theta}(x)$有一定的密度时，逆向KL散度会变得非常大。\n- （2）当$p_{\\theta}(x) \\rightarrow 0$, 不管 $p_r(x)$如何取值，$p_{\\theta}(x) log\\, \\frac{p_{\\theta}(x)}{p_r(x)} \\rightarrow 0$。\n\n因此逆向KL散度会鼓励模型分布$p_{\\theta}(x)$尽可能避开所有真实分布$p_r(x)\\approx 0$的点，而不需要考虑是否覆盖所有分布为$p_r(x) > 0$的点。\n\n下图给出数据真实分布为一个高斯混合分布，模型分布为一个旦高斯分布时，使用前向和逆向KL散度来进行模型优化的示例，蓝色曲线为真实分布$p_r$的等高线，红色曲线为模型分布$p_{\\theta}$的等高线。\n\n![前向和逆向KL散度](https://img-blog.csdnimg.cn/2019123114461746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n## W-GAN\nW-GAN是一种通过使用Wassertein距离替代JS散度来优化训练的生成对抗网络。对于真实分布$p_r$和模型分布$p_{\\theta}$，他们的1st-wassertein距离为：\n$$\nW^1(p_r,p_{\\theta}) = \\underset{\\gamma  \\sim \\prod (P_r, P_g) }{inf} E_{(x,y) \\sim \\gamma} \\left [ ||x-y|| \\right ]\n$$\n其中$\\prod (P_r, P_g)$是边际分布为$p_r$和$p_{\\theta}$的所有可能的联合分布集合。\n\n当两个分布没有重叠或者重叠非常少时，他们之间的KL散度为$+ \\infty$，JS散度为log2，并不随着两个分布之间的距离而变化。而1st-wassertein距离可以依然衡量两个没有重叠分布的距离。\n\nW-GAN的目标函数为：\n$$\n\\underset{\\theta}{ max } E_{z \\sim p(z)} \\left [ f(G(z, \\theta), \\phi) \\right ]\n$$\n因为$f(x, \\phi)$为不饱和函数，所以生成网络参数$\\theta$的梯度不会消失，理论上解决了原始GAN训练不稳定的问题。并且W-GAN中生成网络的目标函数不再是两个分布的比率，在一定程度上缓解了模型坍塌问题，使得生成的样本具有多样性。\n\n下图给出了W-GAN的训练过程，和原始GAN相比，W-GAN的评价网络最后一层不使用sigmoid函数，损失函数不取对数。\n\n![W-GAN的训练过程](https://img-blog.csdnimg.cn/20191231151625579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n# 总结\n深度生成模型是一种有机地融合神经网络和概率图模型的生成模型,将神经网络作为一个概率分布的逼近器,可以拟合非常复杂的数据分布。\n\n变分自编码器是一个有意义的深度生成模型,可以有效地解决含隐变量的概率模型中后验分布难以估计的问题。\n\n生成对抗网络是一个具有开创意义的深度生成模型,突破了以往的概率模型必须通过最大似然估计来学习参数的限制。DC-GAN是一个生成对抗网络的成功实现,可以生成十分逼真的自然图像。对抗生成网络的训练不稳定问题的一种有效解决方法是W-GAN,通过用 Wassertein 距离替代 JS 散度来进行训练。\n\n虽然深度生成模型取得巨大的成功,但是作为一种无监督模型,其主要的缺点是缺乏有效的客观评价,因此不同模型之间的比较很难客观衡量。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(5)-生成对抗网络（上）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(5)-生成对抗网络（上）篇/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/103231385\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n概率生成模型简称生成模型（Generative Model），是概率统计和机器学习中的一类重要模型，指一系列用于随机生成可观测数据的模型。生成模型的思路是根据可观测的样本学习一个参数化的模型$p_{\\theta}(x)$来近似未知分布$p_r(x)$，使得生成的样本和真实的样本尽可能的相似。\n\n深度生成模型就是利用深层神经网络可以近似任意函数的能力来建模一个复杂的分布$p_r(x)$。常见的两种深度生成模型包括**变分自动编码器（Variational Autoencoder, VAE）**和**生成对抗网络（Generative Adversarial Networks, GAN）**（因为他们的目标基本是一致的）。本篇内容主要介绍生成模型的基本功能和变分自动编码器。\n\n# 生成模型的基本功能\n生成模型的两个基本功能为：\n- 密度估计\n- 生成样本\n\n## 密度估计\n给定一组数据$D=\\{x^i\\}, 1 \\leq i \\leq N$，假设他们都是独立地从相同的概率密度函数为$p_r(x)$的未知分布中产生的，密度估计是根据数据集$D$来估计其概率密度函数$p_{\\theta}(x)$。\n\n生成模型也可以应用于监督学习，监督学习的目标是建模输出标签的条件概率密度函数$p(y|x)$。根据贝叶斯公式：\n$$\np(y|x) = \\frac{p(x|y)}{ \\sum_{y}p(x,y)}\n$$\n可以将监督学习问题转化为联合概率密度函数p(x,y)的密度估计问题。在监督学习中比较典型的生成模型有**朴素贝叶斯分类器**、**隐马尔可夫模型**\n\n和生成模型相对应的另一类监督学习模型是判别模型（Discriminative Model），判别式模型直接建模条件概率密度函数$p(y|x)$，并不建模其联合概率密度函数$p(x,y)$，常见的判别模型有logistic 回归、支持向量机、神经网络等。由生成模型可以得到判别模型，但由判别模型得不到生成模型。\n\n## 生成样本\n生成样本就是给定一个概率密度函数为$p_{\\theta}(x)$的分布，生成一些服从这个样本的分布，也称为采样。\n\n对于带隐变量的生成模型，在得到$p(z,\\theta)$和$p(x|z,\\theta)$之后，就可以生成数据$x$，具体可以分为两步执行：\n- 根据隐变量的先验分布$p(z,\\theta)$进行采样，得到样本$z$\n- 根据条件分布$p(x|z,\\theta)$进行采样，得到样本$x$\n\n# 变分自动编码器\n\n> 以下内容来自于：https://spaces.ac.cn/archives/5253 \n\n## 从分布变换看VAE\nVAE和GAN的目标都是：希望构建一个从隐变量$Z$生成目标数据$X$的模型，但是实现上有所不同。它们假设$Z$服从某些常见的数据分布（正态分布，均匀分布等），然后希望训练得到一个模型$X=g(Z)$，这个模型能够将原来的概率分布映射到训练集的概率分布，即，他们的目的都是进行分布之间的变换。\n![分布变换](https://img-blog.csdnimg.cn/20191229104302607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n生成模型的难题就是判断生成分布与真实分布的相似度，因为我们只知道两者的采样结果，不知道它们的分布表达式。\n\n\n\n\n## VAE的通常理解\n首先我们有一批样本$\\{X_1,X_2,...,X_n\\}$，其整体使用$X$描述，我们本想根据$\\{X_1,...,x_n\\}$得到$X$的分布$P(X)$，如果能得到的话，我们直接根据$P(X)$来采样，就可以得到所有可能的$X$了。这是一个理想的终极模型，但很难实现，于是将分布改为：\n$$\np(X)=\\sum_{Z}p(X|Z)p(Z)\n$$\n这里不区分求和还是求积分，意思差不多就行。此时$p(X|Z)$就描述了一个由$Z$来生成$X$的模型，而我们假设$Z$服从标准的正态分布，即：$p(Z)=N(0,1)$，如果这个理想能实现，我们就可以先从标准的正态分布中采样一个$Z$，然后根据$Z$来计算$X$，这也是一个很棒的生成模型。接下来就是结合自编码器来实现重构，保证有效信息没有丢失，再加上一系列的推导，最后把模型实现，框架结构如下所示：\n\n![VAE的通常理解](https://img-blog.csdnimg.cn/20191229161853604.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n但是上图存在一个问题：**我们不清楚经过重新采样生成的$Z_k$是否还对应着原来的$X_k$，所以直接最小化$D(\\hat{X_k},X_k)^2$是很不科学的。而实际上，你看代码，也会发现根本不是这样实现的。**\n\n## VAE应该是什么样的\n\n其实在整个VAE模型中，并没有使用$p(Z)$（隐变量空间的分布）是正态分布的假设，我们用的是假设$p(X|Z)$（后验分布）是正态分布。\n\n具体来说，给定一个真实样本$X_k$，我们假设存在一个专属于$X_k$的分布$p(Z|X_k)$（学名叫后验分布），并进一步假设这个分布是（独立的、多元的）正态分布，这里强调专属因为：后面要训练一个生成器$X=g(Z)$，希望能够把从分布$p(Z|X_k)$采样出来的$Z_k$还原为$X_K$。如果假设$p(Z)$是正态分布，然后从$p(Z)$中采样一个$Z$，那么我们怎么知道这个$Z$是对应哪个真实的$X$呢？**现在$p(Z|X_k)$专属于$X_k$，我们有理由说从这个分布采样出来的$Z$应该要还原到$X_k$中去。**\n\n事实上，在论文《Auto-Encoding Variational Bayes》的应用部分，也特别强调了这一点。\n\n---\nIn this case, we can let the\nvariational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:\n$$\n\\log q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}^{(i)}) = \\log \\mathcal{N}(\\boldsymbol{z} ;\\boldsymbol{\\mu}^{(i)},\\boldsymbol{\\sigma}^{2(i)}\\boldsymbol{I})\\tag{9}\n$$\n（注：这里是直接摘录原论文，本文所用的符号跟原论文不尽一致，望读者不会混淆。）\n\n---\n\n上面介绍到了，每一个$X_k$都配上了一个专属的正态分布，才方便后续的生成器做还原，但这样有多少个$X$就有多少个正态分布了。我们知道正态分布分布有两组参数$\\mu$和$\\sigma^2$，那怎么找出专属$X_k$的正态分布$P(Z|X_k)$的均值和方差呢？并没有什么直接的思路，那么就用神经网络来拟合吧。\n\n于是我们就可以构建两个神经网络$\\mu_k=f_1(X_k), log\\sigma^2 = f_2(X_k)$来计算均值和方差了。这里之所以选择$log \\sigma^2$是因为$\\sigma^2$总是非负的，需要加激活函数处理，而拟合$log\\sigma^2$不需要加激活函数处理，因为可正可负。到这里，能知道专属于$X_k$的均值和方差了，也知道他的正态分布长什么样子了，然后从一个专属分布中采样一个$Z_k$出来，然后经过一个生成器得到$\\hat{X_k}=g(Z_k)$，现在可以放心的最小化$D(\\hat{X_k},X_k)^2$，因为$Z_k$是从专属的$X_k$的分布中采样出来的，这个生成器要把开始的$X_k$还原回来，于是可以画出VAE的示意图为：\n![VAE的示意图](https://img-blog.csdnimg.cn/20191229181437675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n## 分布标准化\n首先，我们希望重构$X$，也就是最小化$D(\\hat{X_k},X_k)^2$，但这个过程容易收到噪声的影响，因为$Z_k$是经过重新采样生成的，不是直接由encoder算出来的。显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构的更好，肯定会想尽办法让方差变为0，而方差为0的话，也就没有随机性了，所以不管怎么样采样都只是得到确定的结果（也就是均值），只拟合一个当然比拟合多个要容易，而均值是通过另外一个神经网络算出来的。\n\n说白了，模型会慢慢退化成普通的AutoEncoder，噪声不再起作用。那这样的话，就不是生成模型。\n\n其实VAE还让所有的$p(Z|x)$都向标准正态分布看齐，这样就防止了噪声为零，同时保证了模型具有生成能力。如下图所示：\n\n![向标准正态分布看齐](https://img-blog.csdnimg.cn/20191229184136920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n那么如何让所有的$p(Z|X)$都向$N(0,1)$看齐呢？原论文直接算了一般（各分量独立的）正态分布与标准正态分布的KL散度$KL(N(\\mu, \\sigma^2)|| N(0,1))$作为这个额外的loss，计算结果为：\n$$\n\\mathcal{L}_{\\mu,\\sigma^2}=\\frac{1}{2} \\sum_{i=1}^d \\Big(\\mu_{(i)}^2 + \\sigma_{(i)}^2 - \\log \\sigma_{(i)}^2 - 1\\Big)\\tag{4}\n$$\n\n这里的$d$指的是隐变量$Z$的维度，而$\\mu_i$和$\\sigma^2_i$分别代表一般正态分布的均值向量和方差向量的第$i$个分量。直接用这个公式做补充loss，就不用考虑均值损失和方差损失的相对比例问题了。\n\n## VAE的本质\nVAE虽然也是AE（AutoEncoder）的一种，但他的做法是独特的，在VAE中，他的Encoder有两个，一个用来计算均值，一个用来计算方差。\n\nVAE从让普通人望而生畏的变分和贝叶斯理论出发，最后落地到一个具体的模型中，虽然走了比较长的一段路，但最终的模型其实是很接地气的：他本质上就是在我们常规的自编码器的基础上，对encoder的结果加上了“高斯噪声”，使得结果decoder能够对噪声有鲁棒性，而那个额外的KL loss事实上就是相当于encoder的一个正则项，希望encoder出来的东西均有零均值。\n\n那另外一个encoder的作用是用来动态调节噪声的强度的，直觉上想，当decoder还没有训练好的时候（重构误差远大于KL loss），就会适当降低噪声（KL loss增加），使得拟合起来更加容易一些（重构误差开始下降）；反之，如果decoder训练得还不错时（重构误差小于KL loss），这时候噪声就会增加（KL loss减少），使得拟合更加困难了（重构误差又开始增加），这时候decoder就要想办法提高他的生成能力了。\n\n![VAE本质](https://img-blog.csdnimg.cn/20191229191312712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n说白了，重构的过程是希望没噪声的，而KL loss则希望有高斯噪声的，两者是对立的，所以VAE跟GAN一样，内部其实时包含了一个对抗的过程，只不过他们两者是混合起来，共同进化的。\n\n## VAE中的变分\n\nVAE叫做“变分自编码器”，他跟变分法有什么联系？在VAE的论文和相关解读中，好像也没看到变分法的存在呀？\n\n如果大家承认KL散度的话，那VAE好像真的跟变分没多大关系了～因为理论上对于KL散度要证明：\n> 固定概率分布$p(x)（或q(x)）$的情况下，对于任意的概率分布$q(x)（或p(x)）$，都有$KL(p(x) || q(x)) \\geq 0$，而且只有当$p(x)=q(x)$时才等于零。\n\n因为$KL(p(x) || q(x))$实际上是一个泛函，要对泛函求极值就要用到变分法，当然，这里的变分法只是普通微积分的平行推广，还没涉及到真正复杂的变分法。而VAE的变分下界，是直接基于KL散度就得到的。所以直接承认了KL散度的话，就没变分什么事了。\n\n所有VAE名字中的变分，是因为他的推导过程用到了KL散度及其性质。\n\n\n> 至此已经把变分自编码的内容捋的差不多了，再下一篇将会更新GAN！\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(4)-深度信念网络（下）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(4)-深度信念网络（下）篇/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/103231385\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n在上一篇文章中介绍了玻尔兹曼机和受限玻尔兹曼机（[阅读详情](https://blog.csdn.net/Gamer_gyt/article/details/103231385)）,这篇文章中介绍一下深度信念网络。\n\n### 深度信念网络介绍\n深度信念网络（Deep Belief Netword, DBN）是一种深层的概率有向图模型，其图结构有多层的节点构成。每层节点的内部没有连接，相邻两层的节点之间为全连接。网络的最底层为可观测的变量，其他层节点都为隐变量。最顶部的两层间的连接是无向的，其他层之间的连接是有向的。下图给出了一个深度信念网络的示例。\n\n![4层结构的深度信念网络](https://img-blog.csdnimg.cn/20191207141248183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n对一个有$L$层隐变量的深度信念网络，令$v=h^{(0)}$表示最底层（第0层）为可观测变量，$h^{(1)}, ..., h^{(L)}$表示其余每层的变量。顶部的两层是一个无向图，可以看作是一个受限玻尔兹曼机，用来产生$p(h^{(L-1)})$的先验分布。除了最顶上两层外，每一层变量$h^{(l)}$依赖于其上面一层$h^{(l+1)}$，即：\n$$\np(h^{(l)} | h^{(l+1)}, ..., h^{(L)}) = p(h^{(l)} | h^{(l+1)})\n$$\n其中$l=\\{0,...,L-2\\}$\n\n深度信念网络中所有变量的联合概率可以分解为：\n$$\np(v,h^{(1)},...,h^{(L)}) = p(v|h^{(1)})( \\prod_{l=1}^{L-2} p(h^{(l)} | h^{(l+1)} ) )p( h^{(L-1)}, h^{(L)})\n\\\\\n= \\prod_{l=0}^{L-1} p(h^{(l)} | h^{(l+1)} ) p(h^{(L-1)},h^{(L)})\n$$\n\n其中$p(h^{l} | h^{(l+1)})$为sigmoid型条件概率分布为：\n$$\np(h^{(l)} | h^{(l+1)}) = \\sigma (a^{(l)}+W^{(l+1)} h^{(l+1)})\n$$\n其中$\\sigma(.)$ 为按位计算的logistic sigmoid函数，$a^{(l)}$为偏置参数，$W^{(l+1)}$为权重参数。这样，每一个层都可以看作是一个Sigmoid信念网络。\n\n### 生成模型\nDBN是一个生成模型，可以用来生成符合特定分布的样本。隐变量用来描述在可观测变量之间的高阶相关性。假如训练数据服从分布$p(v)$，通过训练得到一个深度信念网络。\n\n在生成样本时，首先在最顶两层进行足够多次的吉布斯采样，生成$h^{(L-1)}$，然后依次计算下一层隐变量的分布。因为在给定上一层变量取值时，下一层的变量是条件独立的，因为可以独立采样。这样我们就可以从第$L-1$层开始，自顶向下进行逐层采样，最终得到可观测层的样本。\n\n### 参数学习\n\nDBN最直接的训练方式可以通过最大似然方法使得可观测变量的边际分布$p(v)$在训练集合上的似然达到最大。但在深度信念网络中，隐变量$h$之间的关系十分复杂，由于“贡献度分配问题”，很难直接学习。即使对于简单的单层Sigmoid信念网络\n$$\np(v=1 | h) = \\sigma (b + w^Th)\n$$\n在已知可观察变量时，其隐变量的联合后验概率$p(h|v)$不再相互独立，因此很难精确估计所有隐变量的后验概率。早起深度信念网络的后验概率一般通过蒙特卡洛方法或变分方法来近似估计，但是效率比较低，而导致其参数学习比较困难。\n\n为了有效地训练深度信念网络，我们将每一层的Sigmoid信念网络转换为受限玻尔兹曼机。这样做的好处是隐变量的后验概率是相互独立的，从而可以很容易地进行采样。这样，深度信念网络可以看作是由多个受限玻尔兹曼机从上到下进行堆叠，第$l$层受限玻尔兹曼机的隐层作为第$l+1$层受限玻尔兹曼机可观测层。进一步地，深度信念网络可以采用逐层训练的方式来快速训练，即从最底层开始，每次只训练一层，直到最后一层。\n\n深度信念网络的训练过程可以分为预训练和精调两个阶段。先通过逐层预训练将模型的参数初始化为较优的值，再通过传统学习方法对参数进行精调。\n\n**逐层预训练**\n在预训练阶段，采用逐层训练的方式，将深度信念网络的训练简化为多个玻尔兹曼机的训练。下图给出了深度信念网络的逐层预训练过程。\n\n![深度信念网络的逐层预训练过程](https://img-blog.csdnimg.cn/20191207155415641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n具体的逐层训练过程为自下而上依次训练每一层的受限玻尔兹曼机。假设我们已经训练好了前$l-1$层的受限玻尔兹曼机，那么可以计算隐变量自下而上的条件概率\n$$\np(h^{(i)} | h^{(i-1)}) = \\sigma( b^{(i)} + W^{(i) h^{(i-1)}} ), 1 \\leq i \\leq (l-1)\n$$\n其中$b^{(i)}$为第$i$层受限玻尔兹曼机的偏置，$W^{(i)}$为连接权重。这样，可以按照 $v=h^{(0)} -> h^{(1)} -> ... -> h^{(l-1)}$的顺序生成一组$h^{(l-1)}$的样本，记为$\\hat{H} ^{(l-1)} = {\\hat{h}^{(l,1)}, ..., \\hat{h}^{(l,M)} }$。然后将$h^{(l-1)}$和$h^{(l)}$组成一个受限玻尔兹曼机，用$\\hat{H}^{(l-1)}$作为训练集充分训练第$l$层的受限玻尔兹曼机。\n\n下图的算法流程给出了一种深度信念网络的逐层预训练方法。大量的实践表明，逐层预训练可以产生非常好的参数初始值，从而极大的降低了模型的学习难度。\n\n![算法流程](https://img-blog.csdnimg.cn/20191207161617779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n**精调**\n\n经过预训练之后，再结合具体的任务（监督学习或无监督学习），通过传统的全局学习算法对网络进行精调（fin-tuning），使模型收敛到更好的局部最优点。\n\n作为生成模型的精调，除了顶层的受限玻尔兹曼机，其他层之间的权重被分成向上的认知权重（recognition weights）$W'$和向下的生成权重（generative weights）$W$。认知权重用来进行后验概率计算，而生成权重用来进行定义模型。认知权重的初始值$W'^{(l)}=W^{(l)T}$。\n\n深度信念网络一般采用contrastive wake-sleep算法进行精调，其算法过程是：\n- Wake阶段：认知过程，通过外界输入（可观测变量）和向上认知权重，计算每一层隐变量的后验概率并采样。然后，修改下行的生成权重使得下一层的变量的后验概率最大。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”\n- Sleep阶段：生成过程，通过顶层的采样和向下的生成权重，逐层计算每一层的后验概率并采样。然后，修改向上的认知权重使得上一层变量的后验概率最大。也就是“如果梦中的景象不是我闹中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”\n- 交替进行Wake和Sleep过程，直到收敛\n\n**作为深度神经网络的精调** 深度信念网络的一个应用是作为深度神经网络的预训练部分，提供神经网络的初始权重。\n\n在深度信念网络的最顶层再增加一层输出层，然后再使用反向传播算法对这些权重进行调优。特别是在训练数据比较少的时候，预训练的作用非常大。因为不恰当的初始化权重会显著影响最终模型的性能，而预训练获得的权重在权重空间中比随机权重更接近最优的权重，避免了反向传播算法因随机初始化权重参数而容易陷入局部最优和训练时间长的缺点。这不仅仅提升了模型的性能，也加快了调优阶段的收敛速度。\n\n下图给出了深度信念网络作为生成模型和判断模型的精调过程。\n\n![精调过程](https://img-blog.csdnimg.cn/20191207164923957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n### 总结\n\n玻尔兹曼机能够学习数据的内部表示，并且其参数学习的方式和赫布型学习十分类似。没有任何约束的玻尔兹曼机因为过于复杂，难以应用在实际问题上。通过引入一定的约束（即变为二分图），受限玻尔兹曼机在特征提取、协同过滤、分类等多个任务上取得了广泛的应用。\n\n和深度信念网络十分类似的一种深度概率模型是深度玻尔兹曼机（deep Boltzmann Machines ，DBM）。深度玻尔兹曼机是由多层的受限玻尔兹曼机堆叠而成，是真正的无向图模型，其联合概率是通过能量函数来定义。和深度信念网络相比，深度玻尔兹曼机的学习和推断要更加困难。\n\n尽管深度信念网络作为一种深度学习模型已经很少使用，但其在深度学习发展过程中的贡献十分巨大，并且其理论基础为概率图模型，有非常好的解释性，依然是一种值得深入研究的模型。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(4)-深度信念网络（上）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(4)-深度信念网络（上）篇/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/103231385\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n\n---\n\n\n### 引言\n\n常见的五种神经网络系列第三篇，主要介绍深度信念网络。内容分为上下两篇进行介绍，本文主要是深度信念网络（上）篇，主要介绍以下内容：\n\n- 背景\n- 玻尔兹曼机\n- 受限玻尔兹曼机\n\n### 背景\n对于一个复杂的数据分布，我们往往只能观测到有限的局部特征，并且这些特征通常会包含一定的噪声。如果要对这个数据分布进行建模，就需要挖掘出可观测变量之间复杂的依赖关系，以及可观测变量背后隐藏的内部表示。\n\n而深度信念网络可以有效的学习变量之间复杂的依赖关系。深度信念网络中包含很多层的隐变量，可以有效的学习数据的内部特征表示，也可以作为一种有效的非线性降维方法，这些学习到的内部特征表示包含了数据的更高级的、有价值的信息，因此十分有助于后续的分类和回归等任务。\n\n玻尔兹曼机是生成模型的一种基础模型，和深度信念网络的共同问题是**推断和学习**，因为这两种模型都比较复杂，并且都包含隐变量，他们的推断和学习一般通过MCMC方法来进行近似估计。这两种模型和神经网络有很强的对应关系，在一定程度上也称为随机神经网络（Stochastic Neural Network，SNN）。\n\n因为深度信念网络是有多层玻尔兹曼机组成的，所以本篇文章我们先来了解一下**玻尔兹曼机**和**受限玻尔兹曼机**。\n\n### 玻尔兹曼机\n#### 介绍\n玻尔兹曼机（Boltzmann Machine）可以看作是一种随机动力系统，每个变量的状态都以一定的概率受到其他变量的影响。玻尔兹曼机可以用概率无向图模型来描述，如下所示：\n\n![一个有六个变量的玻尔兹曼机](https://img-blog.csdnimg.cn/20191122090749423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\nBM的三个性质：\n\n- 二值输出：每个随机变量可以用一个二值的随机变量表示\n- 全连接：所有节点之间是全连接的\n- 权重对称：每两个变量之间的相互影响是对称的\n\nBM中的每个变量$X$的联合概率由玻尔兹曼分布得到，即：\n$$\np(x) = \\frac{1}{Z} exp(\\frac{-E(x)}{T})\n$$\n其中$Z$为配分函数，能量函数$E(X)$的定义为：\n$$\nE(x) \\overset{\\bigtriangleup }{=} E(X=x) = -(   \\sum_{i<j } w_{ij}x_ix_j + \\sum_{i} b_i x_i  ) \n$$\n\n其中$w_{ij}$是两个变量之间的连接权重，$x_i \\in \\{0,1\\}$表示状态，$b_i$是变量$x_i$的偏置。\n\n玻尔兹曼机可以用来解决两类问题，一是搜索问题，当给定变量之间的连接权重，需要找到一组二值变量，使得整个网络的能量最低。另一类是学习问题，当给定一部分变量的观测值时，计算一组最优的权重。\n\n#### 生成模型\n在玻尔兹曼机中配分函数$Z$通常难以计算，因此联合概率分布$p(x)$一般通过MCMC（马尔科夫链蒙特卡罗，Markov Chain Monte Carlo）方法来近似，生成一组服从$p(x)$分布的样本。这里介绍基于吉布斯采样的样本生成方法。\n\n**1. 全条件概率**\n吉布斯采样需要计算每个变量$X_i$的全条件概率$p(x_i|x_{\\setminus i})$，其中$x_{\\setminus i}$表示除了$X_i$外其它变量的取值。\n\n对于玻尔兹曼机中的一个变量$X_i$，当给定其他变量$x_{\\setminus i}$时，全条件概率公式$p(x_i|x_{\\setminus i})$为：\n$$\np(x_i=1|x_{\\setminus i}) = \\sigma( \\frac{ \\sum_{j} w_{ij}x_j +b_i }{T} )\n\\\\\np(x_i=0|x_{\\setminus i}) = 1- p(x_i=1|x_{\\setminus i})\n$$\n其中$\\sigma(.)$为sigmoid函数。\n\n\n**2. 吉布斯采样**\n\n玻尔兹曼机的吉布斯采样过程为：随机选择一个变量$X_i$，然后根据其全条件概率$p(x_i|x_{\\setminus i})$来设置其状态，即以$p(x_i=1|x_{\\setminus i})$的概率将变量$X_i$设为1，否则全为0。在固定温度$T$的情况下，在运动不够时间之后，玻尔兹曼机会达到热平衡。此时，任何全局状态的概率服从玻尔兹曼分布$p(x)$，只与系统的能量有关，与初始状态无关。\n\n要使得玻尔兹曼机达到热平衡，其收敛速度和温度$T$相关。当系统温度非常高$T \\rightarrow \\infty$时，$p(x_i|x_{\\setminus i}) \\rightarrow 0.5$，即每个变量状态的改变十分容易，每一种系统状态都是一样的，从而很快可以达到热平衡。当系统温度非常低$T \\rightarrow 0$时，如果$\\Delta E_i(x_{ \\setminus i}) > 0$，则$p(x_i|x_{\\setminus i}) \\rightarrow 1$，如果$\\Delta E_i(x_{ \\setminus i}) < 0$，则$p(x_i|x_{\\setminus i}) \\rightarrow 0$，即：\n$$\nx_i = \\left\\{\\begin{matrix}\n1 &  if \\sum_{j}w_{ij}x_j + b_i  \\geq 0\\\\ \n0 & otherwise\n\\end{matrix}\\right.\n$$\n\n因此，当$ \\rightarrow 0$时，随机性方法变成了确定性方法，这时，玻尔兹曼机退化为一个Hopfield网络。Hopfield网络是一种确定性的动力系统，每次的状态更新都会使系统的能量降低；而玻尔兹曼机时一种随机性动力系统，每次的状态更新则以一定的概率使得系统的能力上升。\n\n**3. 能量最小化与模拟退火**\n\n要使得动力系统达到热平衡，温度$T$的选择十分关键。一个比较好的折中方法是让系统刚开始在一个比较高的温度下运行达到热平衡，然后逐渐降低直到系统在一个比较低的温度下达到热平衡。这样我们就能够得到一个能量全局最小的分布。这个过程被称为模拟退火（Simulated Annealing）。\n\n模拟退火是一种寻找全局最优的近似方法。\n\n\n### 受限玻尔兹曼机\n#### 介绍\n全连接的玻尔兹曼机十分复杂，虽然基于采样的方法提高了学习效率，但在权重更新的过程中仍十分低效。在实际应用中，使用比较广泛的一种带限制的版本，即受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）是一个二分图结构的无向图模型，如下所示。\n![一个有7个变量的受限玻尔兹曼机](https://img-blog.csdnimg.cn/20191122145237428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n首先玻尔兹曼机中的变量也分为隐藏变量和可观测变量。分别用可观测层和隐藏层来表示这两组变量。同一层中的节点之间没有连接，而不同层一个层中的节点与另一层中的所有节点连接，这和两层的全连接神经网络结构相同。\n\n一个受限玻尔兹曼机由$m_1$个可观测变量和$m_2$个隐变量组成，其定义如下：\n\n- 可观测的随机向量$v=[v_1, ..., v_{m_1}]^T$\n- 隐藏的随机向量 $h=[h_1, ... , h_{m_2}]^T$\n- 权重矩阵$W \\in R^{m_1 * m_2}$，其中每个元素$w_{ij}$为可观测变量$v_i$和隐变量$h_j$之间边的权重\n- 偏置$a \\in R^{m_1}$和$b \\in R^{m_2}$，其中$a_i$为每个可观测变量$v_i$得偏置，$b_j$为每个隐变量$h_j$得偏置\n\n受限玻尔兹曼机得能量函数定义为：\n$$\nE(v,h) = - \\sum_{i} a_iv_i - \\sum_{j}b_j h_j - \\sum_{i}\\sum_{j}v_i w_{ij}h_j = -a^Tv -b^Th - v^T W h\n$$\n受限玻尔兹曼机得联合概率分布为$p(v,h)$定义为：\n$$\np(v,h) = \\frac{1}{Z} exp(-E(v,h)) = \\frac{1}{Z} exp(a^Tv)exp(b^Th)exp(v^TWh)\n$$\n其中$Z=\\sum_{v,h}exp(-E(v,h))$为配分函数。\n\n#### 生成模型\n受限玻尔兹曼机得联合概率分布p(v,h)一般也通过吉布斯采样的方法来近似，生成一组服从$p(v,h)$分布的样本。\n\n**1. 全条件概率**\n吉布斯采样需要计算每个变量$V_i$和$H_j$的全条件概率。受限玻尔兹曼机中的同层的变量之间没有连接。从无向图的性质可知，在给定可观测变量时，隐变量之间相互条件独立，同样在给定隐变量时，可观测变量之间也相互条件独立，即有：\n$$\np(v_i | v_{\\setminus i},h) = p(v_i|h)\n\\\\\np(h_j | v,h_{\\setminus j}) = p(h_j|v)\n$$\n其中$v_{\\setminus i}$表示除变量$V_i$外其他可观测变量得取值，$h_{\\setminus j}$为除变量$H_j$外其它隐变量的取值。因此，$V_i$的全条件概率只需要计算$p(v_i|h)$，而$H_j$的全条件概率只需要计算$p(h_j|v)$\n\n在受限玻尔兹曼机中，每个可观测变量和隐变量的条件概率为：\n$$\np(v_i=1|h) = \\sigma (a_i + \\sum_{j}w_{ij} h_j)\n\\\\\np(h_j=1|v) = \\sigma (b_j + \\sum_{i}w_{ij} v_i)\n$$\n\n其中$\\sigma$为sigmoid函数。\n\n**2. RBM中得吉布斯采样**\n受限玻尔兹曼机得采样过程如下：\n\n- （给定）或随机初始化一个可观测的向量$v_0$，计算隐变量得概率，并从中采样一个隐向量$h_0$\n- 基于$h_0$，计算可观测变量得概率，并从中采样一个个可观测的向量$v_1$\n- 重复$t$次后，获得$(v_t, h_t)$\n- 当$t \\rightarrow \\infty$时，$(v_t,h_t)$的采样服从$p(v,h)$分布\n\n下图为上述采样过程的示例：\n![受限玻尔兹曼机得采样过程](https://img-blog.csdnimg.cn/20191122162803614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n**3. 对比散度学习算法**\n\n由于首先玻尔兹曼机得特殊结构，因此可以使用一种比吉布斯采样更高效的学习算法，即对比散度（Contrastive Divergence）。对比散度算法仅需k步吉布斯采样。\n\n为了提高效率，对比散度算法用一个训练样本作为可观测向量的初始值，然后交替对可观测向量和隐藏向量进行吉布斯采用，不需要等到收敛，只需要k步就行了。这就是CD-k算法，通常，k=1就可以学得很好。对比散度得流程如下所示：\n![单步对比散度算法](https://img-blog.csdnimg.cn/20191122163920824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n**4. 受限玻尔兹曼机得类型**\n在具体的不同任务中，需要处理得数据类型不一定是二值得，也可能时连续值，为了能够处理这些数据，就需要根据输入或输出得数据类型来设计新的能量函数。一般来说受限玻尔兹曼机有以下三种：\n\n- “伯努利-伯努利”受限玻尔兹曼机：即上面介绍得可观测变量喝隐变量都为二值类型得受限玻尔兹曼机\n- “高斯-伯努利”受限玻尔兹曼机：假设可观测变量为高斯分布，隐变量为伯努利分布，其能量函数定义为：\n$$\nE(v,h) = \\sum_{i} \\frac{(v_i - \\mu_i)^2}{2 \\sigma_i^2} - \\sum{j} b_jh_j - \\sum_{i}\\sum{j} \\frac{v_i}{\\sigma_i}w_ijh_j\n$$\n其中每个可观测变量$v_i$服从$(\\mu_i, \\sigma_i)$的高斯分布。\n- “伯努利-高斯”受限玻尔兹曼机：假设可观测变量为伯努利分布，隐变量为高斯分布，其能量函数定义为：\n$$\nE(v,h)=\\sum_{i}a_i v_j - \\sum_{j} \\frac{(h_j-u_j)^2}{2\\sigma_j^2} - \\sum_{i}\\sum_{j}v_iw_{ij}\\frac{h_j}{\\sigma_j}\n$$\n其中每个隐变量$h_j$服从$(\\mu_j, \\sigma_j)$的高斯分布\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(3)-循环神经网络（中）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(3)-循环神经网络（中）篇/","content":"\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/100709422\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n---\n### 引言\n\n常见的五种神经网络系列第三种，主要介绍循环神经网络，分为上中下三篇进行介绍，本文主为（中）篇，涉及内容如下：\n\n- 循环神经网络中的参数学习\n- RNN中的长期依赖问题\n- 常见的循环神经网络结构\n\n该系列的其他文章：\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n---\n\n### 参数学习\n\n循环神经网络的参数可以通过梯度下降方法来学习。给定一个样本(x,y)，其中$x_{1:T}=(x_1, x_2, ... ,x_T)$为长度是T的输入序列，其中$y_{1:T}=(y_1, y_2, ... ,y_T)$是长度为T的标签序列，在每个时刻t，都有一个监督信息$y_t$，定义时刻t的损失函数为（公式1-1）：\n$$\nL_t = L(y_t, g(h_t))\n$$\n其中$g(h_t)$为第t时刻的输出，L为可微分的损失函数，比如交叉熵，整个序列上的损失函数为（公式1-2）：\n$$\nL = \\sum_{t=1}^{T} L_t\n$$\n整个序列的损失函数L关于参数U的梯度为（公式1-3）：\n$$\n\\frac{\\partial L}{\\partial U} = \\sum _{t=1}^{T}\\frac{\\partial L_t}{ \\partial U }\n$$\n即每个时刻的损失函数$L_t$对参数U的偏导数之和。\n\n在循环神经网络中主要有两种计算梯度的方式：\n- 随时间反向传播算法（Backpropagation Through Time，BRTT）\n- 实时循环学习（Real-Time Recurrent Learning，RTRL）\n\n#### 随时间反向传播算法\n\n主要通过类似前馈神经网络的错误反向传播算法来进行计算梯度。随时间反向传播算法将循环神经网络看作是一个展开的多层前馈网络，其中“每一层”对应循环网络中的每个时刻，这样循环神经网络就可以按照前馈神经网络中的反向传播算法来计算梯度。与前馈神经网络不同的是，循环神经网络中各层的参数是共享的，因此参数的真实梯度是各个层的参数梯度之和。\n\n先计算公式1-3中第t时刻损失对参数U的偏导数 $\\frac {\\partial L_t}{\\partial U}$，参数U和每个时刻k的净输入$z_k = Uh_{k-1} + Wx_{k} + b$有关，因此第t个时刻损失函数$L_t$关于参数$U_ij$的梯度为（公式1-4）：\n$$\n\\frac{\\partial L_t}{ \\partial U_{ij}} = \\sum_{k=1}^{t} tr( ( \\frac{\\partial L_t}{ \\partial z_k} )^T \\frac{\\partial^+ z_k}{ \\partial U_{ij}}    )\n\\\\\n= \\sum_{k=1}^{t}  ( \\frac{\\partial^+ z_k}{ \\partial U_{ij}}    )^T  \\frac{\\partial L_t}{ \\partial z_k} \n$$\n\n其中$\\frac{\\partial^+ z_k}{ \\partial U_{ij}}$ 表示“直接”偏导数，即公式$z_k = Uh_{k-1} + Wx_{k} + b$中保持$h_{k-1}$不变，对$U_{ij}$进行求偏导数，得到（公式1-5）：\n$$\n \\frac{\\partial^+ z_k}{ \\partial U_{ij}}  = \\begin{bmatrix}\n0\\\\ \n... \\\\ \n[h_{k-1}]_j \\\\ \n... \\\\ \n0\n\\end{bmatrix} \\triangleq I_i([h_{k-1}]_j)\n$$\n其中$[h_{k-1}]_j$为第$k-1$时刻隐状态的第j维，$I_i(x)$除了第j行值为x，之外全为0的向量。\n\n定义$\\delta _{t,k} = \\frac{\\partial L_t}{ \\partial z_k }$为第t时刻损失函数对第k时刻隐藏层神经元净输入$z_k$的导数，则（公式1-6）：\n$$\n\\delta _{t,k} = \\frac{\\partial L_t}{ \\partial z_k }\n\\\\\n= \\frac{ \\partial h_k }{ \\partial z_k} \\frac{\\partial z_{k+1}}{ \\partial h_k } \\frac{ \\partial L_t }{ \\partial z_{k+1} }\n\\\\\n= diag(f'(z_k))U^T \\delta _{t,k+1}\n$$\n\n将（公式1-6） 和 （公式 1-5） 代入（公式1-4）得到（公式1-7）：\n$$\n\\frac{\\partial L_t}{ \\partial U_{ij} } = \\sum_{k=1}^{ t } [\\delta _{t,k}]_i [h_{k-1}]_j\n$$\n将（公式1-7）写成矩阵形式为（公式1-8）：\n$$\n\\frac{\\partial L}{ \\partial U } = \\sum_{k=1}^{ t } \\delta _{t,k} h^T_{k-1}\n$$\n下图为随时间反向传播算法示例：\n\n![随时间反向传播算法示例](https://img-blog.csdnimg.cn/20190910191047315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n将（公式1-8）代入（公式1-3）得到整个序列的损失函数$L$关于参数$U$的梯度（公式1-9）:\n$$\n\\frac{\\partial L}{ \\partial U } = \\sum_{t=1}^{ T}\\sum_{k=1}^{ t } \\delta _{t,k} h^T_{k-1}\n$$\n\n同理可得到$L$关于参数$W$的梯度（公式1-10）：\n$$\n\\frac{\\partial L}{ \\partial W } = \\sum_{t=1}^{ T}\\sum_{k=1}^{ t } \\delta _{t,k} x^T_k\n$$\n\n$L$关于参数$b$的梯度（公式1-11）：\n$$\n\\frac{\\partial L}{ \\partial b } = \\sum_{t=1}^{ T}\\sum_{k=1}^{ t } \\delta _{t,k}\n$$\n\n> 在 随时间反向传播算法中，参数的梯度需要在一个完整的“向前”计算和“向后”计算后才能得到并参数更新。\n\n\n#### 实时循环学习\n与随时间反向传播算法不同的是：实时循环学习（Real-Time Recurrent Learning）是通过前向传播的方式来计算梯度。\n\n假设RNN中第 $t+1$时刻的状态$h_{t+1}$为（公式1-12）：\n$$\nh_{t+1} = f(z_{t+1}) = f(Uh_k + Wx_{k+1} + b)\n$$\n其关于参数$U_{ij$的偏导数为（公式1-13）：\n$$\n\\frac{ h_{t+1} }{ \\partial U_{ij} } = \\frac{ \\partial h_{t+1} }{ \\partial z_{t+1} } ( \\frac{ \\partial^+z_{t+1} }{ \\partial  U_{ij}}   + U \\frac{ \\partial h_t}{ \\partial U_{ij} } )\n\\\\\n= diag( f'(z_{t+1}) ) ( I_i ([h_t]_j)+ U \\frac{ \\partial h_t}{ \\partial U_{ij} }  )\n\\\\\n=f'(z_{t+1}) \\odot  ( I_i ([h_t]_j)+ U \\frac{ \\partial h_t}{ \\partial U_{ij} }  )\n$$\n其中$I_i(x)$为除了第i行之外元素全为0的向量。\n\nRTRL自从第一个时刻开始，除了计算RNN的隐状态之外，还利用（公式1-13）依次前向计算偏导数$\\frac{\\partial h_1}{ \\partial U_{ij}},\\frac{\\partial h_2}{ \\partial U_{ij}},\\frac{\\partial h_3}{ \\partial U_{ij}}...$\n\n这样假设第t个时刻存在一个监督信息，其损失函数为$L_t$，就可以同时计算损失函数对$U_{ij}$的偏导数（公式1-14）：\n$$\n\\frac{\\partial L_t}{ \\partial U_{ij}} =( \\frac{\\partial h_t}{ \\partial U_{ij} } )^T \\frac{\\partial L_t}{ \\partial h_t}\n$$\n这样在第t个时刻就可以实时计算$L_t$关于参数U的梯度，并更新参数。参数W和b的梯度也可以按照上述方法进行计算。\n\n> 两种算法比较：RTRL算法和BPTT算法都是基于梯度求解参数，分别通过前向模式和反向模式应用链式法则来计算梯度。在RNN中一般输出维度要比输入维度少，因此BPTT算法的计算量会很小，但要保存计算过程中的梯度值，空间复杂度较高。RTRL算法不需要进行空间回传，比较适合用在在线学习或无限序列的任务中。\n\n### 长期依赖\n\n在BRTT算法中，将（公式1-6）展开得到（公式1-15）：\n$$\n\\delta _{t,k}=\\prod_{i=k}^{t-1} ( diag('f(z_i ))U^T  )\\delta _{t,t}\n$$\n如果定义$\\gamma \\approx || diag('f(z_i ))U^T   ||$，则（公式1-16）：\n$$\n\\delta _{t,k}=\\gamma ^{t-k} \\delta _{t,t}\n$$\n若$\\gamma >1$，当$t-k \\rightarrow +\\infty$，$\\gamma ^{t-k} \\rightarrow +\\infty$，会造成系统不稳定，称之为梯度爆炸（Gradient Exploding Problem），反之，若$\\gamma < 1$，当$t-k \\rightarrow +\\infty$，$\\gamma ^{t-k} \\rightarrow 0$，会出现和前馈神经网络类似的梯度消失问题（Gradient Vanishing Problem）。\n\n> 注意：在循环神经网络中，梯度消失指的是并不是说$\\frac{ \\partial L_t}{ \\partial U}$的梯度消失了，而是$\\frac{ \\partial L_t}{ \\partial h_k}$的梯度消失，当$t-k$很大时，即参数U的更新主要靠最近的几个状态来更新，长距离的状态对参数U没有影响。\n\n当循环神经网络中使用的激活函数是Logistic或者tanh的时候，由于其导数小于1，并且权重矩阵$||U||$也不会太大，因此，如果时间间隔t-k过大的话，也会出现梯度消失问题。所以一般采用 ReLU激活函数（关于激活函数的介绍可参考：[神经网络中的激活函数介绍](https://blog.csdn.net/Gamer_gyt/article/details/89440152)）。\n\n虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸和梯度消失问题，实际上只能学习到短期的依赖关系，这样如果t时刻的输出$y_t$依赖于$t-k$时刻的输入$x_{t-k}$，当间隔k比较大时，简单神经网络很难建模这种长距离的依赖关系，称之为长期依赖问题（Long-Term Dependences Problem）。\n\n\n改进措施：\n- 选取合适的参数\n- 使用非饱和的激活函数\n\n循环网络的梯度爆炸问题比较容易解决，一般通过梯度截断和权重衰减来避免。而梯度消失很难解决，通常是对模型进行调优来解决。\n\n### 常见的循环神经网络结构\n\n主要包含四种：\n- N：N\n- 1：N\n- N：1\n- N：M\n\n#### N比N结构\n\nN维输入对应N维输出，大致结构如下所示：\n\n![循环神经网络N比N结构](https://img-blog.csdnimg.cn/20190910211158381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n其常常用于处理以下问题：\n- 视频理解中获取视频每一帧标签，输入为视频解码后的图像，通过此结构，获取每一 帧的标签信息。这种场景一般用作视频理解的初期，对视频做初步的处理后， 后续可以基于这些标签信息进行语义分析，构建更为复杂的需求场景。\n- 股票价格预测。基于历史的股票信息输入，预测下一时刻或者未来的股票走势信息。\n\n#### 1比N结构\n\n一维输入，N维输出，大致结构如下图所示：\n![循环神经网络1比N结构](https://img-blog.csdnimg.cn/20190910211546198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n还有一种结构是在同一信息在不同时刻输入到网络中，如下所示：\n![循环神经网络1比N结构](https://img-blog.csdnimg.cn/20190910211800194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n其常常用于处理以下问题：\n- 看图写描述 : 根据输入的 一张图 片，生成对这张图片的描述信息 \n- 自动作曲 : 按照类别生成音乐 \n\n#### N比1结构\n\nN维输入，一维输出，大致结构如下图所示：\n![循环神经网络N比1结构](https://img-blog.csdnimg.cn/20190910211426311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n其常常用于处理以下问题：\n- 视频理解中的获取视频每个场景的描述信息，或者获取整个影片的摘要信息。 \n- 获取用户评价的情感信息，即根据用户的一句话的评论，来判断用户的喜好等情感信息 。\n\n#### N比M结构\n\nN维输入，M维输出，这种结构又被称为Encoder-Decoder模型，也可以称为Seq2Seq模型，这种模型的输入和输出可以不相等，该模型由两部分组成：编码部分和解码部分，大致结构如下图所示：\n![循环神经网络N比M结构](https://img-blog.csdnimg.cn/20190910212035184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n c的前半部分循环神经网络为编码部分，称之为Endcoder, c可以是s3的直接输出，或者 是对s3输出做一定的变换，也可以对编码部分所有的s1、s2、s3进行变换得到，这样c中就包 含了对X1、 Xz、码的编码信息 。c的后半部分循环神经网络为解码部分，称之为Decoder。c作为之前的状态编码，作为初始值，输入到Decoder当中。 Decoder经过循环处理，最终将信息解码输出。\n \n 除了上边所示的解码结构外，还有下图所示的结构：\n ![循环神经网络N比M结构](https://img-blog.csdnimg.cn/20190910212711689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n \n N比M的循环神经网络结构更具有普遍性，现实环境中有很多基于该结构落地的场景，他可以解决如下问题：\n - 机器翻译：将不同语言作为输入，输出为非输入语言的类型，这也是Encoder-Decoder的经典用法\n - 文本摘要：输入一篇文章，输出这篇文章的摘要信息\n - 语音识别：输入一段语音，输出这段语音信息的文字\n \n> 至此，循环神经网络（中）篇已经介绍完了，在下篇中会展开介绍更多的内容，欢迎关注。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(3)-循环神经网络（下）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(3)-循环神经网络（下）篇/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/100943664\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n\n---\n### 引言\n\n常见的五种神经网络系列第三篇，主要介绍循环神经网络，由于循环神经网络包含的内容过多，分为上中下三篇进行介绍，本文主要是循环神经网络（下）篇，主要介绍以下内容：\n\n- 长短期记忆网络（LSTM）\n- 门控循环单元网络（GRU）\n- 递归循环神经网络（RecNN）\n- 图网络（GN）\n\n---\n### LSTM\n\n\n长短期记忆（Long Short-Term Memory，LSTM）网络是循环神经网络的一个变体，可以有效的解决简单循环神经网络的梯度爆炸和梯度消失问题。\n\nLSTM的改进包含两点：\n- 新的内部状态\n- 门机制\n\n#### 新的内部状态\nLSTM网络引入一个新的内部状态（internal state）$c_t$专门进行线性的循环传递，同时（非线性）输出信息给隐藏层的外部状态$h_t$（公式3-1）。\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c_t}\n\\\\\nh_t = o_t \\odot tanh(c_t)\n$$\n其中 $f_t$，$i_t$，$o_t$为三个门来控制信息传递的路径，$\\odot$为向量元素乘积，$c_{t-1}$为上一时刻的记忆单元，$\\tilde{c_t}$是通过非线性函数得到的候选状态（公式3-2）:\n\n$$\n\\tilde{c_t} = tanh( W_c x_t + U_c h_{t-1} + b_c )\n$$\n\n在每个时刻t，LSTM网络的内部状态$c_t$记录了到当前时刻为止的历史信息。\n\n\n#### 门机制\n\nLSTM网络引入门机制来控制信息的传递， $f_t，i_t，o_t$分别为遗忘门，输入门，输出门。电路中门是0或1，表示关闭和开启，LSTM网络中的门是一种软门，取值在(0,1)，表示以一定比例的信息通过，其三个门的作用分别为：\n- $f_t$：控制上一个时刻的内部状态 $c_{t-1}$需要遗忘多少信息 \n- $i_t$：控制当前时刻的候选状态$\\tilde{c_t}$有多少信息需要保存\n- $o_t$：控制当前时刻的状态$c_t$有多少信息需要输出为$h_t$\n\n三个门的计算如下（公式3-3）：\n$$\ni_t=\\sigma (W_i x_t+U_i h_{t-1} + b_i)\n\\\\\nf_t=\\sigma (W_f x_t+U_f h_{t-1}+ b_f )\n\\\\\no_t=\\sigma (W_o x_t+U_o h_{t-1}+b_o)\n$$\n其中$\\sigma$为logsitic函数，其输出区间为(0,1)，$x_t$为当前输入，$h_{t-1}$为上一时刻的外部状态。 \n\n下图（图3-1）给出了LSTM的循环单元结构，其计算分为三个过程：\n1. 利用当前时刻的输入$x_t$和上一时刻的外部状态$h_{t-1}$计算出三个门和候选状态$\\tilde{c_t}$\n2. 结合遗忘门$f_t$和输入门$i_t$来更新记忆单元$c_t$\n3. 结合输出门$o_t$将内部状态信息传递给外部状态$h_t$\n\n![LSTM循环单元结构](https://img-blog.csdnimg.cn/20190917210304438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n通过LSTM循环单元，整个网络可以建立长距离的时序依赖关系，公式3-1～3-3可以简单的描述为（公式3-4）：\n$$\n\\begin{bmatrix}\n\\tilde{c_t}\\\\ \no_t\\\\ \ni_t\\\\ \nf_t\n\\end{bmatrix} = \\begin{bmatrix}\ntanh\\\\ \n\\sigma \\\\ \n\\sigma \\\\ \n\\sigma \n\\end{bmatrix} (W\\begin{bmatrix}\nx_t\\\\ \nh_{t-1}\n\\end{bmatrix} + b)\n\\\\\nc_t = f_t \\odot c_{t-1}+ i_t \\odot \\tilde{c_t}\n\\\\\nh_t = o_t  \\odot tanh(c_t)\n$$\n其中$x_t$为当前时刻的输入，$W$和$b$为网络参数。\n\n> 循环神经网络中的隐状态h存储了历史信息，可以看作是一种记忆（Memeory）。在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作一种短期记忆（Short-term Memeory）。在神经网络中，长期记忆（Long-term Memory）可以看作是网格参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆。而在LSTM网络中，记忆单元c可以在某个时刻捕捉到某个关键信息，并有能力将该信息保存一段时间，记忆单元c中保存的信息要远远长于隐状态h，但又远远短于长期记忆，因此被成为长短期记忆网络（Long Short-term Memory）。\n\n---\n### GRU\n\n门控单元（Gate Recurrent Unit，GRU）网络是一种比LSTM更加简单的循环神经网络。在LSTM中遗忘门和输入门是互补关系，比较冗余，GRU将遗忘门和输入门合并成一个门：更新门。同时GRU也不引入额外的记忆单元，直接在当前的状态$h_t$和上一个时刻的状态$h_{t-1}$之间引入线性依赖关系。\n\n在GRU网络中，当前时刻的候选状态$\\tilde{h_t}$为（公式3-5）：\n$$\n\\tilde{h_t} = tanh( W_h x_h + U_h(r_t\\odot h_{t-1}) + b_h )\n$$\n> 计算$\\tilde{h_t}$时，选用tanh激活函数是因为其导数有比较大的值域，缓解梯度消失问题。\n\n其中$r_t \\in [0,1]$ 为重置门（reset gate），用来控制候选状态$\\tilde {h_t}$的计算是否依赖上一时刻的状态$h_{t-1}$，公式如下（公式3-6）：\n$$\nr_t = \\sigma ( W_r x_t  + U_r h_{t-1} + b_r)\n$$\n\n当 $r_t$为0 时，候选状态$\\tilde{h_t}$只和当前输入$x_t$有关，和历史状态无关，当$r_t$为1时，候选状态$\\tilde{h_t}$和当前输入$x_t$，历史状态$h_{t-1}$都有关，和简单循环网络一致。\n\nGRU网络隐状态$h_t$的更新方式为（公式3-7）：\n$$\nh_t = z_t \\odot h_{t-1}+ (1-z_t) \\odot \\tilde {h_t}\n$$\n其中$z \\in [0,1]$为更新门（update gate），用来控制当前状态需要从历史状态中保留多少信息（不经过非线性变换），以及需要从候选状态中获取多少信息。$z_t$公式如下（公式3-8）：\n$$\nz_t = \\sigma (W_z x_t + U_z h_{t-1} + b_z)\n$$\n- 若$z_t=0$，当前状态$h_t$和历史状态$h_{t-1}$之间为非线性函数。\n- 若$z_t=0，r=1$，GRU退化为简单循环网络\n- 若$z_t=0，r=0$，当前状态$h_t$只和当前输入$x_t$有关，和历史状态$h_{t-1}$无关\n- 若$z_t=1$，当前时刻状态$h_t=h_{t-1}$，和当前输入$x_t$无关\n\n\nGRU网络循环单元结构如下（图3-2）：\n\n![GRU网络循环单元结构](https://img-blog.csdnimg.cn/20190917231241261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n---\n\n### RecNN\n\n> 如果将循环神经网络按时间展开，每个时刻的隐状态$h_t$看做是一个节点，那么这些节点构成一个链式结构，而链式结构是一种特殊的图结构，很容易将这种消息传递的思想扩展到任意的图结构上。\n\n递归神经网络（Recursive Neurnal Network，RecNN）是循环神经网络在有向无循环图上的控制，递归神经网络一般结构为树状的层次结构，如下图所示（图3-3）：\n\n![递归神经网络](https://img-blog.csdnimg.cn/20190917232047624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n以上图(a)为例，包含3个隐藏层$h_1,h_2,h_3$，其中$h_1$由两个输入$x_1,x_2$计算得到，$h_2$由两个输入$x_3,x_4$计算得到，$h_3$由两个隐藏层$h_1,h_2$计算得到。\n\n对于一个节点$h_i$，它可以接受来自子节点集合$\\pi_i$中所有节点的消息，并更新自己的状态，如下所示（公式3-9）：\n$$\nh_i = f(h_{\\pi_i})\n$$\n其中$h_{\\pi_i}$表示$\\pi_i$集合中所有节点状态的拼接，$f(.)$是一个和节点状态无关的非线性函数，可以为一个单层的前馈神经网络，比如图3-3(a)所表示的递归神经网络可以表示为（公式3-10）：\n$$\nh_1 = \\sigma (W \\begin{bmatrix}\nx_1\\\\ \nx_2\n\\end{bmatrix}+ b) \n\\\\\nh_2 = \\sigma (W \\begin{bmatrix}\nx_3\\\\ \nx_4\n\\end{bmatrix}+ b) \n\\\\\nh_3 = \\sigma (W \\begin{bmatrix}\nh_1\\\\ \nh_2\n\\end{bmatrix}+ b) \n$$\n其中$\\sigma$表示非线性激活函数，W和b为可学习的参数，同样输出层y可以为一个分类器，比如（公式3-11）：\n$$\nh_3 = g (W' \\begin{bmatrix}\nh_1\\\\ \nh_2\n\\end{bmatrix}+ b') \n$$\n其中$g(.)$为分类器，$W'$和$b'$为分类器的参数。当递归神经网络的结构退化为图3-3(b)时，就等价于简单神经循环网络。\n\n递归神经网络主要用来建模自然语言句子的语义，给定一个句子的语法结构，可以使用递归神经网络来按照句法的组合关系来合成一个句子的语义，句子中每个短语成分可以分成一些子成分，即每个短语的语义可以由它的子成分语义组合而来，进而合成整句的语义。\n\n同样也可以使用门机制来改进递归神经网络中的长距离依赖问题，比如树结构的长短期记忆模型就是将LSTM的思想应用到树结构的网络中，来实现更灵活的组合函数。\n\n---\n### GN\n在实际应用中，很多数据是图结构的，比如知识图谱，社交网络，分子网络等。而前馈网络和反馈网络很难处理图结构的数据。\n\n**图网络（Graph Network，GN）**是将消息传递的思想扩展到图结构数据上的神经网络。\n\n对于一个图结构$G(V,\\varepsilon )$，其中$V$表示节点结合，$\\varepsilon$表示边集合。每条边表示两个节点之间的依赖关系，节点之间的连接可以是有向的，也可以是无向的。图中每个节点v都用一组神经元来表示其状态$h^{(v)}$ ，初始状态可以为节点v的输入特征$x^{(v)}$，每个节点接受相邻节点的信息，来更新自己的状态，如下所示（公式3-12）：\n$$\nm^{(v)}_t = \\sum_{u \\in N(v)} f( h^{(v)}_{t-1},h^{(u)}_{t-1},e^{(u,v)} )\n\\\\\nh^{(v)}_t = g(h^{(v)}_{t-1},m^{(u)}_t)\n$$\n其中$N(v)$表示节点v的邻居节点，$m^{(v)}_t$ 表示在t时刻节点v接受到的信息，$e^{(u,v)}$为边(v,u)上的特征。\n\n公式3-12是一种同步更新方式，所有结构同时接受信息并更新自己的状态，而对于有向图来说，使用异步的更新方式会更有效率，比如循环神经网络或者递归神经网络，在整个图更新T次后，可以通过一个读出函数g(.)来得到整个网络的表示。\n\n> 至此，循环神经网络（上）（中）（下）篇已经介绍完毕。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(3)-循环神经网络（上）篇","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(3)-循环神经网络（上）篇/","content":"\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/100600661\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n\n---\n### 引言\n\n常见的五种神经网络系列第三种，主要介绍循环神经网络，由于循环神经网络包含的内容过多，分为上中下三篇进行介绍，本文主要是循环神经网络（上）篇，主要介绍以下内容：\n- 循环神经网络概述\n- 如何给神经网络增加记忆能力\n    - 延时神经网络\n    - 有外部输入的非线性自回归模型\n    - 循环神经网络\n- 一般的循环神经网络\n    - 单向循环神经网络\n    - 双向循环神经网络\n    - 深度循环神经网络\n- 循环神经网络应用到机器学习任务\n\n\n### 概述\n在前馈神经网络中，信息在神经元之间的传递是单向，网络的输出只依赖于当前的输入，这样限制虽然使网络变得容易学习，但是却减弱了网络的表达能力。在很多现实任务中，网络的输出不仅和当前的输入有关，也和过去一段时间的输出相关，比如一个**有限状态自动机**不仅和当前的输入有关，也和当前的状态（上一步的输出）有关。如下图（图-1）\n\n![有限状态自动机](https://img-blog.csdnimg.cn/2019090716181761.png)\n\n> 有限状态自动机称为FSM（finite state machine）或者FSA（finite state automaton）\n\n此外，前馈神经网络难以处理时序数据，比如视频，语音，文本等。因为时序数据的长度是不固定的，而前馈神经网络要求输入和输出的维度是固定的。因此当处理这种复杂的时序数据时，就需要一种表达能力能强的模型。\n\n\n**循环神经网络(Recurrent Neural Network,RNN)** 是一类具有短期记忆能力的神经网络，在循环神经网络中，神经元不仅可以接受其他神经元的信息，还可以接受自身的信息，形成一个环路结构。RNN的参数学习可以通过随时间反向传播算法进行学习（下文会具体介绍），随时间反向传播算法按照时间的逆序将错误信息一步步的向前传递，当输入序列时间较长时，会存在梯度消失和梯度爆炸问题（也叫长期依赖问题），为了解决这个问题，人们对RNN进行了许多改进，其中最有效的是引入门控制，比如长短期记忆网络（LSTM）和门控循环单元网络（GRU），将在（下）篇进行介绍。\n\n### 如何给网络增加记忆能力\n\n上边提到前馈神经网络是一个静态网络，不能处理时序数据，那么可以通过以下三种方法给网络增加记忆能力：\n- 延时神经网络\n- 有外部输入的非线性自回归模型\n- 循环神经网络\n\n#### 延时神经网络\n\n一种简单的利用利用历史信息的方法是建立一个额外的延时单元，用来存储网络的历史信息（比如输入，输出，隐状态等），这其中比较有代表性的就是延时神经网络（TDNN，Time Delay Neural Network）。\n\n延时神经网络是在前馈神经网络的非输出层都添加一个延时器，记录最近几次神经元的输出，在第t个时刻，第（l+1）层的神经元和第（l）层神经元的最近p次输出有关，即（公式-1）:\n$$\nh_t^{l+1} = f(h_t^l,h_{t-1}^l,....,h_{t-p+1}^l)\n$$\n通过延时器，前馈神经网络就具有了短期记忆的能力。\n\n#### 有外部输入的非线性自回归模型\n**自回归模型（Autoregressive Model）** 是统计学中常用一类时间序列模型，用一个变量 $y_t$ 的历史信息来预测自己（公式-2）。\n$$\ny_t = w_0 + \\sum_{i=1}^{p}w_p * y_{t-i} + \\varepsilon_t\n$$\n其中p为超参数，$w_p$ 为参数，$\\varepsilon_t～N(0,\\sigma ^2)$ 为第t个时刻的噪声，方差$\\sigma^2$和时间t无关。\n\n有外部输入的非线性自回归模型（Nonlinear Autoregressive Model）是自回归模型的扩展，在每个时刻t都有一个外部输入$x_t$，产出一个输出$y_t$，NART通过一个延时器来记录最近几次的外部输入和输出，第t个时刻的输出$y_t$为（公式-3）：\n$$\ny_t = f(x_t, x_{t-1},..,x_{t-p}, y_{t-1},y_{t-2},....,y_{t-q})\n$$\n其中f(.)为非线性函数，可以是前馈神经网络，p和q为超参数。\n\n#### 循环神经网络\n给定一个输入序列，$x_{1:T}=( x_1, x_2, ... , x_T )$ 循环神经网络通过以下公式（公式-4）更新带反馈边的隐藏层的活性值$h_t$：\n$$\nh_t = (h_{t-1}, x_t)\n$$\n循环神经网络示例如下（图-2）：\n![循环神经网络示例](https://img-blog.csdnimg.cn/20190907182513443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n\n### 一般的循环神经网络\n\n在（图-2）中展示了一个简单的循环神经网络，其整个结构分为3层：输入层，隐藏层和输出层。其中t时刻，隐藏层的状态$h_t$不仅与输入$x_t$有关，还与上一个时刻的隐藏层状态$h_{t-1}$有关。\n\n由于隐藏层多了一个自身到自身的输入，因此该层被称为循环层，（图-2）所示的为一个简单循环神经网络。循环神经网络还有多种类型，基于循环的方向划分为：\n- 单向循环神经网络\n- 双向循环神经网络\n\n基于循环的深度分为：\n- 循环神经网络\n- 深度循环神经网络\n\n#### 单向循环神经网络\n\n（图-2）所示即为一个单向的循环神经网络，对其展开后的效果图如下（图-3）：\n![单向循环神经网络](https://img-blog.csdnimg.cn/2019090809043298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n上图可以理解为网络的输入通过时间往后传递，当前隐藏层的输出$h_t$取决于当前层的输入$x_t$和上一层的输出$h_{t-1}$，因此当前隐藏层的输出信息包含了之前时刻的信息，表达出对之前信息的记忆能力。单向循环神经网络表达如下（公式-5）：\n$$\no_t = g(V*h_t)\n\\\\\nh_t = f(U*x_t + W*h_{t-1})\n$$\n其中$o_t$为输出层的计算公式， $h_t$为隐藏层的计算公式，g(.) 和 f(.)为激活函数。值得说明的是在循环神经网络中U，V，W权重矩阵值每次循环都是一份，因此循环神经网络的每次循环步骤中，这些参数都是共享的，这也是循 环神经网络 的结构特征之一。\n\n#### 双向循环神经网络\n\n在日常的信息推断中，当前信息不仅仅依赖之前的内容，也有可能会依赖后续的内容，比如英语的完形天空。这时候单向的循环神经网络就不能很好的处理，就需要 ** 双向循环神经网络(Bi-directional Recurrent Neural Network)** 。\n\n其主要思想是训练一个分别向前和分别向后的循环神经网络，表示完整的上下文信息，两个循环 网络对应同一个输出层，因此可以理解为两个循环神经网络的叠加，对应的输出结果根据两个神经网络输出状态计算获得，将双向循环神经网络按照时间序列结构展开，如下图所示（图-4）：\n![双向循环神经网络](https://img-blog.csdnimg.cn/20190908145624762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n从上图可以看出，隐藏层需要保留两部分，一部分为由前向后的正向传递$h_t$，一部分为由后向前的反向传递$h'_t$，最新的信息输出$o_t$。双向循环神经网络的表达公式如下（公式-6）：\n$$\no_t = g(V*h_t + V'*h'_t)\n\\\\\nh_t = f(U*x_t + W*h_{t-1})\n\\\\\nh'_t = f(U'*x_t + W'*h'_{t-1})\n$$\n\n\n#### 深度循环神经网络\n\n上边介绍的单向训练神经网络和双向循环神经网络都只有一个隐藏层，但是在实际应用中，为了增强表达能力，往往引入多个隐藏层，即深度循环神经网络，如下图所示（图-5）：\n![深度循环神经网络](https://img-blog.csdnimg.cn/20190908151530509.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n同样可以得到深度循环神经网络的表达式（公式-7）：\n$$\no_t = g(V^{(i)}*h^{(i)}_t + V'^{(i)}*h'^{(i)}_t)\n\\\\\nh^{(i)}_t = f(U^{(i)}*h^{(i-1)}_t + W^{(i)}*h_{t-1})\n\\\\\nh'^{(i)}_t = f(U'^{(i)}*h'^{(i-1)}_t + W'^{(i)}*h'_{t+1})\n\\\\\n...\n\\\\\nh^{(1)}_t = f(U^{(1)} * h_t + W^{(1)}*h_{t-1})\n\\\\\nh'^{(1)}_t = f(U'^{(1)} * h_t + W'^{(1)}*h'_{t+1})\n\\\\\n$$\n从上述公式可以看出，最终的输出依赖两个维 度的计算，横向上内部前后信息的 叠加，即按照时间的计算；纵向上是每一时刻的输入信息在 逐层之间的传递，即按照 空间结构的计算。\n\n\n### 循环神经网络应用到机器学习任务\n循环神经网络可以应用到很多不同类型的机器学习任务，根据这些任务的特点，可以分为以下几种模式：\n- 序列到类别模式\n- 同步的序列到序列模式\n- 异步的序列到序列模式\n\n#### 序列到类别模式\n主要应用在序列数据的分类问题，其输入为序列，输出为类别。比如在文本分类中，输入为单词序列，输出为文本的类别。\n\n假设一个样本 $x_{1:T}=(x_1, x_2, ... , x_T)$为一个长度为T的序列，输出为类别 $y \\in (1, ..., C)$，可以将样本x按不同的时刻输入到循环神经网络中，并得到不同时刻的隐含状态$h_t$，可以将 $h_t$ 看作是整个序列的最终表示，并输入给分类器 g(.) 进行分类，如下所示（公式-8）：\n$$\n\\hat{y} = g(h_T)\n$$\n\n其中g(.) 为简单的线性分类器（比如LR）或者复杂的分类器（前馈神经网络）。\n\n除了将最后时刻的状态作为序列表示之外，我们还可以对整个序列的状态进行平均，并用整个状态的最终平均作为整个序列的表示（公式-9）：\n$$\n\\hat{y} = g( \\frac{1}{T} \\sum_{t=1}^{T} h_t )\n$$\n\n公式-8 和公式-9 分别对应下图（图-6）的（a）和（b）:\n![序列到类别模式](https://img-blog.csdnimg.cn/20190908154219364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n#### 同步的序列到序列模式\n主要用于序列标注（Sequence Labeling）任务，即每一时刻都有输入和输出，输入序列和输出序列的长度相同。比如词性标注（Part-of-Speech Tagging）中，每一个单词都需要标注其对应的词性标签。\n\n假设一个样本 $x_{1:T}=(x_1, x_2, ... , x_T)$为一个长度为T的序列，输出为序列 $y_{1:T}=(y_1, y_2, ... , y_T)$，可以将样本x按不同的时刻输入到循环神经网络中，并得到不同时刻的隐含状态$h_t$，每个时刻的 $h_t$ 代表了当前时刻和历史的信息，并输入给分类器 g(.) 进行分类，得到当前的标签 $\\hat{y}_t$，如下所示（公式-8）：\n$$\n\\hat{y} = g(h_T), \\forall_t \\in [1,T]\n$$\n\n![同步的序列到序列模式](https://img-blog.csdnimg.cn/20190908154735645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n\n#### 异步的序列到序列模式\n\n异步的序列到序列模式也成为编码器-解码器，输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度，比如在机器翻译中，输入为源语音的单词序列，输出为目标语言的单词序列。\n\n假设输入为一个长度为T的序列 $x_{1:T}=(x_1, x_2, ... , x_T)$，输出为长度为M的序列$y_{1:M}=(x_1, x_2, ... , x_M)$，经常通过先编码后解码的形式实现。\n\n先将样本x按不同时刻输入到一个循环神经网络（编码器）中，并得到其编码$h_T$，然后再使用另外一个循环神经网络（解码器）中，得到输出序列$\\hat {y}_{1:M}$。为了建立输出序列之间的依赖关系，在解码器中通常使用非线性的自回归模型。如下所示（公式-9）：\n$$\nh_t = f_1(h_{t-1},x_t), \\forall_t \\in [1,T]\n\\\\\nh_{T+t} = f_2(h_{T+t-1},x_t), \\forall_t \\in [1,M]\n\\\\\n\\hat{y} _t = g(h_{T+t}), \\forall_t \\in [1,M]\n$$\n其中 $f_1(.)$，$f_2(.)$分别为用作编码器和解码器的循环神经网络，g(.)为分类器，$\\hat{y}_t$ 为输出预测$\\hat{y}_t$的表示。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190908160323234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n> 至此，循环神经网络（上）篇已经介绍完了，在（中）篇和下篇中会展开介绍更多的内容，欢迎关注。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(2)-卷积神经网络","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(2)-卷积神经网络/","content":"转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/100600661\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n\n---\n\n> 卷积神经网络（Convolutional Neural Network）是一种具有局部连接，权重共享等特性的深层前馈神经网络。一般是由卷积层，汇聚层，全连接层交叉堆叠而成，使用反向传播算法进行训练。其有三个结构上的特征：局部连接，权重共享以及汇聚。这些特征使得卷积神经网络具有一定程度上的平移，缩放和旋转不变性。较前馈神经网络而言，其参数更少。\n\n卷积神经网络目前主要应用在图像和视频分析的各种任务上，比如图像分类，人脸识别，物体识别，图像分割等，其准确率也远远超过了其他的人工神经网络。近年来，卷积神经网络也应用到自然语言处理和推荐系统等领域。\n\n# 卷积的概念\n卷积（Convolution）也叫摺积，是分析数学中一种重要的运算。在信号处理或者图像处理中，会经常使用一维或二维卷积。\n\n## 一维卷积\n一维卷积经常用在信号处理上，用来计算信号的累积。假设一个信号发生器每个时刻t发生一个信号 $x_t$，其信号衰减率维$w_k$，即在$k-1$时刻后，信息变为原来的$w_k$倍，假设$w_1 = 1, w_2=1/2,w_3=1/4$那么在t时刻收到的信号$y_t$为当前时刻产生的信息之前时刻产生的延迟信息的叠加(公式1.1)。\n$$\ny_t = 1 * x_t + 1/2 * x_{t-1} + 1/4 * x_{t-2}\n\\\\\n=w_1 * x_t + w_2 * x_{t-1} + w_3 * x{t-2}\n\\\\\n= \\sum_{k=1}^{3} w_k * x_{t-k+1}\n$$\n我们把$w_1, w_2, ....$称为滤波器（Filter）或者卷积核（Convolution Kernel）。假设滤波器长度为m，它和一个信号序列$x_1,x_2,...$的卷积为(公式1.2)：\n$$\\sum_{k=1}^{m} w_k * x_{t-k+1}$$\n信号序列x和滤波器w的卷积定义为(公式1.3)：\n$$y = w \\otimes  x$$\n一维卷积示例如下：\n![一维卷积神经网络](https://img-blog.csdnimg.cn/20190904075135300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n## 二维卷积\n卷积也常用在图像处理中，因为图像是一个二维结构，需要对一维卷积进行扩展。给定一个图像$X \\in R^{M*N}$和滤波器$W \\in R^{m*n}$，一般$m << M, n <<N$，其卷积为(公式1.4)：\n$$\ny_{ij}=\\sum_{u=1}^{M}\\sum_{v=1}^{N} w_{uv} * x_{i-u+1,j-v+1}\n$$\n二维卷积示例如下：\n![二维卷积示例](https://img-blog.csdnimg.cn/20190904080806425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n> 注意：上图中的展示的卷积核（3*3矩阵）和二维结构数据相乘时需要逆时针旋转180度！对照着卷积公式可以理解。\n\n\n## 互相关\n在计算卷积过程中，需要进行卷积核翻转，在具体实现上一般会以互相关操作来代替卷积，从而会减少一些不必要的操作或者开销。互相关是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。给定一个图像$X \\in R^{M*N}$和卷积核$W \\in R^{m*n}$，他们的互相关为(公式1.5)：\n$$\ny_{ij}=\\sum_{u=1}^{M}\\sum_{v=1}^{N} w_{uv} * x_{i+u-1,j+v-1}\n$$\n和公式1.4相比，互相关和卷积的区别在于是否对卷积核进行翻转，因此互相关也称为不翻转卷积。\n\n在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行核翻转与其特征抽取能力无关。特别是当卷积核是可学习的参数时，卷积和互相关是等价的，因此为了实现方便，通常使用互相关来代替卷积。事实上很多深度学习工具中卷积操作都是用互相关来代替的。\n\n公式1.5可以表示为：\n\n$$\nY = W \\otimes X \n$$\n\n---\n# 常见的卷积核及特征\n## 常见的卷积核\n1. 对图像无任何影响的卷积核\n$$\n\\begin{bmatrix}\n0  & 0 & 0 \\\\ \n0 &  1 & 0 \\\\ \n0 & 0  & 0\n\\end{bmatrix}\n$$\n2. 对图像进行锐化的滤波器\n$$\n\\begin{bmatrix}\n-1  & -1 & -1 \\\\ \n-1 &  9 & -1 \\\\ \n-1 & -1  & -1\n\\end{bmatrix}\n$$\n3. 浮雕滤波器\n$$\n\\begin{bmatrix}\n-1  & -1 & 0 \\\\ \n-1 &  0 & 1 \\\\ \n0 & 1  & 1\n\\end{bmatrix}\n$$\n4. 均值模糊滤波器\n$$\n\\begin{bmatrix}\n0  & 0.2 & 0 \\\\ \n0.2 &  0.2 & 0.2 \\\\ \n0 & 0.2  & 0\n\\end{bmatrix}\n$$\n> 均值模糊是对像素点周围的像素进行均值化处理，将上下左右及当前像素点分文5份，然后进行平均，每份占0.2，即对当前像素点周围的点进行均值化处理。\n\n5. 高斯模糊滤波器\n> 均值模糊是一种简单的模糊处理方式，但是会现实模糊不够平滑，而高斯模糊可以很好的处理，因此高斯模糊经常用于图像的降噪处理上，尤其是在边缘检测之前，进行高斯模糊，可以移除细节带来的影响。\n- 一维高斯模糊\n$$\nG(x)=\\frac{1}{ \\sqrt{2 \\pi \\sigma ^2}} e^{( -\\frac{x^2}{2\\sigma ^2} )}\n$$\n- 二维高斯模糊\n$$\nG(x)=\\frac{1}{ \\sqrt{2 \\pi \\sigma ^2}} e^{( -\\frac{x^2+y^2}{2\\sigma ^2} )}\n$$\n\n## 卷积核的特征\n> 这里的滤波器就是卷积核\n\n- 当滤波器矩阵中的值相加为0甚至更小时，被滤波器处理之后的图像相对会比原始图像暗，值越小越暗\n- 当滤波器矩阵中的值相加和为1时，被滤波器处理之后的图像与原始图像的亮度相比几乎一致\n- 当滤波器矩阵中的值相加和大于1时，被滤波器处理之后的图像相对会比原始图像的亮度更亮\n\n---\n\n# 卷积的变种\n在卷积的标准定义基础上，还可以引入滤波器的滑动步长和零填充来增加卷积的多样性，可以更加灵活的提取特征。\n- 滤波器的步长（Stride）是指滤波器在滑动时的时间间隔\n- 零填充（Zero Padding）是在输入向量两端进行补零\n\n下图展示为步长为2和零填充的示例：\n![步长为2和零填充示例](https://img-blog.csdnimg.cn/20190904081641651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n假设卷积层的输入神经元个数为n，卷积大小为m，步长为s，输入神经元两端各补p各零，那么该卷积对应的神经元数量为：(n+2p-m)/s + 1。\n\n一般的卷积分为以下三种：\n- 窄卷积（Narrow Convolution）：步长s=1，两端不补零即p=0，卷积后输出长度为：n-m + 1\n- 宽卷积（Wide Convolution）：步长s=1，两端补零p=m-1，卷积后输出长度为：n+m-1\n- 等宽卷积（Equal-Width Convolution）：步长s = 1,两端补零p = (m −1)/2,卷积后输出长度 n。\n\n---\n\n# 卷积的数学性质\n卷积有很多比较好的数学性质，这里主要介绍一些二维的数学性质，同样针对一维卷积也同样适用。\n## 交换性\n如果不限制两个卷积的长度，卷积是具有交换性的。即 $x \\otimes  y = y \\otimes  x$，当输入信息和卷积核有固定长度时，他们的宽卷积依然具有交换性。对于两维图像$X \\in R^{M*N}$和卷积核$W \\in R^{m*n}$，对图像X的两个维度进行零填充，两端各补m-1和n-1个零，得到全填充（Full Padding）的图像$\\tilde{X} \\in R^{(M+2m-2)(N+2n-2)}$。图像X和卷积核W的宽卷积（Wide Convolution）定义为：$W \\tilde{\\otimes } X \\triangleq X \\tilde{\\otimes } W$\n，其中$\\tilde{\\otimes }$为宽卷积操作。宽卷积具有交换性，即：$W \\tilde{\\otimes } X = X \\tilde{\\otimes } W$\n\n## 导数\n假设$Y = W \\otimes X$，其中$X \\in R^{M*N}$，$W \\in R^{m*n}$，$Y \\in R^{(M-m+1)*(N-n+1)}$，函数$f(Y) \\in R$为一个标量函数，则(公式1.6)\n$$\n\\frac{\\partial f(Y)}{\\partial w_{uv}} = \\sum_{i=1}^{M-m+1}\\sum_{j=1}^{N-n+1} \\frac{\\partial f(Y)}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial w_{uv}}\n\\\\\n= \\sum_{i=1}^{M-m+1}\\sum_{j=1}^{N-n+1}  \\frac{\\partial f(Y)}{\\partial y_{ij}}x_{ {i+u-1},{j+v-1}}  \n\\\\\n= \\sum_{i=1}^{M-m+1}\\sum_{j=1}^{N-n+1}  \\frac{\\partial f(Y)}{\\partial y_{ij}}x_{ {u+i-1},{v+j-1}}  \n$$\n从公式1.6可以看出，f(Y)关于W的偏导数为X和$\\frac{\\partial f(Y)}{\\partial Y}$的卷积（公式1.7）\n$$\n\\frac{\\partial f(Y)}{\\partial W} = \\frac{\\partial f(Y)}{ \\partial Y } \\otimes X\n$$\n同理得到（公式1.8）：\n$$\n\\frac{\\partial f(Y)}{\\partial x_{st}} = \\sum_{i=1}^{M-m+1}\\sum_{j=1}^{N-n+1} \\frac{\\partial f(Y)}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial x_{st}}\n\\\\\n= \\sum_{i=1}^{M-m+1}\\sum_{j=1}^{N-n+1}  \\frac{\\partial f(Y)}{\\partial y_{ij}}w_{ {s-i+1},{t-j+1}}  \n$$\n其中当$(s-i+1) < 1$，或$(s-i+1)>m$，或$(t-j+1) <1$，或$(t-j+1) >n$，或$w_{s-i+1,t-j+1}=0$时，即相当于对W进行了 p=(M-m,N-n)的零填充。\n\n从公式1.8可以看出，f(Y)关于X的偏导数为W和$\\frac{\\partial f(Y)}{ \\partial Y }$，公式1.8中的卷积是真正的卷积而不是互相关，为了一致性，我们用互相关的卷积，即(公式1.9)：\n$$\n\\frac{\\partial f(Y)}{\\partial X} = rot180(\\frac{\\partial f(Y)}{\\partial X}) \\tilde{\\otimes }W=rot180(W) \\tilde{\\otimes }\\frac{\\partial f(Y)}{\\partial X}\n$$\n其中rot180(.)表示旋转180度。\n\n----\n\n\n# 卷积神经网络\n卷积神经网络一般由卷积层，汇聚层和全连接层构成。\n## 用卷积代替全连接\n在全连接前馈神经网络中，如果第$l$层有$n^l$个神经元，第$l-1$层有$n^{l-1}$个神经元，连接边就有$n^l * n^{l-1}$也就是权重参数有这么多个，当m和n都很大时，权重矩阵的参数会非常多，训练的效率会非常低。\n\n如果用卷积代替全连接，第$l$层的净输入$z^l$与$l-1$层活性值$a^{l-1}$和滤波器$w^l \\in R^m$的卷积，即$z^l = w^l * a^{l-1} + b^l$,其中滤波器$w^l$\n为可学习的权重向量，$b^l \\in R^{l-1}$为可学习的偏置。\n\n根据卷积的定义，卷积层有两个很重要的性质：\n- 局部连接：在卷积层(假设是第$l$层)中的每一个神经元都只和下一层(第$l − 1$层)中某个局部窗口内的神经元相连,构成一个局部连接网络。\n- 全局共享：作为参数的滤波器 $w^l$，对于第 $l$层的所有的神经元都是相同的。\n\n## 卷积层\n卷积层的作用是提取一个局部区域的特征，不同大小的卷积相当于不同的特征提取器。上文介绍的卷积和神经元都是一维的，但卷积神经网络主要是针对图像处理而言的，而图像通常是二维的，为了充分利用图像的局部特征，通常将神经元组织为三维结构的神经层，其大小 M * 宽度 W * 深度 D，即D个M*N的特征映射组成。\n\n\n对于输入层的而言，特征映射就是图像本身，如果是灰色图像，则深度为1，如果为彩色图像（分别是RGB三个通道的颜色特征映射），则深度为3。\n\n## 汇聚层\n汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是进行特征选择，降低特征数量，从而减少参数数量。\n\n卷积层虽然可以明显减少网络中的连接数量，但是特征映射中的神经元个数并未显著减少。如果后边接一个分类器的话，分类器的输入维数依然很高，很容易出现过拟合。因此有了汇聚层的产生，在卷积后边加一个汇聚层，从而降低特征维数，避免过拟合。\n\n假设汇聚层的输入特征映射组为$X \\in R^{M * N * D}$，对于其中每一个映射$X^d$，将其划分为很多区域$R^d_{m,n}$，1 <= m <= M'，1<= n <= N'，这些区域可以重叠，也可以不重叠。汇聚（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括。常见的汇聚方式有两种：\n- 最大汇聚（Maximum Pooling）：一个区域内所有神经元的最大值\n- 平均汇聚（Mean Pooling）：一个区域内所有神经元的平均值\n\n典型的汇聚层是将每个特征映射划分为2*2大小的不重叠区域，然后使用最大汇聚的方式进行下采样。汇聚层也可以看作是一个特殊的卷积层，卷积核大小为m * m，步长为s * s，卷积核为 max函数或者mean函数。过大的采样区域会急剧减少神经元的数量，会造成过多的信息损失。\n\n下图所示为最大汇聚示例：\n![最大汇聚实例](https://img-blog.csdnimg.cn/20190905133224389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n## 全连接层\n在全连接层中，将最后一层的卷积输出展开，并将当前层的每个节点与下一层的另一个节点连接起来。全连接层只是人工神经网络的另一种说法，如下图所示，全连接层中的操作与一般神经网络中的操作完全相同。\n\n![image](https://img-blog.csdnimg.cn/20190904171044639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n对于output layer中的的每个神经元，其表达式可以记做为(公式1.10)：\n$$\ny = \\sigma (\\sum_{i=1}^{m} w_i ^T x_i + b)\n$$\n如果outptu有多个神经元，最终可以通过softmax进行最终类别的判断。\n\n---\n\n# 典型的卷积网络结构\n\n一个典型的卷积网络是有卷积层，汇聚层，全连接层交叉堆叠而成。目前常用的卷积神经网络结构如下图所示：\n![卷积神经网络](https://img-blog.csdnimg.cn/2019090413573373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n卷积块是由M个卷积层b个汇聚层（M通常在2～之间，b为0或1），一个卷积网络中可以堆叠N个连续的卷积块，然后再接着K个全连接层（N的取值空间比较大，一般是1～100或者更大，K通常为0～2）。\n\n目前整个网络倾向于使用更小的卷积核（比如1*1或者3*3）以及更深的结构（比如层数大于50），此外，卷积操作的灵活性越来越大，汇聚层的作用变得越来越小，因此目前流行的卷积网络中，汇聚层的比例也在逐渐降低，倾向于全连接网络。\n\n---\n\n# 参数学习\n在卷积神经网络中，参数为卷积核中的权重和偏置，和全连接前馈神经网络一样，使用误差反向传播算法来进行参数学习。梯度主要通过每一层的误差项$\\delta$进行反向传播，并进一步计算每一层的梯度。在卷积神经网络中主要有两种功能不同的网络层：卷积层和汇聚层。而参数为卷积核中权重和偏置，因此只需要计算卷积层中参数梯度。\n\n不失一般性，对第$l$层卷积层,第$l-1$层的输入特征映射为$X^{(l-1)} \\in R^{M*N*D}$，通过卷积计算得到第$l$层净输入为$Z^{(l)}\\in  R^{M'*N'*P}$，第$l$层的第p(1<= p <= P)个特征净输入为(公式1.11)\n$$\nZ^{(l,p)} = \\sum_{d=1}^{D} W^{(l,p,d)} \\otimes X^{(l-1,d)}+ b^{(l,p)}$$\n其中$W^{(l,p,d)} ,b^{(l,p)}$为卷积核以及偏置。第$l$层共有P * D 个卷积和P个偏置，可以分别使用链式法则计算其梯度。\n\n根据公式1.7 和 1.11，损失函数关于第$l$层的卷积核$W^{(l,p,d)}$的偏导数为为(公式1.12)：\n$$\n\\frac{\\partial L (Y,\\hat{Y})}{ \\partial W^{(l,p,d)} } = \\frac{\\partial L (Y,\\hat{Y})}{ \\partial Z^{(l,p)} } \\otimes X^{(l-1,d)}=\\delta ^{(l,p)} \\otimes X^{(l-1,d)}\n$$\n其中为(公式1.13)\n$$\n\\delta ^{(l,p)} = \\frac{\\partial L (Y,\\hat{Y})}{ \\partial Z^{(l,p)} } \n$$\n为损失函数关于第$l$层的第p个特征映射净输入$Z^{(l,p)}$的偏导数。\n\n同理可得，损失函数关于第$l$层的第p个偏置$b^{(l,p)}$的偏导数为为(公式1.14)：\n$$\n\\frac{ \\partial L (Y,\\hat{Y}) }{ \\partial b^{(l,p)} } = \\sum_{i,j} [\\delta ^{(l,p)}]_{i,j}\n$$\n卷积网络中，每层参数的梯度依赖其所在层的误差项$\\delta ^{(l,p)}$\n\n## 误差项的计算\n卷积层和汇聚层的误差项计算不同。\n\n### 卷积层\n当$l+1$层为卷积层时，假设特征映射净输入(公式1.15)\n$$\nZ^{(l+1,p)} = \\sum_{d=1}^{D} W^{(l+1,p,d)} \\otimes X^{(l,d)} + b^{(l+1,p)}\n$$\n\n其中$W^{(l+1,p,d)},b^{(l+1,p)}$为第$l$层的卷积核和偏置。第$l+1$层共有 P *D 个卷积核和P个偏置。\n\n第 $l$层的第 $d$个特征映射的误差项$\\delta ^{(l,d)}$的具体推导过程如下(公式1.16):\n$$\n\\delta ^{(l,d)} \\triangleq  \\frac{\\partial L (Y,\\hat{Y})}{ \\partial Z^{(l,d)} }\n\\\\\n=\\frac{\\partial X^{(l,d)} } { \\partial Z^{(l,d)}} \\cdot \\frac{\\partial L (Y,\\hat{Y})}{ \\partial X^{(l,d)} }\n \\\\\n= f'_l (Z^{(l,p)})  \\odot \\sum_{p=1}^{P}( rot180(W^{(l+1,p,d)} ) \\tilde{\\otimes  } \\frac{\\partial L(Y,\\hat{Y})}{ \\partial Z^{(+1,p)}})\n\\\\\n=  f'_l (Z^{(l,p)})  \\odot \\sum_{p=1}^{P}(rot180(W^{(l+1,p,d)} ) \\tilde{\\otimes  }\\delta ^{(l+1,p)})\n$$\n\n其中$\\tilde{\\otimes}$表示宽卷积。\n\n### 汇聚层\n当第$l+1$层为汇聚层时, 因为汇聚层是下采样操作, $l+1$层的每个神经元的误差项 $\\delta$对应于第$l$层的相应特征映射的一个区域。$l$层的第$p$个特征映射中的每个神经元都有一条边和$l+1$层的第$p$个特征映射中的一个神经元相连。\n\n根据链式法则,第$l$层的一个特征映射的误差项$\\delta ^{(l,p)}$，只需要将 $l+1$层对应特征映射的误差项$\\delta ^{(l+1,p)}$进行上采样操作(和第 $l$层的大小一样) ,再和 $l$层特征映射的激活值偏导数逐元素相乘,就得到了 $\\delta ^{ (l,p)}$\n\n第 $l$层的第$p$个特征映射的误差项$\\delta ^{(l,p)}$的具体推导过程如下(公式1.17)：\n$$\n\\delta ^{(l,p)} \\triangleq  \\frac{\\partial L (Y,\\hat{Y})}{ \\partial Z^{(l,p)} }\n\\\\\n=\\frac{\\partial X^{(l,p)} } { \\partial Z^{(l,p)}} \\cdot \\frac{\\partial Z^{(l+1,p)} } { \\partial X^{(l,p)}} \\cdot \\frac{\\partial L (Y,\\hat{Y})}{ \\partial Z^{(l+1,p)} }\n\\\\ \n= f'_l (Z^{(l,p)})  \\odot up(\\delta ^{(l+1,p)})\n$$\n其中$f'_l$为第l层使用的激活函数导数，up为上采样函数(upsampling)，与汇聚层中使用的下采样函数刚好相反，如果下采样是最大汇聚（max pooling），误差项$\\delta ^{(l+1,p)}$中每个值都会传递到上一层对应区域中的最大值所对应的神经元，该区域中其他位置的神经元的误差都设为0，如果下采样是平均汇聚(mean pooling) ,误差项 $\\delta ^{(l+1,p)}$中每个值会被平均分配到上一层对应区域中的所有神经元上。\n\n---\n\n# 几种典型的卷积神经网络\n## LeNet-5\n\nLeNet-5 虽然提出的时间比较早（LeCun et al., 1998），但是一个非常成功的卷积神经网络模型，90年代在许多银行进行使用，用来识别手写数字，其网络结构如下：\n![LeNet-5](https://img-blog.csdnimg.cn/20190904141123853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n## AlexNet\nAlexNet是第一个现代深度卷积网络模型，其首次使用了现在深度卷积网络的一些技巧，比如GPU并行训练，采用ReLU作为非线性激活函数，使用DropOut防止过拟合，使用数据增强来提高模型准确率。AlexNet获得了2012年ImageNet图像分类比赛的冠军，其网络结构如下：\n![AlexNet](https://img-blog.csdnimg.cn/20190904141502612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n## Inception\n在卷积网络中，如何定义一个卷积的大小是一个十分关键的问题，在Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模块， Inception网络是由多个inception模块和汇聚层堆叠而成。\n\nInception模块同时使用1*1，3*3，5*5等大小不同的卷积核，并将得到的特征映射在深度上拼接（堆叠）起来作为输出特征映射。下图给出了v1版本的inception模块结构图，采用了4组平行的特征抽取方式，分别为1*1，3*3，5*5的卷积和3*3的最大汇聚，同时为了提高计算效率，减少参数数量，inception模块在进行3*3，5*5的卷积之前，3*3的最大汇聚之后，进行一次1*1的卷积来减少特征映射的深度。\n![v1版本的inception模块](https://img-blog.csdnimg.cn/20190904142716385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\nInception网络最早的v1版本就是非常著名的GoogleNet，获得了2014年ImageNet图像分类竞赛的冠军。其结构图如下所示：\n![GoogleLeNet](https://img-blog.csdnimg.cn/20190904142927193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n当然Inception网络有多个改进版本，比如Inception-v3网络，Inception-ResNet v2网络和改进版的Inception-v4模型。\n\n## 残差网络\n残差网络（Residual Network，ResNet）是通过给非先行的卷积层增加直连边的方式来提高信息的传播效率。\n\n假设在一个深度网络中，我们期望一个非线性单元$f(x,\\theta)$去逼近一个目标函数为h(x)。如果将目标函数拆分为两部分：恒等函数（Identity）和残差函数（Reside Function）h(x)-x。\n\n$$\nh(x) = \\underset{IdentityFunc}{\\underbrace{x}} +( \\underset{ResidueFunc}{\\underbrace{h(x)-x}})\n$$\n\n根据通用近似定理，一个由神经网络构成的非线性单元有足够的能力来近似逼近原始目标函数或残差函数，但实际中后者更容易血虚。因此原来的优化问题可以转化为：让非线性单元$f(x,\\theta)$去近似残差h(x)-x,并用$f(x,\\theta) +x$去逼近h(x)。\n\n下图给出了一个典型的残差单元示例，残差单元由多个级联的（等长）卷积层和一个跨层的直连边组成，再经过ReLU激活后得到输出。残差网络就是将很多个残差单元串联起来构成的一个非常深的网络。\n![残差单元示例](https://img-blog.csdnimg.cn/20190904144622317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n---\n\n# 其他卷积方式\n## 转置卷积\n我们一般通过卷积操作来实战高维特征到低维特征的转换，但在一些任务中需要把低维特征映射到高维特征，并且希望通过卷积操作来实现。\n\n卷积操作可以通过仿射变换的形式。假设一个5维的向量x，经过大小为3的卷积核w=[w1,w2,w2]^T来进行卷积，得到3维向量z，卷积操作可以写为：\n\n![5维向量x与大小为3的卷积核进行卷积](https://img-blog.csdnimg.cn/20190904154849239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n其中C是一个稀疏矩阵，其非零元素来自于卷积核w中的元素。如果实现3维向量z到5维向量x的映射，可以通过仿射矩阵转置来实现。\n\n![仿射矩阵转置](https://img-blog.csdnimg.cn/20190904155151517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n其中rot180(.)表示旋转180度。\n\n> 我们将低维特征映射到高维特征的卷积操作称之为转置卷积（Transposed Convolution），也叫反转卷积（Deconvolution）。\n\n## 空洞卷积\n\n对于一个卷积层，如果希望增加输出单元的感受野，一般可以通过三种方式实现：\n- 增加卷积核的大小\n- 增加层数\n- 在卷积之前进行汇聚操作\n\n前两种会增加参数数量，最后一种会丢失一些信息。\n\n空洞卷积（Atrous Convolution）也成为膨胀卷积（Dilated Convolution），是一种不增加参数数量，同时增加输出单元感受野的一种方法。\n\n空洞卷积通过给卷积核插入“空洞”来变相的增加其大小，如果在卷积核的每两个元素之间插入d-1个空洞，卷积核的有效大小维：\n$m'=m+ (m-1) * (d-1)$\n其中d称为膨胀率（Dilation Rate）。当d=1时卷积核维普通的卷积核。\n![不同膨胀率的卷积核](https://img-blog.csdnimg.cn/20190904160154440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"常见的五种神经网络(1)-前馈神经网络","url":"/2019/04/23/深度学习/常见的五种神经网络/常见的五种神经网络(1)-前馈神经网络/","content":"\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/100943664\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n\n该系列的其他文章：\n- [常见的五种神经网络(1)-前馈神经网络](https://blog.csdn.net/Gamer_gyt/article/details/89459131)\n- [常见的五种神经网络(2)-卷积神经网络](https://blog.csdn.net/Gamer_gyt/article/details/100531593)\n- [常见的五种神经网络(3)-循环神经网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/100600661)\n- [常见的五种神经网络(3)-循环神经网络(中篇)](https://blog.csdn.net/Gamer_gyt/article/details/100709422)\n- [常见的五种神经网络(3)-循环神经网络(下篇)](https://thinkgamer.blog.csdn.net/article/details/100943664)\n- [常见的五种神经网络(4)-深度信念网络(上篇)](https://blog.csdn.net/Gamer_gyt/article/details/103231385)\n- [常见的五种神经网络(4)-深度信念网络(下篇)](https://blog.csdn.net/Gamer_gyt/article/details/103437985)\n- [常见的五种神经网络(5)-生成对抗网络（上篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n- [常见的五种神经网络(5)-生成对抗网络（下篇）](https://blog.csdn.net/Gamer_gyt/article/details/103754752)\n\n\n> 给定一组神经元，我们可以以神经元为节点来构建一个网络。不同的神经网络模型有着不同网络连接的拓扑结构。一种比较直接的拓扑结构是前馈网络。前馈神经网络（Feedforward Neural Network，FNN）是最早发明的简单人工神经网络。\n\n<!--More-->\n\n# 介绍\n在前馈神经网络中，不同的神经元属于不同的层，每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。第0层叫做输入层，最后一层叫做输出层，中间的叫做隐藏层，整个网络中无反馈，信号从输入层到输出层单向传播，可用一个有用无环图表示。\n\n前馈神经网络也成为多层感知器（Mutlti-Layer Perceptron，MLP）。但是多层感知器的叫法并不准确，因为前馈神经网络其实是由多层Logistic回归模型（连续的非线性模型）组成，而不是有多层感知器模型（非连续的非线性模型）组成。\n\n下图为简单的前馈神经网络图：\n\n![多层前馈神经网络](https://img-blog.csdnimg.cn/20190422193716850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n神经网络中涉及的多个概念：\n- L：表示神经网络的层数\n- m^l：表示第 l 层神经元个数\n- f_l(.)：表示第 l 层神经元的激活函数\n- W^l：表示第 l-1 层到第 l 层的权重矩阵\n- b^l：表示第 l-1 层到第 l 层的偏置\n- z^l：表示第 l 层神经元的净输入（净活性值）\n- a^l：表示第l层的神经元输出（活性值）\n\n神经网络的信息传播公式如下（公式1-1）\n$$\nz^l  = W^l \\cdot a^{l-1} + b^l\n\\\\\na^l = f_l(z^l)\n$$\n\n公式1-1也可以合并写为（公式1-2）：\n$$\nz^l = W^l \\cdot f_{l-1}(z^{l-1}) + b^l\n$$\n或者（公式1-3）\n$$\na^l = f_l(W^l \\cdot a^{l-1} + b^l)\n$$\n这样神经网络可以通过逐层的信息传递，得到网络最后的输出a^L。整个网络可以看做一个符合函数\n$$\n\\phi (x; W,b)\n$$\n将向量x作为第一层的输入a^0，将第 l 层的输入a^0，将第L层的输出a^L 作为整个函数的输出。\n\n$$\nx = a^0 \\rightarrow z^1 \\rightarrow a^1 \\rightarrow z^2 .... \\rightarrow a^{L-1} \\rightarrow  z^L \\rightarrow  a^L = \\phi (x;W,b)\n$$\n\n其中W, b表示网络中所有层的连接权重和偏置。\n\n# 参数学习\n如果采用交叉熵损失函数，对于样本(x，y)，其损失函数为（公式1-4）：\n$$\nL(y,\\hat{y}) = -y^T log (\\hat{y})\n$$\n其中 y 属于{0,1}^T为标签y对应的one-hot向量。\n\n给定训练集D={(x^n,y^n)}, N >= n >=0，将每个样本x^n输入给前馈神经网络，得到网络输出为y^n，其在数据集D上的结构化风险函数为（公式1-5）：\n$$\nR(W,b)=\\frac{1}{N}\\sum_{n=1}^{N} L(y^n,\\hat{y}^n) + \\frac{1}{2}\\lambda \\left \\| W \\right \\|_F^2\n$$\n其中W和b分别表示网络中所有的权重矩阵和偏置向量， (||W||_F)^2是正则化项，用来防止过拟合，lambda是为正数的超参数，lambda越大，W越接近于0。这里的(||W||_F)^2一般使用Frobenius范数：\n$$\n\\left \\| W \\right \\|_F^2= \\sum_{l=1}^{L} \\sum_{i=1}^{m^l} \\sum_{j=1}^{m^{l-1}} (W_{ij}^l)^2\n$$\n有了学习准则和训练样本，网络参数可以通过梯度下降法来进行学习。在梯度下降方法的每次迭代过程中，第l层的参数 W^l 和 b^l 参数更新方式为（公式1-6）：\n$$\nW^l \\leftarrow W^l - \\alpha \\frac{\\partial R(W,b)}{\\partial W^l}\n=W^l - \\alpha ( \\frac{1}{N} \\sum_{n=1}^{N}(\\frac{\\partial L(y^n,\\hat{y}^n)}{\\partial W^l}) + \\lambda W^l )\n\\\\\nb^l \\leftarrow b^l - \\alpha \\frac{\\partial R(W,b)}{\\partial b^l}\n=b^l - \\alpha ( \\frac{1}{N} \\sum_{n=1}^{N}(\\frac{\\partial L(y^n,\\hat{y}^n)}{\\partial b^l}) )\n$$\n\n其中alpha为学习参数。\n\n梯度下降法需要计算损失函数对参数的偏导数，如果通过链式法则逐一对每个参数进行求偏导效率比较低。在神经网络的训练中经常使用反向传播算法来高效的计算梯度。\n\n# 反向传播算法\n\n基于误差的反向传播算法（backpropagation，BP）的前馈神经网络训练过程可以分为以下三步：\n- 前馈计算每一层的净输入z^l  和激活值 a^l，直到最后一层\n- 反向传播计算每一层的误差项\n- 计算每一层参数的偏导数，并更新参数\n\n其具体训练过程如下：\n\n![image](https://img-blog.csdnimg.cn/20190423152427560.png)\n\n# 自动梯度计算\n神经网络中的参数主要是通过梯度下降来进行优化的。当确定了风险函数及网络结构后，我们就可以手动用链式法则来计算风险函数对每个参数的梯度，并用代码进行实现。\n\n目前几乎所有的深度学习框架都包含了自动梯度计算的功能，在使用框架进行神经网络开发时，我们只需要考虑网络的结构并用代码实现，其梯度可以自动进行计算，无需人工干预，这样开发效率就大大提高了。\n\n自动梯度计算方法分为以下三种：\n\n## 数值微分\n数值微分（Numerical Differentiation）是用数值方法计算函数f(x)的导数。函数f(x)的点x的导数定义为：\n$$\nf'(x) = \\underset{\\Delta x \\rightarrow 0}{ lim } \\frac{ f(x + \\Delta x) -f(x) }{ \\Delta x }\n$$\n要计算f(x)在点x的导数，可以对x加上一个很少的非零扰动，然后通过上述定义来直接计算函数f(x)的梯度。数值微分方法非常容易实现，但找到一个合适扰动非常难，如果扰动过小会引起数值计算问题，比如**舍入误差**；如果扰动过大，会增加**截断误差**，使得导数计算不准确，因此数值微分的实用性比较差，在实际应用中，常用以下公式来计算梯度可以减少截断误差。\n$$\nf'(x) = \\underset{\\Delta x \\rightarrow 0}{ lim } \\frac{ f(x + \\Delta x) -f(x -\\Delta x) }{2 \\Delta x }\n$$\n- 舍入误差：是指数值计算中由于数字舍入造成的近似值和精确值之间的差异，比如用浮点数来表示实数。\n- 截断误差：数学模型的理论解与数值计算问题的精确解之间的误差\n\n\n## 符号微分\n符号微分（Symbolic Differentiation）是一种基于符号计算的自动求导方法。符号计算，也叫代数计算，是指用计算机来处理带有变量的数学表达式。\n\n符号计算的输入和输出都是数学表达式的化简、因式分解、微分、积分、解代数方程、求解常微分方程等运算。\n\n比如数学表达式的化简\n- 输入：3x-x+2x+1\n- 输出：4x+1\n\n符号计算一般来讲是对输入的表达式，通过迭代或递归使用一些事先定义的规则进行转换。当转换结果不能再继续使用变换规则时，便停止计算。\n\n## 自动微分\n自动微分（Automatic Differentiation，AD）是一种可以对一个（程序）函数进行计算导数的方法。符号微分的处理对象是数学表达式，而自动微分的处理对象是一个函数或一段程序。而自动微分可以直接在原始程序代码进行微分。自动微分的基本原理是所有的数值计算可以分解为一些基本操作，包含+,−,×, / 和一些初等函数exp, log, sin, cos 等。\n\n自动微分也是利用链式法则来自动计算一个复合函数的梯度。我们以一个神经网络中常见的复合函数的例子来说明自动微分的过程。为了简单起见，令复合函数f(x;w, b) 为\n\n$$\nf(x;w,b)=\\frac{1}{ exp(-(wx+b))+1 }\n$$\n其中x 为输入标量，w和b 分别为权重和偏置参数。\n\n复合函数f(x;w,b) 可以拆解为：\n\n![image](https://img-blog.csdnimg.cn/20190423161240321.png)\n\n继而就可以通过链式求导法则进行复合函数求导。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["常见的五种神经网络"],"categories":["技术篇"]},{"title":"神经网络中的激活函数介绍","url":"/2019/04/21/深度学习/神经网络笔记/神经网络中的激活函数介绍/","content":"\n> 人工神经元（Artifical Neuron）简称神经元（Neuron），是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接受一组输入信息并产出输出。\n\n<!--More-->\n\n# 激活函数（Activation Function）\n\n是神经元中非常重要的一部分，为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质：\n- 连续并可导的非线性函数，可导的激活函数可以直接利用数值优化的方法来学习网络参数。\n- 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。\n- 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和温度性。\n\n## Sigmoid型激活函数\n\nS型曲线函数，常见的Sigmoid函数有Logistic函数和tanh函数。\n\n> 知识点：对于函数f(x)，若x趋向于负无穷大，其导数f'(x)趋向于0，则称其为左饱和。若x趋向于正无穷大，其导数f'(x)趋向于0，则称其为右饱和。同时满足左右饱和时，称为两端饱和。\n\n- Logistic 函数\n$$\n\\sigma (x) = \\frac{1} { 1+ exp(-x)}\n$$\n- tanh函数\n    \n$$\ntanh(x) = \\frac{ exp(x)-exp(-x) }{ exp(x) + exp(-x) }\n$$\n\ntanh函数可以看作是放大并平移的Logistic函数，其值域是(-1，1)。\n$$\ntanh(x) = 2 \\sigma(2x) - 1\n$$\n\ntanh函数的输出是零中心化的（Zero-Centered），而Logistic函数的输出值恒大于0。非零中心化的输出会使得最后一层的神经元的输入发生位置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。\n\n![image](https://img-blog.csdnimg.cn/2019042122140374.jpg)\n\n\n## 修正线性单元\nRectified Linear Unit（ReLU）也叫rectifier函数，是目前深层神经网络中经常使用的激活函数。ReLU实际上是一个斜坡函数，定义为：\n$$\nReLU(x) = \\begin{cases}\nx & \\text{ if } x \\geq 0 \\\\ \n0 & \\text{ if } x < 0\n\\end{cases}\n= max(0,x)\n$$\n\n---\n\nReLU的优缺点：\n- 优点\n> 采用ReLU的神经元只需要进行加，乘，和，比较的操作，计算上更加高效。Sigmoid型激活函数会导致一个非稀疏的神经网络，而ReLU却具有很好的稀疏性，大约50%的神经元会处于激活状态。\n\n> 在优化方面，由于Sigmoid型函数的两端饱和，ReLU函数为左饱和函数，且在x>0时导数为1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。\n\n- 缺点\n> ReLU的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。此外ReLU神经元在训练时比较容易死亡。在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。这种现象称为死亡ReLU问题（Dying ReLU Problem），并且也有kennel会发生在其他隐藏层。\n\n---\n在实际使用中，为了避免上述情况，有集中ReLU的变种也会被广泛使用。\n\n### 带泄漏的ReLU\n带泄漏的ReLU在输入x<0时，保持一个很小的梯度 lambda。这样当神经元非激活时也能又一个非零的梯度可以更新参数，避免永远不能被激活。带泄漏的ReLU的定义如下：\n$$\nLeakyReLU(x) = \\begin{cases}\nx & \\text{ if } x > 0 \\\\ \n\\gamma x & \\text{ if } x  \\leq 0\n\\end{cases}\n= max(0,x) + \\gamma min(0,x)\n$$\n其中 gamma是一个很小的常数，比如0.01。当gamma < 1时，带泄漏的ReLU也可以写为：\n$$\nLeakyReLU(x) = max(x, \\gamma x)\n$$\n相当于是一个比较简单的maxout单元。\n\n### 带参数的ReLU\n\n带参数的ReLU引入一个可学习的参数，不同神经元可以有不同的参数，对于第i个神经元，其PReLU的定义为：\n\n$$\nPReLU(x) = \\begin{cases}\nx & \\text{ if } x > 0 \\\\ \n\\gamma _ix & \\text{ if } x  \\leq 0\n\\end{cases}\n= max(0,x) + \\gamma_imin(0,x)\n$$\n其中γi为x≤0时函数的斜率。因此，PReLU是非饱和函数。如果γi =0，那 么 PReLU 就退化为 ReLU。如果 γi 为一个很小的常数，则 PReLU 可以看作带 泄露的 ReLU。PReLU 可以允许不同神经元具有不同的参数，也可以一组神经 元共享一个参数。\n\n---\n\n### ELU\n指数线性单元（Exponential Linear Unit）是一个近似的零中心化的非线性函数，其定义为：\n$$\nELU(x) = \\begin{cases}\nx & \\text{ if } x > 0 \\\\ \n\\gamma (exp(x)-1) & \\text{ if } x  \\leq 0\n\\end{cases}\n= max(0,x) + min(0,\\gamma(exp(x)-1))\n$$\n其中 γ ≥ 0是一个超参数，决定x ≤ 0时的饱和曲线，并调整输出均值在0附\n近。\n\n---\n\n#### Softplus函数\nSoftplus函数可以看作是rectifier函数的平滑版本，其定义为：\n$$\nSoftplus(x) = log(1 + exp(x))\n$$\nSoftplus函数及其导数刚好是Logistic函数。Softplus函数虽然也具有单侧抑制，宽兴奋边界的特征，却没有稀疏激活性。\n\n下图为几种激活函数的示例：\n\n![激活函数对比](https://img-blog.csdnimg.cn/20190421213409642.jpg)\n\n## Swish函数\n\nSwish函数是一种自门控（self-Gated）激活函数，其定义为：\n$$\nswish(x) = x \\sigma (\\beta x)\n$$\n其中 sigma(.)为logistic函数，beta为可学习的参数或一个固定超参数。 sigma(.) 属于 (0,1)可以看做是一种软性的门控机构。当sigma(beta x)接近于1时，门处于“开”状态，激活函数的输出近似于x本身；当sigma(beta x)接近于0时，门的状态为“关”，激活函数的输出近似于0。\n\n下图为Swish函数的示例：\n![image](https://img-blog.csdnimg.cn/20190421214743755.jpg)\n\n当 beta=0时，Swish函数变成线性函数 x/2。 当 beta=1时，Swish 函数在 x>0时近似线性，在x < 0时近似饱和，同时具有一定的非单调性。当beta趋向于正无穷大时， sigma(beta x)趋向于离散的0-1函数，Switch函数近似为ReLU函数。\n\n因此Swish函数可以看作时线性函数和ReLU函数之间的非线性插值函数，其程度由参数beta控制。\n\n---\n\n### Maxout单元\nMaxout单元也是一种分段线性函数。Sigmoid型函数， ReLU等激活函数的输入是神经元的净输入z，是一个标量。而maxout单元的输入是上一层神经元的全部原始输入，是一个向量x=[x1;x2;...;x_d]。\n\n每个maxout单元有K个权重向量w_k 属于 R^d 和偏置 b_k（1 <= k <= K）。对于输入x，可以得到K个净输入z_k，1 <= k <=K。\n$$\nz_k = w^T_kx + b_k\n$$\n其中\n$$\nw_k = [w_{k,1},w_{k,2},...,w_{k,d}]^T\n$$\n为第k个权重向量。\nMaxout单元的非线性函数定义为：\n$$\nmaxout(x) = \\underset{k\\in [1,K]}{max} (z_k)\n$$\nMaxout单元不单是净输入到输出之间的非线性映射，而是整体学习输入到输出之间的非线性映射关系。Maxout激活函数可以看作任意凸函数的分段线性近似，并且在有限的点上是不可微的。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"线性模型篇之SVM数学公式推导","url":"/2019/04/21/机器学习/线性模型篇之SVM数学公式推导/","content":"\n> 支持向量机（Support Vector Machine，SVM）是一个经典两类分类算法，其找到的分割超平面具有更好的鲁棒性，因此广泛使用在很多任务上，并表现出了很强优势。\n\n<!--More-->\n\n# 介绍\n给定一个两分类数据集D={(x^n, y^n)}，n属于N，其中y_n 属于{+1,-1}，如果两类样本是线性可分的，即存在一个超平面（公式-1）\n$$\nw^Tx + b =0\n$$\n将两类样本分开，那么对于每个样本都有\n$$\ny^n(w^Tx^n + b) > 0\n$$\n\n数据集D中的每个样本x^n 到分隔超平面的距离为：\n$$\n\\gamma ^n = \\frac{\\left \\| w^Tx^n +b \\right \\|}{ \\left \\| w \\right \\|} = \\frac{y^n(w^Tx^n + b)}{ \\left \\| w \\right \\| }\n$$\n\n我们定义整个数据集D中所有样本到分隔超平面的最短距离为间隔（Margin）（公式-2）\n$$\n\\gamma = \\underset{n}{min} \\gamma ^ n\n$$\n如果间隔 gamma越大，其分隔超平面对两个数据集的划分越稳定，不容易受噪声等因素影响，支持向量机的目的是找到一个超平面(w^* , b^ *)使得gamma最大，即（公式-3）\n$$\n\\underset{w,b}{max} \\qquad \\gamma \n\\\\\ns.t.  \\qquad  \\frac{y^n (w^Tx^n + b)}{\\left \\| w \\right \\|} \\geq \\gamma,\\forall_n\n$$\n令 \n$$\n\\left \\| w \\right \\| . \\gamma =1\n$$\n则（公式-3）等价于（公式-4）\n$$\n\\underset{w,b}{max} \\qquad \\frac{1}{ \\left \\| w \\right \\| ^2} \n\\\\\ns.t.  \\qquad  y^n(w^Tx^n + b) \\geq 1, \\forall_n\n$$\n数据集中所有满足 y^n (w^T x^n +b) =1 的样本点，都称为支持向量（support vertor）\n\n对于一个线性可分数据集，其分隔的超平面有多个，但是间隔最大的超平面是唯一的。下图给定了支持最大间隔分隔超平面的示例，其红色样本点为支持向量。\n\n<center>\n\n![支持向量机示例](https://img-blog.csdnimg.cn/20190417110614374.png)\n\n</center>\n\n# 参数学习\n\n**凸函数 & 凹函数**\n关于凹凸函数的定义和性质可以参考下图：\n<center>\n\n![image](https://img-blog.csdnimg.cn/20190420165249483.jpg)\n</center>\n\n为了找到最大间隔分割超平面，将公式-4改写为凸优化问题（公式-5）：\n$$\n\\underset{w,b}{min} \\qquad \\frac{1}{ 2} {\\left \\| w \\right \\| ^2} \n\\\\\ns.t.  \\qquad  1-y^n(w^Tx^n + b) \\leq 0, \\forall n\n$$\n使用拉格朗日乘数法，公式-5的拉格朗日函数为（公式-6）：\n$$\n\\Lambda (w,b,\\lambda )=\\frac{1}{2} \\left \\| w^2 \\right \\| + \\sum_{n-1}^{N} \\lambda _n( 1-y^n(w^Tx^n + b) )\n$$\n其中\n$$\n\\lambda _1 \\geq 0,...,\\lambda _N \\geq 0\n$$\n为拉格朗日乘数。计算公式-6关于w和b的导数，并令其等于0得到（公式-7）\n$$\nw = \\sum_{n=1}^{ N }\\lambda _n y^nx^n\n$$\n和（公式-8）\n$$\n0 = \\sum_{n=1}^{N} \\lambda _n y^n\n$$\n将公式-7代入公式-6，并利用公式-8，得到拉格朗日对偶函数（公式-9）：\n$$\n\\Gamma(\\lambda) = -\\frac{1}{2} \\sum_{n=1}^{N}\\sum_{m=1}^{N} \\lambda_n \\lambda_m y^m y^n (x^m)^Tx^n + \\sum_{n=1}^{N}\\lambda_n\n$$\n支持向量机的主优化问题为凸优化问题，满足强对偶性，即主优化问题可以通过最大化对偶函数\n$$\nmax_{\\lambda \\geq 0} \\Gamma(\\lambda)\n$$\n对偶函数 Gamma(lambda)是一个凹函数，因此最大化对偶数是一个凸优化问题，可以通过多种凸优化方法进行求解，得到拉格朗日乘数的最优值 lambda^* 。但由于其约束条件的数量为训练样本数量，一般的优化方法代价比较高，因此在实践中通常采样比较高效的优化方法，比如SMO(Sequential Minimal Optimization)算法等。\n\n根据KKT条件中的互补松弛条件，最优解满足(公式-10)\n$$\n \\lambda_n ^*(1-y^n(w^{*T}x^n+b^*))=0\n$$\n如果样本x^n 不在约束边界上，(lambda_n)^*，其约束失效；如果样本x^n在约束边界上，(lambda_n)^* >=0。这些在约束边界上的样本点称为支持向量（support vector），即离决策平面距离最近的点。\n\n再计算出 lambda^*后，根据公式-7计算出最优权重w^*，最优偏置b^*可以通过任选一个支持向量(x,y)计算得到（公式-11）\n$$\nb^* = \\tilde{y} - w^{*T}\\tilde{x}\n$$\n最优参数的支持向量机的决策函数为（公式-12）\n$$\nf(x)=sgn(w^{*T}x+b^*)=sgn(\\sum_{n=1}^{N} \\lambda_n^* y^n(x^n)^Tx + b^* )\n$$\n支持向量机的决策函数只依赖 lambda_n^*>0的样本点，即支持向量。\n\n支持向量机的目标函数可以通过SMO等优化方法得到全局最优解，因此比其他分类器的学习效率更高。此外，支持向量机的决策函数只依赖与支持向量。与训练样本总数无关，分类速度比较快。\n\n# 核函数\n\n支持向量机还有一个重要的优点是可以使用核函数(kernal)隐式的将样本从原始特征空间映射到更高维的空间，并解决原始特征空间中的线性不可分问题。比如在一个变换后的特征空间中，支持向量机的决策函数为（公式-13）\n$$\nf(x)=sgn(w^{*T} \\phi(x)+b^*)=sgn(\\sum_{n=1}^{N} \\lambda_n^* y^n K(x^n,x) + b^* )\n$$\n其中\n$$\nK(x,z)=\\phi(x)^T \\phi(z) \n$$\n为核函数，通常不需要显式的给出φ(x)的具体形式，可以通过核技巧(kernel trick)来构造。比如以x,z属于R^2为例，我们可以构造一个核函数（公式-14）\n$$\nK(x,z)=(1+x^Tz)^2=\\phi(x)^T\\phi(z)\n$$\n来隐式的计算x,z在特征空间φ中的内积，其中:\n$$\n\\phi(x)=[1,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_1x_2,x_1^2,x_2^2]^T\n$$\n# 软间隔\n\n在支持向量机的优化问题中，约束条件比较严格。如果训练集中的样本在特征空间中不是线性可分的，就无法找到最优解。为了能够容忍部分不满足约束的样本，我们可以引入松弛变量，将优化问题变为（公式-15）：\n$$\n\\underset{w,b}{min} \\qquad \\frac{1}{ 2} {\\left \\| w \\right \\| ^2} + C \\sum_{n=1}^{N}\\xi _n\n\\\\\ns.t.  \\qquad  1-y^n(w^Tx^n + b) -\\xi _n \\leq 0, \\forall n\n\\\\\n\\xi _n \\geq 0, \\forall n\n$$\n其中参数C>0用来控制间隔和松弛变量惩罚的平衡，引入松弛变量的间隔称为软间隔（soft margin）。公式-15也可以表示为经验风险+正则化项的形式（公式-16）。\n$$\n\\underset{w,b}{min} \\qquad \\sum_{n=1}^{N}max(0,1-y^n(w^Tx^n + b)) + \\frac{1}{C}.\\frac{1}{2}\\left \\| w \\right \\|^2\n$$\n其中\n$$\nmax(0,1-y^n(w^Tx^n + b))\n$$\n称为hinge损失函数，1/C可以看作是正则化系数。软间隔支持向量机的参数学习和原始支持向量机类似，其最终决策函数也只和支持向量有关，即满足\n$$\n1-y^n(w^Tx^n + b) - \\xi_n = 0\n$$\n的样本。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"线性模型之PLA数学公式推导","url":"/2019/04/16/机器学习/线性模型之PLA数学公式推导/","content":"> 本文主要介绍线性模型之PLA数学公式推导。\n\n<!--More-->\n\n# 介绍\n感知机（Perceptron）是一种广泛使用的线性分类器，相当于最简单的人工神经网络，只有一个神经元。其全称是PLA（Perceptron Linear Algorithm），线性感知机算法。\n\n感知机是对生物神经元的简单数学模型，有与生物神经元相对应的部件，比如权重（突触）、偏置（阈值）及激活函数（细胞体），输出值为 +1 或者 -1。\n\n$$\n\\hat{y} = sgn(w^Tx)\n$$\n\n对于二分类问题，可以使用感知机算法来解决。PLA的原理是逐点解决，首先在超平面上随意取一条分类面，统计分类错误的点，然后随机对某个错误点修正，即变换直线的位置，使该错误点被修正，接着再随机选取另外一个错误点进行修正，分类面不断变化，直到所有点都分类正确了，就得到了最佳分类面。\n\n利用二维平面进行解释，第一种情况是错误的将正样本（y=1）分类为负样本（y=-1）。此时wx<0，即w与x的夹角大于90度，分类线L的两侧。修正的方法是让夹角变小，修正w值，使二者位于直线同侧。\n\n$$\nw:=w+x=w+yx\n$$\n\n修正过程如下：\n![修正过程](https://img-blog.csdnimg.cn/20190415193607475.jpg)\n\n第二种情况就是错误的将负样本（y=-1）分类为正样本（y=1）。此时，wx>0，即w与x的夹角小于90度，分类线L的同一侧。修正的方法是让夹角变大，修正w值，使二者位于分类线同侧。\n\n$$\nw:=w-x=w+yx\n$$\n修正过程如下：\n![修正过程](https://img-blog.csdnimg.cn/20190415193855372.jpg)\n\n经过上边两种情况分析，PLA每次更新参数w的表达式是一致的，掌握了每次w的优化表达式，那么PLA就能不断地将所有错误的分类样本纠正并分类正确。\n\n# 参数学习\n给定N个训练集样本{(x^n, y^n)},n<=N，其中y^n 属于{+1,-1}，感知机试图学习到参数w*，使得对于每个样本(x^n,y^n)\n有：\n\n$$\ny^n w^{*T}x^n>0,\\forall n\\in [1,N]\n$$\n感知机算法是一种错误驱动的在线学习算法，先初始化一个权重向量w<-0（通常是全零向量），然后每次分错一个样本（x,y）时，就用这个样本来更新权重。\n\n$$\nw \\leftarrow w + yx\n$$\n\n具体的感知机算法伪代码如下（==算法-1==）：\n![感知机算法伪代码](https://img-blog.csdnimg.cn/20190415202606770.png)\n\n根据感知器的学习策略，可以反推出感知器的损失函数为：\n$$\nL (w;x,y)=max(0, -yw^Tx)\n$$\n采用随机梯度的下降，其每次更新的梯度为：\n$$\n\\frac{\\partial L (w;x,y) }{ \\partial w}=\\begin{cases}\n0 & \\text{ if } y^Tx>0 \\\\ \n-yx & \\text{ if } y^Tx<0 \n\\end{cases}\n$$\n下图给出了感知机参数学习的过程，其中红色实心为正例，蓝色空心点为负例。黑色箭头表示权重向量，红色虚线箭头表示权重的更新方向。\n\n![感知机参数学习的过程](https://img-blog.csdnimg.cn/20190415203526997.png)\n\n# 感知机的收敛\nNovikoff证明对于两类问题，如果训练集是线性可分的，那么感知器\n算法可以在有限次迭代后收敛。然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。\n\n当数据集是两类线性可分时，对于数据集D={(x^n,y^n)}，n属于N，其中x^n为样本的增广特征向量，y^n属于{-1，+1}，那么存在一个正的常数r(r>0)和权重向量w，并且||w*||=1，对所有n都满足(w^*)(y^n x^n)>r。\n\n可以证明如下定理（定理-1）。\n\n---\n\n给定一个训练集\n$$\nD={(x^n,y^n)},n\\in {1,N}\n$$\n假设R是训练集中最大的特征向量的模\n$$\nR=\\underset{n}{max} \\left \\| x^n \\right \\| \n$$\n如果训练集D线性可分，感知机学习算法-1的权重更新次数不超过 R^2/ r^2\n\n---\n\n证明：\n感知机算法的权重更新方式为（==公式-1==）：\n$$\nw_k = w_{k-1}+y^kx^k\n$$\n其中x^k, y^k表示第k个错误分类的条件。\n因为初始权重为0，在第K次更新时感知器的权重向量为（==公式-2==）：\n$$\nw_k = =\\sum_{k=1}^{K}y^kx^k\n$$\n分别计算||w||^2的上下界：\n\n**计算其上界**（公式-2）：\n$$\n\\left \\| w_k^2 \\right \\| \n$$\n$$\n= \\left\\| w_{K_1} + y^K x^K \\right \\|^2\n$$\n$$\n= \\left \\|  w_{K-1} \\right \\| ^2 + \\left \\| y^Kx^K \\right \\| ^2+2y^Kw_{K-1}x^K \n$$\n$$\n\\leqslant \\left \\| w_{K-1} \\right \\|^2 + R^2\n$$\n$$\n\\leqslant\\left \\| w_{K-2} \\right \\|^2+2R^2\n$$\n$$\n\\leqslant KR^2\n$$\n\n**计算其下界**（公式-3）：\n$$\n\\left \\| w_k^2 \\right \\| \n$$\n$$\n=\\left \\| w^* \\right \\|^2 .\\left \\| w_K \\right \\|^2\n$$\n$$\n\\geqslant \\left \\| w^{*T}w_K \\right \\| ^2\n$$\n$$\n=\\left \\| w^{*T}\\sum_{k=1}^{K}(y^Kx^K) \\right \\|^2\n$$\n$$\n=\\left \\| \\sum_{k=1}^{K}w^{*T}(y^Kx^K) \\right \\|^2\n$$\n$$\n\\geq K^2r^2\n$$\n==附==：两个向量内积的平方一定小于等于这两个向量的模的乘积。\n\n由公式-2和公式-3得到（公式-4）\n$$\nK^2r^2 \\leq \\left \\| w_K \\right \\|^2\\leq KR^2\n$$\n取最左和最右的两项，进一步得到K^2r^2 <= K^2R^2，然后两边同时除以K，最终得到（公式-5）：\n$$\nK\\leq \\frac{R^2}{r^2}\n$$\n因此在线性可分的情况下，算法-1会在R^2 / r^2步内收敛。\n\n虽然感知机线性模型在线性可分的数据上可以保证收敛，但其存在以下不足：\n- 在数据集线性可分时，感知器虽然可以找到一个超平面把两类数据分开， 但并不能保证能其泛化能力。\n- 感知器对样本顺序比较敏感。每次迭代的顺序不一致时，找到的分割超平 面也往往不一致。\n- 如果训练集不是线性可分的，就永远不会收敛。\n\n# 参数平均感知机\n根据定理3.1，如果训练数据是线性可分的，那么感知器可以找到一个判别 函数来分割不同类的数据。如果间隔 γ 越大，收敛越快。但是感知器并不能保 证找到的判别函数是最优的(比如泛化能力高)，这样可能导致过拟合。\n\n感知机的学习到的权重向量和训练样本的顺序相关。在迭代次序上排在后 面的错误样本，比前面的错误样本对最终的权重向量影响更大。比如有 1, 000 个 训练样本，在迭代 100 个样本后，感知器已经学习到一个很好的权重向量。在 接下来的 899 个样本上都预测正确，也没有更新权重向量。但是在最后第 1, 000 个样本时预测错误，并更新了权重。这次更新可能反而使得权重向量变差。\n\n为了改善这种情况，可以使用“参数平均”的策略提高感知ji的鲁棒性，也叫投票感知机。\n\n投票感知机记录第k次更新参数之后的权重w_k在之后的训练过程中正确分类样本的次数c_k。这样最后的分类器形式为（公式-6）：\n$$\n\\hat{y} = sgn(\\sum_{k=1}^{K}c_ksgn(w_k^Tx))\n$$\n其中sgn(.)为符号函数。\n\n投票感知机虽然提高了模型的泛化能力，但是需要保存K个权重向量。在实际的操作中会带来额外的开销。因此经常会使用一个简化的版本，即平均感知机。其表达式如下（公式-7）：\n$$\n\\hat{y} = sgn(\\sum_{k=1}^{K}c_k (w_k^Tx))\n=sgn( (\\sum_{k=1}^{K}c_k w_k )^Tx )\n=sgn( \\bar{w}^Tx)\n$$\n其中 \n$$\n\\bar{w}\n$$\n为平均的权重向量。\n\n假设w_{t,n}是在第t轮更新到第n个样本时的权重向量值，平均的权重向量也可以表示为（公式-8）：\n$$\n\\bar{w} = \\frac{\\sum_{t=1}^{T} \\sum_{n=1}^{N}w_{t,n}}{nT}\n$$\n这个方法实现简单，只需要在算法-1中增加一个平均向量，并且在处理每一个样本后，进行更新（公式-9）：\n$$\n\\bar{w} = \\bar{w}+ w_{t,n}\n$$\n但这个方法需要在处理每一个样本时都要更新平均权重，因为\n$$\n \\bar{w} ,w_{t,n}\n$$\n都是稠密向量，因此更新操作比较费时。为了提高迭代速度，有很多改进的办法，让这个更新只需要在错误预测时才进行。下图给了一个改进的平均感知机算法的训练过程（算法-2）。\n![改进的平均感知机算法的训练过程](https://img-blog.csdnimg.cn/20190416184043264.png)\n\n# 扩展到多分类\n\n原始的感知机是二分类模型，但也很容易的扩展到多分类，甚至是更一般的结构化学习问题。\n\n之前介绍的线性分类模型中，分类函数都是在输入x的特征空间上。为了使得感知机可以处理更加复杂的输出，我们引入一个构建输入输出联合空间上的特征函数，将样本(x,y)对映射到一个特征向量空间。\n\n在联合特征空间中，我们可以建立一个广义的感知机模型（公式-10）：\n$$\n\\hat{y} = \\underset{y\\in Gen(x)}{ arg max} w^T \\phi(x,y)\n$$\n其中w为权重向量，Gen(x)表示输入x所有的输出目标集合。当处理C类分类问题时，Gen(x)={1,....,C}\n\n在C分类中，一种常用的特征函数（公式-11）\n$$\n\\phi(x,y)\n$$\n是y和x的内积，其中y为类别的one-hot向量表示（公式-12）。\n$$\n \\phi(x,y) = vec(yx^T)\\in R^{(d\\times C)}\n$$\n其中vec是向量化算子。\n\n给定样本(x,y)，若\n$$\nx \\in R^d\n$$\ny为第c维为1的one-hot向量，则：\n![y为第c维为1的one-hot向量](https://img-blog.csdnimg.cn/20190416190108143.png)\n\n广义感知器算法的训练过程如算法-3所示：\n\n![广义感知器算法的训练过程](https://img-blog.csdnimg.cn/20190416190225604.png)\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"线性模型篇之softmax数学公式推导","url":"/2019/04/07/机器学习/线性模型篇之softmax数学公式推导/","content":"\n> Softmax回归也称多项（multinomial）或者多类（multi-class）的Logistic回归，是Logistic回归在多类分类问题上的推广。和逻辑回归一样属于线性模型。\n\n<!--More-->\n\n# SoftMax回归简介\n对于多类问题，类别标签\n\n\n$$\ny \\in {1,2,3,...,C} \n$$\n\n可以用C个取值，给定一个样本x，softmax回归预测的是属于类别c的概率为(公式-1)：\n\n$$\np(y=c|x)=softmax(w_c^Tx)=\\frac{exp(w_c^Tx)}{\\sum_{c=1}^{C}exp(w_c^Tx)}\n$$\n\n其中w_c是第c类的权重向量。\n\nsoftmax回归的决策函数可以表示为(公式-2)：\n\n$$\n\\hat{y}=  \\underset{c=1}{ \\overset{C}{arg max} } \\ p(y=c|x) =\\underset{c=1}{ \\overset{C}{arg max} } \\ w_c^T x \n$$\n\n---\n\nsoftMax与Logistic回归的关系：\n\n当类别个C=2时，softMax回归的决策函数为(公式-3)：\n\n\n$$\n\\hat{y} = \\underset{y\\in {0,1}}{ arg max } \\ w_y^Tx=I(w_1^Tx - w_0^Tx >0 )=I((w_1 - w_0)^Tx >0 )\n$$\n\n其中I(.)是指示函数，对比二分类决策函数(公式-4)\n\n\n$$\ng(f(x,w))=sgn(f(x,w))=\\begin{cases}\n & +1 \\text{ if } f(x,w)>0 \\\\ \n & -1 \\text{ if } f(x,w)<0 \n\\end{cases}\n$$\n\n其中sgn表示符号函数(sign function)，可以发现两类分类中的权重向量w=w1-w0\n\n---\n\n向量表示：\n\n公式-1用向量形式可以写为(公式-5)\n\n\n$$\n\\hat{y}=softmax(W^Tx)=\\frac{erp(W^Tx)}{1^Texp(W^Tx)}\n$$\n\n其中W=[w_1,w_2,...,w_C]是由C个类的权重向量组成的矩阵，1为全1的向量，\n\n\n$$\n\\hat{y}\\in  R^C\n$$\n\n为所有类别的预测条件概率组成的向量，第c维的值是第c类的预测条件概率。\n\n# 参数学习\n给定N个训练样本{(x^n, y^n)},n<=N，softmax回归使用交叉熵损失函数来学习最优的参数矩阵W。\n\n这里用C维的one-hot向量\n\n\n$$\ny \\in {0,1} ^C\n$$\n\n来表示类别标签，其向量表示为(公式-6)：\n\n$$\ny = [I(1=c),I(2=c),...,I(C=c)]^T\n$$\n其中I(.)为指示函数。\n\n采用交叉熵损失函数，softmax的经验风险函数为(公式-7)：\n\n$$\nR(W)=-\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{c=1}^{C}y_c^nlog\\hat{y}_c^n\nR(W)=-\\frac{1}{N}\\sum_{n=1}^{N} (y^n)^Tlog\\hat{y}^n\n$$\n\n其中\n\n$$\n\\hat{y}^n = softmax(W^Tx^n)\n$$\n\n为样本x^n在每个类别的后验概率。\n\n==说明：公式-7第一个式变换到第二个式是因为y_c类别中只有一个为1，其余为0，所以将第二个求和去除。==\n\n风险函数R(W)关于W的梯度为(公式-8)：\n\n$$\n\\frac{\\partial R(W)}{\\partial W} = -\\frac{1}{N}\\sum_{n=1}^{N}x^n(y^n-\\hat{y}^n)^T\n$$\n\n==**证明：**==\n\n计算公式-8中的梯度，关键在于计算每个样本的损失函数\n\n$$\nL^n(W)=-(y^n)^Tlog\\hat{y}^n\n$$\n\n关于参数W的梯度，其中需要用到两个导数公式为：\n- 若y=softmax(z)，则\n\n$$\n\\frac{\\partial y}{\\partial z}=diag(y)-yy^T\n$$\n\n- 若\n\n\n$$\nz=W^Tx=[w_1^Tx,w_2^Tx,...,w_C^Tx]^T\n$$\n\n则\n\n$$\n\\frac{\\partial y}{\\partial w_c}\n$$\n\n为第c列为x，其余为0的矩阵。\n\n$$\n\\frac{\\partial z}{\\partial w_c} = [ \\frac{\\partial w_1^Tx}{\\partial w_c},\\frac{\\partial w_2^Tx}{\\partial w_c},...,\\frac{\\partial w_C^Tx}{\\partial w_c} ]\n=[0,0,..,x,...,0]\n=M_c(x)\n$$\n\n根据链式法则，\n\n$$\nL^n(W) = -(y^n)^T log\\hat{y}^n\n$$\n\n关于w_c的偏导数为(公式-12)：\n\n$$\n\\frac{\\partial L^n(W) }{\\partial w_c}\n= -\\frac{ \\partial ((y^n)^T log \\hat{y}^n) }{\\partial w_c}\n$$\n$$\n= -\\frac{\\partial z^n}{ \\partial w_c } \\frac{\\partial \\hat{y}^n}{ \\partial z^n }\\frac{\\partial log \\hat{y}^n}{ \\partial \\hat{y}^n } y^n\n$$\n$$\n=-M_c(x^n)(diag(\\hat{y}^n)-\\hat{y}^n(\\hat{y}^n)^T)(diag(\\hat{y}^n))^{-1} y^n\n$$\n$$\n=-M_c(x^n)(I-\\hat{y}^n1^T)y^n\n$$\n$$\n=-M_c(x^n)(y^n-\\hat{y}^n1^Ty^n)\n$$\n$$\n=-M_c(x^n)(y^n-\\hat{y}^n)\n$$\n$$\n=-x^n[y^n-\\hat{y}^n]_c\n\n$$\n公式-12也可以表示为非向量形式(公式-13)：\n\n$$\n\\frac{\\partial L^n(W) }{\\partial w_c}= -x^n(I(y^n=c)-\\hat{y}_c^n)\n\n$$\n\n其中I(.)为指示函数，根据公式-12可以得到(公式-14)\n\n$$\n\\frac{\\partial L^n(W) }{\\partial W} = -x^n(y^n-\\hat{y}^n)^T\n$$\n\n采用梯度下降法，softmax回归的训练过程为：初始化W_0 <- 0，然后通过下式进行迭代更新。\n\n$$\nW_{t+1} = W_t + \\alpha (\\frac{1}{N} \\sum_{n=1}^{N}x^n(y^n - \\hat{y}_{W_t} ^ n)^T)\n$$\n\n其中a是学习率，\n\n$$\n\\hat{y}_{W_t}^n\n$$\n是当参数为W_t时，softmax回归模型的输出。\n\n---\n\n**注意：**\n> softmax回归中使用的C个权重向量是冗余的，即对所有权重向量都减去一个同样的向量v，不改变其输出结果。因此，softmax往往需要正则化来约束参数。此外，可以利用这个特性来避免计算softmax函数时在数值计算上溢出问题。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"线性模型篇之Logistic Regression数学公式推导","url":"/2019/04/02/机器学习/线性模型篇之Logistic Regression数学公式推导/","content":"\n> 本文主要介绍 线性模型篇之Logistic Regression数学公式推导。\n<!--More-->\n\n# 两分类与多分类\n\n- 两类分类（Binary Classification）\n  - 类别标签y只有两种取值，通常设为{0，1}\n  - 线性判别函数，即形如 y = w^T*x + b\n  - 分割超平面（hyper plane）,由满足f(w,x)=0的点组成\n  - 决策边界（Decision boundary）、决策平面（Decision surface）：即分分割超平面，决策边界将特征空间一分为二，划分成两个区域，每个区域对应一个类别。\n  - 有向距离（signed distance）\n- 多样分类（Multi-class Classification）\n  - 分类的类别个数大于2，多分类一般需要多个线性判别函数，但设计这些判别函数有很多方式。eg：\n    - 一对其余：属于和不属于\n    - 一对一\n    - argmax（改进的一对其余）：属于每个类别的概率，找概率最大值\n  - 参考：[多分类实现方式介绍和在Spark上实现多分类逻辑回归](https://blog.csdn.net/Gamer_gyt/article/details/86378882)\n\n# Logistic回归\n## LR回归\nLogistic回归（Logistic Regression，LR）是一种常见的处理二分类的线性回归模型。\n\n为了解决连续的线性回归函数不适合做分类的问题，引入函数g：R^d -> (0,1)来预测类别标签的后验概率p(y=1 | x)\n\n其中g(.)通常称为激活函数（activation function），其作用是把线性函数的值域从实数区间“挤压”到了（0，1）之间，可以用概率表示。在统计文献中，g(.)的逆函数g(.)^-1也称为联系函数（Link Function）\n\n在逻辑回归中使用Logistic作为激活函数，标签y=1的后验概率为(公式-1)：\n$$\np(y=1 | x) = \\sigma (w^T x) \n$$\n$$\np(y=1 | x)= \\frac{1}{1+exp(-w^T x)}\n$$\n\n标签 y=0的后验概率为(公式-2)：\n$$\np(y=0 | x) =1-p(y=0 | x)\n$$\n$$\np(y=0 | x)= \\frac{exp(-w^T x)}{1+exp(-w^T x)}\n$$\n将公式-1进行等价变换，可得(公式-3)：\n$$\nw^T x = log \\frac{p(y=1 | x)}{1-p(y=1 | x)} \n$$\n$$\nw^T x = log \\frac { p(y=1 | x)}{p(y=0|x)}\n$$\n其中\n$$\n\\frac { p(y=1 | x)}{p(y=0|x)}\n$$\n为样本x正反例后验概率的比例，称为几率（odds），几率的对数称为对数几率（log odds或者logit），公式-3中第一个表达式，左边是线性函数，logistic回归可以看做是预测值为“标签的对数几率”的线性回归模型，因为Logistic回归也称为对数几率回归（Logit Regression）。\n\n附公式-1到公式-3的推导：\n$$\np(y=1 | x)= \\frac{1}{1+exp(-w^T x)} \n$$\n$$\n=> exp(-w^Tx) = \\frac{1-p(y=1 | x)}{p(y=1 | x)} \n$$\n$$\n=> - w^T x = log \\frac{1- p(y=1 | x)}{p(y=1 | x)}\n$$\n$$\n=>  w^T x = log (\\frac{1- p(y=1 | x)}{p(y=1 | x)})^{-1}\n$$\n$$\n=> w^T x = log \\frac{p(y=1 | x)}{1-p(y=1 | x)}\n$$\n$$\n=> w^T x = log \\frac{p(y=1 | x)}{p(y=0 | x)}\n$$\n\n## 参数学习\nLR回归采用交叉熵作为损失函数，并使用梯度下降法对参数进行优化。给定N个训练样本{x_i,y_i}，i<=N，使用LR对每个样本进行预测，并用输出x_i的标签为1的后验概率，记为y'_i(x)  (公式-4)\n$$\ny'_i(x) = \\sigma(w^Tx_i),i\\in N\n$$\n由于y_i属于{0，1}，样本{x_i,y_i}的真实概率可以表示为(公式-5)：\n$$\np_r(y_i =1 | x_i) = y_i\n$$\n$$\np_r(y_i =0 | x_i) = 1- y_i\n$$\n使用交叉熵损失函数，其风险函数为(公式-6)：\n$$\nR(w)= - \\frac{1}{N}\\sum_{n=1}^{N} (p_r(y_i =1 | x_i) log(y_i') + p_r(y_i =0 | x_i) log(1-y_i') )\n$$\n$$\n= - \\frac{1}{N}\\sum_{n=1}^{N} ( y_i log(y_i') + (1-y_i') log(1-y_i') )\n$$\n风险函数R(w)关于参数w的导数为(公式-7)：\n$$\n\\frac{ \\partial R(w)}{ \\partial w} = - \\frac{1}{N}\\sum_{n=1}^{N}( y_i \\frac{y_i'(1-y_i')}{y_i'}x_i -(1-y_i)\\frac{y_i'(1-y_i')}{1-y_i'}x_i  )\n$$\n$$\n= - \\frac{1}{N}\\sum_{n=1}^{N}( y_i(1-y_i')x_i -(1-y_i)y_i'x_i)\n$$\n$$\n= - \\frac{1}{N}\\sum_{n=1}^{N}x_i(y_i-y_i')\n$$\n采用梯度下降算法，Logistic的回归训练过程为：初始化w_0 为0，然后通过下式来更新迭代参数(公式-8)。\n$$\nw_{t+1} \\leftarrow w_t + \\alpha \\frac{1}{N}\\sum_{n=1}^{N} x_i(y_i-y_{w_t}')\n$$\n其中a是学习率，y_{wt}'是当参数为w_t 时，Logistic回归的输出。\n\n从公式-6可知，风险函数R(w)是关于参数w的连续可导的凸函数，因此除了梯度下降算法外，Logistic还可以使用高阶的优化算法，比如牛顿法来进行优化。\n\n说明:\n- 两个未知数相乘求导：\n$$\n(ab)' = a'b + ab'\n$$\n- sigmoid函数求导后为：\n$$\n\\sigma ' = \\sigma (1-\\sigma )x\n$$\n\n---\n## 参考\n- https://zhuanlan.zhihu.com/p/44591359\n- https://blog.csdn.net/wgdzz/article/details/48816307\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"机器学习算法分类","url":"/2019/03/26/机器学习/机器学习算法分类/","content":"\n> 机器学习算法可以按照不同的标准进行分类。比如按函数f(X)的不同，机器学习算法可以分为线性模型和非线性模型；按照学习准则的不同，机器学习算法也可以分为统计方法和非统计方法。\n> \n> 但一般而言，会按照训练样本提供的信息以及反馈方式不同，将机器学习算法分为以下几类，下面将一一细说。\n\n<!--More-->\n- 监督学习\n- 无监督学习\n- 强化学习\n\n\n\n# 监督学习\n如果机器学习的目标是通过建模样本的特征x和标签y之间的关系：\n$$\ny=f(x,\\theta )\n$$\n或\n$$\np(y|x,\\theta)\n$$\n并且训练集中的每个样本都有标签，那么这类学习称之为监督学习（Supervised Learning）。根据标签类型的不同，监督学习又可以分为回归和分类两种。\n## 回归（Regression）\n\n回归问题中的标签y是连续值（实数或者连续整数）\n$$\ny=f(x,\\theta ) \n$$\n的输出也是连续值。\n## 分类（Classification）\n分类问题中的标签y是离散的类别，在分类问题中，学习到的模型也成为分类器（Classifier）。分类问题根据其类别的数量又可以分为二分类（Binary Clssification）和多分类（Mutil-class Classification）。\n\n## 机构化学习（Structured Learning）\n结构化学习的输出对象是结构化的对象，比如序列、树、图等，由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将x,y映射为该空间中的联合特征向量（x,y）,预测模型可以写为：\n$$\n\\hat{y}=\\underset{y \\in Gen(x)}{arg max f(\\phi (x,y),\\theta )}\n$$\n其中gen(x)表示x所有可能的输出目标集合。计算arg max的过程也称为解码（decoding）过程，一般通过动态规划的方法来计算。\n\n# 无监督学习\n无监督学习（Unsupervised Learning）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有聚类、密度估计、特征学习、降维等。\n\n# 强化学习\n强化学习（Reinforcement Learning）是一类通过交互来学习的机器学习算法。在强化学习中，智能体根据环境的状态作出一个动作，并得到即时或延时的奖励。智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报。\n\n下表给出了三种机器学习类型\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190326174240856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n监督学习需要每个样本都有标签，而无监督学习则不需要标签。一般而言，监督学习通常大量的有标签数据集，这些数据集是一般都需要由人工进行标注，成本很高。因此，也出现了很多弱监督学习（Weak Supervised Learning）和半监督学习（Semi-Supervised Learning）的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。强化学习和监督学习的不同在于强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"从线性回归看偏差-方差分解（Bias-Variance Decomposition）","url":"/2019/03/25/机器学习/从线性回归看偏差-方差分解（Bias-Variance Decomposition）/","content":"> 本文主要介绍从线性回归看偏差-方差分解（Bias-Variance Decomposition）\n\n<!--More-->\n\n# 概述\n对于数字序列1，3，5，7，？，正常情况下大家脑海里蹦出的是9，但是217314也是其一个解\n9对应的数学公式为\n$$\nf(x)=2x-1\n$$\n217314对应的数学公式为\n$$\nf(x)=\\frac{18111}{2} x^{4}-90555x^{3}+\\frac{633885}{2}x^{2}-452773x+217331\n$$\n\nPython 实现为：\n```\n>>> def f(x):\n...     return 18111/2 * pow(x,4) -90555 * pow(x,3) + 633885/2 * pow(x,2) -452773 * x +217331\n... \n>>> f(1)\n1.0\n>>> f(2)\n3.0\n>>> f(3)\n5.0\n>>> f(4)\n7.0\n>>> f(5)\n217341.0\n```\n\n当机器学习模型进行预测的时候，通常都需要把握一个非常微妙的平衡，一方面我们希望模型能够匹配更多的训练数据，相应的增加其复杂度，否则会丢失相关特征的趋势（即模型过拟合）；但是另一方面，我们又不想让模型过分的匹配训练数据，相应的舍弃部分复杂的，因为这样存在过度解析所有异常值和伪规律的风险，导致模型的泛化能力差（即模型欠拟合）。因此在模型的拟合能力和复杂度之前取得一个比较好的权衡，对于一个模型来讲十分重要。而偏差-方差分解（Bias-Variance Decomposition）就是用来指导和分析这种情况的工具。\n# 偏差和方差定义\n- 偏差（Bias）：即预测数据偏离真实数据的情况。\n- 方差（Variance）：描述的是随机变量的离散程度，即随机变量在其期望值附近的波动程度。\n\n# 偏差-方差推导过程\n以回归问题为例，假设样本的真实分布为p_r(x,y)，并采用平方损失函数，模型f(x)的期望错误为(公式2.1)：\n$$\nR(f) = E_{(x,y)\\sim p_r{(x,y)}} \\left [ (y-f(x))^2 \\right ]\n$$\n那么最优模型为(公式2.2)：\n$$\nf^*(x) = E_{y\\sim p_r{(y|x)}} \\left [ y \\right ]\n$$\n其中p_r(y|x)为真实的样本分布，f^*(x)为使用平方损失作为优化目标的最优模型，其损失为(公式2.3)：\n$$\n\\varepsilon  = E_{(x,y)\\sim p_r{(x,y)}} \\left [ (y-f^*(x))^2 \\right ]\n$$\n损失\n$$\n\\varepsilon \n$$\n通常是由于样本分布及其噪声引起的，无法通过优化模型来减少。\n期望错误可以分解为(公式2.4)：\n$$\nR(f)\n$$\n$$\n= E_{(x,y)\\sim p_r{(x,y)}} \\left [ (y- f^*(x) + f^*(x) -f(x))^2 \\right ]\n$$\n$$\n= E_{x\\sim p_r{(x)}}\\left [ (f(x) - f^*(x))^2 \\right ] + \\varepsilon \n$$\n公式2.4中的第一项是机器学习可以优化的真实目标，是当前模型和最优模型之间的差距。\n\n在实际训练一个模型f(x)时，训练集D是从真实分布p_r(x,y)上独立同分布的采样出来的有限样本集合。不同的训练集会得到不同的模型。令f_D(x)表示在训练集D上学习到的模型，一个机器学习算法（包括模型和优化算法）的能力可以通过模型在不同训练集上的平均性能来体现。\n\n对于单个样本x，不同训练集D得到的模型f_D(x)和最优模型f^*(x)的上的期望差距为（公式2.5）：\n$$\nE_D[( f_D(x)-f^*(x) )^2]\n$$\n$$\n=E_D\\left [  ( f_D(x)  - E_D[f_D(x)] +E_D[f_D(x)]   -f^*(x) )^2   \\right ]\n$$\n$$\n=( E_D[f_D(x)]   -f^*(x) )^2 )  + E_D[(f_D(x)  - E_D[f_D(x)] )^2]\n$$\n公式2.5最后一行中的第一项为偏差(bias)，是指一个模型在不同训练集上的平均性能和与最优模型的差异，偏差可以用来衡量一个模型的拟合能力；第二项是方差（variance），是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。\n\n集合公式2.4和公式2.5，期望错误可以分解为(公式2.6)：\n$$\nR(f)= (bias)^2 + variance +  \\varepsilon \n$$\n其中\n$$\n(bias)^2 =  E_X[E_D[f_D(x)]   -f^*(x) )^2 ]\n$$\n$$\nvariance = E_X [ E_D[(f_D(x)  - E_D[f_D(x)] )^2] ]\n$$\n最小化期望错误等价于最小化偏差和方差之和。\n\n# 偏差和方差分析\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190325231703314.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n上图为机器学习中偏差和方差的四种不同情况。每个图的中心点为最优模型f*(x)，蓝点为不同训练集D 上得到的模型f_D(x)。\n- (a)给出了一种理想情况，方差和偏差都比较小\n- (b)为高偏差低方差的情况，表示模型的泛化能力很好，但拟合能力不足\n- (c)为低偏差高方差的情况，表示模型的拟合能力很好，但泛化能力比较差。当训练数据比较少时会导致过拟合\n- (d)为高偏差高方差的情况，是一种最差的情况\n\n方差一般会随着训练样本的增加而减少。当样本比较多时，方差比较少，我们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往往都比较有限，最优的偏差和最优的方差就无法兼顾。\n\n随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而导致过拟合。以结构错误最小化为例，我们可以调整正则化系数λ来控制模型的复杂度。当λ变大时，模型复杂度会降低，可以有效地减少方差，避免过拟合，但偏差会上升。当λ过大时，总的期望错误反而会上升。因此，正则化系数λ需要在偏差和方差之间取得比较好的平衡。下图给出了机器学习模型的期望错误、偏差和方差随复杂度的变化情况。最优的模型并不一定是偏差曲线和方差曲线的交点。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190325231720819.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n偏差和方差分解给机器学习模型提供了一种分析途径，但在实际操作中难以直接衡量。一般来说，当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型。当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型复杂度，加大正则化系数，引入先验等方法来缓解。此外，还有一种有效的降低方差的方法为集成模型，即通过多个高方差模型的平均来降低方差。 \n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"基于神经网络实现Mnist数据集的多分类","url":"/2019/03/09/深度学习/TensorFlow/基于神经网络实现Mnist数据集的多分类/","content":"\n> 在之前的文章中介绍了基于Logistic Regression实现Mnist数据集的多分类，本篇文章主要介绍基于TensorFlow实现Mnist数据集的多分类。\n\n<!--More-->\n\n一个典型的神经网络训练图如下所示：\n![神经网络训练图](https://img-blog.csdnimg.cn/20190308005540655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n只不过在Mnist数据集是十分类的，起输出由y1和y2换成y1，....，y10。本文实现的神经网络如下所示：\n\n这是使用的是两层的神经网络，第一层神经元个数是256，第二层为128，最终输出的是10个类别。对应的神经网络结果如下图所示：\n<img src=\"https://img-blog.csdnimg.cn/20190309205947261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n接着我们创建一个MutilClass类，并初始化相关参数用来实现基于神经网络的多分类函数。\n\n```\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n \nclass MutilClass:\n    def __init__(self):\n        # 加载数据集\n        self.Mnsit = input_data.read_data_sets(\"./data/\", one_hot=True)\n \n        # 设置神经网络层参数\n        self.n_hidden_1 = 256\n        self.n_hidden_2 = 128\n        self.n_input = 784\n        self.n_classes = 10\n \n        self.x = tf.placeholder(dtype=float, shape=[None, self.n_input], name=\"x\")\n        self.y = tf.placeholder(dtype=float, shape=[None, self.n_classes], name=\"y\")\n        # random_normal 高斯分布初始化权重\n        self.weights = {\n            \"w1\": tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1],stddev = 0.1)),\n            \"w2\": tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2], stddev = 0.1)),\n            \"out\": tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes], stddev = 0.1))\n        }\n        self.bias = {\n            \"b1\": tf.Variable(tf.random_normal([ self.n_hidden_1 ])),\n            \"b2\": tf.Variable(tf.random_normal([ self.n_hidden_2 ])),\n            \"out\": tf.Variable(tf.random_normal([ self.n_classes ]))\n        }\n \n        print(\"参数初始化完成！\")\n```\n\n神经网络首次循环，是根据初始化的参数和偏置，向前传播，经过两层隐层，最终的到一个对应各个类别的概率，然后再根据反向传播，最小化损失函数求解参数，所以这里创建一个前向传播和反向传播的函数，如下所示：\n```\n    # 定义一个MLP，前向感知器\n    def _multilayer_perceptron(self,_X, _weights, _bias):\n        layer_1 = tf.nn.sigmoid(tf.add ( tf.matmul(_X, _weights[\"w1\"]), _bias[\"b1\"] ) )\n        layer_2 = tf.nn.sigmoid(tf.add ( tf.matmul(layer_1, _weights[\"w2\"]), _bias[\"b2\"] ) )\n        return (tf.matmul( layer_2 ,_weights[\"out\"] ) + _bias[\"out\"])\n \n    # 定义反向传播\n    def _back_propagation(self):\n        pred = self._multilayer_perceptron(self.x, self.weights, self.bias)\n        # logits 未归一化的概率\n        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=self.y) )\n        optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.001).minimize(cost)\n        corr = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1) )\n        accr =tf.reduce_mean(tf.cast(corr, dtype=float))\n \n        init = tf.global_variables_initializer()\n        return init, optimizer,cost, accr\n```\n\n接着就是训练网络了，指定的迭代次数为：100，batch_size：100，其对应的函数未：\n\n```\n    # 训练模型\n    def _train_model(self, _init, _optimizer, _cost, _accr):\n        epochs = 100\n        batch_size = 100\n        display_steps = 1\n        sess = tf.Session()\n        sess.run(_init)\n \n        for epoch in range(epochs):\n            avg_cost = 0\n            total_batch = int (self.Mnsit.train.num_examples / batch_size)\n            for i in range(total_batch):\n                batch_xs, batch_ys = self.Mnsit.train.next_batch(batch_size)\n                feeds = {self.x: batch_xs, self.y: batch_ys}\n                sess.run(_optimizer, feed_dict=feeds)\n                avg_cost += sess.run(_cost, feed_dict=feeds)\n            avg_cost = avg_cost / total_batch\n \n            if (epoch +1) % display_steps ==0:\n                print(\"Epoch: {} / {}, cost: {}\".format(epoch, epochs, avg_cost))\n                feeds = {self.x: batch_xs, self.y: batch_ys}\n                train_acc = sess.run(_accr, feed_dict=feeds)\n                print(\"Train Accuracy: {}\".format(train_acc))\n \n                feeds = {self.x : self.Mnsit.test.images, self.y: self.Mnsit.test.labels}\n                test_acc = sess.run(_accr, feed_dict= feeds)\n                print(\"Test Accuracy: {}\".format(test_acc))\n                print(\"-\" * 50)\n```\n创建主函数，进行迭代训练\n```\nif __name__ == \"__main__\":\n    network = MutilClass()\n    init, optimizer, cost, accr = network._back_propagation()\n    network._train_model(init, optimizer,cost, accr)\n```\n\n最后的迭代结果为：\n```\nEpoch: 0 / 100, cost: 2.4407591546665537\nTrain Accuracy: 0.12999999523162842\nTest Accuracy: 0.12960000336170197\n--------------------------------------------------\nEpoch: 1 / 100, cost: 2.290777679356662\nTrain Accuracy: 0.12999999523162842\nTest Accuracy: 0.1469999998807907\n--------------------------------------------------\nEpoch: 2 / 100, cost: 2.2774649468335237\nTrain Accuracy: 0.17000000178813934\nTest Accuracy: 0.21799999475479126\n--------------------------------------------------\n \n.......\n \n--------------------------------------------------\nEpoch: 98 / 100, cost: 0.7186844098567963\nTrain Accuracy: 0.8299999833106995\nTest Accuracy: 0.8371999859809875\n--------------------------------------------------\nEpoch: 99 / 100, cost: 0.7124480505423112\nTrain Accuracy: 0.8100000023841858\nTest Accuracy: 0.8377000093460083\n--------------------------------------------------\n```\n\n从结果中可以看出，cost是一直在减少，训练集和测试集评估模型的准确率也在一直提高。当然我们也可以通过调节epoch，batch_size来重新训练模型。\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["TensorFlow"],"categories":["技术篇"]},{"title":"【技术分享】机器学习在微博信息流推荐中的应用实践","url":"/2019/03/05/Share/【技术分享】机器学习在微博信息流推荐中的应用实践/","content":"\n> 本文分为四部分介绍机器学习在微博信息流中的应用实践，分别为：微博信息流推荐场景介绍，内容理解与用户画像，大规模推荐系统实践和总结展望。\n\n<!--More-->\n\n# 微博信息流推荐场景介绍\n> 微博的feed流内容形态各异，有视频，图片，文字，长文，问答等，其用户量也很大，2018年Q2统计DAU（日活）为1.9亿，MAU（月活）为4.3亿，这么庞大的用户量，如何做好首页feed流的个性化推荐就显得格外重要。\n\n\n<img src=\"https://img-blog.csdnimg.cn/20190305162718873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305162740161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n# 内容理解与用户画像\n> 由于个性化推荐是给用户推荐其感兴趣的内容，所以对于微博的内容理解和用户画像部分就显得格外重要。内容理解即通过文本内容理解和视觉理解技术，对微博内容进行细粒度表征，即形成每篇微博内容的表征向量。\n\n<img src=\"https://img-blog.csdnimg.cn/20190305162850198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305162905934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n> 用户画像即基于用户的发博内容，行为数据，自填信息等进行深度挖掘，精准分析刻画用户，从而在进行微博内容推送时能够实现其个性化。\n\n<img src=\"https://img-blog.csdnimg.cn/20190305162934416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n# 大规模推荐系统实践\n\n> 目前推荐架构的实现思路都是先从海量原始数据中，依据用户画像，召回用户偏好的数据，在利用排序算法对其进行排序，最终选择top K返回给用户。微博推荐亦是如此。其整体的流程图如下所示：\n\n> 物料召回：即从候选物料集合中粗筛物料，作为进行模型的待排序物料。\n\n<img src=\"https://img-blog.csdnimg.cn/20190305163036955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n<img src=\"https://img-blog.csdnimg.cn/20190305163059488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n<img src=\"https://img-blog.csdnimg.cn/20190305163120489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n> 算法排序则是结合相关特征对物料召回的内容进行预估排序，其特征主要分为：用户特征，内容特征，环境特征，组合特征和上下文特征等。\n\n<img src=\"https://img-blog.csdnimg.cn/20190305163427457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163455977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163515432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163530356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163645420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163705966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163719819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163749462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163805882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163818878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190305163833250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n\n# 总结与展望\n- 总结\n\t- 业务和数据决定了模型算法的应用场景\n\t- 模型算法殊途同归\n\t- 工程能力和算法架构是基本保障\n\n- 展望\n\t- 采用多模型融合，能更好的对非结构化内容进行表征\n\t- 更多的融合网络结构适用于CTR预估场景\n\n---\n\n完整内容请阅读原文：https://blog.csdn.net/Gamer_gyt/article/details/88164127 \n\n------\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>","tags":["技术分享"],"categories":["技术篇"]},{"title":"TensorFlow实现Mnist数据集的多分类逻辑回归模型","url":"/2019/02/27/深度学习/TensorFlow/TensorFlow实现Mnist数据集的多分类逻辑回归模型/","content":"\n> 多分类逻辑回归基于逻辑回归（Logistic Regression，LR）和softMax实现，其在多分类分类任务中应用广泛，本篇文章基于tf实现多分类逻辑回归，使用的数据集为Mnist。\n\n<!--More-->\n\n多分类逻辑回归的基础概要和在Spark上的实现可参考：\n\n- 多分类逻辑回归（Multinomial Logistic Regression）\n- 多分类实现方式介绍和在Spark上实现多分类逻辑回归（Multinomial Logistic Regression）\n\n<!--More-->\n\n本篇文章涉及到的tf相关接口函数及释义如下：\n\n## tf.nn.softmax\nSoftmax 在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类（C > 2）问题，分类器最后的输出单元需要Softmax 函数进行数值处理。关于Softmax 函数的定义如下所示：\n\n...\n\n---\n\n完整内容请阅读原文：https://blog.csdn.net/Gamer_gyt/article/details/87970776 \n\n\n-----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["TensorFlow"],"categories":["技术篇"]},{"title":"深度学习中的epochs、batch_size、iterations详解","url":"/2019/02/26/深度学习/神经网络笔记/深度学习中的epochs、batch_size、iterations详解/","content":"> 深度学习中涉及到很多参数，如果对于一些参数不了解，那么去看任何一个框架都会有难度，在TensorFlow中有一些模型训练的基本参数，这些参数是训练模型的前提，也在一定程度上影响着模型的最终效果。下面主要介绍几个参数。\n\n<!--More-->\n\n- batch_size\n- iterations\n- epochs\n\n# batch_size\n\n深度学习的优化算法，其实就是梯度下降，在之前的文章中我们也介绍过梯度下降，这里就不详细说明。梯度下降分为三种：\n\n- 批量梯度下降算法（BGD，Batch gradient descent algorithm）\n- 随机梯度下降算法（SGD，Stochastic gradient descent algorithm）\n- 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）\n\n批量梯度下降算法，每一次计算都需要遍历全部数据集，更新梯度，计算开销大，花费时间长，不支持在线学习。\n\n随机梯度下降算法，每次随机选取一条数据，求梯度更新参数，这种方法计算速度快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。\n\n为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。\n\ntf框架中的batch_size指的就是更新梯度中使用的样本数。当然这里如果把batch_size设置为数据集的长度，就成了批量梯度下降算法，batch_size设置为1就是随机梯度下降算法。\n\n# iterations\n迭代次数，每次迭代更新一次网络结构的参数。\n\n迭代是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以到达所需的目标或结果。\n\n每一次迭代得到的结果都会被作为下一次迭代的初始值。\n\n一个迭代 = 一个（batch_size）数据正向通过（forward）+ 一个（batch_size）数据反向（backward）\n\n![神经网络](https://img-blog.csdnimg.cn/2019022523170956.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n\n前向传播：构建由（x1,x2,x3）得到Y（hwb(x)）的表达式\n \n反向传播：基于给定的损失函数，求解参数的过程\n\n\n# epochs\nepochs被定义为前向和反向传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次前向和反向传递。\n\n简单说，epochs指的就是训练过程中数据将被“轮”多少次\n\n例如在某次模型训练过程中，总的样本数是10000，batch_size=100，epochs=10，其对应的伪代码如下：\n```\ndata = \nbatch_size = 100\nfor i in range(epochs):\n    for j in range(int(data_length / batch_size - 1)):\n        x_data = data[begin:end, ]\n        y_data = data[begin:end, ]\n        mode.train(x_data, y_data)\n        begin += batch_size\n        end += batch_size\n```\n\n其中iterations = data_length / batch_size\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["神经网络"],"categories":["技术篇"]},{"title":"Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍","url":"/2019/01/29/RecSys/Spark与推荐系统/Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍/","content":">【Spark排序算法系列】主要介绍的是目前推荐系统或者广告点击方面用的比较广的几种算法，和他们在Spark中的应用实现，本篇文章主要介绍GBDT算法.\n\n<!--More-->\n# 前言\n本系列还包括（持续更新）：\n- Spark排序算法系列之LR（逻辑回归）\n- Spark排序算法系列之模型融合（GBDT+LR）\n- Spark排序算法系列之XGBoost\n- Spark排序算法系列之FTRL（Follow-the-regularized-Leader）\n- Spark排序算法系列之FM与FFM\n\n在本篇文章中你可以学到：\n- Spark MLLib包中的GBDT使用方式\n- 模型的通过保存、加载、预测\n- PipeLine\n- ML包中的GBDT\n\n\n# 概述\nLR因为其容易并行最早应用到推荐排序中的，但学习能力有限，需要大量的特征工程来增加模型的学习能力。但大量的特征工程耗时耗力，且不一定带来效果的提升，因此在如何能有效的发现特征组合，来缩短LR特征实验周期的背景下，GBDT被应用了起来。GBDT模型全称是Gradient Boosting Decision Tree，梯度提升决策树。是属于Boosing算法中的一种，关于Boosting的介绍可以参考文章集成学习（Ensemble Learning)\n\n关于GBDT算法理解可参考：\n- Spark排序算法系列之GBTs基础——梯度上升和梯度下降\n- 梯度提升决策树GBDT（Gradient Boosting Decision Tree）\n\n其实相信很多人对Spark 机器学习包（ml和mllib）中的GBDT傻傻分不清楚，这里我们先来捋一捋。Spark中的GBDT较GBTs——梯度提升树，因为其是基于决策树（Decision Tree，DT）实现的，所以叫GBDT。Spark 中的GBDT算法存在于ml包和mllib包中，mllib是基于RDD的，ml包则是针对DataFrame的，ml包中的GBDT分为分类和回归，在实际使用过程中，需要根据具体情况进行衡量和选择。由于在实际生产环境中使用基于RDD的较多，所以下面将着重介绍下MLLib包中的GBTs，ML包中的将进行简单说明。\n\n--------------------- \n完整内容请阅读原文：https://blog.csdn.net/Gamer_gyt/article/details/86695837\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"【资源分享】从数理统计到DL、RL，还不快来！","url":"/2019/01/28/Share/【资源分享】从数理统计到DL、RL，还不快来/","content":"\n> 之前在自己的年度总结里写到：19年的目标就是技术沉淀与突破。技术突破不仅包含现有技术的总结和反思，更是对未知技术的探索和求知，希望19年能够更上一层楼。\n\n<!--More-->\n\n这个repo是我一直维护和整理的一个技术资料分享的repo，是我包括群友一块整理的一个免费技术资料分享的库，不仅包含了机器学习，数据挖掘，深度学习，还包含了大数据，数理统计，强化学习等，希望在技术这条路上你能跑的更快。\n\nrepo：https://github.com/Thinkgamer/books\n\n先来张图片镇楼！！！\n<img src=\"https://img-blog.csdnimg.cn/20190128031446639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70\">\n\n# Why\n建立该Repo的目的有两个：\n\n- 本人在各个平台共享书籍，进行一个统一管理\n- 分享给各个搞技术的朋友，知识无私藏之说\n\n----\n\n# What\n该Repo会涉及包含以下类别书籍：\n\n- 机器学习\n- 数据挖掘\n- 深度学习\n- NLP\n- 云计算\n- 统计学概率论\n- 收藏的论文\n- 杂乱无章\n\n----\n\n# List\n注明：排名无先后顺序\n\n## 机器学习\n- scikit-learn 中英文\n- 机器学习-周志华\n- 机器学习实战\n- 机器学习导论\n- 集体智慧编程中文版\n- [英文版]叶斯思维：统计建模的Python学习法\n\n\n## 数据挖掘\n- python数据分析与挖掘实战\n- 利用python进行数据分析\n- 面向程序员的数据挖掘指南\n- 数据挖掘：概念与技术（中文第三版）\n- 数据挖掘应用20个案例分析\n- 数据挖掘与数据化运营实战_思路_方法_技巧与应用_完整版\n\n## 深度学习\n- 神经网络与机器学习（加）Simon Haykin\n- TensorFlow实战-黄文坚\n- 深度学习 中文版\n- 神经网络与深度学习\n\n## NLP\n- 模式识别与机器学习 中文版\n- NLP汉语自然语言处理原理与实践\n- PYTHON自然语言处理中文版\n\n## 推荐系统\n- 推荐系统实践\n- learning-to-rank.pdf \n- Recommender Systems Handbook.pdf\n- Context-Aware-Recommender-Systems-chapter-7.pdf\n\n## 云计算\n- 《快学Scala》\n- Learning PySpark.pdf\n- SparkMLlib机器学习\n- Spark快速大数据分析\n- 数据算法  Hadoop Spark大数据处理技巧\n\n## 统计学与概率论\n- 《概率论与数理统计》浙大版（第四版）教材\n- 《概率论与数理统计习题全解指南》.浙大版（第四版）\n- 数理统计与数据分析原书第3版\n- 应用商务统计分析 王汉生(2008).pdf\n\n## 收藏的论文\n- 平滑系数自适应的二次指数平滑模型及其应用\n- The Structure of Collaborative Tagging Systems\n- FM\n- FFM\n- DeepFFM\n- Focal Loss for Dense Object Detection\n- Attentive Group Recommendation.pdf\n- Real-time Personalization using Embeddings for Search.pdf\n\n## 杂乱无章\n- 阿里技术之瞳-p260\n- 数据敏感性测试\n- 正则表达式经典实例.（美）高瓦特斯，（美）利维森\n- 阿里广告中的机器学习平台.pdf\n- 广告数据上的大规模机器学习.pdf\n- 绿盟大数据安全分析平台 产品白皮书.pdf\n- Xdef2013-基于机器学习和NetFPGA的智能高速入侵防御系统.ppt\n- 04-程佳-推荐广告机器学习实践\n- A Gentle Introduction to Gradient Boosting.pdf\n- GBDT算法原理与系统设计简介.pdf\n- [微博] 机器学习在微博信息流推荐应用实践.pdf\n- [知乎] 首页信息流系统的框架及机器学习技术在推荐策略中的应用.pdf\n\n## 强化学习\n- 强化学习在阿里的技术演进与业务创新.pdf\n\n## 技术集锦\n- AAAI2018.pdf\n- 数字经济下的算法力量.pdf\n- 2018美团点评算法系列.pdf\n- Learning To Rank在个性化电商搜索中的应用\n\n---\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>","tags":["资源分享"],"categories":["技术篇"]},{"title":"多分类实现方式介绍和在Spark上实现多分类逻辑回归","url":"/2019/01/12/机器学习/多分类实现方式介绍和在Spark上实现多分类逻辑回归/","content":"> 本文主要介绍多分类实现方式介绍和在Spark上实现多分类逻辑回归。\n\n<!--More-->\n\n# 背景\n在之前的文章中介绍了多分类逻辑回归算法的数据原理，参考文章链接\n\nCSDN文章链接：https://blog.csdn.net/Gamer_gyt/article/details/85209496\n该篇文章介绍一下Spark中多分类算法，主要包括的技术点如下\n\n- 多分类实现方式\n\t- 一对一 （One V One）\n\t- 一对其余（One V Remaining）\n\t- 多对多 （More V More）\n\n\n- Spark中的多分类实现\n# 多分类实现方式\n## 一对一\n假设某个分类中有N个类别，将这N个类别两两配对（继而转化为二分类问题），这样可以得到 N（N-1）/ 2个二分类器，这样训练模型时需要训练 N（N-1）/ 2个模型，预测时将样本输送到这些模型中，最终统计出现次数较多的类别结果作为最终类别。\n\n假设现在有三个类别：类别A，类别B，类别C，类别D。一对一实现多分类如下图所示：\n<img src=\"https://img-blog.csdnimg.cn/20190112220044121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70\">\n\n--------------------- \n完整内容请阅读原文：https://blog.csdn.net/Gamer_gyt/article/details/86378882 \n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"Hive Join 分析和优化","url":"/2019/01/03/Spark/Hive Join 分析和优化/","content":"> Sku对应品牌进行关联，大表对应非大表（这里的非大表并不能用小表来定义）\n\n<!--More-->\n \n# 问题分析\n进行表左关联时，最后一个reduce任务卡到99%，运行时间很长，发生了严重的数据倾斜。\n\n什么是数据倾斜？数据倾斜主要表现在，map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。\n\n\n完整内容请阅读原文：https://blog.csdn.net/Gamer_gyt/article/details/85690885 \n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"TensorFlow 特征工程:feature_column","url":"/2019/01/03/深度学习/TensorFlow/TensorFlow 特征工程: feature_column/","content":"> 在使用很多模型的时候，都需要对输入的数据进行必要的特征工程处理。最典型的就是:one-hot处理，还有hash分桶等处理。为了方便处理这些特征，tensorflow提供了一些列的特征工程方法来方便使用.\n\n<!--More-->\n\n## 公共的import\n```\nimport tensorflow as tf\nfrom tensorflow.python.estimator.inputs import numpy_io\nimport numpy as np\nimport collections\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import coordinator\nfrom tensorflow import feature_column\n\nfrom tensorflow.python.feature_column.feature_column import _LazyBuilder\n```\n\n\n## numeric_column\n```\nnumeric_column(\n    key,\n    shape=(1,),\n    default_value=None,\n    dtype=tf.float32,\n    normalizer_fn=None\n)\n```\n- key：特征的名字。也就是对应的列名称\n- shape：该key所对应的特征的shape. 默认是1，但是比如one-hot类型的，shape就不是1，而是实际的维度。总之，这里是key所对应的维度，不一定是1\n- default_value：如果不存在使用的默认值\n- normalizer_fn：对该特征下的所有数据进行转换。如果需要进行normalize，那么就是使用normalize的函数.这里不仅仅局限于normalize，也可以是任何的转换方法，比如取对数，取指数，这仅仅是一种变换方法\n\n完整内容请阅读：https://blog.csdn.net/Gamer_gyt/article/details/85689840\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["TensorFlow"],"categories":["技术篇"]},{"title":"集成学习（Ensemble Learning)","url":"/2019/01/03/机器学习/集成学习（Ensemble Learning)/","content":"\n> 在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。\n\n<!--More-->\n\n集成学习在各个规模的数据集上都有很好的策略。\n- 数据集大：划分成多个小数据集，学习多个模型进行组合\n- 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合\n\n这篇博客介绍一下集成学习的几类：Bagging，Boosting以及Stacking。\n\n# Bagging\nBagging是bootstrap aggregating的简写。先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。具体步骤如下\n- 采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本\n- 根据抽出的样本计算想要得到的统计量T\n- 重复上述N次（一般大于1000），得到N个统计量T\n- 根据这N个统计量，即可计算出统计量的置信区间\n\n在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。\n\n例如随机森林（Random Forest）就属于Bagging。随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。\n\n在我们学习每一棵决策树的时候就需要用到Bootstrap方法。在随机森林中，有两个随机采样的过程：对输入数据的行（数据的数量）与列（数据的特征）都进行采样。对于行采样，采用有放回的方式，若有N个数据，则采样出N个数据（可能有重复），这样在训练的时候每一棵树都不是全部的样本，相对而言不容易出现overfitting；接着进行列采样从M个feature中选择出m个（m<<M）。最近进行决策树的学习。\n\n预测的时候，随机森林中的每一棵树的都对输入进行预测，最后进行投票，哪个类别多，输入样本就属于哪个类别。这就相当于前面说的，每一个分类器（每一棵树）都比较弱，但组合到一起（投票）就比较强了。\n\n# Boosting\n提升方法（Boosting）是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是AdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。具体可以参考《统计学习方法》。\n\n# Stacking\nStacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。\n\n如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，称之为Tier 1分类器（可以采用交叉验证的方式学习），然后将输出用于训练Tier 2 分类器。\n\n完整内容请阅读： https://blog.csdn.net/Gamer_gyt/article/details/85689424 \n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"基于TF-IDF算法的短标题关键词提取","url":"/2019/01/03/NLP/基于TF-IDF算法的短标题关键词提取/","content":"\n> TF-IDF（Term Frequency–InverseDocument Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n\n<!--More-->\n\n完整内容请阅读：https://blog.csdn.net/Gamer_gyt/article/details/85690389 \n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["NLP"],"categories":["技术篇"]},{"title":"【内附PDF资料】Python实现下载图片并生产PDF文件","url":"/2019/01/02/Python/【内附PDF资料】Python实现下载图片并生产PDF文件/","content":"\n> 背景：2018AICon大会中的一些PPT，官方没有提供完整的PDF文件，而是一张张图片，不方便下载和后续阅读，这里使用Python爬取相关演讲的图片，并生产PDF文件\n\n<!--More--> \n\n# 下载函数\n创建下载图片函数，使用的是Python的urllib库，代码如下\n\n```\n# 下载图片\ndef save_img(img_url,file_path=\"./img/\"):\n    if not os.path.exists(file_path):\n        print(\"保存图片的文件不存在，创建该目录\")\n        os.mkdir(file_path)\n    # 图片后缀\n    file_suffix = os.path.splitext(img_url)[1]\n    file_name = str ( int(img_url.split(\"-\")[1].split(\".\")[0]) )\n    # 拼接图片名（包含路径）\n    filename = '{}{}{}'.format(file_path,file_name,file_suffix)\n    # urllib\n    urllib.request.urlretrieve(img_url, filename=filename)\n```\n\n# 生成PDF函数\n创建图片生成PDF文件的函数，使用的是Python的reportlab库，代码如下\n```\n# 生成pdf\ndef convert_img_to_pdf(img_path,pdf_path):\n    pages = 0\n    (w, h) = landscape(portrait(A4))\n    can = canvas.Canvas(pdf_path, pagesize=landscape(portrait(A4)))\n    # 获取img_path下文件，并进行排序\n    files = os.listdir(img_path)\n    files.sort(key=lambda x: int(x[:-4]))\n    # 遍历每个文件\n    for f_name in files:\n        # 拼装成完整的file路径\n        file_path = img_path + os.sep + str(f_name)\n        can.drawImage(file_path, 0, 0, w, h)\n        can.showPage()\n        pages = pages + 1\n    can.save()\n```\n需要注意的是，pagesize不能直接指定值portrait(A4)，因为一张图片会完整的嵌套在一页A4纸张上，极其不美观，这里需要将A4的大小进行转置，pagesize=landscape(portrait(A4))\n\n# 定义要生成的PDF相关信息\n```\n_list = (\n    {\n        \"pdf_name\": \"FFM及DeepFFM模型在推荐系统的探索及实践.pdf\",\n        \"img_path\": \"./img_1/\",\n        \"page\": 52,\n        \"id\": \"3670025915\"\n    },{\n        \"pdf_name\": \"知乎推荐系统的实践及重构之路.pdf\",\n        \"img_path\": \"./img_2/\",\n        \"page\": 38,\n        \"id\": \"4291192513\"\n    },{\n        \"pdf_name\": \"深度树匹配——下一代推荐技术的探索和实践.pdf\",\n        \"img_path\": \"./img_3/\",\n        \"page\": 33,\n        \"id\": \"3621355867\"\n    },{\n        \"pdf_name\": \"基于知识的搜索推荐技术及应用.pdf\",\n        \"img_path\": \"./img_4/\",\n        \"page\": 31,\n        \"id\": \"436657700\"\n    },{\n        \"pdf_name\": \"瓜子二手车个性化推荐的挑战与应对.pdf\",\n        \"img_path\": \"./img_5/\",\n        \"page\": 40,\n        \"id\": \"1433077746\"\n    },{\n        \"pdf_name\": \"机器学习在苏宁搜索平台中的实践.pdf\",\n        \"img_path\": \"./img_6/\",\n        \"page\": 55,\n        \"id\": \"1155556309\"\n    }\n)\n```\n\n# 生成PDF\n遍历要生成PDF的每个信息\n```\nfor one in _list:\n    print(one)\n    for i in range(1,one[\"page\"]+1):\n        img_path = one[\"img_path\"]\n        pdf_path = one[\"pdf_name\"]\n        if i < 10:\n            img_url = \"https://static001.geekbang.org/con/37/pdf/\" + one[\"id\"] + \"/image/page-00\" + str(i) + \".jpg\"\n        else:\n            img_url = \"https://static001.geekbang.org/con/37/pdf/\" + one[\"id\"] + \"/image/page-0\" + str(i) + \".jpg\"\n        print(img_url)\n        # 下载图片\n        save_img(img_url,img_path)\n        # 合成pdf\n        convert_img_to_pdf(img_path, pdf_path)\n    print(\"Image转PDF完成！\")\n```\n\nPDF文件资料获取方式：百度网盘\n\n链接：https://pan.baidu.com/s/1MC_bH2x5jjsP1LvBB5cZCA\n\n提取码：58zq\n\n---\n\n原文链接：https://blog.csdn.net/Gamer_gyt/article/details/85637436\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Python","技术分享"],"categories":["技术篇"]},{"title":"2018年终总结-趁“未老”，再认真一次，我觉得你能更好！","url":"/2018/12/29/随手记/2018年终总结-趁“未老”，再认真一次，我觉得你能更好！/","content":"> 每一年的年尾，我都会问自己：这一年，我收获了什么？            你呢？\n\n<!--More-->\n\n---\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/85333470\n博主微博：http://weibo.com/234654758\nGithub：https://github.com/thinkgamer\n公众号：搜索与推荐Wiki\n个人网站：http://thinkgamer.github.io\n\n---\n\n# Chapter 1\n\n这一年，我收获了很多东西，也学会和体验到了很多，用户八个字来概括就是：不忘粗心，方得始终。\n\n- 小白\n\n4月29号那天，我发了个朋友圈，说买了个小白，不说钱、不说事，只说这是我人生路上的另外一个小伙伴。如今7个月过去了，小白也从最开始跑到了现在的8000公里，虽然在北京受到限号等政策的影响，但他还是发挥了很大的价值。\n\n- 露营\n\n不得不说这是我2018年玩过最刺激的游戏，找一个风和日丽的日子，背上行囊，前往露营圣地，18年总共出去了两次，一次是北京第一高峰——灵山，一次是北京第二高峰——海坨山。2019会进行更多的露营活动，玩的就是心跳\n\n- 海坨山\n\n穿过云烟雾绕的盘山路，看到了那一片世外与桃园。对于海坨山向往已久，终于拉上小伙伴开始了人生第一次露营，只能说真刺激，中午到达时下雨，上山时下雨，晚上下雨，第二天早上下雨，第一次露营简直就是与山上挨冻淋雨的，好歹看到的风景不错，经历的事情比较有趣，记录的情节比较美好，这一切便是值得的。\n\n<center><img src=\"https://img-blog.csdnimg.cn/20181229042241463\"></center>\n\n- 灵山\n\n去灵山时记忆比较深刻的是，负重20kg，爬了4个小时，在接近山顶的地方安营扎寨，然后，然后那天晚上又遇到了大雨，当时心中简直是一万只草泥马在驷马奔腾，不过有趣的是第一次在室外煮了个面，然后又在雨中担惊受怕的睡了一晚。第二天早上，雨过天晴，看到的云，听到的风，让人觉得这所经历的一切都是值得的。\n<center><img src=\"https://img-blog.csdnimg.cn/20181229042241593\"></center>\n\n- 济州岛\n\n人生的第一次出国，选的是韩国的济州岛，选择这个地方的原因主要是不需要办签证，方便，离北京近，飞机三个小时就到了，而且最重要的是这里是纯天然的自然风景，比较符合我欣赏风景的调调。\n<center><img src=\"https://img-blog.csdnimg.cn/20181229042241728\"></center>\n\n第一次出国整体非常顺利，无论是Airbnb订房，到达之后的房东的款待，还是在那的沟通交流，都让人非常的惊喜和愉快。济州岛的生活状态很缓慢，让人非常舒服，最重要的还是自然景色不错，文化生活体验不错，心情不错。\n\n在济州岛最让我惊讶的一件事情是：在发展如此迅速的时代，海女真的存在，他们潜入海底，带回一些水产品，然后出售给海岛上的饭店，赚取生活的费用。由于长期潜水，他们的脸部已经发生了变形，出于尊重，我没有拍照。而且韩国载册的海女记录人数是在不断减少的，韩国济州岛的海女文化被列入联合国教科文组织人类非物质文化遗产名录。同时海女在二战期间，爆发了抗日独立运动，对反法西斯战争有着不同寻常的意义。\n\n真的是每一种文化都值得被尊敬。\n\n- 618\n\n在京东经历了第一个618大促，当时是出于好奇参加的，可能是职业的特殊性，我们算法这边感觉没那么忙，不过数据端和服务端倒是一直绷着神经。\n\n同样是618，第一次近距离的接触了一些明星，有胡夏，金志文，张信哲，苏运莹，第一次离明星们那么近，多多少少还是很激动的。\n\n- 拄拐\n\n这一年，也第一次用了双拐，因为打篮球把右脚崴着了，拄着拐杖上了两周班，说实话很尴尬呀\n\n# Chapter 2\n最近刚好组内进行2018的总结汇报，就顺道把自己18年的工作和技术成长都总结了下。\n\n2018年，自己在技术上也成长了很多，不管是对于推荐算法的理解掌握，还是架构的迁移理解，都学会了很多东西，在整个过程中对于自己的工程开发能力也有了一定的提高，算法的应用和业务理解都有很深刻的认知。整个过程中还是非常感谢组长娜姐对我的帮助和指导，从娜姐和其他人的身上真的学到了很多东西，算法理论的扎实，流程的规范和优秀的问题解决能力。\n\n2018年也对自己过往知识进行了总结，整个文档和章节会在后续给出，想进行阅读的朋友可以持续关注本公众号。\n\n# Chapter 3\n\n2019是一个新的开始，希望自己能够重新的去沉淀一下技术，同时走更多的路，看更多的风景，阅更丰富的人生。\n\n- 露营持续进行\n- 自驾，探索户外\n- 相关论文阅读，要求至少一月一篇\n- 深度在线排序研究和实践\n- 实时在线学习研究和实践\n- 深度架构探索和应用\n- 认真学习，总结文章，用心经营微信公众号\n\n2019，加油，相信你可以在匆匆的人流中，驻足欣赏那交错斑影。\n\n# Chapter 4\n人生没有一帆风顺的，也没有顺心顺意的，那些无谓的事，无谓的人，你又何必care呢，做好自己，不值得交的人不必凡事耿耿于心，不必要嫉妒的事，不必事事亲为，做好自己，就好了。\n\n认真对待生活中的每一件小事，任何一件小事，只要你坚持下去，这便是很值得人尊敬和钦佩了。\n\n19年，一起加油！\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["年终总结"],"categories":["随手记"]},{"title":"2019年终总结-埋下的种子是讲给自己的故事","url":"/2018/12/29/随手记/2019年终总结-埋下的种子是讲给自己的故事/","content":"\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200101231601533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70)\n窗外有漆黑的夜，心中是明亮的灯。其实每个人的内心都有一颗属于自己的灯塔，不渡远方的邮轮，不引迷路的灵魂。\n\n不知不觉的，时间又过了一年，年初兴高采烈给自己2019立flag的情景还在眼前晃着，一不小心这一年就要彻底沦为过往了。回想过去这一年，许多人匆匆到来，也有很多人悄悄离去，有过好友相聚的狂欢，也有无数捧着孤独无处存放的夜晚，有过收获的满足，也有在崩溃边缘挣扎的心酸，有过山野隔绝的轻松，也有追逐路上磕倒的羁绊。\n\n2019年于我而言，好像也没什么特别之处，经历了一些事，看透了一些人，明白了一些理，懂得了一些情，到最后才发现，原来所有的一些都只不过是过往云烟，曾经那个意气风发的少年，早已被落了很远很远。\n\n这一年，踏过他国的路，走过异乡的桥\n这一年，爬过梦中的山，涉过远方的水\n这一年，有在星空下指着月亮，看天文台的无声\n这一年，有在荒野里闪着灯光，听虫鸣和流水声\n这一年，看过草原的日出，也看过火山的晚霞\n这一年，看过草原的天路，也看过华山的凶险\n这一年，旅途细无声，视界变光明\n\n这一年，经历了三件大事吧，换工作（[陆月，我想说的和我要改名了！](https://mp.weixin.qq.com/s/NvpZiaJ_0StAkkor1Q1aVQ)【点击阅读】）、书出版（[官宣：处女座《推荐系统开发实战》上市了](https://mp.weixin.qq.com/s/8V-nu1s6jCgIkwE0liFT4Q)【点击阅读】，[《推荐系统开发实战》上市近两个月的碎碎念](https://mp.weixin.qq.com/s/0ZCvn_fXP-CkbLq4KVkLAg)【点击阅读】）、装修房子。\n\n在这一年经历的所有事情中，有几点体会和感悟吧，分享出来，希望对你有帮助。\n\n## 迈出第一步\n事情再小，也要认真对待，事情再大，也不要害怕，勇敢的迈出第一步，然后见招拆招就好了。当初开始装修房子之前，我简直就懵逼了，面对毛胚房，无从下手，刚开始从网上各种搜资料，注意事项等，五一请了几天假，回家找人开始整，现在已经都搞好了，看起来感觉还不错，当然整个过程中也会有一些小插曲，但整体上还是很顺利的。\n\n## 认真用心\n其实不管是平时做事，还是工作，仔细认真一些，你会避免一些小问题。在开始工作之前，多一些思考和梳理，整体工作就会更加顺利，也会避免很多错误和坑。当然做事也一样，凡事多思考，事前思考、事后总结，总会有一些意向不到的生活。\n\n## 平凡与伟大\n这一年也接触了一些人，他们都很优秀，很厉害，在他们的专业领域当然是top级选手。但是我发现他们都有一个特征：坚持做一件平凡的事，做到极致。所以不管你从事什么细分领域，做什么样的工作，坚持做好工作和生活中的每一件小事，你也能过化平凡为伟大。\n\n## 不忘初心\n相信你身边有很多在各自领域做的很优秀的人，保持敬畏之心，但不要过度去追捧、崇拜、模仿，你要明白那些过的好的人，只不过是承受了更大的压力，只不过是在坚持和放弃的路口选择了坚持。所以，保持自己的初心和规划，坚持下去，按照自己的计划走，活出自己的英雄主义。\n\n\n\n天亮之后，那个不带有一丝阴霾的，全新的2020会如约而至，新的一年，希望我们能放下过往，卸下悲伤，重新整装出发，希望我们都能在疲倦的生活里活出自己的英雄主义。新的一年，祝你平安顺遂，喜笑开颜。\n\n\n----\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<center>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n\n---\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108185024479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20191115142715859.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" />\n</center>","tags":["年终总结"],"categories":["随手记"]},{"title":"多分类逻辑回归（Multinomial Logistic Regression）","url":"/2018/12/21/机器学习/多分类逻辑回归（Multinomial Logistic Regression）/","content":"分类从结果的数量上可以简单的划分为：\n- 二分类（Binary Classification）\n- 多分类（Multinomial  Classification）。\n\n<!--More-->\n\n# 前言\n\n其中二分类是最常见且使用最多的分类场景，解决二分类的算法有很多，比如：\n- 基本的KNN、贝叶斯、SVM\n- Online Ranking中用来做二分类的包括FM、FFM、GBDT、LR、XGBoost等\n\n多分类中比如：\n- 改进版的KNN、改进版的贝叶斯、改进版的SVM等\n- 多类别的逻辑回归\n啰嗦了这么多，其实就是为了说这个多分类的逻辑回归。\n\n# 简介\n在统计学里，多类别逻辑回归是一个将逻辑回归一般化成多类别问题得到的分类方法。用更加专业的话来说，它就是一个用来预测一个具有类别分布的因变量不同可能结果的概率的模型。 \n\n另外，多类别逻辑回归也有很多其它的名字，包括polytomous LR，multiclass LR，softmax regression，multinomial logit，maximum entropy classifier，conditional maximum entropy model。 \n\n在多类别逻辑回归中，因变量是根据一系列自变量（就是我们所说的特征、观测变量）来预测得到的。具体来说，就是通过将自变量和相应参数进行线性组合之后，使用某种概率模型来计算预测因变量中得到某个结果的概率，而自变量对应的参数是通过训练数据计算得到的，有时我们将这些参数成为回归系数。\n\n模型分析\n--------------------- \n\n原文：https://blog.csdn.net/Gamer_gyt/article/details/85209496 \n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"【论文】基于三部图网络结构的知识推荐算法","url":"/2018/07/13/论文/【论文】基于三部图网络结构的知识推荐算法/","content":"\n> 该论文是北京一高校学生的论文，其主要是基于用户-物品-标签这样的三部图网络展开描述的。下面主要介绍一些其中提出的可用的点，其余的详细介绍可参考原文章。\n\n<!--More-->\n\n### 背景\n推荐算法是个性化推荐的核心，现有的推荐算法（除了点击率预估之外的）包括：\n- 基于内容的推荐算法\n- 基于项目的协同过滤算法\n- 基于用户的协同过滤算法\n- 基于模型的协同过滤算法\n- 基于社会网络分析方法的算法\n- 基于网络结构的推荐算法\n\n基于网络结构的推荐算法大部分是基于二部图来对用户进行个性化推荐，这种推荐算法不能有效地解决用户冷启动和冷门物品推荐的问题，本文针对现有算法的不足，提出了一种加权的三部图网络结构的知识推荐算法，在三部图网络模型的基础上设计了新的推荐算法。\n\n### 三部图网络结构算法\n\n项目-用户-标签的三部图可以表示为：{U,I,T}，其中U表示用户集合，I表示物品结合，T表示标签集合。在此三部图结构中，不仅用户与项目以及 用户与标签之间存在联系，项目与项目、用户与用户以及标签 与标签之间也存在联系。\n<center><img src=\"https://img-blog.csdnimg.cn/20190805185607264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=300px></center>\n\n如图 1所示，使用三部图对项目—用户—标签的三元组进行相关模型建立，对三部图模型进行用户—项目、用户—标签 和项目—标签的二部图的扩散。最终的推荐算法是建立在用户—项目、用户—标签和项目—标签二部图网络结构的基础上。\n\n项目的度和权重对推荐算法的影响\n#### 1. 项目的评分相似性及改进\n基于用户-项目网络结构的推荐系统把用户和项目仅仅看成抽象的节点，所有算法利用的信息都隐藏在用户和项目的关系之中。用户-物品的二部图提出了一种资源分配的算法。假设 j 选择过的所有产品都具有某种向 i推荐其他产品的能力，这个 抽象的能力看做是相关产品上的某种可分的资源，表示产品 j 愿意分配给产品 i的资源配额(产品的相似性)。\n\n$$\nsim(i,j) = \\frac{1}{ k_j } \\sum_{l=1}^{m}\\frac{ a_{il} b_{jl} }{k_l}\n$$\n\n其中：\n- k_j 表示物品j的度（即被多少用户产生过行为）\n- k_l 表示用户l的度（即用户u对多少个物品产生过行为）\n- a_il 表示用户 l 是否对物品i产生过行为，产生过行为为1，没有产生过行为为0\n\n传统的二部网络图结构只注意到了用户是否对物品产生过行为，忽略了用户对物品的评分，容易造成信息的损失。本文中保留了用户对物品的评分信息，对用户到物品的边增加一个权重w，w的大小根据用户对物品的评分确定。\n\n假设用户对物品的评分最多为5分，则w=score/5，所以w的范围是[0,1]，score表示用户对物品的评分。则改进后的物品相似度计算公式为：\n\n$$\nsim'{(i,j)} = \\frac{1}{ d(I_j) } \\sum_{a=1}^{m}\\frac{ w_{ia} w_{ja} }{d(u_a)}\n$$\n\n其中：\n\n$$\nd(I_j) = \\sum_{k=1}^{l} w_{kj} \n$$\n表示项目I_j的所有权重之和，即对物品j有行为的用户到物品j的权重之和。\n\n$$\nd(u_a) = \\sum_{k=1}^{m} w_{ak} \n$$\n表示用户u_a的所有权重之和，即用户u对其用行为物品的用户到物品的权重之和。\n\n\n#### 2.项目的属性相似性\n\n在现实的生产系统中，当用户进入一个系统时，可能从未对系统内的任何一个物品产生过行为，也可能只对很少的物品产生了行为。这时候系统无法根据其历史信息获取相关兴趣。\n\n这个时候可以对物品进行属性描述和标签描述，利用项目的属性相似性和利用标签相似性向用户提供更好的推荐服务。\n\n假设用户对物品的属性矩阵如下：\n<center><img src=\"https://img-blog.csdnimg.cn/20190805223801681.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=500px></center>\n\n上表中1 表示物品拥有该属性，0表示物品不拥有该属性。假设物品i和j所拥有的属性分别表示为Qi 和 Qj，则项目i和j的属性相似度计算公式如下：\n\n$$\nsim_{at}(i,j) = \\frac{ |Q_i \\bigcap Q_j | }{|Q_i \\bigcup  Q_j |}\n$$\n\n其中：\n- Q_i 交 Q_j 表示物品i和物品j的属性交集\n- Q_i 并 Q_j 表示物品i和物品j的属性并集\n\n$$\nsim_a'(i,j) = \\sum_{k=1}^{ m } sim_{ak}(i,j)\n$$\n\n从上式中可以看出，当某一个项目属性相似性与所有项目的属性相似度之和 sim_a′(i，j)非常大时，表明该项目在整个系统中是非常活跃的，易被首先向其他用户和项目推荐。因此该项目可视做在整个推荐系统中的初始推荐资源，由此可解决用户冷启动问题。\n\n> 关于标签相似性，可以阅读原论文，这里不展开介绍。\n\n#### 3. 项目的相似性和评分\n\n从上边的1 和 2中的描述可以看出，项目的相似性衡量可以从两方面入手\n- 项目 i与 j的评分相似性。评分越相似，说明用户对这两个项目的喜好越相似。而且如果同时对它们评分的用户个 数越多，说明它们的相似性越高。\n- 项目 i与 j的属性相似性。如果项目 i与 j的属性属于 相同的类别，那么它们的相似性就越高。\n\n所以本文里采用的是项目的评分相似性和属性相似性组合的方法来计算项目的相似性。项目相似性的计算公式为：\n$$\n\\bar S(i,j) = sim'(i,j) + sim_a'(i,j)\n$$\n\n如果两个用户同时对一个热门物品产生了行为，并不能说明这两个用户的兴趣相同；相反如果这两个用户同时选择了一个冷门物品，则这两个用户拥有相似的独特兴趣。算法中应该考虑项目的度（即热门程度），增强小度项目的推荐能力，降低大度项目的推荐能力，提高推荐的多样性。\n\n引入调节函数f(φ)对物品的相似度进行调节，计算相似度时考虑物品的度和物品的权重之和的比值φ，其中φ的定义为：\n$$\n\\varphi =\\frac{k(I_j)}{ d(I_j )}\n$$\n\n其中：\n- k(I_j) 表示项目I_j的度\n- d(I_j) 表示物品I_j的所有权重之和\n\n当所有用户对I_j没有行为时，φ取得最大值 1/w ，当所有用户都对I_j有行为且认为很重要时，φ取得最小值 1。即φ越小，用户选择该物品的概率越大，φ越大，用户不选择该物品的概率越大，算法中应该降低这种物品的推荐能力。\nf(φ)的表达式为：\n$$\nf(\\varphi) =\\varepsilon + \\delta ln(\\varphi +e)\n$$\n\n经过多次实验总结，当 δ=0.5时，可以减少 f(φ)对项目相似度的影响，效果最佳。则计算项目相似度的最终公式为：\n$$\n\\bar S(i,j) = sim'(i,j) + sim_a'(i,j) =  \\frac{1}{ d(I_j) } \\sum_{a=1}^{m}\\frac{ w_{ia} w_{ja} }{d^{f(\\varphi )}(u_a)} + \\sum_{k=1}^{ m } sim_{ak}(i,j)\n$$\n\n其中：\n- φ的取值为（1，1/w）\n- ε 为 一 个 可 调 参 数 ，用 于 调 节 标 签度对推荐结果的影响，ε>0表示大度标签的推荐能力被压制， ε<0表示大度标签的推荐能力得到提高\n\n计算目标用户ui对未评分项目的评分 r_iα的公式为：\n$$\nr_{ia} = \\frac{ \\sum_{j=1,j \\neq i}^{n} \\bar S(i,j) * w_{ja} } {\\sum_{j=1,j \\neq i}^{n}  \\bar S(i,j)  }\n$$\n\n标签的度和权值对推荐算法的影响 参考原论文，这里不过多介绍，论文标题为：《基于三部图网络结构的知识推荐算法 》\n\n### 实验结论\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190805223958482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190805223911246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20190805224043927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=400px></center>\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["论文"],"categories":["技术篇"]},{"title":"点击率预估中的FM算法和FFM算法","url":"/2018/07/13/RecSys/Spark与推荐系统/点击率预估中的FM&FFM算法/","content":"\n>特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已\n\n<!--More-->\n# CTR方法概览\n  1.  广义线性模型+人工特征组合（LR+FeatureEngineering）\n  2. 非线性模型（GBDT，FM，FFM，DNN，MLR）\n  3. 广义线性模型+非线性模型组合特征（模型融合，常见的是LR+GBDT）\n\n其中 2（非线性模型）又可以分为：\n      ○ 矩阵分解类 （FM，FFM）\n      ○ 回归类        （GBDT，MLR）\n      ○ 神经网络类 （DNN）\n\n# FM算法\n## 背景\nFM算法（Factorization Machine）一般翻译为“因子分解机”，2010年，它由当时还在日本大阪大学的Steffen Rendle提出。此算法的主要作用是可以把所有特征进行高阶组合，减少人工参与特征组合的工作，工程师可以将精力集中在模型参数调优。FM只需要线性时间复杂度，可以应用于大规模机器学习。经过部分数据集试验，此算法在稀疏数据集合上的效果要明显好于SVM。\n## 要解决的问题\n假设一个广告分类问题，根据用户和广告位相关的特征，预测用户是否点击了广告。\n\nClicked?|  Country | Day | Ad_type\n--- | --- | ---| ---\n1 |  USA | 26/11/15 | Movie\n0 | China | 1/7/14 | Game\n1 | China | 19/2/15 | Game\n\n\"Clicked?\"是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。\n\nClicked? | Country=USA  |Country=China | Day=26/11/15 | Day=1/7/14 | Day=19/2/15 |Ad_type=Movie | Ad_type=Game\n---| ---| ---|---\n1 |1 |0| 1 |0| 0 |1| 0\n0 |0 |1 |0 |1 |0 |0 |1\n1 |0| 1 |0| 0 |1| 0 |1\n\n经过one-hot编码之后，特征变得稀疏，上边实例中每个样本有7维的特征，但平均仅有3维为非0值，在电商场景中预测sku是否被点击的过程中，特征往往要比上例中多的多，可见数据稀疏性是工业环境中不可避免的问题。另外一个问题就是特征在经过one-hot编码之后，维度会变得非常大，比如说三级品类有3k个，那么sku的所属三级品类特征经过编码之后就会变成3k维，特征空间就会剧增。\n\n依旧分析上边的例子，特征在经过关联之后，与label之间的相关性就会增加。例如“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。\n\n综上，FM所解决的问题是\n1)：特征稀疏\n2)：特征组合\n## 模型形式\n对于特征集合x=（x1,x2,x3,x4,....,xn）和标签y，希望得到x和y的对应关系，最简单的是建立线性回归模型，\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/fBKAMR8S_7Dba.png\"></img>\n\n但是，一般线性模型无法学习到高阶组合特征，所以会将特征进行高阶组合，这里以二阶为例(理论上，FM可以组合任意高阶，但是由于计算复杂度，实际中常用二阶，后面也主要介绍二阶组合的内容)。模型形式为，\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/cOBUyUJj_TmqU.png\"></img>\n\n其中n代表样本的特征数量，x_i是第i个特征，w_0，w_i，w_ij是模型参数\n从公式(2)中可以看出，组合特征参数一共有n(n-1)/2个，任意两个参数之间都是独立的，在数据特别稀疏的情况下，二次项参数的训练是十分困难的，原因为每个wij的训练都需要大量的非零x_i，x_j样本，由于样本稀疏，满足条件的样本很少，就会导致参数w_ij不准确，最终将影响模型的性能。\n如何解决二次项参数的训练？可以利用矩阵分解的思路。\n在model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子中，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量的点积就是矩阵中user对item的打分。\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/UhsawK7r_t2gO.png\"></img>\n\n所有的二次项参数 w_ij 可以组成一个对称矩阵W，那么这个矩阵可以分解为W=V^T * V，V的第j列便是第j维特征的隐向量，换句话说就是每个参数w_ij=<v_i,v_j>，这里的v_i和v_j是分别是第i，j个特征的隐向量，这就是FM的核心思想，因此FM的模型方程为（二阶形式）\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/ta3RTSJ7_P6vQ.png\"></img>\n\n其中，V_i是第i维特征的隐向量，<*,*>表示两个向量内积，隐向量的长度k（k<<n），包含k个描述特征的因子，这样二次项的参数便从n^2 减少到了nk，远少于多项式模型的参数数量。\n另外，参数因子化使得 x_h，x_i的参数和 x_i，x_j 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，x_h，x_i 和 x_i，x_j 的系数分别为 ⟨v_h,v_i⟩ 和 ⟨v_i,v_j⟩，它们之间有共同项 vi。也就是说，所有包含“xi的非零组合特征”（存在某个 j≠i，使得 x_i x_j≠0）的样本都可以用来学习隐向量 v_i，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，w_hi 和 w_ij 是相互独立的。\n显而易见，公式(3)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用MSE（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge/Cross-Entropy损失（铰链损失，互熵损失）来求解分类问题。当然，在进行二元分类时，FM的输出需要经过sigmoid变换，这与Logistic回归是一样的。直观上看，FM的复杂度是 O(kn^2)。但是，通过公式(3)的等式，FM的二次项可以化简，其复杂度可以优化到 O(kn)。由此可见，FM可以在线性时间对新样本作出预测。\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/ZvytpfVA_wCpX.png\"></img>\n\n从(3)—>(4)推导如下：\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/MS5AAkAm_kPrI.png\"></img>\n\n解读第（1）步到第（2）步，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角与上三角相等，有下式成立：\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/RpSGrOmQ_AfmL.png\"></img>\n如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/T8MAnmvn_apWg.png\"></img>\n\n其中，v_j,f 是隐向量 v_j 的第 f 个元素。由于 ∑nj=1vj,fxj 只与 f 有关，而与 i 无关，在每次迭代过程中，只需计算一次所有 f 的 ∑nj=1vj,fxj，就能够方便地得到所有 v_i,f 的梯度。显然，计算所有 f 的 ∑nj=1vj,fxj 的复杂度是 O(kn)；已知 ∑nj=1vj,fxj 时，计算每个参数梯度的复杂度是 O(1)；得到梯度后，更新每个参数的复杂度是 O(1)；模型参数一共有 nk+n+1 个。因此，FM参数训练的复杂度也是 O(kn)。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。\n## FM总结\n### FM降低了交叉项参数学习不充分的影响\none-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数w_ij用对应特征隐向量的内积表示，即⟨v_i,v_j⟩（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数w_ij的过程，转变为学习n个单特征对应k维隐向量的过程。\n很明显，单特征参数（k维隐向量v_i）的学习要比交叉项参数w_ij学习得更充分。示例说明：\n假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：\n\n共现交叉特征 | 样本数 |注\n--- | ---| ---\n<女性，汽车> | 500| 同时出现<女性，汽车>的样本数\n<女性，化妆品> | 1000| 同时出现<女性，化妆品>的样本数\n<男性，汽车> | 1500 |同时出现<男性，汽车>的样本数\n<男性，化妆品>| 0 |样本中无此特征组合项\n<女性，汽车>的含义是女性看汽车广告。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。\n因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响\n### FM提升了模型预估能力\n依然看上面的示例，样本中没有<男性，化妆品>交叉特征，即没有男性看化妆品广告的数据。如果用多项式模型来建模，对应的交叉项参数W_男性,化妆品是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告场景给出准确地预估。\nFM模型是否能得到交叉项参数W_男性,化妆品呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为W_男性,化妆品=⟨v_男性,v_化妆品⟩。\n用男性特征隐向量v_男性和v_化妆品特征隐向量v化妆品的内积表示交叉项参数 W_男性,化妆品。\n由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用⟨v_男性,v_化妆品⟩得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估能力。\n### FM提升了参数学习效率\n这个显而易见，参数个数由(n^2+n+1)变为(nk+n+1)个，模型训练复杂度也由O(mn^2)变为O(mnk)。m为训练样本数。对于训练样本和特征数而言，都是线性复杂度。\n此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。\n从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。\n### 总结\nFM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力\n\n# FFM算法\n## 背景\nFFM（Field-aware Factorization Machine）最初的概念来自Yu-Chin Juan（阮毓钦，毕业于中国台湾大学，现在美国Criteo工作）与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。\n## 模型形式\n通过引入field的概念，FFM把相同性质的特征，归结于同一个field。还是以FM中的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，三级品类有3k个，那么sku的所属三级品类特征经过编码之后就会变成3k维，那么这3k维可以放到一个field中，简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field。\n在FFM中，每一维特征x_i，针对其他特征的每一种field f_j，都会学习到一个隐向量V_i,f_ j。因此，隐向量不仅与特征有关，还与filed有关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type\"特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。\n假设样本的 n 个特征属于 f 个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/jUKaHRqz_pIvY.png\"></img>\n\n其中，f_ j 是第 j 个特征所属的field。如果隐向量的长度为 k，那么FFM的二次参数有 f * kn 个，远多于FM模型的 kn 个。此外，由于隐向量与field相关，FFM二次项并不能化简，其预测复杂度是 O(kn^2)。\n下面以一个例子简单说明FFM的特征组合方式。输入记录如下\nUser Movie Genre Price\nYuChin 3Idiots Comedy, Drama $9.99\n这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。\n\nField name | Field index | Feature name | Feature index\n--- | ---|---|---\nUser | 1 | User=YuChin |1\nMovie | 2 |  Movie=3Idiots | 2\nGenre | 3 | Genre=Comedy | 3\nPrice | 4 | Genre=Drama | 4\n | |  Price | 5\n那么，FFM的组合特征有10项，如下图所示。\n\n<img src=\"https://bdn.135editor.com/files/users/360/3608534/201807/BW3Vpww7_n3ZY.png\"></img>\n\n其中，红色是field编号，蓝色是特征编号，绿色是此样本的特征取值。二次项的系数是通过与特征field相关的隐向量点积得到的，二次项共有 n(n−1)/2 个。\n## 应用场景\n在DSP的场景中，FFM主要用来预估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。\nCTR和CVR预估模型都是在线下训练，然后用于线上预测。两个模型采用的特征大同小异，主要有三类：\n  ● -用户相关的特征\n用户相关的特征包括年龄、性别、职业、兴趣、品类偏好、浏览/购买品类等基本信息，以及用户近期点击量、购买量、消费额等统计信息。\n  ● 商品相关的特征\n商品相关的特征包括所属品类、销量、价格、评分、历史CTR/CVR等信息。\n  ● 用户-商品匹配特征\n用户-商品匹配特征主要有浏览/购买品类匹配、浏览/购买商家匹配、兴趣偏好匹配等几个维度。\n为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。\n除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。\nCTR、CVR预估样本的类别是按不同方式获取的。CTR预估的正样本是站内点击的用户-商品记录，负样本是展现但未点击的记录；CVR预估的正样本是站内支付（发生转化）的用户-商品记录，负样本是点击但未支付的记录。构建出样本数据后，采用FFM训练预估模型，并测试模型的性能。\n\n | #(field) |  #(feature) |  AUC  |Logloss\n--- | ---|---|---|---\n站内CTR |39 |2456 |0.77 |0.38\n站内CVR |67| 2441 |0.92 |0.13\n由于模型是按天训练的，每天的性能指标可能会有些波动，但变化幅度不是很大。这个表的结果说明，站内CTR/CVR预估模型是非常有效的。\n在训练FFM的过程中，有许多小细节值得特别关注。\n第一，样本归一化。FFM默认是进行样本数据的归一化，即 pa.norm 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。\n第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1] 是非常必要的。\n第三，省略零值特征。从FFM模型的表达式(8)可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。\n\n# 参考资料\n  1. http://bourneli.github.io/ml/fm/2017/07/02/fm-remove-combine-features-by-yourself.html\n  2. https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html\n  3. http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/\n  4. https://cloud.tencent.com/developer/article/1104673?fromSource=waitui\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"回归分析之逻辑回归-Logistic Regression","url":"/2018/04/28/机器学习/回归分析之逻辑回归-Logistics Regression/","content":"\n转载请注明出处：http://blog.csdn.net/gamer_gyt \n博主微博：http://weibo.com/234654758 \nGithub：https://github.com/thinkgamer\n\n\n> 逻辑回归与线性回归本质上是一样的，都是通过误差函数求解得到最优的系数，在形式上只不过是在线性回归上套了一个逻辑函数。线性回归的相关知识可以参考上边的给出链接文章，与线性回归相比，逻辑回归（Logistic Regression）更适用于因变量为二分变量的模型，Logistic 回归系数可用于估计模型中每个自变量的几率比。\n\n<!--More-->\n\n[回归分析之理论篇](http://blog.csdn.net/gamer_gyt/article/details/78008144)\n[回归分析之线性回归（N元线性回归）](http://blog.csdn.net/gamer_gyt/article/details/78135354)\n[回归分析之Sklearn实现电力预测](http://blog.csdn.net/gamer_gyt/article/details/78467021)\n\n# Sigmoid函数\n## 数学表达式和图形\n这里选用Sigmoid函数（海维赛德阶跃函数）作为LR的模型函数，是因为在二分类情况下输出为0和1，其函数的数学表达式为：\n$$\nf(x)=\\frac{1}{1+e^{-x}}\n$$\n其图形为：\n\n```\nimport math\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n  return 1 / (1 + math.exp(-x))\n\n# python2 中range生成的是一个数组，py3中生成的是一个迭代器，可以使用list进行转换\nX = list(range(-9,10))\nY = list(map(sigmoid,X))\n\n#画图\nplt.plot(X,Y)\nplt.show()\n\n```\n<center>![sigmoid](http://img.blog.csdn.net/20171219192329328?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n\n从上图可以看到sigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0/1。\n\n## LR为什么要使用Sigmoid\n首先Sigmoid函数满足LR模型的需要，在自变量小于0时，因变量小于0.5，自变量大于0时，因变量大于0.5。\n以下一段引用 [Logistic Regression 模型简介](https://tech.meituan.com/intro_to_logistic_regression.html) 的介绍对逻辑回归是一种概率模型做了阐述。\n\n>逻辑回归是一种判别模型，表现为直接对条件概率P(y|x)建模，而不关心背后的数据分布P(x,y)。而高斯贝叶斯模型（Gaussian Naive Bayes）是一种生成模型，先对数据的联合分布建模，再通过贝叶斯公式来计算样本属于各个类别的后验概率，即：\n$$\np(y|x) = \\frac{P(x|y)P(y)}{\\sum{P(x|y)P(y)}}\n$$\n通常假设P(x|y)是高斯分布，P(y)是多项式分布，相应的参数都可以通过最大似然估计得到。如果我们考虑二分类问题，通过简单的变化可以得到：\n$$\n\\log\\frac{P(y=1|x)}{P(y=0|x)} = \\log\\frac{P(x|y=1)}{P(x|y=0)} + \\log\\frac{P(y=1)}{P(y=0)}  \\ = -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\ + \\theta_0\n$$\n如果 σ1=σ0，二次项会抵消，我们得到一个简单的线性关系：\n$$\n\\log\\frac{P(y=1|x)}{P(y=0|x)} = \\theta^T x\n$$\n由上式进一步可以得到：\n$$\nP(y=1|x) = \\frac{e^{\\theta^T x}}{1+e^{\\theta^T x}} = \\frac{1}{1+e^{-\\theta^T x}}\n$$\n可以看到，这个概率和逻辑回归中的形式是一样的。这种情况下GNB 和 LR 会学习到同一个模型。实际上，在更一般的假设（P(x|y)的分布属于指数分布族）下，我们都可以得到类似的结论。\n\n至于逻辑回归为什么采用Sigmoid函数作为激活函数，因为LR是在伯努利分布和广义线性模型的假设下推导而来的，伯努利分布是属于指数分布簇的。\n\n具体的推导可以参考 [机器学习算法之：指数族分布与广义线性模型][1] 的介绍\n\n# Logistic Regression 分类器\n即决策函数。\n一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是：\n$$\nP(y=1|x;\\theta) = g(\\theta^T x) = \\frac{1}{1 + e ^ {-\\theta^T * x}}\n$$\n这里的 g(h)是上边提到的 sigmoid 函数$\\theta$ 表示的是一组参数，$\\theta^T $表示参数的转置矩阵，$\\theta^T x$其实表示的就是$\\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2} x_{2}....$，相应的决策函数为：\n$$\ny^* = 1, \\, \\textrm{if} \\, P(y=1|x) > 0.5\n$$\n选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。\n\n# 参数求解\n确定好决策函数之后就是求解参数了。\n\n模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然估计，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）越大。在逻辑回归模型中，似然度可表示为：\n$$\nL(\\theta) = P(D|\\theta) = \\prod P(y|x;\\theta) = \\prod g(\\theta^T x) ^ y (1-g(\\theta^T x))^{1-y} \n$$\n取对数可以得到对数似然度：\n$$\nl(\\theta) = \\sum {y\\log{g(\\theta^T x)} + (1-y)\\log{(1-g(\\theta^T x))}}\n$$\n另一方面，在机器学习领域，我们更经常遇到的是损失函数的概念，其衡量的是模型预测错误的程度。常用的损失函数有0-1损失，log损失，hinge损失等。其中log损失在单个数据点上的定义为\n$$\n-y\\log{p(y|x)}-(1-y)\\log{1-p(y|x)}\n$$\n如果取整个数据集上的平均log损失，我们可以得到\n$$\nJ(\\theta) = -\\frac{1}{N} l(\\theta)\n$$\n即在逻辑回归模型中，我们最大化似然函数和最小化log损失函数实际上是等价的。对于该优化问题，存在多种求解方法，这里以梯度下降的为例说明。梯度下降(Gradient Descent)又叫作最速梯度下降，是一种迭代求解的方法，通过在每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。基本步骤如下：\n\n- 选择下降方向（梯度方向，$\\nabla {J(\\theta)}$）\n- 选择步长，更新参数 $\\theta^i = \\theta^{i-1} - \\alpha^i \\nabla {J(\\theta^{i-1})}$\n- 重复以上两步直到满足终止条件\n\n![此处输入图片的描述][2]\n\n其中损失函数的梯度计算方法为：\n$$\n\\frac{\\partial{J}}{\\partial{\\theta}} = -\\frac{1}{n}\\sum_i (y_i - y_i^*)x_i + \\lambda \\theta\n$$\n沿梯度负方向选择一个较小的步长可以保证损失函数是减小的，另一方面，逻辑回归的损失函数是凸函数（加入正则项后是严格凸函数），可以保证我们找到的局部最优值同时是全局最优。此外，常用的凸优化的方法都可以用于求解该问题。例如共轭梯度下降，牛顿法，LBFGS等。\n\n# 分类边界\n知道如何求解参数后，我们来看一下模型得到的最后结果是什么样的。很容易可以从sigmoid函数看出，当$\\theta^T x > 0$ 时，y=1，否则 y=0。$\\theta^T x = 0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。下面两个图的对比说明了线性分类曲线和非线性分类曲线（通过特征映射）。\n![此处输入图片的描述][3]\n![此处输入图片的描述][4]\n左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是在特征转换 $[x_1, x_2] => [x_1, x_2, x_1^2, x_2^2, x_1x_2]$ 后的空间是线性可分的，对应的原始空间中分类边界为一条类椭圆曲线。\n\n# 总结\n逻辑回归的数学模型和求解都相对比较简洁，实现相对简单。通过对特征做离散化和其他映射，逻辑回归也可以处理非线性问题，是一个非常强大的分类器。因此在实际应用中，当我们能够拿到许多低层次的特征时，可以考虑使用逻辑回归来解决我们的问题。\n\n-----\n# 资料参考\n\nhttp://blog.csdn.net/han_xiaoyang/article/details/49123419\n\nhttps://www.cnblogs.com/sxron/p/5489214.html\n\nhttps://tech.meituan.com/intro_to_logistic_regression.html\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: https://blog.csdn.net/u011467621/article/details/48197943\n  [2]: https://tech.meituan.com/img/lr_intro/gradient_descent.png\n  [3]: https://tech.meituan.com/img/lr_intro/decision_boundary_1.png\n  [4]: https://tech.meituan.com/img/lr_intro/decision_boundary_2.png","tags":["机器学习"],"categories":["技术篇"]},{"title":"TensorFlow的MNIST学习","url":"/2018/04/22/深度学习/TensorFlow/TensorFlow的MNIST学习/","content":"\n> MNIST是在机器学习领域中的一个经典问题。该问题解决的是把28x28像素的灰度手写数字图片识别为相应的数字，其中数字的范围从0到9.\n\n<!--More-->\n\n数据集\t| 目的\n--|--\ndata_sets.train|\t55000 组 图片和标签, 用于训练。\ndata_sets.validation|\t5000 组 图片和标签, 用于迭代验证训练的准确性。\ndata_sets.test\t|10000 组 图片和标签, 用于最终测试训练的准确性。\n\n# 数据集简介\nMNIST数据集加载有两种办法，第一是直接从网上下载，第二是下载到本地进行load（跟第一种类似，只不过是事先下载好，从本地进行加载）。从网上下载到本地方式如下：\n\n```\n# 加载mnist数据集\nfrom tensorflow.examples.tutorials.mnist import input_data\nprint(\"load finish\")\n\nmnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\nprint(type(mnist))\n```\n输出为：\n```\nload finish\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n```\n\n---\n\n```\nprint(\"MNIST 训练集数据条数：\" ,mnist.train.num_examples)\nprint(\"MNIST 测试集数据条数：\" ,mnist.test.num_examples)\n\ntrain_img = mnist.train.images\ntrain_label = mnist.train.labels\nprint(\"训练集类型：\",type(train_img))\nprint(\"训练集维度：\",train_img.shape)\n\ntest_img = mnist.test.images\ntest_label = mnist.test.labels\nprint(\"测试集类型：\",type(test_img))\nprint(\"测试集维度：\",test_img.shape)\n```\n输出为：\n```\nMNIST 训练集数据条数： 55000\nMNIST 测试集数据条数： 10000\n训练集类型： <class 'numpy.ndarray'>\n训练集维度： (55000, 784)\n测试集类型： <class 'numpy.ndarray'>\n测试集维度： (10000, 784)\n```\n打开当前运行代码的目录，我们会发现一个MNIST_data的文件夹，里边包含的文件如下：\n文件\t| 内容\n--|--\ntrain-images-idx3-ubyte.gz |\t训练集图片 - 55000 张 训练图片, 5000 张 验证图片\ntrain-labels-idx1-ubyte.gz |\t训练集图片对应的数字标签\nt10k-images-idx3-ubyte.gz |\t测试集图片 - 10000 张 图片\nt10k-labels-idx1-ubyte.gz |\t测试集图片对应的数字标签\n\n使用next_batch函数加载指定条数的数据集\n```\n# 关于next_batch函数\nbatchSize = 100\nbatch_x,batch_y = mnist.train.next_batch(batch_size=batchSize)\nprint(batch_x.shape)\nprint(batch_y.shape)\n```\n输出为：\n```\n(100, 784)\n(100, 10)\n```\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["TensorFlow"],"categories":["技术篇"]},{"title":"TensorFlow实现线性回归","url":"/2018/04/22/深度学习/TensorFlow/TensorFlow实现线性回归/","content":"\n> TensorFlow写简单的代码是大财小用，需要很繁琐的代码方能实现简单的功能，但对于复杂的机器学习算法和深度神经网络却是十分的简单，下边看一个tf实现线性回归的Demo\n\n<!--More-->\n\n----\n首先引入tf包的相关的package，并初始化100个点。这里我们假定线性回归中的w为0.1 b为0.3，并设置一个随机数保证在y=0.1X + 0.3 上下浮动。\n```\n# coding: utf-8\n'''\ncreate by: Thinkgamer\ncreate time: 2018/04/22\ndesc: 使用tensorflow创建线性回归模型\n'''\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# 随机生成100个点\nnum_points = 100\nvectors_set = list()\nfor i in range(num_points):\n    x1 = np.random.normal(0.00,00.55)\n    y1 = x1* 0.1 + 0.3 + np.random.normal(0.0,0.03)\n    vectors_set.append([x1,y1])\n\n# 生成一些样本\nx_data = [v[0] for v in vectors_set]\ny_data = [v[1] for v in vectors_set]\n# print(x_data)\n# print(y_data)\nplt.scatter(x_data,y_data,c='r')\nplt.show()\n```\n生成的图如下所示：\n![这里写图片描述](https://img-blog.csdn.net/20180422142414284?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n用刚才生成的数据进行线性回归拟合\n\n```\n# 构造线性回归模型\n\n# 生成一维的W矩阵，取值是[-1,1]之间的随机数\nW = tf.Variable(tf.random_uniform([1],-1,1),name='W')\n# 生成一维的b矩阵，初始值为0\nb = tf.Variable(tf.zeros([1]),name='b')\n# 经过计算得出预估值y\ny=W*x_data + b\n\n# 定义损失函数，以预估值y和y_data之间的均方误差作为损失\nloss = tf.reduce_mean(tf.square(y - y_data),name='loss')\n# 采用地图下降算法来优化参数\noptimizer = tf.train.GradientDescentOptimizer(0.5)\n# 训练过程就是最小化误差\ntrain = optimizer.minimize(loss,name='train')\n\nsess = tf.Session()\n\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# 初始化W 和 b\nprint(\"初始化值：  W = \",sess.run(W), \"b= \",sess.run(b))\nfor step in range(20):\n    sess.run(train)\n    # 打印出每次训练后的w和b\n    print(\"第 %s 步：  W = \" % step,sess.run(W), \"b= \",sess.run(b))\n    \n# 展示\nplt.scatter(x_data,y_data,c='r')\nplt.plot(x_data,sess.run(W)*x_data+sess.run(b),c='b')\nplt.show()\n```\n对应的输出结果为：\n```\n初始化值：  W =  [-0.300848] b=  [0.]\n第 0 步：  W =  [-0.17744449] b=  [0.3065566]\n第 1 步：  W =  [-0.09421768] b=  [0.30550653]\n第 2 步：  W =  [-0.03631891] b=  [0.3047983]\n第 3 步：  W =  [0.0039596] b=  [0.3043056]\n第 4 步：  W =  [0.0319802] b=  [0.3039629]\n第 5 步：  W =  [0.05147332] b=  [0.30372444]\n第 6 步：  W =  [0.06503413] b=  [0.30355856]\n第 7 步：  W =  [0.07446799] b=  [0.30344316]\n第 8 步：  W =  [0.08103085] b=  [0.30336288]\n第 9 步：  W =  [0.08559645] b=  [0.30330706]\n第 10 步：  W =  [0.0887726] b=  [0.3032682]\n第 11 步：  W =  [0.09098216] b=  [0.30324116]\n第 12 步：  W =  [0.09251929] b=  [0.30322236]\n第 13 步：  W =  [0.09358863] b=  [0.30320928]\n第 14 步：  W =  [0.09433253] b=  [0.3032002]\n第 15 步：  W =  [0.09485004] b=  [0.30319384]\n第 16 步：  W =  [0.09521006] b=  [0.30318946]\n第 17 步：  W =  [0.09546052] b=  [0.3031864]\n第 18 步：  W =  [0.09563475] b=  [0.30318424]\n第 19 步：  W =  [0.09575596] b=  [0.30318278]\n```\n生成的图如下：\n![这里写图片描述](https://img-blog.csdn.net/20180422142454345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["TensorFlow"],"categories":["技术篇"]},{"title":"TensorFlow安装、变量学习和常用操作","url":"/2018/04/17/深度学习/TensorFlow/TensorFlow安装、变量学习和常用操作/","content":"\n> 本文主要介绍TensorFlow的安装 ，变量学习和一些常用操作。\n\n<!--More-->\n\n# 安装、入门\n环境说明：\n\n- deepin 15.4\n- python 3.5.4\n- tensorflow 1.7.0\n\n安装：\n```\npip3 install https://pypi.python.org/packages/dd/ed/9e6c6c16ff50be054277438669542555a166ed9f95a0dcaacff24fd3153a/tensorflow-1.7.0rc1-cp35-cp35m-manylinux1_x86_64.whl#md5=ec44ad9b0d040caef8ca0fac5b822b0d\n```\n测试：\n```\n>>> import tensorflow as tf\n>>> hello = tf.constant(\"hello , Thinkgamer\")\n>>> with tf.Session() as sess:\n...     print(sess.run(hello))\n...     sess.close()\n... \n2018-03-28 00:50:16.728894: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nb'hello , Thinkgamer'\n\n```\n使用 TensorFlow, 你必须明白 TensorFlow:\n\n- 使用图 (graph) 来表示计算任务.\n- 在被称之为 会话 (Session) 的上下文 (context) 中执行图.\n- 使用 tensor 表示数据.\n- 通过 变量 (Variable) 维护状态.\n- 使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.\n\n**session和InteractiveSession的区别：**\n\nsession：启动图的操作，有run()、close()\nInteractiveSession：默认会话\n```\n>>> sess = tf.InteractiveSession()\n>>> a = tf.constant(5.0)\n>>> b = tf.constant(6.0)\n>>> c = a * b\n>>> print(c.eval())\n30.0\n>>> sess.close()\n>>>\n>>> sess = tf.Session()\n>>> a = tf.constant(5.0)\n>>> b = tf.constant(6.0)\n>>> c = a * b\n>>> sess.run(c)\n30.0\n>>> sess.close()\n```\n\n# Tensorflow之变量\n## tensor的理解\ntensor即张量，tf中所有的数据通过数据流进行传输，可以声明任何一个张量，只有当一个张量进行run的时候，这个张量所涉及的tensor便会触发，这一点和spark的DAG很是相似，在同一条链上的某个节点进行触发操作时，该节点之前的所有节点便会参与计算。\n\n在tensorflow中，张量的维数被描述为“阶”，张量是以list的形式存储的。list有几重中括号，对应的张量就是几阶。如t=[ [1,2,3],[4,5,6],[7,8,9] ]，t就是一个二阶张量。\n\n我们可以认为，一阶张量，如[1,2,3]，相当于一个向量，二阶张量，如[ [1,2,3],[4,5,6],[7,8,9] ]，相当于一个矩阵。\n\n对于t=[ [1,2,3],[4,5,6],[7,8,9] ]来说，它的shape==>[3,3]，shape可以理解成：当脱去最外层的一对中括号后，里面有3个小list，然后每个小list里又有3个元素，所以该张量的shape==>[3,3]。\n\n举几个例子，如[ [1,2,3],[4,5,6] ] 的shape=[2,3](因为当脱去最外层的一对中括号后，里面有2个小list，然后每个小list里又有3个元素，所以该张量的shape==>[2,3]。）\n\n又如：\n```\n[\n    [ \n        [ [ 2 ], [ 2 ] ] ,\n        [ [ 2 ], [ 2 ] ] , \n        [ [ 2 ], [ 2 ] ] \n    ] , \n    [ \n        [ [ 2 ], [ 2 ] ] , \n        [ [ 2 ], [ 2 ] ] , \n        [ [ 2 ], [ 2 ] ] \n    ] ,\n    [ \n        [ [ 2 ], [ 2 ] ] , \n        [ [ 2 ], [ 2 ] ] , \n        [ [ 2 ], [ 2 ] ]\n    ] ,\n    [ \n        [ [ 2 ], [ 2 ] ] ,\n        [ [ 2 ], [ 2 ] ] ,\n        [ [ 2 ], [ 2 ] ] \n    ] \n]\n```\n的shape==>[4,3,2,1] (因为当脱去最外层的一对中括号后，里面有4个第二大的list，每个第二大的list里又有3个第三大的list，每个第三大的list里有2个第四大的list，每个第四大的list里有1个元素，所以该张量的shape==>[4,3,2,1]。\n\n```\n#coding: utf-8\n\n'''\ncreate by: thinkgamer\ncreate time: 2018/04/16\ndescription: 关于tensorflow变量的学习\n'''\n\nimport tensorflow as tf\n\na = tf.Variable([ [1,2,3],[4,5,6],[7,8,9] ])\nb = tf.Variable([ [1,2,3],[4,5,6] ])\nc = tf.Variable([[ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ,\n                 [ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ,\n                 [ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ,\n                 [ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ])\ninit_op = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\nprint(\"a shape: \",a.eval)\nprint(\"b shape: \",b.eval)\nprint(\"c shape: \",c.eval)\n```\n打印出的结果如下：\n```\na shape:  <bound method Variable.eval of <tf.Variable 'Variable_19:0' shape=(3, 3) dtype=int32_ref>>\nb shape:  <bound method Variable.eval of <tf.Variable 'Variable_20:0' shape=(2, 3) dtype=int32_ref>>\nc shape:  <bound method Variable.eval of <tf.Variable 'Variable_21:0' shape=(4, 3, 2, 1) dtype=int32_ref>>\n```\n## tf实现乘法\n```\n# tf 实现矩阵乘法\nval1 = tf.Variable([[1,2]])\nval2 = tf.Variable([[1],[2]])\nresult1 = tf.matmul(val1,val2)\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run(result1))\n    sess.close()\n```\n\n## tf的常用操作\n```\n# tf的常用操作，建议变量以float32为主,CPU,GPU均支持，否则容易出现一些错误,float32需要从tf中引入\nfrom tensorflow import float32\n\n# 创建一个shape=（3,4）的tensor\nt1 = tf.zeros([3,4],float32)\n\n# 创建一个shape类似于tensor的tensor，值分为全为0或1\ntensor = tf.Variable([ [1,2,3],[4,5,6],[7,8,9] ])\nt2 = tf.zeros_like(tensor)\nt3 = tf.ones_like(tensor)\n\n# 创建tensorflow支持的常量\nt4 = tf.constant([1,2,3,4])\n\n# 创建常量，指定值全为-1\nt5 = tf.constant(-1.0,shape=[2,3])\n\n# 创建数组\nt6 = tf.linspace(1.0,6.0,3,name=\"linspace\")\n# start=1, limit=9, delta=3\nt7 = tf.range(1,9,3)\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    print(\"t1: \",sess.run(t1))\n    print(\"\\ntermsor: \",sess.run(tensor))\n    print(\"\\nt2: \",sess.run(t2))\n    print(\"\\nt3: \",sess.run(t3))\n    print(\"\\nt4: \",sess.run(t4))\n    print(\"\\nt5: \",sess.run(t5))\n    print(\"\\nt6: \",sess.run(t6))\n    print(\"\\nt7: \",sess.run(t7))\nsess.close()\n\n#-----------------------------------------\n# tf创建符合指定正态分布的tensor\nt8 = tf.random_normal([2,3],mean=-1 ,stddev=4)\n\n# tf shuffle\nt9 = tf.constant([[1,2],[3,4],[5,6]])\nt10 = tf.random_shuffle(t9)\n\nsess = tf.Session()\nprint(\"\\nt8:\",sess.run(t8))\nprint(\"\\nt9:\",sess.run(t9))\nprint(\"\\nt10\",sess.run(t10))\n```\n打印出的值为：\n```\nt1:  [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\ntermsor:  [[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nt2:  [[0 0 0]\n [0 0 0]\n [0 0 0]]\n\nt3:  [[1 1 1]\n [1 1 1]\n [1 1 1]]\n\nt4:  [1 2 3 4]\n\nt5:  [[-1. -1. -1.]\n [-1. -1. -1.]]\n\nt6:  [1.  3.5 6. ]\n\nt7:  [1 4 7]\n\nt8:  [[  0.8365586   5.152416   -3.9977255]\n [ -1.7592524 -12.675492   -5.949805 ]]\n\nt9:  [[1 2]\n [3 4]\n [5 6]]\n\nt10:  [[3 4]\n [5 6]\n [1 2]]\nIn [ ]:\n\n```\n\n## tf实现i++\n```\n# tf 实现i++\nstate= tf.Variable(1)\nnew_value = tf.add(state,tf.Variable(1))\nupdate = tf.assign(state,new_value)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(state))\n    for _ in range(3):\n        sess.run(update)\n        print(sess.run(state))\n```\n打印结果为：\n```\n1\n2\n3\n4\n```\n\n## numpy转tensor\n```\n# numpy 转化为 tensor\nimport numpy as np\nnp1 = np.zeros((3,3))\nta = tf.convert_to_tensor(np1)\nwith tf.Session() as tf:\n    print(sess.run(ta))\n```\n\n# 测试遇到的问题\n\n## 1：tensorFlow与python版本不适应\n现象：NameError: name 'XXX' is not defined\n原因是：当前tensorflow版本与python版本不适应。\n解决办法：重新安装tf\n```\npip3 install --upgrade tensorflow\n\n------------\nCollecting tensorflow\n  Downloading tensorflow-1.6.0-cp35-cp35m-manylinux1_x86_64.whl (45.8MB)\n```\n## 2：变量未初始化\n```\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_58\n```\n解决办法：将变量进行初始化\n```\ninit = tf.global_variables_initializer()\ntf.Session().run(init)\n```\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["TensorFlow"],"categories":["技术篇"]},{"title":"机器学习中的AUC理解","url":"/2018/04/15/机器学习/机器学习中的AUC理解/","content":"> 最近在做GBDT模型，里边用到胡模型评价方法就是AUC，刚好趁此机会，好好学习一下。\n\n<!--More-->\n\n# 混淆矩阵(Confusion matrix)\n混淆矩阵是理解大多数评价指标的基础，毫无疑问也是理解AUC的基础。丰富的资料介绍着混淆矩阵的概念，下边用一个实例讲解什么是混淆矩阵\n\n如有100个样本数据，这些数据分成2类，每类50个。分类结束后得到的混淆矩阵为：\n![这里写图片描述](https://img-blog.csdn.net/20180415002706538?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n说明：\n40个为0类别的，预测正确，60个事实是0类别的给预测为1类别的\n40个为1类别的，预测正确，60个事实是1类别的给预测为0类别的\n\n其对应的混淆矩阵如下：\n![这里写图片描述](https://img-blog.csdn.net/2018041500331853?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n混淆矩阵包含四部分的信息：\n\n- True negative(TN)，称为真阴率，表明实际是负样本预测成负样本的样本数\n- False positive(FP)，称为假阳率，表明实际是负样本预测成正样本的样本数\n- False negative(FN)，称为假阴率，表明实际是正样本预测成负样本的样本数\n- True positive(TP)，称为真阳率，表明实际是正样本预测成正样本的样本数\n\n从上边的图可以分析出，对角线带True的为判断对了，斜对角线带False的为判断错了。\n\n像常见的准确率，精准率，召回率，F1-score，AUC都是建立在混淆矩阵上的。\n\n准确率（Accuracy）：判断正确的占总的数目的比例【（TN+TP）/100=(40+40)/100=0.8】\n精准率（precision）：判断正确的正类占进行判断的数目的比例（针对判别结果而言的，表示预测为正的数据中有多少是真的正确）【TP/(TP+FP) = 40/(40+60 )=0.4】\n召回率（recall）: 判断正确正类占应该应该判断正确的正类的比例（针对原样本而言，表示样本中的正例有多少被判断正确了）【TP/(TP+FN)=40/(40+60)=0.4】\nF1-Measure：精确值和召回率的调和均值【2*R*R/(P+R)=2*0.4*0.4/(0.4+0.4)=1】\n\n# AUC & ROC\nAUC是一个模型评价指标，只能用于二分类模型的评价，对于二分类模型，还有损失函数（logloss），正确率（accuracy），准确率（precision），但相比之下AUC和logloss要比accuracy和precision用的多，原因是因为很多的机器学习模型计算结果都是概率的形式，那么对于概率而言，我们就需要去设定一个阈值来判定分类，那么这个阈值的设定就会对我们的正确率和准确率造成一定成都的影响。\n\nAUC(Area under Curve)，表面上意思是曲线下边的面积，这么这条曲线是什么？——ROC曲线（receiver operating characteristic curve，接收者操作特征曲线）。\n\n接下来分析下面这张图（图片来自百度百科）：\n![这里写图片描述](https://img-blog.csdn.net/20180415015350726?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\nX轴是假阳率：FP/(FP+TN)\nY轴是真阳性：TP(TP+FN)\nROC曲线给出的是当阈值(分类器必须提供每个样例被判为阳性或者阴性的可信程度值)变化时假阳率和真阳率的变化情况，左下角的点所对应的是将所有样例判为反例的情况，而右上角的点对应的则是将所有样例判断为正例的情况。\nROC曲线不但可以用于比较分类器，还可以基于成本效益分析来做出决策。在理想情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在假阳率很低的同时获得了很高的真阳率。\n\n# AUC计算\n以下部分引用 [ROC曲线与AUC计算][1] 中的举例\n\n------------------\n假设有6次展示记录，有两次被点击了，得到一个展示序列（1:1,2:0,3:1,4:0,5:0,6:0），前面的表示序号，后面的表示点击（1）或没有点击（0）。\n然后在这6次展示的时候都通过model算出了点击的概率序列。\n\n下面看三种情况。\n\n一、如果概率的序列是（1:0.9,2:0.7,3:0.8,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）\n1 0.9\n1 0.8\n0 0.7\n0 0.6\n0 0.5\n0 0.4\n绘制的步骤是：\n1）把概率序列从高到低排序，得到顺序（1:0.9,3:0.8,2:0.7,4:0.6,5:0.5,6:0.4）；\n2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；\n3）从概率最大开始，再取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.0；\n4）再从最大开始取一个点作为正类，取到点2，计算得到TPR=1.0，FPR=0.25;\n5）以此类推，得到6对TPR和FPR。\n然后把这6对数据组成6个点(0,0.5),(0,1.0),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。\n这6个点在二维坐标系中能绘出来。\n![这里写图片描述](https://img-blog.csdn.net/20180415024426171?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n看看图中，那个就是ROC曲线。\n\n二、如果概率的序列是（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）\n1 0.9\n0 0.8\n1 0.7\n0 0.6\n0 0.5\n0 0.4\n绘制的步骤是：\n6）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；\n7）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；\n8）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；\n9）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;\n10）     以此类推，得到6对TPR和FPR。\n然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。\n这6个点在二维坐标系中能绘出来。\n![这里写图片描述](https://img-blog.csdn.net/20180415024524722?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n看看图中，那个就是ROC曲线。\n\n三、如果概率的序列是（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）。与原来的序列一起，得到序列（从概率从高到低排）\n0 0.9\n0 0.8\n0 0.7\n0 0.6\n1 0.5\n1 0.4\n绘制的步骤是：\n11）把概率序列从高到低排序，得到顺序（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）；\n12）从概率最大开始取一个点作为正类，取到点6，计算得到TPR=0.0，FPR=0.25；\n13）从概率最大开始，再取一个点作为正类，取到点5，计算得到TPR=0.0，FPR=0.5；\n14）再从最大开始取一个点作为正类，取到点4，计算得到TPR=0.0，FPR=0.75;\n15）以此类推，得到6对TPR和FPR。\n然后把这6对数据组成6个点(0.25,0.0),(0.5,0.0),(0.75,0.0),(1.0,0.0),(1.0,0.5),(1.0,1.0)。\n这6个点在二维坐标系中能绘出来。\n![这里写图片描述](https://img-blog.csdn.net/20180415024654878?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n看看图中，那个就是ROC曲线\n\n意义：\n如上图的例子，总共6个点，2个正样本，4个负样本，取一个正样本和一个负样本的情况总共有8种。\n\n上面的第一种情况，从上往下取，无论怎么取，正样本的概率总在负样本之上，所以分对的概率为1，AUC=1。再看那个ROC曲线，它的积分是什么？也是1，ROC曲线的积分与AUC相等。\n\n上面第二种情况，如果取到了样本2和3，那就分错了，其他情况都分对了；所以分对的概率是0.875，AUC=0.875。再看那个ROC曲线，它的积分也是0.875，ROC曲线的积分与AUC相等。\n\n上面的第三种情况，无论怎么取，都是分错的，所以分对的概率是0，AUC=0.0。再看ROC曲线，它的积分也是0.0，ROC曲线的积分与AUC相等。\n\n很牛吧，其实AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。\n\n绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了。\n\n\n  [1]: https://blog.csdn.net/guohecang/article/details/52858019\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"梯度提升决策树-GBDT（Gradient Boosting Decision Tree）","url":"/2018/04/12/RecSys/Spark与推荐系统/梯度提升决策树-GBDT（Gradient Boosting Decision Tree）/","content":"\n> 研究GBDT的背景是业务中使用到了该模型，用于做推荐场景，当然这里就引出了GBDT的一个应用场景-回归，他的另外一个应用场景便是分类，接下来我会从以下几个方面去学习和研究GBDT的相关知识，当然我也是学习者，只是把我理解到的整理出来。本文参考了网上各路大神的笔记，在此感谢！\n\n<!--More-->\n# Boosting&Bagging\n集成学习方法不是单独的一个机器学习算法，而是通过构建多个机器学习算法来达到一个强学习器。集成学习可以用来进行分类，回归，特征选取和异常点检测等。随机森林算法就是一个典型的集成学习方法，简单的说就是由一个个弱分类器（决策树）来构建一个强分类器，从而达到比较好的分类效果。\n\n那么如何得到单个的学习器，一般有两种方法：\n\n- 同质（对于一个强学习器而言，所用的单个弱学习器都是一样的，比如说用的都是决策树，或者都是神经网络）\n- 异质（相对于同质而言，对于一个强学习器而言，所用的单个弱学习器不全是一样的，比如说用的决策树和神经网络的组合）\n\n相对异质而言，同质学习期用的最为广泛，我们平时所讨论的集成学习方法指的就是同质个体学习器，同质个体学习器按照个体学习器之间的依赖关系分为串行（有强依赖关系）和并行（不存在关系或者有很弱的依赖关系），而在串行关系中有代表性的就是boosting系列算法，并行关系中具有代表性的就是bagging和随机森林（random forest）\n\nBoosting流程图\n![这里写图片描述](https://img-blog.csdn.net/20180410010257504?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\nBagging流程图 \n![这里写图片描述](https://img-blog.csdn.net/20180410010305827?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n上边简单的介绍了集成学习方法和boosting&bagging的区别，那么对于单个学习器采用何种策略才能得到一个强学习器呢？\n\n- 平均法（加权（个体学习器性能相差较大），简单（性能相近））\n- 投票法（绝对多数（超过半数标记。否则拒绝预测），相对多数，加权投票）\n- 学习法（通过另一个学习器来进行结合，Stacking算法）\n\n> Stacking算法：\n基本思想：先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而出事样本的标记仍被当作样例标记。\n<br>\n注意点：若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；一般会通过交叉验证等方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。\n\n--------\n# Gradient Boosting\n\nGradient Boosting是一种Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。而让损失函数持续下降，就能使得模型不断改性提升性能，其最好的方法就是使损失函数沿着梯度方向下降（讲道理梯度方向上下降最快）。\n\nGradient Boost是一个框架，里面可以套入很多不同的算法。\n\n# 分类树&回归树&分类回归树\n## 分类树\n三种比较常见的分类决策树分支划分方式包括：ID3, C4.5, CART。\n> 以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature<=阈值，和feature>阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。\n\n总结：分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。\n\n## 回归树\n> 回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。\n\n总结：回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。\n\n## 分类回归树\n> Classification And Regression Trees，即既能做分类任务又能做回归任务，CART也是决策树的一种，是一种二分决策树，但是也可以用来做回归，CART同决策树类似，不同于 ID3 与 C4.5 ,分类树采用基尼指数来选择最优的切分特征，而且每次都是二分。至于怎么利用基尼系数进行最优的特征切分，大家可以参考这篇文章的详细介绍 [决策树之 CART][1]\n\n# 损失函数\n机器学习中的损失函数有很多，常见的有\n\n- 0-1损失函数（0-1 loss function）\n$$\nL(Y,f(X))=\\left\\{ \\begin{aligned}&1,\\quad Y\\ne f(X)\\\\& 0,\\quad Y=f(X) \\end{aligned} \\right.\n$$\n\n> 该损失函数的意义就是，当预测错误时，损失函数值为1，预测正确时，损失函数值为0。该损失函数不考虑预测值和真实值的误差程度，也就是只要预测错误，预测错误差一点和差很多是一样的。\n\n- 平方损失函数（quadratic loss function）\n$$\nL(Y,f(X))=(Y-f(X))^2\n$$\n> 取预测差距的平方\n\n- 绝对值损失函数（absolute loss function）\n$$\nL(Y,f(X))=|Y-f(X)|\n$$\n> 取预测值与真实值的差值绝对值，差距不会被平方放大\n\n- 对数损失函数（logarithmic loss function）\n$$\nL(Y,P(Y|X))=-logP(Y|X)\n$$\n> 该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。\n\n- 全局损失函数 \n> 上面的损失函数仅仅是对于一个样本来说的。而我们的优化目标函数应当是使全局损失函数最小。因此，全局损失函数往往是每个样本的损失函数之和，即： \n$$\nJ(w,b)=\\frac{1}{m} \\sum_{i=1}^m L(Y,f(X))\n$$\n对于平方损失函数，为了求导方便，我们可以在前面乘上一个1/2，和平方项求导后的2抵消，即： \n$$\nJ(w,b)=\\frac{1}{2m} \\sum_{i=1}^m L(Y,f(X))\n$$\n- 逻辑回归中的损失函数\n> 在逻辑回归中，我们采用的是对数损失函数。由于逻辑回归是服从伯努利分布(0-1分布)的，并且逻辑回归返回的sigmoid值是处于(0,1)区间，不会取到0,1两个端点。因此我们能够将其损失函数写成以下形式： \n$$\nL(\\hat y,y)=-(y\\log{\\hat y}+(1-y)\\log(1-\\hat y))\n$$\n\n# GBDT思想\n> 以下部分学习于 [GBDT算法原理深入解析][2] ，原文作者讲的很好，照搬过来，毕竟笔者不是推导数学公式的料，哈哈\n\nGBDT 可以看成是由K棵树组成的加法模型：\n$$\n\\hat{y}_i=\\sum_{k=1}^K f_k(x_i), f_k \\in F \\tag 0\n$$\n\n其中F为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\\Theta=\\{f_1,f_2, \\cdots, f_K \\}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合\n\n上述加法模型的目标函数定义为：$Obj=\\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)$，其中$\\Omega$表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。\n\n如何来学习加法模型呢？\n\n解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下： \n$$\n\\begin{split}\n\\hat{y}_i^0 &= 0 \\\\\n\\hat{y}_i^1 &= f_1(x_i) = \\hat{y}_i^0 + f_1(x_i) \\\\\n\\hat{y}_i^2 &= f_1(x_i) + f_2(x_i) = \\hat{y}_i^1 + f_2(x_i) \\\\\n& \\cdots \\\\\n\\hat{y}_i^t &= \\sum_{k=1}^t f_k(x_i) = \\hat{y}_i^{t-1} + f_t(x_i) \\\\\n\\end{split}\n$$\n\n那么，在每一步如何决定哪一个函数$f$被加入呢？指导原则还是最小化目标函数。 \n在第$t$步，模型对$x_i$的预测为：$\\hat{y}_i^t= \\hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为：\n$$\n\\begin{split}\nObj^{(t)} &= \\sum_{i=1}^nl(y_i, \\hat{y}_i^t) + \\sum_{i=i}^t \\Omega(f_i) \\\\\n&=  \\sum_{i=1}^n l\\left(y_i, \\hat{y}_i^{t-1} + f_t(x_i) \\right) + \\Omega(f_t) + constant\n\\end{split}\\tag{1}\n$$\n举例说明，假设损失函数为平方损失（square loss），则目标函数为：\n$$\n\\begin{split}\nObj^{(t)} &= \\sum_{i=1}^n \\left(y_i - (\\hat{y}_i^{t-1} + f_t(x_i)) \\right)^2 + \\Omega(f_t) + constant \\\\\n&= \\sum_{i=1}^n \\left[2(\\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \\right] + \\Omega(f_t) + constant\n\\end{split}\\tag{2}\n$$\n\n其中，$(\\hat{y}_i^{t-1} - y_i)$称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。\n> 泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在点$a$处$n+1$次可导，那么对于这个区间上的任意$x$都有：$\\displaystyle f(x)=\\sum _{n=0}^{N}\\frac{f^{(n)}(a)}{n!}(x-a)^ n+R_ n(x)$，其中的多项式称为函数在$a$处的泰勒展开式，$R_ n(x)$是泰勒公式的余项且是$(x-a)^ n$的高阶无穷小。\n\n根据泰勒公式把函数$f(x+\\Delta x)$在点$x$处二阶展开，可得到如下等式： \n$$\nf(x+\\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac12 f''(x)\\Delta x^2 \\tag 3\n$$\n由等式(1)可知，目标函数是关于变量$\\hat{y}_i^{t-1} + f_t(x_i)$的函数，若把变量$\\hat{y}_i^{t-1}$看成是等式(3)中的$x$，把变量$f_t(x_i)$看成是等式(3)中的$\\Delta x$，则等式(1)可转化为：\n$$\nObj^{(t)} = \\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{t-1}) + g_if_t(x_i) + \\frac12h_if_t^2(x_i) \\right]  + \\Omega(f_t) + constant \\tag 4\n$$\n其中$g_i$，定义为损失函数的一阶导数，即$g_i=\\partial_{\\hat{y}^{t-1}}l(y_i,\\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\\partial_{\\hat{y}^{t-1}}^2l(y_i,\\hat{y}^{t-1})$。 \n假设损失函数为平方损失函数，则$g_i=\\partial_{\\hat{y}^{t-1}}(\\hat{y}^{t-1} - y_i)^2 = 2(\\hat{y}^{t-1} - y_i)$，$h_i=\\partial_{\\hat{y}^{t-1}}^2(\\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。\n由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得： \n$$\nObj^{(t)} \\approx \\sum_{i=1}^n \\left[ g_if_t(x_i) + \\frac12h_if_t^2(x_i) \\right]  + \\Omega(f_t) \\tag 5\n$$\n由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数$f(x)$，从而根据加法模型等式(0)可得最终要学习的模型。\n\n# GBDT在Scikit-learn中的调用\n关于GBDT在Scikit-learn中的实现原文在 [点击查看][3]\nGBDT在sklearn中导入的包不一样，分类是  from sklearn.ensemble import GradientBoostingClassifier，回归是 from sklearn.ensemble import GradientBoostingRegressor\n\n## 参数说明\nGBDT的参数分为boosting类库参数和弱学习器参数，其中有GBDT的弱学习器为CART，所以弱学习器参数基本为决策树的参数，参考[点击阅读][4]\n###　类库参数\n\n- loss：损失函数，对于分类模型，有对数似然损失函数\"deviance\"和指数损失函数\"exponential\"两者输入选择。默认是对数似然损失函数\"deviance\"。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的\"deviance\"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。\n- learning_rate：即每个弱学习器的权重缩减系数νν，也称作步长。\n- n_estimators：就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n- subsample：子采样，取值是[0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。\n- init：即我们的初始化的时候的弱学习器，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。\n- verbose：默认是0，代表启用详细输出，若为1，代表偶尔输出进度信息\n- warm_start：默认为false\n- random_state：如果int，random_state随机数生成器使用的种子；如果randomstate实例，random_state是随机数发生器；如果没有，随机数生成器使用的np.random的randomstate实例。\n- presort：默认情况下会在密集的数据上使用，默认是在稀疏数据正常排序。设置对true的稀疏数据将会引起错误。\n\n\n### 决策树参数\n- max_depth：决策树最大深度\n- criterion：衡量分裂指标的度量方法，支持的是均方误差\n- min_samples_split：内部节点再划分所需最小样本数。这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。\n- min_samples_leaf：叶子节点最少样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。\n- min_weight_fraction_leaf：叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。\n- max_features：划分时考虑的最大特征数。可以使用很多种类型的值，默认是\"None\",意味着划分时考虑所有的特征数；如果是\"log2\"意味着划分时最多考虑log2Nlog2N个特征；如果是\"sqrt\"或者\"auto\"意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的\"None\"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。\n- max_leaf_nodes：最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是\"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。\n- min_impurity_split：节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。\n- min_impurity_decrease：默认值为0。如果分裂\n\n## 分类\n```\n>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)                 \n0.913...\n```\n\n## 回归\n```\n>>> import numpy as np\n>>> from sklearn.metrics import mean_squared_error\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n>>> X_train, X_test = X[:200], X[200:]\n>>> y_train, y_test = y[:200], y[200:]\n>>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n>>> mean_squared_error(y_test, est.predict(X_test))    \n5.00...\n```\n\n## 模型评价\n```\nfrom sklearn import cross_validation, metrics\nmetrics.accuracy_score(y.values, y_pred) # 准确度\nmetrics.roc_auc_score(y, y_predprob)  # AUC大小\n```\n\n----\n参考资料：\n\n-  [集成学习原理小结][5]\n-  [Regression Tree 回归树][6]\n-  [浅析机器学习中各种损失函数及其含义][7]\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n---\n\n<center>\n    <img src=\"http://img.blog.csdn.net/20180204165223361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" width=\"35%\" height=\"35%\" />\n    <br>\n打开微信扫一扫，加入数据与算法交流大群\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n  [1]: https://www.cnblogs.com/ooon/p/5647309.html\n  [2]: https://www.zybuluo.com/yxd/note/611571\n  [3]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n  [4]: https://www.cnblogs.com/DjangoBlog/p/6201663.html\n  [5]: https://www.cnblogs.com/pinard/p/6131423.html\n  [6]: https://blog.csdn.net/weixin_40604987/article/details/79296427\n  [7]: https://blog.csdn.net/qq547276542/article/details/77980042","tags":["Spark与推荐系统"],"categories":["技术篇"]},{"title":"Softmax-Regression","url":"/2018/03/28/机器学习/Softmax-Regression/","content":"> 在本节中，我们介绍Softmax回归模型\n\n<!--More-->\n\n# 简介\n该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签 \\textstyle y 可以取两个以上的值。 Softmax回归模型对于诸如MNIST手写数字分类等问题是很有用的，该问题的目的是辨识10个不同的单个数字。Softmax回归是有监督的，不过后面也会介绍它与深度学习/无监督学习方法的结合。（译者注： MNIST 是一个手写数字识别库，由NYU 的Yann LeCun 等人维护。http://yann.lecun.com/exdb/mnist/ ）\n\n回想一下在 logistic 回归中，我们的训练集由 \\textstyle m 个已标记的样本构成：$ \\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\} $ ，其中输入特征$ x^{(i)} \\in \\Re^{n+1} $（我们对符号的约定如下：特征向量\n$ \\textstyle x $ 的维度为 \\textstyle n+1，其中 \\textstyle x_0 = 1 对应截距项 。） 由于 logistic 回归是针对二分类问题的，因此类标记 $ y^{(i)} \\in \\{0,1\\}$。假设函数(hypothesis function) 如下：\n\n\n\n$$\n\\begin{align}\nh_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)},\n\\end{align}\n$$\n\n我们将训练模型参数$ \\textstyle \\theta $，使其能够最小化代价函数 ：\n$$\n\\begin{align}\nJ(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right]\n\\end{align}\n$$\n在 softmax回归中，我们解决的是多分类问题（相对于 logistic 回归解决的二分类问题），类标$ \\textstyle y $可以取$ \\textstyle k $ 个不同的值（而不是 2 个）。因此，对于训练集$ \\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\} $，我们有$ y^{(i)} \\in \\{1, 2, \\ldots, k\\} $。（注意此处的类别下标从 1 开始，而不是 0）。例如，在 MNIST 数字识别任务中，我们有$ \\textstyle k=10 $个不同的类别。\n\n\n对于给定的测试输入$ \\textstyle x$，我们想用假设函数针对每一个类别j估算出概率值$ \\textstyle p(y=j | x)$。也就是说，我们想估计$ \\textstyle x $ 的每一种分类结果出现的概率。因此，我们的假设函数将要输出一个 $ \\textstyle k $维的向量（向量元素的和为1）来表示这$ \\textstyle k$ 个估计的概率值。 具体地说，我们的假设函数$ \\textstyle h_{\\theta}(x) $ 形式如下：\n\n$$\n\\begin{align}\nh_\\theta(x^{(i)}) =\n\\begin{bmatrix}\np(y^{(i)} = 1 | x^{(i)}; \\theta) \\\\\np(y^{(i)} = 2 | x^{(i)}; \\theta) \\\\\n\\vdots \\\\\np(y^{(i)} = k | x^{(i)}; \\theta)\n\\end{bmatrix}\n=\n\\frac{1}{ \\sum_{j=1}^{k}{e^{ \\theta_j^T x^{(i)} }} }\n\\begin{bmatrix}\ne^{ \\theta_1^T x^{(i)} } \\\\\ne^{ \\theta_2^T x^{(i)} } \\\\\n\\vdots \\\\\ne^{ \\theta_k^T x^{(i)} } \\\\\n\\end{bmatrix}\n\\end{align}\n$$\n其中$ \\theta_1, \\theta_2, \\ldots, \\theta_k \\in \\Re^{n+1}$ 是模型的参数。请注意$ \\frac{1}{ \\sum_{j=1}^{k}{e^{ \\theta_j^T x^{(i)} }} } $这一项对概率分布进行归一化，使得所有概率之和为 1 。\n\n\n为了方便起见，我们同样使用符号$ \\textstyle \\theta$ 来表示全部的模型参数。在实现Softmax回归时，将$ \\textstyle \\theta$ 用一个$ \\textstyle k \\times(n+1) $的矩阵来表示会很方便，该矩阵是将 $ \\theta_1, \\theta_2, \\ldots, \\theta_k $ 按行罗列起来得到的，如下所示：\n\n$$\n\\theta = \\begin{bmatrix}\n\\mbox{---} \\theta_1^T \\mbox{---} \\\\\n\\mbox{---} \\theta_2^T \\mbox{---} \\\\\n\\vdots \\\\\n\\mbox{---} \\theta_k^T \\mbox{---} \\\\\n\\end{bmatrix}\n$$\n# 代价函数\n现在我们来介绍 softmax 回归算法的代价函数。在下面的公式中，$\\textstyle 1\\{\\cdot\\}$ 是示性函数，其取值规则为：\n$ \\textstyle 1\\{ 值为真的表达式 \\textstyle \\}=1 $\n\n，$ \\textstyle 1\\{ 值为假的表达式 \\textstyle \\}=0 $。举例来说，表达式$ \\textstyle 1\\{2+2=4\\} $的值为1 ，$ \\textstyle 1\\{1+1=5\\} $的值为 0。我们的代价函数为：\n\n$$\n\\begin{align}\nJ(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k}  1\\left\\{y^{(i)} = j\\right\\} \\log \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)} }}\\right]\n\\end{align}\n$$\n值得注意的是，上述公式是logistic回归代价函数的推广。logistic回归代价函数可以改为：\n\n$$\n\\begin{align}\nJ(\\theta) &= -\\frac{1}{m} \\left[ \\sum_{i=1}^m   (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right] \\\\\n&= - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=0}^{1} 1\\left\\{y^{(i)} = j\\right\\} \\log p(y^{(i)} = j | x^{(i)} ; \\theta) \\right]\n\\end{align}\n$$\n可以看到，Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 $\\textstyle k $个可能值进行了累加。注意在Softmax回归中将 \\textstyle x 分类为类别 $\\textstyle j $的概率为：\n\n$$\np(y^{(i)} = j | x^{(i)} ; \\theta) = \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)}} }\n.\n$$\n对于$ \\textstyle J(\\theta)$ 的最小化问题，目前还没有闭式解法。因此，我们使用迭代的优化算法（例如梯度下降法，或 L-BFGS）。经过求导，我们得到梯度公式如下：\n\n$$\n\\begin{align}\n\\nabla_{\\theta_j} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = j\\}  - p(y^{(i)} = j | x^{(i)}; \\theta) \\right) \\right]  }\n\\end{align}\n$$\n让我们来回顾一下符号 \"$ \\textstyle \\nabla_{\\theta_j}$\" 的含义。$\\textstyle \\nabla_{\\theta_j} J(\\theta)$ 本身是一个向量，它的第$ \\textstyle l $个元素$ \\textstyle \\frac{\\partial J(\\theta)}{\\partial \\theta_{jl}} $是$ \\textstyle J(\\theta)对\\textstyle \\theta_j $的第 $\\textstyle l $个分量的偏导数。\n\n\n有了上面的偏导数公式以后，我们就可以将它代入到梯度下降法等算法中，来最小化$ \\textstyle J(\\theta)$。 例如，在梯度下降法的标准实现中，每一次迭代需要进行如下更新: $\\textstyle \\theta_j := \\theta_j - \\alpha \\nabla_{\\theta_j} J(\\theta)(\\textstyle j=1,\\ldots,k）$。\n\n当实现 softmax 回归算法时， 我们通常会使用上述代价函数的一个改进版本。具体来说，就是和权重衰减(weight decay)一起使用。我们接下来介绍使用它的动机和细节。\n\n\n# Softmax回归模型参数化的特点\nSoftmax 回归有一个不寻常的特点：它有一个“冗余”的参数集。为了便于阐述这一特点，假设我们从参数向量 $\\textstyle \\theta_j $中减去了向量 $\\textstyle \\psi$，这时，每一个$ \\textstyle \\theta_j $都变成了 $\\textstyle \\theta_j - \\psi(\\textstyle j=1, \\ldots, k)$。此时假设函数变成了以下的式子：\n\n$$\n\\begin{align}\np(y^{(i)} = j | x^{(i)} ; \\theta)\n&= \\frac{e^{(\\theta_j-\\psi)^T x^{(i)}}}{\\sum_{l=1}^k e^{ (\\theta_l-\\psi)^T x^{(i)}}}  \\\\\n&= \\frac{e^{\\theta_j^T x^{(i)}} e^{-\\psi^Tx^{(i)}}}{\\sum_{l=1}^k e^{\\theta_l^T x^{(i)}} e^{-\\psi^Tx^{(i)}}} \\\\\n&= \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)}}}.\n\\end{align}\n$$\n换句话说，从$ \\textstyle \\theta_j $中减去$ \\textstyle \\psi$ 完全不影响假设函数的预测结果！这表明前面的 softmax 回归模型中存在冗余的参数。更正式一点来说， Softmax 模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数$ \\textstyle h_\\theta$。\n\n\n进一步而言，如果参数 $\\textstyle (\\theta_1, \\theta_2,\\ldots, \\theta_k) $是代价函数$ \\textstyle J(\\theta)$ 的极小值点，那么 $\\textstyle (\\theta_1 - \\psi, \\theta_2 - \\psi,\\ldots,\n\\theta_k - \\psi) $同样也是它的极小值点，其中 $\\textstyle \\psi$ 可以为任意向量。因此使 $\\textstyle J(\\theta)$ 最小化的解不是唯一的。（有趣的是，由于$ \\textstyle J(\\theta)$ 仍然是一个凸函数，因此梯度下降时不会遇到局部最优解的问题。但是 Hessian 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题）\n\n\n注意，当$ \\textstyle \\psi = \\theta_1$ 时，我们总是可以将 $\\textstyle \\theta_1$替换为$\\textstyle \\theta_1 - \\psi = \\vec{0}$（即替换为全零向量），并且这种变换不会影响假设函数。因此我们可以去掉参数向量 $\\textstyle \\theta_1 $（或者其他 $\\textstyle \\theta_j $中的任意一个）而不影响假设函数的表达能力。实际上，与其优化全部的$ \\textstyle k\\times(n+1) $个参数 $\\textstyle (\\theta_1, \\theta_2,\\ldots, \\theta_k)$ （其中 $\\textstyle \\theta_j \\in \\Re^{n+1}）$，我们可以令$ \\textstyle \\theta_1 =\n\\vec{0}$，只优化剩余$的 \\textstyle (k-1)\\times(n+1)$ 个参数，这样算法依然能够正常工作。\n\n\n在实际应用中，为了使算法实现更简单清楚，往往保留所有参数$ \\textstyle (\\theta_1, \\theta_2,\\ldots, \\theta_n)$，而不任意地将某一参数设置为 0。但此时我们需要对代价函数做一个改动：加入权重衰减。权重衰减可以解决 softmax 回归的参数冗余所带来的数值问题。\n\n\n# 权重衰减\n我们通过添加一个权重衰减项$\\textstyle \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^{n} \\theta_{ij}^2$ 来修改代价函数，这个衰减项会惩罚过大的参数值，现在我们的代价函数变为：\n\n$$\n\\begin{align}\nJ(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k} 1\\left\\{y^{(i)} = j\\right\\} \\log \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)} }}  \\right]\n              + \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^n \\theta_{ij}^2\n\\end{align}\n$$\n有了这个权重衰减项以后 ($\\textstyle \\lambda > 0$)，代价函数就变成了严格的凸函数，这样就可以保证得到唯一的解了。 此时的 Hessian矩阵变为可逆矩阵，并且因为$\\textstyle J(\\theta)$是凸函数，梯度下降法和 L-BFGS 等算法可以保证收敛到全局最优解。\n\n\n为了使用优化算法，我们需要求得这个新函数$ \\textstyle J(\\theta)$ 的导数，如下：\n$$\n\n\\begin{align}\n\\nabla_{\\theta_j} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} ( 1\\{ y^{(i)} = j\\}  - p(y^{(i)} = j | x^{(i)}; \\theta) ) \\right]  } + \\lambda \\theta_j\n\\end{align}\n\n通过最小化 \\textstyle J(\\theta)，我们就能实现一个可用的 softmax 回归模型。\n\n\n# Softmax回归与Logistic 回归的关系\n当类别数$ \\textstyle k = 2 $时，softmax 回归退化为 logistic 回归。这表明 softmax 回归是 logistic 回归的一般形式。具体地说，当 $\\textstyle k = 2 $时，softmax 回归的假设函数为：\n\n$$\n\\begin{align}\nh_\\theta(x) &=\n\\frac{1}{ e^{\\theta_1^Tx}  + e^{ \\theta_2^T x^{(i)} } }\n\\begin{bmatrix}\ne^{ \\theta_1^T x } \\\\\ne^{ \\theta_2^T x }\n\\end{bmatrix}\n\\end{align}\n$$\n利用softmax回归参数冗余的特点，我们令$ \\textstyle \\psi = \\theta_1$，并且从两个参数向量中都减去向量$ \\textstyle \\theta_1$，得到:\n\n$$\n\\begin{align}\nh(x) &=\n\\frac{1}{ e^{\\vec{0}^Tx}  + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } }\n\\begin{bmatrix}\ne^{ \\vec{0}^T x } \\\\\ne^{ (\\theta_2-\\theta_1)^T x }\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\frac{1}{ 1 + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } } \\\\\n\\frac{e^{ (\\theta_2-\\theta_1)^T x }}{ 1 + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } }\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\frac{1}{ 1  + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } } \\\\\n1 - \\frac{1}{ 1  + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } } \\\\\n\\end{bmatrix}\n\\end{align}\n$$\n因此，用$ \\textstyle \\theta$'来表示$\\textstyle \\theta_2-\\theta_1$，我们就会发现 softmax 回归器预测其中一个类别的概率为 $\\textstyle \\frac{1}{ 1  + e^{ (\\theta')^T x^{(i)} } }$，另一个类别概率的为$ \\textstyle 1 - \\frac{1}{ 1 + e^{ (\\theta')^T x^{(i)} } }$，这与 logistic回归是一致的。\n\n\n# Softmax 回归 vs. k 个二元分类器\n如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？\n\n这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）\n\n如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。\n\n现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？\n\n在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。\n\n\n# 中英文对照\n\n- Softmax回归 Softmax Regression\n- 有监督学习 supervised learning\n- 无监督学习 unsupervised learning\n- 深度学习 deep learning\n- logistic回归 logistic regression\n- 截距项 intercept term\n- 二元分类 binary classification\n- 类型标记 class labels\n- 估值函数/估计值 hypothesis\n- 代价函数 cost function\n- 多元分类 multi-class classification\n- 权重衰减 weight decay\n\n-------\n\n原文链接：http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\n英文链接：http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"推荐系统的一些思考","url":"/2018/03/26/RecSys/冷启动/推荐系统的一些思考/","content":"> 推荐系统一直以来都是电商网站必不可少的一项，在提升用户转化，增加GMV方面可谓功不可没，那么一个好的推荐算法必然会创造更大的价值，刚好最近听了一个关于推荐算法的讲座，写出来一些思考吧，算是分享一下。\n<!--More-->\n\n# 学术界的推荐系统\n\n其实在大学期间也看过一些推荐的算法，还帮别人实现过关于推荐系统的毕设，但终究都是停留在协同过滤的层面，顶多是加了一些热门推荐来防止冷启动。不得不说，协同过滤打开了我对推荐系统认知的大门，当然在真实环境中这是远远不够的。\n\n传统的推荐系统无非就是评分和排序两种方案，评分即计算出用户对item的可能评分，根据评分的高低进行排序，排序则不关心具体的评分是多少，只是为了得到一个顺序（其实这一点和推荐系统即为相似）。传统的推荐算法典型的有协同过滤和基于内容的过滤。如果你不明白什么是协同过滤算法可以参考：https://blog.csdn.net/gamer_gyt/article/details/51346159 ,如果你不知道协同过滤与基于内容的过滤的区别可参考：https://www.zhihu.com/question/19971859。\n\n大学数学术界关于推荐算法的论文都是对协同过滤的改进，而最终得到一个相对于原先的算法有很大的提升的结果，但是这些都过于理想化了，真实的环境远比实验要复杂的多，网上最有名的推荐系统数据集莫过于那个电影评分数据了，里边只有用户对电影的评分，和电影的一些信息数据。建立在这些数据上的推荐算法其实有点理想化了，他并不能模拟出真实的电商环境，数据的缺乏也是导致协同过滤算法大行其道的原因。\n\n# 工业界的推荐系统\n\n工业界的推荐系统，需要的是明确的价值走向，比如说电商网站的推荐系统是为了增加交易总额，那么在进行推荐的时候是不是应该适当过滤一些极其廉价的商品，是否应该根据用户对不同价格段的需求进行不同价格段商品的推送；如果电商的推荐系统是为了增加用户停留时间，提交CTR，那么推荐系统就不应该考虑过多别的因素，只找到用户最感兴趣的商品或者评论等，当然如何找到用户最感兴趣的也是一个问题，但是有一点不可否认的是这些如果用传统的协同过滤来做是很难满足需求的。这时候就需要开发出新的推荐架构，来适应不同的需求。\n\n目前工业界用的最多的算法莫过于GBDT，LR，DNN等，但所有的推荐算法都会面临一个海量数据的情况，这个时候的做法便是对数据集进行数据召回，得到用户比较感兴趣的一些数据，然后再根据我们的推荐模型进行素材偏好度排序，过滤掉用户已经购买过的类别数据，继而推送给用户。\n\n那么如何进行数据召回呢？这就需要一些基础的模型进行数据准备，比如说用户肖像，用户的价格段偏好，用户购买力水平等。根绝已有的用户数据和特征进行数据召回，适度拉取一些新的数据，保证召回结果的多样性，得到召回池数据之后，便是对模型的训练，其实模型本身难度不大，难度大的是如何选取有效的特征来作为模型的输入数据。你组合得到的有效特征越多，对于模型的训练结果就越准确。\n\n# 推荐系统的多样性\n如何保证推荐结果的多样性呢，首先我们要先认识到推荐的可能性，比如说电商网页首页的推荐，商品详细页面的推荐，不同年龄下的推荐，推荐的结果和被评估的指标都是不一样的。这个时候不能单一对所有情况下使用同一种算法或者特征，而是要找到能够区分出不同位置，不同年龄的推荐结果的特征，进行模型训练。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["冷启动"],"categories":["技术篇"]},{"title":"Scala解析XML","url":"/2018/02/04/Spark/Scala解析XML/","content":"\n> 在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。\n\n<!--More-->\nscala提供了类似于Xpath的语法来解析xml文件，其中很重要的两个操作符是\"\\\"\n和 \"\\\\\"\n\n- \\ ：根据搜索条件得到下一个节点\n- \\\\：根据条件获取所有的节点\n\n```\n<configure>\n    <input>\n        <name>app_feature_goods</name>\n        <hdfs>/user/path/to/goods</hdfs>\n    </input>\n    <input>\n        <name>app_feature_user</name>\n        <hdfs>/user/path/to/user</hdfs>\n    </input>\n</configure>\n```\n\n```\nval input = args(0)\nval xml = XML.load(input)\n\n\n// 找到所有的一级节点 input\nval input_list = xml\\\"input\"\ninput_list.foreach(println)\n\n// 遍历每个一级节点，得到具体的值\nfor(one <- input_list){\n    println(one\\\"name\")\n    println((one\\\"name\").text)\n    println(one\\\"hdfs\")\n    println((one\\\"hdfs\").text)\n}\n\n// 得到所有的name\nval name_list = xml\\\\\"name\"\nname_list.map(one => one.text).foreach(println)\n\n// 获取所有hdfs\nval hdfs_list = xml\\\\\"hdfs\"\nhdfs_list.map(one => one.text).foreach(println)\n\n// 获取具有class的值\nprintln(xml\\\"input\"\\\"name\"\\\\\"@class\")\n\n// 打印出具有class属性的name值和hdfs值\nprintln((xml\\\\\"name\").filter(_.attribute(\"class\").exists(_.text.equals(\"test\"))).text)\nprintln((xml\\\\\"hdfs\").filter(_.attribute(\"class\").exists(_.text.equals(\"test\"))).text)\n```\n\n打印出的信息为：\n```\n<input>\n        <name>app_feature_goods</name>\n        <hdfs>/user/path/to/goods</hdfs>\n    </input>\n<input>\n        <name>app_feature_user</name>\n        <hdfs>/user/path/to/user</hdfs>\n    </input>\n<input>\n        <name class=\"test\">app_feature_user_test</name>\n        <hdfs class=\"test\">/user/path/to/user_test</hdfs>\n    </input>\n-------------\n<name>app_feature_goods</name>\napp_feature_goods\n<hdfs>/user/path/to/goods</hdfs>\n/user/path/to/goods\n<name>app_feature_user</name>\napp_feature_user\n<hdfs>/user/path/to/user</hdfs>\n/user/path/to/user\n<name class=\"test\">app_feature_user_test</name>\napp_feature_user_test\n<hdfs class=\"test\">/user/path/to/user_test</hdfs>\n/user/path/to/user_test\n-------------\napp_feature_goods\napp_feature_user\napp_feature_user_test\n-------------\n/user/path/to/goods\n/user/path/to/user\n/user/path/to/user_test\n-------------\ntest\n-------------\napp_feature_user_test\n/user/path/to/user_test\n-------------\n\nProcess finished with exit code 0\n\n```\n\n当然还存在一种情况就是XML文件存在于hdfs之上，这时候就不能直接load xml文件里，不过可以通过下面一种方法获得\n```\nvar rdd = sc.textFile(xml_path)\nval xml = XML.loadString(rdd.collect().mkString(\"\\n\"))\n```\n接下来便可以通过上边的方法进行解析了。\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"Spark求统计量的两种方法","url":"/2018/02/04/Spark/Spark求统计量的两种方法/","content":"> Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家\n\n<!--More-->\n\n# DataFrame形式\n## 加载Json数据源\nexample.json文件格式如下\n```\n{\"name\":\"thinkgamer\",\"age\":23,\"math\":78,\"chinese\":78,\"english\":95}\n{\"name\":\"think\",\"age\":25,\"math\":95,\"chinese\":88,\"english\":93}\n{\"name\":\"gamer\",\"age\":24,\"math\":93,\"chinese\":68,\"english\":88}\n```\n```\n// persist(StorageLevel.MEMORY_AND_DISK) 当内存不够时cache到磁盘里\nval df = spark.read.json(\"/path/to/example.json\").persist(StorageLevel.MEMORY_AND_DISK)\ndf.show()\ndf.describe()\n```\n我们便可以看到如下的形式\n```\n+---+-------+-------+----+----------+\n|age|chinese|english|math|      name|\n+---+-------+-------+----+----------+\n| 23|     78|     95|  78|thinkgamer|\n| 25|     88|     93|  95|     think|\n| 24|     68|     88|  93|     gamer|\n+---+-------+-------+----+----------+\n\n+-------+----+-------+-----------------+-----------------+----------+\n|summary| age|chinese|          english|             math|      name|\n+-------+----+-------+-----------------+-----------------+----------+\n|  count|   3|      3|                3|                3|         3|\n|   mean|24.0|   78.0|             92.0|88.66666666666667|      null|\n| stddev| 1.0|   10.0|3.605551275463989| 9.29157324317757|      null|\n|    min|  23|     68|               88|               78|     gamer|\n|    max|  25|     88|               95|               95|thinkgamer|\n+-------+----+-------+-----------------+-----------------+----------+\n```\n如果是想看某列的通知值的话，可以用下面的方式\n```\ndf.select(\"age\").describe().show()\n```\n```\n+-------+----+\n|summary| age|\n+-------+----+\n|  count|   3|\n|   mean|24.0|\n| stddev| 1.0|\n|    min|  23|\n|    max|  25|\n+-------+----+\n```\n\n# RDD形式\n假设同样还是上边的数据，只不过现在变成按\\t分割的普通文本\n```\nthinkgamer  23  78  78  95\nthink   25  95  88  93\ngamer   24  93  68  88\n```\n这里可以将rdd转换成dataframe洗形式，也可以使用rdd计算，转化为df的样例如下\n```\nval new_data = data_txt\n      .map(_.split(\"\\\\s+\"))\n      .map(one => Person(one(0),one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble))\n      .toDF()\n```\n接下来就是进行和上边df一样的操作了。\n\n那么对于rdd形式的文件如何操作：\n```\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n\nval data_txt = SparkSC.spark.sparkContext.textFile(input_txt).persist(StorageLevel.MEMORY_AND_DISK)\n    val new_data = data_txt\n      .map(_.split(\"\\\\s+\"))\n      .map(one => Vectors.dense(one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble))\n    val summary: MultivariateStatisticalSummary = Statistics.colStats(new_data)\n    \nprintln(\"Max:\"+summary.max)\nprintln(\"Min:\"+summary.min)\nprintln(\"Count:\"+summary.count)\nprintln(\"Variance:\"+summary.variance)\nprintln(\"Mean:\"+summary.mean)\nprintln(\"NormL1:\"+summary.normL1)\nprintln(\"Norml2:\"+summary.normL2)\n```\n输出结果为：\n```\nMax:[25.0,95.0,88.0,95.0]\nMin:[23.0,78.0,68.0,88.0]\nCount:3\nVariance:[1.0,86.33333333333331,100.0,13.0]\nMean:[24.0,88.66666666666667,78.0,92.0]\nNormL1:[72.0,266.0,234.0,276.0]\nNorml2:[41.593268686170845,154.1363033162532,135.83813897429545,159.43023552638942]\n```\n这里可以得到相关的统计信息，主要区别在于dataframe得到的是标准差，而使用mllib得到的统计值中是方差，但这并不矛盾，两者可以相互转化得到。\n\n当然如果要求四分位数，可以转化成df，使用sql语句进行查询\n```\nSelect PERCENTILE(col,<0.25,0.75>) from tableName;\n```\n\n\n# 自己实现\n下面是我自己实现的一个方法，传入的参数是一个rdd，返回的是一个字符串\n```\n// 计算最大值，最小值，平均值，方差，标准差，四分位数\ndef getStat(data: RDD[String]):String= {\n   val sort_data = data\n      .filter(one => Verify.istoDouble(one))\n      .map(_.toDouble)\n      .sortBy(line=>line)\n      .persist(StorageLevel.MEMORY_AND_DISK)  // 默认是true 升序，false为降序\n\n   val data_list = sort_data.collect()\n   val len = data_list.length\n   val min = data_list(0)\n   val max = data_list(len-1)\n   val mean = sort_data.reduce((a,b) => a+b) / len\n   val variance = sort_data.map(one => math.pow(one-mean,2)).reduce((a,b)=>a+b)/len\n   val stdder = math.sqrt(variance)\n   var quant = \"\"\n   if(len<4){\n      val q1 = min\n      val q2 = min\n      val q3 = max\n      quant = q1+\"\\t\"+q2+\"\\t\"+q3\n   }else {\n      val q1 = data_list((len * 0.25).toInt - 1)\n      val q2 = data_list((len * 0.5).toInt - 1)\n      val q3 = data_list((len * 0.75).toInt - 1)\n      quant = q1+\"\\t\"+q2+\"\\t\"+q3\n   }\n   max+\"\\t\"+min+\"\\t\"+mean+\"\\t\"+variance+\"\\t\"+stdder+\"\\t\"+quant\n}\n```\n\n\n\n# 本地碰见的一个错误\n1：错误1\n```\nscala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/Array\n```\n\n原因是Spark中spark-sql_2.11-2.2.1 ，是用scala 2.11版本上编译的，而我的本地的scala版本为2.12.4，所以就错了，可以在\n里边把相应的scala版本就行修改就行了\n\n\n2：错误2\n```\njava.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V\n```\n原因也是因为我下载安装的scala2.12版本，换成scala2.11版本就可以了\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"记一次百G数据的聚类算法实施过程","url":"/2018/01/27/Spark/记一次百G数据的聚类算法实施过程/","content":"\n> 如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需要高效的完成。那么问题就来了，如何保证高效和准确性。（声明小编对Spark也不是说很熟悉）\n\n<!--More-->\n\n# 需求\n\n数据格式为三列，第一列为类别ID，第二列为商品ID，第三列为价格，数据格式如下\n```\n1000    2000    45.3\n1000    2001    121.3\n1001    2002    4125.3\n1000    2003    225.3\n1001    2004    3415.3\n1000    2005    12245.3   \n...     ...     ....\n```\n数据有很多条，数据量为将近100G，存储在hdfs上，第一列品类ID不唯一，每个品类ID下有多个商品ID，商品ID唯一，价格为浮点型数据\n\n现在要对每个品类下的价格进行聚类，得到1~7个价格level（7level的价格要比6level的价格高，以此类推）\n\n# 第一次尝试\n第一次尝试很天真，思路也很正常，如下：\n1：全量加载数据，形成rdd\n2：数据split之后，按key进行groupby\n3：针对每个key（也就是类别ID）进行kmeans聚类和预测，并将结果写入hdfs\n4：加载每个类别的结果，进行聚合形成最终结果\n\n那么开始写代码。papapa写了一堆，发现groupBy之后的数据格式是CompactBuffer，转化成spark kmeans train所需要的格式之后，代码卡着不会动，不明所以（我估计是格式没有转正确，不是kmeans 所需要的格式，但是如果不是kmeans 需要的格式，应该会报错呀），后来当我把代码打包，提交到集群上运行时，提示我kmeans train所在的函数中没有指定master url，可是我明明指定了，后来才发现是因为，我在rdd操作过程中能够，嵌套了函数，函数中又重新使用了rdd，也就是说rdd 不能嵌套rdd使用，具体可参考 [Spark 为什么 不允许 RDD 嵌套-如 RDD\\[RDD\\[T\\]\\]](https://www.zhihu.com/question/54439266)，而我在本地测试时指的都是local，没有进行报错，至此这条路行不通，也就是说不能按这样的思路执行\n\n在该思路的基础上进行改进：\n既然rdd不能嵌套rdd使用，何不先得到所有的类别id，然后在全量数据总filter单个类别id进行kmeans操作呢？\n\n该代码，测试，伪代码如下：\n```\nleibieID_list = XXXXX\nleibieID_list.map(one => kmeans(one,path))\n```\n\n需要注意的是 leibieID_list.map 操作并不是分布式的，而是for 循环，这样3000个类别id运行完，时间可想而知，是极其耗时的，所以这条路也失败了（不是说行不通，是因为耗时）\n\n# 第二次尝试\n\n经过上边的尝试发现不行，那么我想是不是先全量读取数据，然后按照类别ID，将同一个类别ID的数据写到一个文件（或者文件夹下），然后再对之操作\n\n开始写classify by ID 的代码，这里遇到了问题是如何让同一个类别ID的数据写到一个文件中，上网查了一些资料，可以参考之前整理的笔记\n\n[ Spark多路径输出和二次排序](http://blog.csdn.net/gamer_gyt/article/details/79157055)\n\n这里边有实现的办法，但是还有一个问题，对全量数据（100G）进行shuffle的时候，由于数据量特别大，也特别占用资源，往往会出现一些内存上的错误。\n\n这里采用的策略是将全量数据rdd进行random split，然后for循环遍历split之后的rdd，进行saveAsTestFile，保存的目录这样设计\n```\n/path/split=0/\n/path/split=1/\n/path/split=2/\n/path/split=3/\n    ...  ...\n```\n这样的话，就可以避免大量数据 shuffle 耗费资源的问题了，而且也不影响后续数据的使用，同时这一步也会把类别id提取出来，保存在hdfs上，供下一步使用。\n\n经历了上一步的数据准备，开始step 2的开发，第二步的思路：\n加载第一步保存的类别id list文件，分成5份，启动5个spark任务进行train，至此，思路是正确的，但却忽略了一个很严正的问题：数据倾斜\n\n由于是随机对类别 id 进行分组操作，那么不能保证没组中每个类别id对应的数据条数的大概一致性，也就是存在某个ID 数据条数只有几十条，而有些ID 数据条数千万条，这种情况下就会导致代码在运行过程中，有些task很快运行完了，有些执行了好久也没完事。\n\n# 第三次尝试\n有了第二次的经验，想法就是如何将数据条数差不多的分到同一组里，我采用的方法是进行统计，按照10的X次方形式进行分组，比如说\n```\n1~10    1\n10~100  2\n100~1000    3\n....\n```\n但是这样也有一个问题，就是这样大概符合正太分布，4、5、6这样的组里数据条数比较多，1、2、3和7、8、9这样的数据条数少，这样就会因为4、5、6组的程序运行时间较长，整体任务运行时间也较长。\n\n所以这里采用合并和拆分的策略，比如说将1,2,3合并到一组，4、5、6分别拆成两组，7、8、9合成一组，这样就会保证每组运行的时间是差不多的。（实际情况中，要根据数据的分布进行合理的拆分和合并）\n\n# 总结\n至此，问题算是最终解决了，相比原先的MR版本，时间缩减了将近8个小时，在整个优化的过程中，其实对于经验足够的开发者来说，可能很快就会解决，但对于我们这些新手，可能就要耗费些时间，涨涨记性了，在整个过程中对spark也算是有进一步的了解了。\n\n其他的相关笔记：\n\n- http://blog.csdn.net/gamer_gyt/article/details/79157055\n\n- http://blog.csdn.net/gamer_gyt/article/details/79135118\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"Spark多路径输出和二次排序","url":"/2018/01/25/Spark/Spark多路径输出和二次排序/","content":"> 在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。\n\n<!--More-->\n\n# 二次排序\n假设我们的数据是这样的：\n```\n1   2\n1   3\n1   1\n1   6\n1   4\n2   5\n2   8\n2   3\n```\n\n我们想要实现第一列按降序排列，当第一列相同时，第二列按降序排列\n\n\n定义一个SecondSortKey类：   \n```\nclass SecondSortKey(val first: Int, val second: Int)\n  extends Ordered[SecondSortKey] with Serializable {\n  override def compare(that: SecondSortKey): Int = {\n    if (this.first - that.first == 0) {\n      this.second - that.second\n    } else {\n      this.first - that.first\n    }\n  }\n}\n```\n\n然后这样去使用\n```\nval lines = sc.textFile(\"test.txt\")\nval pairs = lines.map { x =>\n      (new SecondSortKey(x.split(\"\\\\s+\")(0).toInt,\n        x.split(\"\\\\s+\")(1).toInt), x)\n    }\nval sortedPairs = pairs.sortByKey(false);\nsortedPairs.map(_._2).foreach(println)\n```\n\n当然这里如果想按第一列升序，当第一列相同时，第二列升序的顺序排列，只需要对SecondSoryKey做如下修改即可\n```\nclass SecondSortKey(val first: Int, val second: Int)\n  extends Ordered[SecondSortKey] with Serializable {\n  override def compare(that: SecondSortKey): Int = {\n    if (this.first - that.first !== 0) {\n      this.second - that.second\n    } else {\n      this.first - that.first\n    }\n  }\n}\n```\n当时使用的使用去掉\n```\npairs.sortByKey(false)\n```\n中的false\n\n# Top N\n\n同样还是上边的数据，假设我们要得到第一列中的前五位\n```\nval lines = sc.textFile(\"test.txt\")\nval rdd = lines\n        .map(x => x.split(\"\\\\s+\"))\n        .map(x => (x(0),x(1)))\n        .sortByKey()\nrdd.take(N).foreach(println)\n```\n\n# 多路径输出\n自己在使用的过程中，通过搜索发现了两种方法\n1：调用saveAsHadoopFile函数并自定义一个OutputFormat类\n\n自定义RDDMultipleTextOutputFormat类\n\nRDDMultipleTextOutputFormat类中的generateFileNameForKeyValue函数有三个参数，key和value就是我们RDD的Key和Value，而name参数是每个Reduce的编号。本例中没有使用该参数，而是直接将同一个Key的数据输出到同一个文件中。\n```\nimport org.apache.hadoop.mapred.lib.MultipleTextOutputFormat  \n  \nclass RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] {  \n  override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String =  \n    key.asInstanceOf[String]  \n}  \n```\n调用\n```\nsc.parallelize(List((\"w\", \"www\"), (\"b\", \"blog\"), (\"c\", \"com\"), (\"w\", \"bt\")))  \n      .map(value => (value._1, value._2 + \"Test\"))  \n      .partitionBy(new HashPartitioner(3))  \n      .saveAsHadoopFile(\"/iteblog\", classOf[String],classOf[String],classOf[RDDMultipleTextOutputFormat])  \n```\n这里的\n```\nnew HashPartitioner(3)\n```\n中的3是有key的种类决定的，当然在实际应用场景中，我们可能并不知道有多少k，这个时候就可以通过一个rdd 的 distinct操作来得到唯一key的数目。\n\n2：使用dataframe\n```\npeople_rdd = sc.parallelize([(1, \"alice\"), (1, \"bob\"), (2,\"charlie\")])\npeople_df = people_rdd.toDF([\"number\", \"name\"])\npeople_df.write.partitionBy(\"number\").format(\"text\").save(path  )\n```\n\n当然这两种方法都有一个缺陷，就是当数据量特别大的时候，数据在repartition的过程中特别耗费资源，也会容易出现任务failed的情况，小编采用的解决办法是，适当的对原rdd进行split，然后遍历每个rdd，进行multioutput操作\n\n形似如下：\n```\nval rdd = sc.textFile(input)\nvar split_rdd = rdd.randomSplit(Array(1.0,1.0,1.0,1.0))\nfor (one <- Array(1,2,3,4))\n{\n    split_rdd(one)XXXX\n}\n```\n------\n参考：\n\n- [Spark学习笔记——二次排序，TopN，TopNByGroup](https://andone1cc.github.io/2017/03/04/Spark/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8Ftopn/)\n- [Spark多文件输出(MultipleOutputFormat)](https://www.iteblog.com/archives/1281.html)\n- [scala - Write to multiple outputs by key Spark - one Spark job](https://code.i-harness.com/en/q/16e22a0)\n- [Write to multiple outputs by key Spark - one Spark job\n](https://stackoverflow.com/questions/23995040/write-to-multiple-outputs-by-key-spark-one-spark-job/26051042#26051042)\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"Spark提交参数说明和常见优化","url":"/2018/01/23/Spark/Spark提交参数说明和常见优化/","content":"\n\n> 最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制，以及海量数据背景下对算法的认知都有很大的提升，这一篇文章主要是总结一些Spark在shell 终端提交jar包任务的时候的相关知识，在后续文章会具体涉及到相关的”实战经历“。\n\n<!--More-->\n\n# 对Spark的认识\n\n由于之前接触过Hadoop，对Spark也是了解一些皮毛，但中间隔了好久才重新使用spark，期间也产生过一些错误的认识。\n\n之前觉得MapReduce耗费时间，写一个同等效果的Spark程序很快就能执行完，很长一段时间自己都是在本地的单机环境进行测试学习，所以这种错误的认知就会更加深刻，但事实却并非如此，MR之所以慢是因为每一次操作数据都写在了磁盘上，大量的IO造成了时间和资源的浪费，但是Spark是基于内存的计算引擎，相比MR，减少的是大量的IO，但并不是说给一个Spark程序足够的资源，就可以为所欲为了，在提交一个spark程序时，不仅要考虑所在资源队列的总体情况，还要考虑代码本身的高效性，要尽量避免大量的shuffle操作和action操作，尽量使用同一个rdd。\n\n会用spark，会调api和能用好spark是两回事，在进行开发的过程中，不仅要了解运行原理，还要了解业务，将合适的方法和业务场景合适的结合在一起，才能发挥最大的价值。\n\n\n# spark-submit\n进入spark的home目录，执行以下命令查看帮助\n```\nbin/spark-submit --help\n```\nspark提交任务常见的两种模式\n1：local/local[K]\n\n- 本地使用一个worker线程运行spark程序\n- 本地使用K个worker线程运行spark程序\n\n此种模式下适合小批量数据在本地调试代码\n\n2：yarn-client/yarn-cluster\n\n- yarn-client：以client方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver在client运行。\n- yarn-cluster：以cluster方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver也在集群中运行。\n\n注意：若使用的是本地文件需要在file路径前加：file://\n\n在提交任务时的几个重要参数\n\n- executor-cores —— 每个executor使用的内核数，默认为1\n- num-executors —— 启动executors的数量，默认为2\n- executor-memory —— executor内存大小，默认1G\n- driver-cores —— driver使用内核数，默认为1\n- driver-memory —— driver内存大小，默认512M\n\n下边给一个提交任务的样式\n```\nspark-submit \\\n  --master local[5]  \\\n  --driver-cores 2   \\\n  --driver-memory 8g \\\n  --executor-cores 4 \\\n  --num-executors 10 \\\n  --executor-memory 8g \\\n  --class PackageName.ClassName XXXX.jar \\\n  --name \"Spark Job Name\" \\\n  InputPath      \\\n  OutputPath\n  \n如果这里通过--queue 指定了队列，那么可以免去写--master\n  \n```\n以上就是通过spark-submit来提交一个任务\n\n# 几个参数的常规设置\n\n- executor_cores*num_executors\n  表示的是能够并行执行Task的数目\n  不宜太小或太大！一般不超过总队列 cores 的 25%，比如队列总 cores 400，最大不要超过100，最小不建议低于 40，除非日志量很小。\n\n- executor_cores \n  不宜为1！否则 work 进程中线程数过少，一般 2~4 为宜。\n\n- executor_memory\n  一般 6~10g 为宜，最大不超过20G，否则会导致GC代价过高，或资源浪费严重。\n\n- driver-memory \n  driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，除非你是 spark-shell，否则一般 1-2g\n\n\n---\n增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：\n  \n- 1、如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，\n甚至不写入磁盘。减少了磁盘IO。\n- 2、对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。\n- 3、对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。\n \n# 常规注意事项\n\n- 预处理数据，丢掉一些不必要的数据\n- 增加Task的数量\n- 过滤掉一些容易导致发生倾斜的key\n- 避免创建重复的RDD\n- 尽可能复用一个RDD\n- 对多次使用的RDD进行持久化\n- 尽量避免使用shuffle算子\n- 在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响.\n-  使用高性能的算子\n\n\n\n-----\n参考：\n1：http://www.cnblogs.com/haozhengfei/p/e570f24c43fa15f23ebb97929a1b7fe6.html\n2：https://www.jianshu.com/p/4c584a3bac7d\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Spark与大数据"],"categories":["技术篇"]},{"title":"2017年终总结-用大把的时间仿徨，却用几个瞬间成长","url":"/2017/12/31/随手记/2017年终总结-用大把的时间仿徨，却用几个瞬间成长/","content":"\n> 人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。\n\n<!--More-->\n\n\n提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及的地方。\n\n2017年对我个人来讲是比较重要的一年，可以说是完成了自我的一个改变吧，但是脱壳之后，需要的是更加努力的完善自我，这样才能够避免被淘汰。为什么这么说？因为这一年我觉得对我个人影响最大的两件事是：买了首套房（虽然只是付了首付）；在年末之时选择离开，侥幸的入职了京东。一个是为“家庭”做了必要的准备，一个是为“事业”做了点缀。而至于其他的大大小小的事情，则是17年的五味杂粮，并不是那么重要，但却也缺一不可。感谢17年，那些我认识的，或者我不认识的，帮助过我的，或者我帮助过的人。\n\n\n转载请注明出处：https://thinkgamer.blog.csdn.net/article/details/78940968\n博主微博：http://weibo.com/234654758 \nGithub：https://github.com/thinkgamer\n\n\n# 过往\n人上了年纪，总爱回忆！\n\n刚好这两天，“18岁”刷爆了朋友圈和QQ空间，或许这是连腾讯有没有预料到的起死回生或者苟延残喘吧。趁机翻了下我的QQ空间相册，突然发现了，这二十多年来以来，我也是经历颇丰。\n\n从上海到江苏，从沈阳到长春，从南京到天津，最后到北京，这一条路一走便是4年。\n从初中到高中，从大学到社会，这一条路一走便是11年。\n\n过往的这条路上遇见了很多人，碰见很多事，但终究还是走散了，看淡了，不过庆幸的是那些一直还有联系的，我们还能彼此叫出姓名的人，不管是18岁之前，还是18岁之后，我想我们是幸运的，\n\n年少的我们曾经难免会埋下羞涩的种子，在记忆深处藏着一些不可告人的秘密，直到有一天我们盘膝而坐，三巡酒过，才道出那些现在我们认为可笑的不能再可笑的羞涩，从此，稚嫩青春里的唯一一朵留恋，也该告一段落。\n\n一杯敬明天，一杯敬过往。灵魂不再无处安放。\n# 遇见\n人有了目标，便爱胡闹！\n\n2017年，不管是工作，还是自我学习都收获了挺多知识，感谢万维接受了我这个毫无工作经验的“学生”吧，在这里的确收获了挺多，让我明白没有结合业务的技术，只是向别人吹嘘而毫无创造价值的垃圾，不管你在哪，技术都是为了推动业务的增长。\n\n2017年，还是习惯性的写一些学习笔记，只不过是频率明显了降了下来，这只能归结于自己变得懒惰了，从开通博客到现在，所有的表层的收获只能通过这些浅显的数据来表达：\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190720101149580.png\">\n</center>\n只是没有17年尾的时候访问量达到100W，不过一切随缘吧。\n\n下边这几张图是最近一个月的访问分析情况，不管怎样，你学习了，也帮助其他人了，这就是成长。\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20190720101344293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190720101550114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/2019072010163376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n<img src=\"https://img-blog.csdnimg.cn/20190720101658948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\">\n</center>\n\n2017年，开通了微信公众号【数据与算法联盟】（原名为码农故事多），没有刻意的运营，没有刻意的传播积累用户，一切随缘。当然如果你想加入我们的数据与算法学习交流群的话，欢迎加我的微信，拉你入群，群里有很多大牛，不定时进行“扯淡”。我们的宗旨就是以技术会友，分享与进步！感谢2017年和以往遇见的所有好友！\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n\n2017年，很侥幸的加入了JD这个大家庭，未来的一切都是未知数，但你能做的就是向他人学习，围绕着工作进行深度的学习和成长。\n\n\n# 新生\n人过了18，要努力发芽！\n\n过去的一年里定了太多的目标，结果大部分都没有实现，哎，分析一下，大部门的目标都是盲目的，没有围绕工作的目标（学习规划吧算是），其实实现起来是有难度的，所以新的一年里调整计划，重新出发。\n\n2018不会定太多的目标，主要是想在技术和业务层面提升下自己，不管是学习大数据还是算法，都会围绕功能工作和一条主旋律进行展开。\n\n感谢一路以来，遇见的所有人！\n\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["年终总结"],"categories":["随手记"]},{"title":"Hexo-Yilia加入相册功能","url":"/2017/12/14/随手记/Hexo-Yilia加入相册功能/","content":"参考：[点击查看](http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD)\n\n<!--More-->\n\n但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）\n\n# 主页新建相册链接\n\n主题_config.json文件的menu 中加入 相册和对应的链接\n```\nthemes/yilia/_config.json\n\nmenu:\n  主页: /\n  ... ...\n  相册: /photos\n```\n\n# 新建目录并拷贝相应文件\n使用的是litten 大神的博客 photos文件夹，对应的路径为：\nhttps://github.com/litten/BlogBackup/tree/master/source/photos\n\n自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。\n\n# 文件修改\n\n 1. 修改 ins.js 文件的 render()函数\n 这个函数是用来渲染数据的\n修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)\n例如我的为：\n```\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\nvar src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n\n```\n# 生成json\n1：下载相应python工具文件\n\n- tools.py\n- ImageProcess.py\n\n下载地址：https://github.com/Thinkgamer/GitBlog\n\n2：新建photos和min_photos文件夹\n在项目根目录下创建，用来存放照片和压缩后的照片\n```\nmkdir photos\nmkdir min_photos\n```\n3：py文件和文件夹都放在项目根目录下\n\n4：生成json\n执行\n```\npython tools.py\n```\n如果提示：\n```\nTraceback (most recent call last):\n  File \"tools.py\", line 13, in <module>\n    from PIL import Image\nImportError: No module named PIL\n```\n说明你没有安装pillow，执行以下命令安装即可\n```\npip install pillow\n```\n\n如果报错：\n```\nValueError: time data 'DSC' does not match format '%Y-%m-%d'\n```\n说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）\n```\n2016-10-12_xxx.jpg/png\n```\nok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多\n\n# 本地预览和部署\n## 本地预览\n项目根目录下执行\n```\nhexo s\n```\n浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg\n\n改正方法\nins.js/render 函数\n```\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n\n换成\n\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n\n注释掉该行：\nsrc += '.jpg'; \n```\n\n到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字\n\n```\nsrc=\"/assets/img/empty.png\n```\n\nok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911\n\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["随手记"],"categories":["随手记"]},{"title":"梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降","url":"/2017/12/14/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降/","content":"\n在机器学习领域，体梯度下降算法分为三种\n\n- 批量梯度下降算法（BGD，Batch gradient descent algorithm）\n- 随机梯度下降算法（SGD，Stochastic gradient descent algorithm）\n- 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）\n<!--More-->\n\n# 批量梯度下降算法\nBGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\\theta$代替$\\theta_i$)，\n\n\n$$\n\\jmath (\\theta _0,\\theta _1,...,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2        \n$$\n$$\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n$$\n$$\n公式(1)\n$$\n\n这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。\n\n特点：\n\n- 能达到全局最优解，易于并行实现\n- 当样本数目很多时，训练过程缓慢\n\n# 随机梯度下降算法\nSGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。\n\n特点：\n- 训练速度快\n- 准确度下降，并不是最优解，不易于并行实现\n\n# 小批量梯度下降算法\nMBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。\n\n相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。\n\n\n# 补充\n在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。\n\n在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。\n\n梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。\n\n梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。\n\n# sklearn中的SGD\nsklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：[SGD](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)，这也有一个中文的 [SGD](http://sklearn.lzjqsdd.com/modules/sgd.html)\n\n```\nIn [1]: from sklearn.linear_model import SGDClassifier\n\nIn [2]: X = [[0., 0.], [1., 1.]]\n\nIn [3]: y = [0, 1]\n\nIn [4]: clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n\nIn [5]: clf.fit(X, y)\nOut[5]: \nSGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n       verbose=0, warm_start=False)\n\nIn [6]:  clf.predict([[2., 2.]])\nOut[6]: array([1])\n\nIn [7]: clf.coef_ \nOut[7]: array([[ 9.91080278,  9.91080278]])\n\nIn [8]: clf.intercept_ \nOut[8]: array([-9.97004991])\n```\n参考：\n\n- https://www.cnblogs.com/pinard/p/5970503.html\n- http://blog.csdn.net/uestc_c2_403/article/details/74910107\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"梯度算法之梯度上升和梯度下降","url":"/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/","content":"\n> 第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。\n\n<!--More-->\n\n# 高数中的导数\n设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成\n$$\nx_{0} + \\Delta x\n$$\n函数y=f(x)的增量\n\n\n$$\n\\Delta y = f(x_0 + \\Delta x) - f(x_0)\n$$\n与自变量的增量 $ \\Delta x $ 之比：\n\n\n$$\n\\frac{ \\Delta y }{ \\Delta x } = \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }\n$$\n称为f(x)的平均变化率。\n如 $ \\Delta x \\rightarrow 0 $ 平均变化率的极限\n$$\n\\lim_{\\Delta x \\rightarrow 0} \\frac{ \\Delta y }{ \\Delta x } = \\lim_{\\Delta x  \\rightarrow 0} \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }\n$$\n存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。\n\n关于导数的说明\n\n1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都\n\n2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导\n\n3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数\n\n4）导函数在x1 处 为 0，若 x<1 时，f'(x) > 0 ，这 f(x) 递增，若f'(x)<0 ，f(x)递减\n\n5）f'(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率\n\n\n# 偏导数\n\n函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \\Delta x $ 时，相应的有函数增量\n$$\nf(x_0 + \\Delta x, y_0) - f(x_0,y_0)\n$$\n如果\n$$\n\\lim_{\\Delta x\\rightarrow 0 } \\frac {f(x_0 + \\Delta x, y_0) - f(x_0,y_0)}{\\Delta x}\n$$\n存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $\n\n\n如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做\n$$\n\\frac{ \\partial z }{ \\partial x } , \\frac{ \\partial f }{ \\partial x } , z_x , f_x(x,y), \n$$\n\n偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处\n$$\nf_x(x,y,z)=\\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x,y,z) -f(x,y,z)}{\\Delta x}\n$$\n$$\nf_y(x,y,z)=\\lim_{\\Delta y \\rightarrow 0} \\frac{f(x,y + \\Delta y,z) -f(x,y,z)}{\\Delta y}\n$$\n$$\nf_z(x,y,z)=\\lim_{\\Delta z \\rightarrow 0} \\frac{f(x,y,z + \\Delta z) -f(x,y,z)}{\\Delta z}\n$$\n可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。\n\n区别：\n导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；\n偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。\n\n偏导数的几何意义：\n偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率\n偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率\n\n例子：\n求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。\n$$\n\\frac{ \\partial z}{\\partial x} = 2x +3y\n$$\n$$\n\\frac{ \\partial z}{\\partial y} = 2y +3x\n$$\n所以:\n$z_x(x=1,y=2) = 8$  \n$z_y(x=1,y=2) = 7$\n\n# 方向导数\n$$\n\\frac{ \\partial }{ \\partial l }  f(x_0,x_1,...,x_n) = \\lim_{\\rho \\rightarrow 0} \\frac{\\Delta y}{ \\Delta x } = \\lim_{\\rho \\rightarrow 0} \\frac{ f(x_0 + \\Delta x_0,...,x_j + \\Delta x_j,...,x_n + \\Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \\rho }\n$$\n$$\n\\rho = \\sqrt{ (\\Delta x_0)^{2} +...+(\\Delta x_j)^{2}+...+(\\Delta x_n)^{2}}\n$$\n前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。\n\n通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 \n　\n\n# 梯度\n与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \\partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $\n( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y })^T\n$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y },\\frac{ \\partial f }{ \\partial z })^T$,以此类推。\n\n那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。\n\n例如：\n函数 $f(x,y) = \\frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：\n$$\n \\frac{ \\partial f }{ \\partial x}=-\\frac{2x}{ (x^2+y^2)^2}\n$$\n$$\n \\frac{ \\partial f }{ \\partial y}=-\\frac{2y}{ (x^2+y^2)^2}\n$$\n所以\n$$\ngrad( \\frac{1}{x^2+y^2} ) = (-\\frac{2x}{ (x^2+y^2)^2} ,-\\frac{2y}{ (x^2+y^2)^2})\n$$\n函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 \n\n\n注意点：\n1）梯度是一个向量\n2）梯度的方向是最大方向导数的方向\n3）梯度的值是最大方向导数的值\n\n# 梯度下降与梯度上升\n在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。\n\n\n\n## 梯度下降\n### 关于梯度下降的几个概念\n1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度\n2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y\n3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,...n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。\n4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,...n),采用线性回归，损失函数为：\n$$\n\\jmath (\\theta _0,\\theta _1)=\\sum_{i=0}^{m}( h_\\theta(x_i)-y_i )^2\n$$\n其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\\theta(x_i)$ 为假设函数。\n\n### 梯度下降的代数方法描述\n\n 1. 先决条件：确定优化模型的假设函数和损失函数\n    这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,...x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2...n)$ 为模型参数(公式中用$\\theta$代替)，$x_i(i=0,1,2...n)$为每个样本的n个特征值。\n    \n    则对应选定得损失函数为：\n    $$\n    \\jmath (\\theta _0,\\theta _1,...,,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2\n    $$\n\n 2. 算法相关参数的初始化\n    主要是初始化 $ \\theta _0,\\theta _1...,\\theta _n$，算法终止距离 $\\varepsilon $ 以及步长 $ \\alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。\n \n 3. 算法过程\n \n - 1)：确定当前损失函数的梯度，对于$\\theta _i $，其梯度表达式为：\n $$\n\\frac{\\partial }{\\partial \\theta _i}\\jmath (\\theta _1,\\theta _2,...,\\theta _n)\n $$\n \n - 2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即\n $$\n \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n $$\n \n - 3)：确定是否所有的$\\theta _i$ ，梯度下降的距离都小于 $ \\varepsilon $，如果小于$ \\varepsilon $，则算法停止，当前所有的 $\\theta _i(i=1,2,3,...,n)$ 即为最终结果。否则执行下一步。\n \n - 4)：更新所有的 $\\theta$，对于$\\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。\n $$\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n $$\n \n ### 梯度下降的矩阵方式描述\n  \n  1. 先决条件：确定优化模型的假设函数和损失函数\n    这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,...x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2...n)$ 为模型参数，$x_i(i=0,1,2...n)$为每个样本的n个特征值。\n    假设函数对应的矩阵表示为：$ h_\\theta (x) = X \\theta $，假设函数 $h_\\theta(x)$ 为mx1的向量，$\\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。\n    则对应选定得损失函数为：\n    $$\n    \\jmath (\\theta)=(X \\theta −Y)^T (X \\theta−Y)\n    $$\n 其中YY是样本的输出向量，维度为m*1\n <br>\n 2.算法相关参数初始化:  \n  $\\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\\varepsilon $ ，步长 $\\alpha$ 和 “梯度下降的代数方法”描述中一致。\n <br>\n 3.算法过程\n  \n - 1)：确定当前位置的损失函数的梯度，对于 $ \\theta $ 向量,其梯度表达式如下：\n$$\n\\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)\n$$\n - 2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)$ \n - 3)：确定 $\\theta$ 向量里面的每个值,梯度下降的距离都小于 $\\varepsilon$，如果小于 $\\varepsilon$ 则算法终止，当前 $\\theta$ 向量即为最终结果。否则进入步骤4)\n - 4)：更新 $\\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)\n $$\n \\theta =\\theta - \\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)\n $$\n \n## 梯度上升\n梯度上升和梯度下降的分析方式是一致的，只不过把 $ \\theta $ 的更新中 减号变为加号。\n\n## 梯度下降的算法优化\n1. 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。\n\n2. 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。\n\n3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\\bar{x}$ 和标准差std(x)，然后转化为：\n$$\n\\frac{x - \\bar{x}}{std(x)}\n$$\n这样特征的新期望为0，新方差为1，迭代次数可以大大加快。\n\n----------\n\n\nhttp://blog.csdn.net/walilk/article/details/50978864\n\nhttps://www.zhihu.com/question/24658302\n\nhttps://www.cnblogs.com/pinard/p/5970503.html\n\nhttp://www.doc88.com/p-7844239247737.html\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"异常检测之指数平滑（利用elasticsearch来实现）","url":"/2017/11/20/ELK/异常检测之指数平滑（利用elasticsearch来实现）/","content":"\n> 指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。\n\n<!--More-->\n\n# 关于指数平滑的得相关资料：\n\n- ES API接口：\n> https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md\n<br><br>\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html\n\n- 理论概念\n> http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html\n\n\n# ES移动平均聚合：Moving Average的四种模型\n## simple\n就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + ... + an) / n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\":{\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"simple\"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 线性模型：Linear\n对窗口内的值先做线性变换处理，再求平均：(a1 * 1 + a2 * 2 + ... + an * n) / (1 + 2 + ... + n)\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"linear\"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 指数平滑模型\n### 指数模型：EWMA (Exponentially Weighted)\n即： 一次指数平滑模型\n\nEWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3\n\n计算模型：s2 = α * x2 + (1 - α) * s1\n\n其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"ewma\",\n                        \"settings\" : {\n                            \"alpha\" : 0.5\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n### 二次指数平滑模型: Holt-Linear\n计算模型：\n\ns2 = α * x2 + (1 - α) * (s1 + t1)\n\nt2 = ß * (s2 - s1) + (1 - ß) * t1\n\n默认alpha = 0.3 and beta = 0.1\n\n二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt\",\n                        \"settings\" : {\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n### 三次指数平滑模型：Holt-Winters无季节模型\n三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。\n\n1: Additive Holt-Winters：Holt-Winters加法模型\n\n下面是累加的三次指数平滑\n```\nsi=α(xi-pi-k)+(1-α)(si-1+ti-1)\nti=ß(si-si-1)+(1-ß)ti-1\npi=γ(xi-si)+(1-γ)pi-k\n```\n其中k为周期\n\n累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k)\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt_winters\",\n                        \"settings\" : {\n                            \"type\" : \"add\",\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5,\n                            \"gamma\" : 0.5,\n                            \"period\" : 7\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n2: Multiplicative Holt-Winters：Holt-Winters乘法模型\n\n下式为累乘的三次指数平滑：\n```\nsi=αxi/pi-k+(1-α)(si-1+ti-1)\nti=ß(si-si-1)+(1-ß)ti-1\npi=γxi/si+(1-γ)pi-k  其中k为周期\n```\n累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k)\n\nα，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。\n\ns,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1.\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt_winters\",\n                        \"settings\" : {\n                            \"type\" : \"mult\",\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5,\n                            \"gamma\" : 0.5,\n                            \"period\" : 7,\n                            \"pad\" : true\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n## 预测模型：Prediction\n使用当前值减去前一个值，其实就是环比增长\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"simple\",\n                        \"predict\" : 10\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 最小化：Minimization\n某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。\n\n出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。\n\newma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。\n\n通过最小化参数启用/禁用最小化：\"minimize\" : true\n\n# 原始数据\n数据为SSH login数据其中 IP／user已处理\n```\n{\n    \"_index\": \"logstash-sshlogin-others-success-2017-10\",\n    \"_type\": \"sshlogin\",\n    \"_id\": \"AV-weLF8c2nHCDojUbat\",\n    \"_version\": 2,\n    \"_score\": 1,\n    \"_source\": {\n        \"srcip\": \"222.221.238.162\",\n        \"dstport\": \"\",\n        \"pid\": \"20604\",\n        \"program\": \"sshd\",\n        \"message\": \"dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2\",\n        \"type\": \"zhongcai-sshlogin\",\n        \"ssh_type\": \"ssh_successful_login\",\n        \"forwarded\": \"false\",\n        \"manufacturer\": \"others\",\n        \"IndexTime\": \"2017-10\",\n        \"path\": \"/home/logstash/log/logstash_data/audit10/sshlogin/11.txt\",\n        \"number\": 1,\n        \"hostname\": \"176.231.228.130\",\n        \"protocol\": \"ssh2\",\n        \"@timestamp\": \"2017-10-19T15:38:02.000Z\",\n        \"ssh_method\": \"publickey\",\n        \"_hostname\": \"dwasw-ibb01\",\n        \"@version\": \"1\",\n        \"host\": \"localhost\",\n        \"srcport\": \"49484\",\n        \"dstip\": \"\",\n        \"category\": \"sshlogin\",\n        \"user\": \"nmuser\"\n    }\n}\n```\n\n# 利用ES API接口去调用查询数据\n\n\"interval\": \"hour\": hour为单位，这里可以是分钟，小时，天，周，月\n\n\"format\": \"yyyy-MM-dd HH\": 聚合结果得日期格式\n\n```\n\"the_sum\": {\n    \"sum\": {\n        \"field\": \"number\"\n    }\n}\n```\nnumber为要聚合得字段\n\n```\ncurl -POST  'localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"size\": 0,\n  \"query\": {\n    \"term\": {\n      \"ssh_type\": \"ssh_successful_login\"\n    }\n  },\n  \"aggs\": {\n    \"hour_sum\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"hour\",\n        \"format\": \"yyyy-MM-dd HH\"\n      },\n      \"aggs\": {\n        \"the_sum\": {\n          \"sum\": {\n            \"field\": \"number\"\n          }\n        },\n        \"the_movavg\": {\n          \"moving_avg\": {\n            \"buckets_path\": \"the_sum\",\n            \"window\": 30,\n            \"model\": \"holt\",\n            \"settings\": {\n              \"alpha\": 0.5,\n              \"beta\": 0.7\n            }\n          }\n        }\n      }\n    }\n  }\n}'\n```\n得到的结果形式为：\n```\n{\n  \"took\" : 35,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 206821,\n    \"max_score\" : 0.0,\n    \"hits\" : [ ]\n  },\n  \"aggregations\" : {\n    \"hour_sum\" : {\n      \"buckets\" : [\n        {\n          \"key_as_string\" : \"2017-09-30 16\",\n          \"key\" : 1506787200000,\n          \"doc_count\" : 227,\n          \"the_sum\" : {\n            \"value\" : 227.0\n          }\n        },\n        {\n          \"key_as_string\" : \"2017-09-30 17\",\n          \"key\" : 1506790800000,\n          \"doc_count\" : 210,\n          \"the_sum\" : {\n            \"value\" : 210.0\n          },\n          \"the_movavg\" : {\n            \"value\" : 113.5\n          }\n        },\n        {\n          \"key_as_string\" : \"2017-09-30 18\",\n          \"key\" : 1506794400000,\n          \"doc_count\" : 365,\n          \"the_sum\" : {\n            \"value\" : 365.0\n          },\n          \"the_movavg\" : {\n            \"value\" : 210.0\n          }\n        },\n    ...\n    }\n}\n```\n\n# 对应得python代码（查询数据到画图）\n```\n# coding: utf-8\nfrom elasticsearch import Elasticsearch\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontManager, FontProperties\n\nclass Smooth:\n    def __init__(self,index):\n        self.es = Elasticsearch(['localhost:9200'])\n        self.index = index\n        \n    # 处理mac中文编码错误\n    def getChineseFont(self):\n        return FontProperties(fname='/System/Library/Fonts/PingFang.ttc')\n    \n    # 对index进行聚合\n    def agg(self):\n        # \"format\": \"yyyy-MM-dd HH:mm:SS\"\n        dsl = '''\n                {\n                  \"size\": 0,\n                  \"query\": {\n                    \"term\": {\n                      \"ssh_type\": \"ssh_successful_login\"\n                    }\n                  },\n                  \"aggs\": {\n                    \"hour_sum\": {\n                      \"date_histogram\": {\n                        \"field\": \"@timestamp\",\n                        \"interval\": \"day\",\n                        \"format\": \"dd\"\n                      },\n                      \"aggs\": {\n                        \"the_sum\": {\n                          \"sum\": {\n                            \"field\": \"number\"\n                          }\n                        },\n                        \"the_movavg\": {\n                          \"moving_avg\": {\n                            \"buckets_path\": \"the_sum\",\n                            \"window\": 30,\n                            \"model\": \"holt_winters\",\n                            \"settings\": {\n                              \"alpha\": 0.5,\n                              \"beta\": 0.7\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n                '''\n        res = self.es.search(index=self.index, body=dsl)\n        return res['aggregations']['hour_sum']['buckets']\n    \n    # 画图\n    def draw(self):\n        x,y_true,y_pred = [],[],[]\n        for one in self.agg():\n            x.append(one['key_as_string'])\n            y_true.append(one['the_sum']['value'])\n            if 'the_movavg' in one.keys():       # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值\n                y_pred.append(one['the_movavg']['value'])\n            else:\n                y_pred.append(one['the_sum']['value'])\n        \n        x_line = range(len(x))\n        \n        plt.figure(figsize=(10,5))\n        plt.plot(x_line,y_true,color=\"r\")\n        plt.plot(x_line,y_pred,color=\"g\")\n        \n        plt.xlabel(u\"每单位时间\",fontproperties=self.getChineseFont()) #X轴标签 \n        plt.xticks(range(len(x)), x)\n        plt.ylabel(u\"聚合结果\",fontproperties=self.getChineseFont()) #Y轴标签  \n        plt.title(u\"10月份 SSH 主机登录成功聚合图\",fontproperties=self.getChineseFont()) # 标题\n        plt.legend([u\"True value\",u\"Predict value\"])\n        plt.show()\n\nsmooth = Smooth(\"logstash-sshlogin-others-success-2017-10\")\nprint smooth.draw()\n```\n结果图示为：\n![这里写图片描述](http://img.blog.csdn.net/20171120171404972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["ELK"],"categories":["技术篇"]},{"title":"Elasticsearch-DSL部分集合","url":"/2017/11/14/ELK/Elasticsearch-DSL部分集合/","content":"\nELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。\n\n点击阅读：[ELK Stack 从入门到放弃](http://blog.csdn.net/column/details/13079.html)\n<!--More-->\n\n# DSL中遇到的错误及解决办法\n## 分片限制错误\n\n```\nTrying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time.\n```\n\n\n解决办法：\n```\n修改该限制数目\n\ncurl -k -u admin:admin -XPUT 'http://localhost:9200/_cluster/settings' -H 'Content-Type: application/json' -d' \n{\n    \"persistent\" : {\n        \"action.search.shard_count.limit\" : \"5000\"\n    }\n}\n'\n\n-k -u admin:admin 表述如果有权限保护的话可以加上\n```\n\n## Fileddate 错误\n```\nFielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\n```\n\n解决办法：\n```\ncars: 索引名\ntransactions：索引对应的类型\ncolor：字段\n\ncurl -XPUT -k -u admin:admin 'localhost:9200/cars/_mapping/transactions?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"properties\": {\n    \"color\": { \n      \"type\":     \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n'\n```\n\n# 指定关键词查询，排序和函数统计\n## 指定关键词\n\nfrom 为首个偏移量，size为返回数据的条数\n```\nhttp://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty\n\n{\n    \"from\":0,size\":1000,\n    \"query\" : {\n        \"term\" : { \n        \t\"major\" : \"55\"\n        }\n    }\n}\n```\n\n## 添加排序\n\n(需要进行mapping设置，asc 为升序  desc为降序)\n```\n{\n    \"from\":0,\"size\":1000,\n    \"sort\":[\n        {\"offset\":\"desc\"}\n    ],\n    \"query\" : {\n        \"term\" : { \n            \"major\" : \"55\"\n        }\n    }\n}\n```\n\n## mode 方法\nmode方法包括 min／max／avg／sum／median\n\n假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。\n\n```\n{\n   \"query\" : {\n      \"term\" : { \"product\" : \"chocolate\" }\n   },\n   \"sort\" : [\n      {\"price\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\n   ]\n}\n```\n\n# IP范围和网段查询\n## IP range 搜索\n\n错误：\n```\nFielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\n```\n\n解决办法：\n```\ncurl -k -u admin:admin -XPUT '10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"properties\": {\n    \"clientip\": { \n      \"type\":     \"text\",\n      \"fielddata\": true,\n      \"norms\": false\n    }\n  }\n}\n'\n```\n\n查看某个索引的mapping\n```\ncurl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty\n```\n\n(当IP为不可解析使就会出现错误)\n```\nhttp://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty\n{\n    \"size\":100,\n    \"aggs\" : {\n        \"ip_ranges\" : {\n            \"ip_range\" : {\n                \"field\" : \"clientip\",\n                \"ranges\" : [\n                    { \"to\" : \"40.77.167.73\" },\n                    { \"from\" : \"40.77.167.75\" }\n                ]\n            }\n        }\n    }\n}\n```\n\n## 网段查询\n```\nhttp://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty\n{\n    \"aggs\" : {\n        \"ip_ranges\" : {\n            \"ip_range\" : {\n                \"field\" : \"ip\",\n                \"ranges\" : [\n                    { \"mask\" : \"172.21.202.0/24\" },\n                    { \"mask\" : \"172.21.202.0/24\" }\n                ]\n            }\n        }\n    }\n}\n```\n\n# 关于索引的操作\n\n## 删除某个索引\n-k -u admin:admin 为用户名：密码\n```\ncurl -XDELETE  -k -u admin:admin 'http://localhost:9200/my_index'\n```\n\n\n## 查看某个索引的Mapping\n```\ncurl -XGET \"http://127.0.0.1:9200/my_index/_mapping?pretty\"\n```\n\n## 索引数据迁移\n\nEs索引reindex(从ip_remote上迁移到本地)\n```\ncurl -XPOST 'localhost:9200/_reindex?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"source\": {\n    \"remote\": {\n      \"host\": \"http://ip_remote:9200\",\n      \"username\": \"username\",\n      \"password\": \"passwd\"\n    },\n    \"index\": \"old_index\"\n  },\n  \"dest\": {\n    \"index\": \"new_index\"\n  }\n}\n'\n```\n\n## 为某个索引添加字段\n添加number字段：\n### 唯一ID\n```\ncurl -POST 'http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty' -H 'Content-Type: application/json' -d'\n{\n   \"doc\" : {\n      \"number\" : 1\n   }\n}\n'\n```\n### 批量操作\n```\ncurl -XPOST 'localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"script\": {\n    \"inline\": \"ctx._source.number=1\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match_all\": {\n    }\n  }\n}\n'\n\n```\n\n# 根据指定条件进行聚合\n每小时成功登录的次数进行聚合\n```\ncurl -POST 'http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"term\": {\n      \"ssh_type\": \"ssh_successful_login\"\n    }\n  },\n  \"aggs\": {\n    \"sums\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"hour\",\n        \"format\": \"yyyy-MM-dd HH\"\n      }\n    }\n  }\n}\n'\n```\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["ELK"],"categories":["技术篇"]},{"title":"数据结构算法之链表","url":"/2017/11/13/数据结构/数据结构算法之链表/","content":"\n> 链表面试总结，使用python实现，参考：https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html\n\n<!--More-->\n\n\n```\n#coding:utf-8\n\n# 定义链表\nclass ListNode:\n    def __init__(self):\n        self.data = None\n        self.pnext = None\n\n# 链表操作类\nclass ListNode_handle:\n    def __init__(self):\n        self.cur_node = None\n    \n    # 链表添加元素\n    def add(self,data):\n        ln = ListNode()\n        ln.data = data\n        \n        ln.pnext = self.cur_node\n        self.cur_node = ln\n        return ln\n    \n    # 打印链表\n    def prt(self,ln):\n        while ln:\n            print(ln.data,end=\"  \")\n            ln = ln.pnext\n    # 逆序输出\n    def _reverse(self,ln):\n        _list = []\n        while ln:\n            _list.append(ln.data)\n            ln = ln.pnext\n        ln_2 = ListNode()\n        ln_h = ListNode_handle()\n        for i in _list:\n            ln_2 = ln_h.add(i)\n        return ln_2\n    \n    # 求链表的长度\n    def _length(self,ln):\n        _len = 0\n        while ln:\n            _len += 1\n            ln = ln.pnext\n        return _len\n    \n    # 查找指定位置的节点\n    def _find_loc(self,ln,loc):\n        _sum = 0\n        while ln and _sum != loc:\n            _sum += 1\n            ln = ln.pnext\n        return ln.data\n    \n    # 判断某个节点是否在链表中\n    def _exist(self,ln,data):\n        flag = False\n        while ln and data != ln.data:\n            ln = ln.pnext\n        return flag\n\n# 创建链表   \nln = ListNode()\nln_h = ListNode_handle()\na = [1,4,2,5,8,5,7,9]\nfor i in a:\n    ln = ln_h.add(i)\n\nprint(\"正序输出...\")\nln_h.prt(ln)\n\nprint(\"\\n\\n逆序输出...\")\nln_2 = ln_h._reverse(ln)\nln_h.prt(ln_2)\n\n# 求链表ln的长度\nlength = ln_h._length(ln)\nprint(\"\\n\\nln的长度为:\",length)\n\n# 查找链表ln中的倒数第３个节点\ndata = ln_h._find_loc(ln,ln_h._length(ln)-3)\nprint(\"\\n\\n倒数第三个节点为:\",data)\n\n# 返回某个节点在链表中的位置\nloc = ln_h._loc(ln,5)\n\n#　判断某个节点是否在链表中\nflag = ln_h._exist(ln,5)\nprint(\"\\n\\n５是否存在与链表ln中:\",end=\" \")\nif flag:\n    print(\"Yes\")\nelse:\n    print(\"No\")\n```\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["数据结构"],"categories":["技术篇"]},{"title":"数据结构算法之合并两个有序序列","url":"/2017/11/13/数据结构/数据结构算法之合并两个有序序列/","content":"\n> 有序序列的合并，python实现。\n\n<!--More-->\n\n\n```\n#coding:utf-8\n\na = [2,4,6,8,10]\nb = [3,5,7,9,11,13,15]\nc = []\n\ndef merge(a,b):\n    i,j = 0,0\n    while i<=len(a)-1 and j<=len(b)-1:\n        if a[i]<b[j]:\n            c.append(a[i])\n            i+=1\n        else:\n            c.append(b[j])\n            j+=1\n    if i<=len(a)-1:\n        for m in a[i:]:\n            c.append(m)\n    \n    if j<=len(b)-1:\n        for n in b[j:]:\n            c.append(n)\n    print(c)\n\nmerge(a,b)\n```\n\n运行结果为：\n```\n[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]\n```\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["数据结构"],"categories":["技术篇"]},{"title":"数据结构算法之排序","url":"/2017/11/13/数据结构/数据结构算法之排序/","content":"> 数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。\n\n<!--More-->\n\n# 冒泡排序\n```\n#coding：utf-8\n\n# 冒泡排序\ndef maopao():\n    a = [2,1,4,3,9,5,6,8,7]\n    for i in range(len(a)-1):\n        for j in range(len(a)-1-i):\n            if a[j]>a[j+1]:\n                temp = a[j]\n                a[j] = a[j+1]\n                a[j+1] = temp\n    print(a)\nmaopao()\n```\n结果为：\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n---\n# 归并排序\n```\n# 归并排序\ndef merge(a,b):\n    i,j = 0,0\n    c = []\n    while i<=len(a)-1 and j<=len(b)-1:\n        if a[i]<b[j]:\n            c.append(a[i])\n            i+=1\n        else:\n            c.append(b[j])\n            j+=1\n    if i<=len(a)-1:\n        for m in a[i:]:\n            c.append(m)\n    \n    if j<=len(b)-1:\n        for n in b[j:]:\n            c.append(n)\n    return c\n\ndef guibing(a):\n    if len(a)<=1:\n        return a\n    center = int(len(a)/2)\n    left = guibing(a[:center])\n    right = guibing(a[center:])\n    return merge(left,right)\n\nprint(guibing([2,1,4,3,9,5,6,8,7]))\n```\n\n结果为：\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n---\n#　快速排序 \n> 快速排序Python实现\n\n\n\n```\n#　快速排序    \ndef kpsort(left,right,a):\n    based = a[left]\n    i = left\n    j = right\n    while i < j:\n        # 从数组右边开始遍历\n        while a[j]>=based and i<j:\n            j -= 1\n        a[i] = a[j]\n        while a[i]<=based and i<j:\n            i += 1\n        \n        a[j]= a[i]\n        a[i] = based\n\n    return i\n    \ndef kuaipai(left,right,a):\n    if left<right:\n        p = kpsort(left,right,a)\n        kuaipai(left,p-1,a)\n        kuaipai(p+1,right,a)\n\n    return a\n            \nprint(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))\n```\n结果为\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["数据结构"],"categories":["技术篇"]},{"title":"数据结构算法之二叉树","url":"/2017/11/13/数据结构/数据结构算法之二叉树/","content":"\n> 数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。\n\n<!--More-->\n\n注意：py2中\n```\nprint root.elem,\n```\n在py3中要换成\n```\nprint (root.elem,end=\"  \")\n```\n\n```\n# coding:utf-8\n\n# 定义节点类\nclass Node:\n    def __init__(self,elem = -1,):\n        self.elem = elem\n        self.left = None\n        self.right = None\n        \n# 定义二叉树\nclass Tree:\n    def __init__(self):\n        self.root = Node()\n        self.myqu = []\n    \n    # 添加节点\n    def add(self,elem):\n        node = Node(elem)\n        if self.root.elem == -1:         # 判断如果是根节点\n            self.root  = node\n            self.myqu.append(self.root)\n        else:\n            treenode = self.myqu[0]\n            if treenode.left == None:\n                treenode.left = node\n                self.myqu.append(treenode.left)\n            else:\n                treenode.right = node\n                self.myqu.append(treenode.right)\n                self.myqu.pop(0)\n        \n    # 利用递归实现树的先序遍历\n    def xianxu(self,root):\n        if root == None:\n            return\n        print root.elem,\n        self.xianxu(root.left)\n        self.xianxu(root.right)\n        \n    # 利用递归实现树的中序遍历\n    def zhongxu(self,root):\n        if root == None:\n            return \n        self.zhongxu(root.left)\n        print root.elem,\n        self.zhongxu(root.right)\n        \n    # 利用递归实现树的后序遍历\n    def houxu(self,root):\n        if root == None:\n            return \n        self.houxu(root.left)\n        self.houxu(root.right)\n        print root.elem,\n    \n    # 利用队列实现层次遍历\n    def cengci(self,root):\n        if root == None:\n            return\n        myq = []\n        node = root\n        myq.append(node)\n        while myq:\n            node = myq.pop(0)\n            print node.elem,\n            if node.left != None:\n                myq.append(node.left)\n            if node.right != None:\n                myq.append(node.right)\n    \n    # 求树的叶子节点\n    def getYeJiedian(self,root):\n        if root == None:\n            return 0\n        if root.left == None and root.right == None:\n            return 1\n\n        return self.getYeJiedian(root.left) + self.getYeJiedian(root.right)\n\n    # 由先序和中序,还原二叉树\n    def preMidToHou(self,pre,mid):\n        if len(pre)==0:\n            return None\n        if len(pre)==1:\n            Node(mid[0])\n        root = Node(pre[0])\n        root_index = mid.index(pre[0])\n        root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index])\n        root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:])\n        return root\n\n    # 由后序和中序,还原二叉树\n    def preMidToHou(self,mid,hou):\n        if len(hou)==0:\n            return None\n        if len(hou)==1:\n            Node(mid[0])\n        root = Node(hou[-1])\n        root_index = mid.index(hou[-1])\n        root.left = self.preMidToHou(mid[:root_index],hou[:root_index])\n        root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:])\n        return root\n\n# 创建一个树，添加节点\ntree = Tree()\nfor i in range(10):\n    tree.add(i)\n    \nprint(\"二叉树的先序遍历:\")\nprint(tree.xianxu(tree.root))\n\nprint(\"二叉树的中序遍历:\")\nprint(tree.zhongxu(tree.root))\n\nprint(\"二叉树的后序遍历:\")\nprint(tree.houxu(tree.root))\n\nprint(\"二叉树的层次遍历\")\nprint(tree.cengci(tree.root))\n\nprint(\"\\n二叉树的叶子节点为:\")\nprint(tree.getYeJiedian(tree.root))\n\nprint(\"\\n已知二叉树先序遍历和中序遍历，求后序:\")\nprint(\"先序:\")\nprint(tree.xianxu(tree.root))\nprint(\"中序:\")\nprint(tree.zhongxu(tree.root))\nprint(\"后序:\")\nroot = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])\nprint(tree.houxu(root))\n\nprint(\"\\n已知二叉树后序遍历和中序遍历，求前序:\")\nprint(\"后序:\")\nprint(tree.houxu(tree.root))\nprint(\"中序:\")\nprint(tree.zhongxu(tree.root))\nprint(\"前序:\")\nroot = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])\nprint(tree.xianxu(root))\n```\n\n运行结果为：\n```\n二叉树的先序遍历:\n0 1 3 7 8 4 9 2 5 6 None\n\n二叉树的中序遍历:\n7 3 8 1 9 4 0 5 2 6 None\n\n二叉树的后序遍历:\n7 8 3 9 4 1 5 6 2 0 None\n\n二叉树的层次遍历\n0 1 2 3 4 5 6 7 8 9 None\n\n二叉树的叶子节点为:\n5\n\n已知二叉树先序遍历和中序遍历，求后序:\n先序:\n0  1  3  7  8  4  9  2  5  6  None\n中序:\n7  3  8  1  9  4  0  5  2  6  None\n后序:\n1  3  7  8  4  9  0  5  2  6  None\n\n已知二叉树后序遍历和中序遍历，求前序:\n后序:\n7  8  3  9  4  1  5  6  2  0  None\n中序:\n7  3  8  1  9  4  0  5  2  6  None\n前序:\n0  1  3  7  8  4  9  6  2  5  None\n```\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["数据结构"],"categories":["技术篇"]},{"title":"回归分析之Sklearn实现电力预测","url":"/2017/11/07/机器学习/回归分析之Sklearn实现电力预测/","content":"\n参考原文：http://www.cnblogs.com/pinard/p/6016029.html\n这里进行了手动实现，增强记忆。\n<!--More-->\n\n# 1：数据集介绍\n使用的数据是UCI大学公开的机器学习数据\n\n数据的介绍在这： http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\n\n数据的下载地址在这：http://archive.ics.uci.edu/ml/machine-learning-databases/00294/\n\n里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。\n\n\n我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:\n\n$$\nPE = \\theta _{0} + \\theta _{0} * AT + \\theta _{0} * V +\\theta _{0} * AP +\\theta _{0}*RH\n$$\n\n而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。\n\n---\n# 2：准备数据\n下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理\n\nsklearn的归一化处理参考：http://blog.csdn.net/gamer_gyt/article/details/77761884\n\n---\n\n# 3：使用pandas来进行数据的读取\n\n```\nimport pandas as pd\n# pandas 读取数据\ndata = pd.read_csv(\"Folds5x2_pp.csv\")\ndata.head()\n```\n然后会看到如下结果，说明数据读取成功：\n\n```\n\tAT\tV\tAP\tRH\tPE\n0\t8.34\t40.77\t1010.84\t90.01\t480.48\n1\t23.64\t58.49\t1011.40\t74.20\t445.75\n2\t29.74\t56.90\t1007.15\t41.91\t438.76\n3\t19.07\t49.69\t1007.22\t76.79\t453.09\n4\t11.80\t40.66\t1017.13\t97.20\t464.43\n```\n\n---\n\n# 4：准备运行算法的数据\n```\nX = data[[\"AT\",\"V\",\"AP\",\"RH\"]]\nprint X.shape\ny = data[[\"PE\"]]\nprint y.shape\n```\n\n```\n(9568, 4)\n(9568, 1)\n```\n\n说明有9658条数据，其中\"AT\",\"V\",\"AP\",\"RH\" 四列作为样本特征，\"PE\"列作为样本输出。\n\n---\n# 5：划分训练集和测试集\n\n```\nfrom sklearn.cross_validation import train_test_split\n\n# 划分训练集和测试集\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)\nprint X_train.shape\nprint y_train.shape\nprint X_test.shape\nprint y_test.shape\n```\n```\n(7176, 4)\n(7176, 1)\n(2392, 4)\n(2392, 1)\n```\n75%的数据被划分为训练集，25的数据划分为测试集。\n\n---\n# 6：运行sklearn 线性模型\n```\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train,y_train)\n\n# 训练模型完毕，查看结果\nprint linreg.intercept_\nprint linreg.coef_\n```\n\n```\n[ 447.06297099]\n[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]\n```\n\n即我们得到的模型结果为：\n$$\nPE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH\n$$\n\n---\n# 7：模型评价\n我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏\n\n```\ny_pred = linreg.predict(X_test)\nfrom sklearn import metrics\n\n# 使用sklearn来计算mse和Rmse\nprint \"MSE:\",metrics.mean_squared_error(y_test, y_pred)\nprint \"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n```\n\n```\nMSE: 20.0804012021\nRMSE: 4.48111606657\n```\n得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。\n\n---\n# 8：交叉验证\n\n我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：\n\n```\n# 交叉验证\nfrom sklearn.model_selection import cross_val_predict\npredicted = cross_val_predict(linreg,X,y,cv=10)\nprint \"MSE:\",metrics.mean_squared_error(y, predicted)\nprint \"RMSE:\",np.sqrt(metrics.mean_squared_error(y, predicted))\n```\n\n```\nMSE: 20.7955974619\nRMSE: 4.56021901469\n```\n\n可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。\n\n---\n# 9：画图查看结果\n```\n# 画图查看结果\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(y, predicted)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()\n```\n![这里写图片描述](http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"回归分析之线性回归（N元线性回归）","url":"/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/","content":"\n在上一篇文章中我们介绍了 [ 回归分析之理论篇][1]，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。\n\n<!--More-->\n\n# 一元线性回归\n预测房价：\n\n输入编号\t| 平方米\t| 价格\n-|-|-\n1 |\t150 |\t6450\n2\t| 200\t| 7450\n3|\t250\t|8450\n4|\t300\t|9450\n5|\t350\t|11450\n6|\t400\t|15450\n7|\t600|\t18450\n\n针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为\n$$\nH(x) = k*x + b\n$$\n其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：\n$$\nk =\\frac{ \\sum_{1}^{n} (x_{i} - \\bar{x} )(y_{i} - \\bar{y}) } { \\sum_{1}^{n}(x_{i}-\\bar{x})^{2} }\n$$\n自己使用python代码实现为：\n```\ndef leastsq(x,y):\n    \"\"\"\n    x,y分别是要拟合的数据的自变量列表和因变量列表\n    \"\"\"\n    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值\n    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值\n\n    xSum = 0.0\n    ySum = 0.0\n\n    for i in range(len(x)):\n        xSum += (x[i] - meanX) * (y[i] - meanY)\n        ySum += (x[i] - meanX) ** 2\n\n    k = ySum/xSum\n    b = ySum - k * meanX\n\n    return k,b\n```\n\n使用python的scipy包进行计算:\n```\nleastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n\nfrom scipy.optimize import leastsq\nimport numpy as np\n\ndef fun(p, x):\n    \"\"\"\n    定义想要拟合的函数\n    \"\"\"\n    k,b = p    # 从参数p获得拟合的参数\n    return k*x + b\n\ndef err(p, x, y):\n    return fun(p,x) - y\n\n#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间\np0 = [1,1]\n\n#将list类型转换为 numpy.ndarray 类型，最初我直接使用\n#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转\n#换为numpy的类型\n\nx1 = np.array([150,200,250,300,350,400,600])\ny1 = np.array([6450,7450,8450,9450,11450,15450,18450])\n\nxishu = leastsq(err, p0, args=(x1,y1))\n\nprint xishu[0]\n\n```\n\n当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：http://www.cnblogs.com/NanShan2016/p/5493429.html\n\n# 多元线性回归\n总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说\n$$y = a * x^2 + b * x + c$$\n$$y = a * x_1^2 + b * x_1 + c * x_2 + d$$\n$$y = a * x_1^3 + b * x_1^2 + c * x_1 + d$$\n在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例\n$$y = a * x_1^2 + b * x_1 + c * x_2 + d$$\n\n对应的python 代码是：\n```\nfrom scipy.optimize import leastsq\nimport numpy as np\n\n\ndef fun(p, x1, x2):\n    \"\"\"\n    定义想要拟合的函数\n    \"\"\"\n    a,b,c,d = p    # 从参数p获得拟合的参数\n    return a * (x1**2) + b * x1 + c * x2 + d\n\ndef err(p, x1, x2, y):\n    return fun(p,x1,x2) - y\n\n#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间\np0 = [1,1,1,1]\n\n#将list类型转换为 numpy.ndarray 类型，最初我直接使用\n#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转\n#换为numpy的类型\n\nx1 = np.array([150,200,250,300,350,400,600])    # 面积\nx2 = np.array([4,2,7,9,12,14,15])               # 楼层\ny1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米\n\nxishu = leastsq(err, p0, args=(x1,x2,y1))\n\nprint xishu[0]\n```\n\n# sklearn中的线性回归应用\n## 普通最小二乘回归\n这里我们使用的是sklearn中的linear_model来模拟$$y=a * x_1 + b * x_2 + c$$\n\n```\nIn [1]: from sklearn.linear_model import LinearRegression\n\nIn [2]: linreg = LinearRegression()\n\nIn [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n\nIn [4]: linreg.coef_\nOut[4]: array([ 0.5,  0.5])\n\nIn [5]: linreg.intercept_\nOut[5]: 1.1102230246251565e-16\n\nIn [6]: linreg.predict([4,4])\nOut[6]: array([ 4.])\n\nIn [7]: zip([\"x1\",\"x2\"], linreg.coef_)\nOut[7]: [('x1', 0.5), ('x2', 0.49999999999999989)]\n```\n所以可得$$ y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16$$\n\nlinreg.coef_  为系数 a,b\n\nlinreg.intercept_ 为截距 c\n\n缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。\n\n\n## 多项式回归：基函数扩展线性模型\n机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。\n\n例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：\n$$\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}\n$$\n\n如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:\n$$\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2\n$$\n\n我们发现，这仍然是一个线性模型，想象着创建一个新变量：\n$$\nz = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]\n$$\n\n可以把线性回归模型写成下边这种形式：\n$$\ny(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}\n$$\n我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。\n\n使用如下代码，将二维数据进行二元转换,转换规则为：\n$$\n[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\n$$\n\n```\nIn [15]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [16]: import numpy as np\n\nIn [17]: X = np.arange(6).reshape(3,2)\n\nIn [18]: X\nOut[18]: \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\nIn [19]: poly = PolynomialFeatures(degree=2)\n\nIn [20]: poly.fit_transform(X)\nOut[20]: \narray([[  1.,   0.,   1.,   0.,   0.,   1.],\n       [  1.,   2.,   3.,   4.,   6.,   9.],\n       [  1.,   4.,   5.,  16.,  20.,  25.]])\n```\n\n验证：\n```\nIn [38]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [39]: from sklearn.linear_model import LinearRegression\n\nIn [40]: from sklearn.pipeline import Pipeline\n\nIn [41]: import numpy as np\n\nIn [42]: \n\nIn [42]: model = Pipeline( [ (\"poly\",PolynomialFeatures(degree=3)),(\"linear\",LinearRegression(fit_intercept=False)) ] )\n\nIn [43]: model\nOut[43]: Pipeline(steps=[('poly', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('linear', LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])\n\nIn [44]: x = np.arange(5)\n\nIn [45]: y = 3 - 2 * x + x ** 2 - x ** 3\n\nIn [46]: y\nOut[46]: array([  3,   1,  -5, -21, -53])\n\nIn [47]: model = model.fit(x[:,np.newaxis],y)\n\nIn [48]: model.named_steps['linear'].coef_\nOut[48]: array([ 3., -2.,  1., -1.])\n```\n我们可以看出最后求出的参数和一元三次方程是一致的。\n\n这里如果把degree改为2，y的方程也换一下，结果也是一致的\n```\nIn [51]: from sklearn.linear_model import LinearRegression\n\nIn [52]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [53]: from sklearn.pipeline import Pipeline\n\nIn [54]: import numpy as np\n\nIn [55]: model = Pipeline( [ (\"poly\",PolynomialFeatures(degree=2)),(\"linear\",LinearRegression(fit_intercept=False)) ] )\n\nIn [56]: x = np.arange(5)\n\nIn [57]: y = 3 + 2 * x + x ** 2\n\nIn [58]: model = model.fit(x[:, np.newaxis], y)\n\nIn [59]: model.named_steps['linear'].coef_\nOut[59]: array([ 3., 2.,  1.])\n```\n\n\n## 线性回归的评测\n在[上一篇文章](http://note.youdao.com/)中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。\n\n这里我们定义预测值和真实值分别为：\n```\ntrue = [10, 5, 3, 2]\npred = [9, 5, 5, 3]\n```\n\n1: 平均绝对误差（Mean Absolute Error, MAE）\n$$\n\\frac{1}{N}(\\sum_{1}^{n} |y_i - \\bar{y}|)\n$$\n\n2: 均方误差（Mean Squared Error, MSE）\n$$\n\\frac{1}{N}\\sum_{1}^{n}(y_i - \\bar{y})^2\n$$\n\n3: 均方根误差（Root Mean Squared Error, RMSE）\n$$\n\\frac{1}{N} \\sqrt{ \\sum_{1}^{n}(y_i - \\bar{y})^2 }\n$$\n\n```\nIn [80]: from sklearn import metrics\n\nIn [81]: import numpy as np\n\nIn [82]: true = [10, 5, 3, 2]\n\nIn [83]: pred = [9, 5, 5, 3]\n\nIn [84]: print(\"MAE: \", metrics.mean_absolute_error(true,pred))\n('MAE: ', 1.0)\n\nIn [85]: print(\"MAE By Hand: \", (1+0+2+1)/4.)\n('MAE By Hand: ', 1.0)\n\nIn [86]: print(\"MSE: \", metrics.mean_squared_error(true,pred))\n('MSE: ', 1.5)\n\nIn [87]: print(\"MSE By Hand: \", (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)\n('MSE By Hand: ', 1.5)\n\nIn [88]: print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(true,pred)))\n('RMSE: ', 1.2247448713915889)\n\nIn [89]: print(\"RMSE By Hand: \", np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))\n('RMSE By Hand: ', 1.2247448713915889)\n```\n\n---\n# 总结\n线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！\n\n  [1]: http://blog.csdn.net/gamer_gyt/article/details/78008144\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"几种距离计算公式在数据挖掘中的应用场景分析","url":"/2017/09/20/机器学习/几种距离计算公式在数据挖掘中的应用场景分析/","content":"\n本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》\n\n- 曼哈顿距离\n- 欧几里得距离\n- 闵可夫斯基距离\n- 皮尔逊相关系数\n- 余弦相似度\n\n\n<!--More-->\n\n之前整理过一篇关于距离相关的文章：[机器学习算法中的距离和相似性计算公式，分析以及python实现]()\n\n# 闵可夫斯基距离\n\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n$$ \\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}  $$\n\n其中p是一个变参数。\n\n当p=1时，就是曼哈顿距离\n\n当p=2时，就是欧氏距离\n\n当p→∞时，就是切比雪夫距离\n\n根据变参数的不同，闵氏距离可以表示一类的距离。\n\np值越大，单个维度的差值大小会对整体距离有更大的影响\n\n# 曼哈顿距离／欧几里得距离的瑕疵\n在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：\n![这里写图片描述](http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。\n\n现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：\n\n```\nDis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9\n```\n\n同样使用欧式距离计算为：\n```\nDis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3\n```\n当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。\n\n---\n# 用户问题／皮尔逊相关系数／分数膨胀\n\n## 现象——用户问题\n仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：\n\n- Bill没有打出极端的分数，都在2-4分之间\n- Jordyn似乎喜欢所有的乐队，打分都在4-5之间\n- Hailey是一个有趣的人，他的评分不是1就是4\n\n那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！\n\n## 解决该现象\n解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：\n\n![这里写图片描述](http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：\n![这里写图片描述](http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。\n\n皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。\n\n皮尔逊相关系数的计算公式为：\n![这里写图片描述](http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：\n![这里写图片描述](http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n---\n\n# 余弦相似度／稀疏数据\n假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。\n\n# 总结\n这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：\n\n- 如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数\n- 如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离\n- 如果数据是稀疏的，就使用余弦相似度\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"机器学习中非常有名的理论或定理你知道几个","url":"/2017/09/20/机器学习/机器学习中非常有名的理论或定理你知道几个/","content":"\n在机器学习中,有一些非常有名的理论或定理,对理解机器学习的内在特性非常有帮助。\n\n# PCA学习理论\n当使用机器学习算法来解决某个问题时，通常靠经验或者多次实验来得到合适的模型，训练样本数量和相关的参数。但是经验判断成本较高，且不太可靠，因此希望有一套理论能够分析问题，计算模型能力，为算法提供理论保证。这就是计算学习理论（Computational Learning Theory），其中最基础的就是近似正确学习理论（Probably Approximately Coorrect，PCA）。\n\n<!--More-->\n\n机器学习中一个很重要的问题就是期望错误与经验错误之间的误差，称为泛化误差（Generalization Error），用来衡量一个机器学习模型能否很好的泛化到未知数据。\n\n根据大数定理，当训练的数据集D接近于无穷大时，泛化错误趋向于0，即经验风险趋向于期望风险。由于我们并不知道真实的数据分布，因此从有限的数据样本学习到一个期望错误为0的模型是很难的，因此需要降低对模型的期望，只要求学习到的模型能够以一定的概率学习到一个近似正确的假设，这就是PCA学习理论。\n\nPCA学习理论包含了两部分：近似正确和可能。\n\n\n# 没有免费午餐定理\n没有免费午餐定理（No Free Lunch Theorem，NFL）是由Wolpert和Macerday在最优化理论中提出的，NFL证明：对于基于迭代的最优化算法不会存在某种算法对所有问题（有限的搜索空间内）都有效。如果一个算法对某些问题有效，那么他一定在另一些问题上比纯随机搜索算法更差。也就是说，不能脱离具体问题来讨论算法的优劣，任何算法都有优劣性，必须要“具体问题具体分析”。\n\n\n# 丑小鸭定理\n丑小鸭定理（Ugly Duckling Theorem）是1969年由渡边慧提出的[Watan-able, 1969]。“丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大”。这个定理初看好像不符合常识，但是仔细思考后是非常有道理的。因为世界上不存在相似性的客观标准，一切相似性的标准都是主观的。如果以体型大小的角度来看，丑小鸭和白天鹅的区别大于两只白天鹅的区别；但是如果以基因的角度来看，丑小鸭与它父母的差别要小于他父母和其他白天鹅之间的差别。\n\n# 奥卡姆剃刀\n奥卡姆剃刀（Occam's Razor）是由14世界逻辑学家William of Occam提出的一个解决问题的法则：“如无必要，勿增实体”。\n\n奥卡姆剃刀的思想和机器学习上正则化思想十分相似：简答的模型泛化能力更好。如果有两个性能相近的模型，我们更倾向于选择简单的模型。因此在机器学习准则上，我们经常会引入参数正则化（比如L2正则）来限制模型能力，避免过拟合。\n\n> 这里需要区分下L1正则和L2正则的区别，如果需要小编回答，可在评论区留言！\n\n奥卡姆剃刀的一种形式化是最小描述长度（Minimum Description Length, MDL）原则，即对一个数据集D，最好的模型f属于F是会使得数据集的压缩效果最好，即编码长度最小。\n\n最小描述长度也可以通过贝叶斯学习的观点来解释，模型 f 在数据集 D 上的对数后验概率为：\n$$\n\\underset{f}{max\\log p(f|D)} = \\underset{f}{max\\log p(D|f)} + logp(f) =  \\underset{f}{min}-log \\ p(D|f) - log \\ p(f)\n$$\n其中 -log p(f)和-log p(D|f)可以分别看作是模型f的编码长度和在该模型下数据集D的编码长度，也就是说我们不但要使得模型f可以编码数据集D，也要使模型f尽可能的简单。\n\n\n# 归纳偏置\n在机器学习中，很多算法会对学习的问题做一些假设，这些假设就称为归纳偏置（Inductive Bias）。比如在最近邻分类器中，我们会假设在特征空间内，一个小的局部区域中的大部分样本都属于同一类。在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是相互独立的。\n\n归纳偏置在贝叶斯学习中也成为先验（priors）。\n\n# 大数定理\n假设X1,X2,....是独立同分布的随机变量，记他们的均值为 $\\mu$，方差为：$\\sigma ^2$,则对于任意的正数$\\varepsilon$，有\n$$\n\\displaystyle \\lim_{ n \\to \\infty } P(| \\bar{X}_n - \\mu| \\geq \\varepsilon) = 0 \n$$\n\n我们通常对数据进行抽样估计利用的则是大数定理思想。\n\n# 中心极限定理\n中心极限定理是研究独立随机变量和的极限分布为正态分布的命题。经过科学家长期的观察和总结，发现服从正态分布的随机现象往往是由独立(或弱相依)的随机变量产生的。\n\n这类随机现象往往可视为独立随机变量之和\n$$\n\\sum_{i=1}^{n} x_i\n$$\n在什么条件下渐进于正态分布的问题。为使问题规范化，数学家们将问题归结为讨论规范和\n$$\n\\frac{\\sum_{i=1}^{n}x_i - E(\\sum_{i=1}^{n}x_i ) }{\\sqrt {D(\\sum_{i=1}^{n}x_i )} }\n$$\n有渐进分布N(0,1)的条件，并称有此结论的随机序列{x_n}服从中心极限定理。即：\n$$\n\\frac{\\sum_{i=1}^{n}x_i - E(\\sum_{i=1}^{n}x_i ) }{\\sqrt {D(\\sum_{i=1}^{n}x_i )} } \\sim  N(0,1)\n$$\n\n独立同分布的中心极限定理和德莫佛-拉普拉斯中心极限定理参考：\n- https://blog.csdn.net/baishuiniyaonulia/article/details/83998635\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"回归分析之理论篇","url":"/2017/09/17/机器学习/回归分析之理论篇/","content":"2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。\n\n<!--More-->\nCSDN链接：[点击阅读](http://blog.csdn.net/gamer_gyt/article/details/78008144)\n\n# 一：一些名词定义\n\n## 1）指数分布族\n\n\n指数分布族是指可以表示为指数形式的概率分布。\n$$\nf_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )\n$$\n其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。\n\n伯努利分布与高斯分布是两个典型的指数分布族\n\n### 伯努利分布\n又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则\n其概率质量函数为：\n```\n{\\displaystyle f_{X}(x)=p^{x}(1-p)^{1-x}=\\left\\{{\\begin{matrix}p&{\\mbox{if }}x=1,\\\\q\\ &{\\mbox{if }}x=0.\\\\\\end{matrix}}\\right.}\n```\n其期望值为：\n$$\n{\\displaystyle \\operatorname {E} [X]=\\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}\n$$\n\n其方差为：\n$$\n{\\displaystyle \\operatorname {var} [X]=\\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}\n$$\n\n\n### 正态分布(高斯分布)\n若随机变量X服从一个位置参数为 ${\\displaystyle \\mu }$ 、尺度参数为 ${\\displaystyle \\sigma } $ 的概率分布，记为：\n$$\nX \\sim N(\\mu,\\sigma^2),\n$$\n\n其概率密度函数为:\n```\nf(x) = {1 \\over \\sigma\\sqrt{2\\pi} }\\,e^{- {{(x-\\mu )^2 \\over 2\\sigma^2}}}\n```\n\n正态分布的数学期望值或期望值$ {\\displaystyle \\mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\\displaystyle \\sigma ^{2}} $ 的开平方或标准差$ {\\displaystyle \\sigma }$ 等于尺度参数，决定了分布的幅度。\n\n### 标准正态分布：\n\n如果$ {\\displaystyle \\mu =0} $ 并且 $ {\\displaystyle \\sigma =1} $ 则这个正态分布称为标准正态分布。简化为：\n```\nf(x) = \\frac{1}{\\sqrt{2\\pi}} \\, \\exp\\left(-\\frac{x^2}{2} \\right)\n```\n如下图所示：\n\n![image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png)\n\n正态分布中一些值得注意的量：\n\n- 密度函数关于平均值对称\n- 平均值与它的众数（statistical mode）以及中位数（median）同一数值。\n- 函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。\n- 95.449974%的面积在平均数左右两个标准差 $ {\\displaystyle 2\\sigma } $ 的范围内。\n- 99.730020%的面积在平均数左右三个标准差$ {\\displaystyle 3\\sigma } $ 的范围内。\n- 99.993666%的面积在平均数左右四个标准差$ {\\displaystyle 4\\sigma } $ 的范围内。\n- 函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。\n\n## 2）多重共线性和完全共线性\n\n多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。\n\n完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。\n\n两者在某种特殊情况下是有交集的。\n\n## 3）T检验\nT检验又叫student T 检验，主要用于样本含量小，总标准差 $\\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。\n参考: http://blog.csdn.net/shulixu/article/details/53354206\n\n\n\n## 4）关系\n- 函数关系\n> 确定性关系，y=3+2x\n- 相关关系\n>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。\n\n## 5）虚拟变量\n定义：\n>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）\n\n作用：\n>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。\n\n设置：\n>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。\n性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。\n\n例子：\n针对上边所说的体重和身高，性别的关系。\n\n构建模型：\n- 1）加法模型\n```\nw = a + b * h + c * isman\n```\n针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。\n\n- 2）乘法模型\n```\nw = a + b * h + c * isman * h + d * iswoman * h\n```\n同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b*h + c * h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。\n\n- 3）混合模型\n```\nw = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h\n```\n假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b*h + c + e * h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。\n\n# 二：什么是回归（分析）\n回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3...）和另外一组随机变量Y（Y1，Y2，Y3...）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。\n\n回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：\n\n## 1）线性回归\n线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。\n\n一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。\n\n多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。\n\n## 2）非线性回归\n有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。\n\n## 3）广义线性回归\n一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。\n典型的代表是Logistic回归。\n\n## 4）如何衡量相关关系既判断适不适合使用线性回归模型？\n使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy>0,代表正相关，Rxy<0,代表负相关）\n\n$$\nr_{XY} = \\frac{ \\sum (X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) }{ \\sqrt{ \\sum (X_{i}-\\bar{X})^2) \\sum (Y_{i}-\\bar{Y})^2) } }\n$$\n\n# 三：回归中困难点\n## 1）选定变量\n> 假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：http://blog.csdn.net/gamer_gyt/article/details/51418069 ，基本是整理《机器学习实战》这本书的笔记）\n\n## 2）发现多重共线性\n(1).方差扩大因子法( VIF)\n\n>一般认为如果最大的VIF超过10，常常表示存在多重共线性。\n\n(2).容差容忍定法\n\n>如果容差（tolerance）<=0.1，常常表示存在多重共线性。\n\n(3). 条件索引\n\n>条件索引(condition index)>10，可以说明存在比较严重的共线性\n\n\n\n## 3）过拟合与欠拟合问题\n过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。\n\n在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。\n\n这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。\n\n### 如何限制过拟合？\n> 过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。\n过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。\n当评价机器学习算法时我们有两者重要的技巧来限制过拟合\n使用重采样来评价模型效能\n保留一个验证数据集\n最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。\n验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。\n\n## 4）检验模型是否合理\n验证目前主要采用如下三类办法：\n1、拟合优度检验\n主要有R^2，t检验，f检验等等\n这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。\n2、预测值和真实值比较\n主要是差值和比值，一般差值和比值都不超过5%。\n3、另外的办法\nGEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。\nGEH=(2(M-C)^2/(M+C))^(1/2)\n其中M是预测值，C是实际观测值\n如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。\nhttp://blog.sina.com.cn/s/blog_66188c300100hl45.html\n\n## 5）线性回归的模型评判\n- 误差平方和（残差平方和）\n\n例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。\n\n点到直线的距离公式为： \n$$\n \\frac{\\left | A_{x_{0}}+B_{y_{0}} +C \\right |}{\\sqrt{A^2 + B^2 }}\n$$\n由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：\n$$\nRSS = \\sum_{i=1}^{n}(y_{i}- \\hat{y_{i}} )^2 = \\sum_{i=1}^{n}[y_{i} - (\\alpha +\\beta x_{i})]^2\n$$\n\n- AIC准则（赤池信息准则）\n$$\nAIC=n ln (RSSp/n)+2p\n$$\nn为变量总个数，p为选出的变量个数，AIC越小越好\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"数据归一化和其在sklearn中的处理","url":"/2017/09/01/特征工程/数据归一化和其在sklearn中的处理/","content":"> 本文主要介绍数据归一化和其在sklearn中的处理。\n\n<!--More-->\n\n# 一：数据归一化\n\n数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。\n\n归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。\n\n## 1）min-max标准化\n\n\nmin-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：\n\n$$\nX_{scale} = \\frac{x-min}{max-min}\n$$\n\n对应的python实现为\n```\n# x为数据 比如说 [1,2,1,3,2,4,1]\ndef Normalization(x):\n    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]\n```\n\n如果要将数据转换到[-1,1]之间，可以修改其数学公式为：\n\n$$\nX_{scale} = \\frac{x-x_{mean}}{max-min}\n$$\nx_mean 表示平均值。\n\n对应的python实现为\n```\nimport numpy as np\n\n# x为数据 比如说 [1,2,1,3,2,4,1]\ndef Normalization(x):\n    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]\n```\n\n其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n\n该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。\n\n当然还有一些其他的办法也能实现数据的标准化。\n\n## 2）z-score标准化\nz-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为\n\n$$\nX_{scale} = \\frac{x-\\mu }{\\sigma }\n$$\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\n其对应的python实现为：\n```\nimport numpy as np\n\n#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ndef z_score(x):\n    return (x - np.mean(x) )/np.std(x, ddof = 1)\n```\nz-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。\n\n## 3）改进的z-score标准化\n\n将标准分公式中的均值改为中位数，将标准差改为绝对偏差。\n\n$$\nX_{scale} = \\frac{x-x_{center} }{\\sigma_{1} }$$\n中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。\n\nσ1为所有样本数据的绝对偏差,其计算公式为：\n$$\n\\frac{1}{N} \\sum_{1}^{n}|x_{i} - x_{center}|\n$$\n\n----\n# 二：sklearn中的归一化\n\nsklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。\n\n## 1）均值-标准差缩放\n\n即我们上边对应的z-score标准化。\n在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。\n\n实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。\n\n\n例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。\n\n例子：\n```\n>>> from sklearn import preprocessing\n>>> import numpy as np\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n>>> X_scaled = preprocessing.scale(X_train)\n>>> X_scaled\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n标准化后的数据符合标准正太分布\n```\n>>> X_scaled.mean(axis=0)\narray([ 0.,  0.,  0.])\n>>> X_scaled.std(axis=0)\narray([ 1.,  1.,  1.])\n```\n\n预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。\n```\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\n>>> scaler\nStandardScaler(copy=True, with_mean=True, with_std=True)\n>>> scaler.mean_\narray([ 1.        ,  0.        ,  0.33333333])\n>>> scaler.scale_\narray([ 0.81649658,  0.81649658,  1.24721913])\n>>> scaler.transform(X_train)\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n\n使用转换器可以对新数据进行转换\n```\n>>> X_test = [[-1., 1., 0.]]\n>>> scaler.transform(X_test)\narray([[-2.44948974,  1.22474487, -0.26726124]])\n```\n\n## 2）min-max标准化\n\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n\n```\n\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                      [ 2.,  0.,  0.],\n...                      [ 0.,  1., -1.]])\n>>> min_max_scaler = preprocessing.MinMaxScaler()\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n>>> X_train_minmax\narray([[ 0.5       ,  0.        ,  1.        ],\n       [ 1.        ,  0.5       ,  0.33333333],\n       [ 0.        ,  1.        ,  0.        ]])\n```\n上边我们创建的min_max_scaler 同样适用于新的测试数据\n```\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_minmax = min_max_scaler.transform(X_test)\n>>> X_test_minmax\narray([[-1.5       ,  0.        ,  1.66666667]])\n```\n可以通过scale_和min方法查看标准差和最小值\n```\n>>> min_max_scaler.scale_ \narray([ 0.5       ,  0.5       ,  0.33333333])\n>>> min_max_scaler.min_\narray([ 0.        ,  0.5       ,  0.33333333])\n```\n\n## 3）最大值标准化\n\n对于每个数值／每个维度的最大值\n\n```\n>>> X_train\narray([[ 1., -1.,  2.],\n       [ 2.,  0.,  0.],\n       [ 0.,  1., -1.]])\n>>> max_abs_scaler = preprocessing.MaxAbsScaler()\n>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n>>> X_train_maxabs\narray([[ 0.5, -1. ,  1. ],\n       [ 1. ,  0. ,  0. ],\n       [ 0. ,  1. , -0.5]])\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_maxabs = max_abs_scaler.transform(X_test)\n>>> X_test_maxabs                 \narray([[-1.5, -1. ,  2. ]])\n>>> max_abs_scaler.scale_         \narray([ 2.,  1.,  2.])\n```\n\n## 4）规范化\n规范化是文本分类和聚类中向量空间模型的基础\n\n```\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> X_normalized = preprocessing.normalize(X, norm='l2')\n>>> X_normalized\narray([[ 0.40824829, -0.40824829,  0.81649658],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.70710678, -0.70710678]])\n```\n\n解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。\n\n机器学习中的范数规则：[点击阅读](http://blog.csdn.net/zouxy09/article/details/24971995/)<br>\n其他对应参数：[点击查看](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize)\n\n\npreprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。\n```\n>>> normalizer = preprocessing.Normalizer().fit(X)\n>>> normalizer\nNormalizer(copy=True, norm='l2')\n>>> normalizer.transform(X)\narray([[ 0.40824829, -0.40824829,  0.81649658],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.70710678, -0.70710678]])\n>>> normalizer.transform([[-1,1,0]])\narray([[-0.70710678,  0.70710678,  0.        ]])\n```\n\n## 5）二值化\n将数据转换到0-1 之间\n```\n>>> X\n[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]\n>>> binarizer = preprocessing.Binarizer().fit(X)\n>>> binarizer\nBinarizer(copy=True, threshold=0.0)\n>>> binarizer.transform(X)\narray([[ 1.,  0.,  1.],\n       [ 1.,  0.,  0.],\n       [ 0.,  1.,  0.]])\n```\n可以调整二值化的门阀\n```\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\n>>> binarizer.transform(X)\narray([[ 0.,  0.,  1.],\n       [ 1.,  0.,  0.],\n       [ 0.,  0.,  0.]])\n```\n\n## 6）编码的分类特征\n\n通常情况下，特征不是作为连续值给定的。例如一个人可以有\n```\n[\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]\n```\n这些特征可以被有效的编码为整数，例如\n```\n[\"male\", \"from US\", \"uses Internet Explorer\"] => [0, 1, 3]\n[\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\n```\n这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。\n\n```\n>>> enc = preprocessing.OneHotEncoder()\n>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n       handle_unknown='error', n_values='auto', sparse=True)\n>>> enc.transform([[0,1,3]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])\n```\n默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。\n```\n>>> enc = preprocessing.OneHotEncoder(n_values=[2,3,4])\n>>> enc.fit([[1,2,3],[0,2,0]])\nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n>>> enc.transform([[1,0,0]]).toarray()\narray([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])\n```\n\n## 7）填补缺失值\n由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import Imputer\n>>> imp = Imputer(missing_values='NaN',strategy='mean',verbose=0)\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\nImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n>>> print(imp.transform(X))                           \n[[ 4.          2.        ]\n [ 6.          3.66666667]\n [ 7.          6.        ]]\n```\n\nImputer同样支持稀疏矩阵\n```\n>>> import scipy.sparse as sp\n>>> X = sp.csc_matrix([[1,2],[0,3],[7,6]])\n>>> imp = Imputer(missing_values=0,strategy='mean',axis=0)\n>>> imp.fit(X)\nImputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)\n>>> X_test = sp.csc\nsp.csc          sp.csc_matrix(  \n>>> X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])\n>>> print(imp.transform(X_test))\n[[ 4.          2.        ]\n [ 6.          3.66666667]\n [ 7.          6.        ]]\n```\n\n## 8）生成多项式特征\n通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。\n\n其遵循的原则是 \n$$ \n(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\n$$\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X                                                 \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)                             \narray([[  1.,   0.,   1.,   0.,   0.,   1.],\n       [  1.,   2.,   3.,   4.,   6.,   9.],\n       [  1.,   4.,   5.,  16.,  20.,  25.]])\n```\n\n有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成\n```\n>>> X = np.arange(9).reshape(3, 3)\n>>> X                                                 \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n>>> poly.fit_transform(X)                             \narray([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])\n```\n其遵循的规则是：\n$$\n(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\n$$\n\n---\n\n对应的scikit-learn资料为： http://scikit-learn.org/stable/modules/preprocessing.html\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["特征工程"],"categories":["技术篇"]},{"title":"MachingLearning中的距离和相似性计算以及python实现","url":"/2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/","content":"> 写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。\n<!--More-->\n\n# 前言\n\n在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。\n\n\n\n文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。\n\n参考：http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html\n\n推荐：https://my.oschina.net/hunglish/blog/787596\n\n# 欧氏距离\n也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。\n## 二维空间的欧氏距离 \n二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离\n\n$$\nd12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}\n$$\npython 实现为：\n\n```\n# coding: utf-8\n\nfrom numpy import *\n\ndef twoPointDistance(a,b):\n\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )\n\treturn d\n\nprint 'a,b 二维距离为：',twoPointDistance((1,1),(2,2))\n```\n\n\n## 三维空间的欧氏距离\n三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离\n\n$$d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}$$\npython 实现为：\n```\ndef threePointDistance(a,b):\n\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )\n\treturn d\n\nprint 'a,b 三维距离为：',threePointDistance((1,1,1),(2,2,2))\n```\n\n## 多维空间的欧氏距离\n两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离\n\n$$ \n\\sqrt{\\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }\n$$\npython 实现为：\n\n```\ndef distance(a,b):\n\tsum = 0\n\tfor i in range(len(a)):\n\t\tsum += (a[i]-b[i])**2\n\treturn sqrt(sum)\n\nprint 'a,b 多维距离为：',distance((1,1,2,2),(2,2,4,4))\n```\n这里传入的参数可以是任意维的，该公式也适应上边的二维和三维\n\n# 标准欧氏距离\n标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：\n\n　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：\n　　\n$$\nX^* = \\frac{X-m}{s}\n$$\n标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差\n\n经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：\n\n$$\nd_{12} =\\sqrt {\\sum_{k=1}^{n} (\\frac{x_{1k}-x_{2k}}{s_{k}})^2}\n$$\n如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。\n\npython 实现为\n```\ndef moreBZOSdis(a,b):\n    sumnum = 0\n    for i in range(len(a)):\n        # 计算si 分量标准差\n        avg = (a[i]-b[i])/2\n        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )\n        sumnum += ((a[i]-b[i])/si ) ** 2\n\t\n    return sqrt(sumnum)\n\nprint 'a,b 标准欧式距离：',moreBZOSdis((1,2,1,2),(3,3,3,4))\n```\n\n# 曼哈顿距离\n又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。\n\n在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。\n\n这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，[一种人脸表情分类的新方法——Manhattan距离](http://download.csdn.net/detail/gamer_gyt/9899825)\n\n## 二维曼哈顿距离\n二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n\n$$\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right |\n$$\npython实现为\n```\ndef twoMHDdis(a,b):\n    return abs(a[0]-b[0])+abs(a[1]-b[1])\n\nprint 'a,b 二维曼哈顿距离为：', twoMHDdis((1,1),(2,2)) \n```\n\n\n## 三维曼哈顿距离\n三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离\n\n$$\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right | + \\left |z_{1}-z_{2}  \\right |\n$$\npython实现为\n```\ndef threeMHDdis(a,b):\n\treturn abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])\n \nprint 'a,b 三维曼哈顿距离为：', threeMHDdis((1,1,1),(2,2,2)) \n```\n\n## 多维曼哈顿距离\n多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n$$\nd12 = \\sum_{k=1}^{n} \\left | x_{1k} - x_{2k} \\right |\n$$\npython实现为\n```\ndef moreMHDdis(a,b):\n    sum = 0 \n    for i in range(len(a)):\n        sum += abs(a[i]-b[i])\n    return sum\n\nprint 'a,b 多维曼哈顿距离为：', moreMHDdis((1,1,1,1),(2,2,2,2)) \n```\n由于维距离计算是比较灵活的，所以也同样适合二维和三维。\n\n# 切比雪夫距离\n切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , ... ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中[曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别](http://blog.csdn.net/jerry81333/article/details/52632687) 给了一个很形象的解释如下：\n```\n比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。\n\n但是按照切比雪夫距离，这是完全不同的概念了。\n\n譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。\n\n这样来看的话，距离是不是就不一样了呢？\n\n或者还是不清楚，我再说的详细点。\n\n同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。\n```\n\n## 二维切比雪夫距离\n\n二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离\n\n$$\nd_{12} = max( \\left | x_{1} - x_{2} \\right | , \\left | y_{1} - y_{2} \\right |)\n$$\npython 实现为\n\n```\ndef twoQBXFdis(a,b):\n    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))\n\nprint 'a,b二维切比雪夫距离：' , twoQBXFdis((1,2),(3,4))\n```\n\n\n## 多维切比雪夫距离\n\n两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离\n$$\nd12 = max_{i\\epsilon n}( \\left | x_{1i} - x_{2i} \\right | )\n$$\n\npython 实现为\n```\ndef moreQBXFdis(a,b):\n    maxnum = 0\n    for i in range(len(a)):\n        if abs(a[i]-b[i]) > maxnum:\n            maxnum = abs(a[i]-b[i])\n    return maxnum\n\nprint 'a,b多维切比雪夫距离：' , moreQBXFdis((1,1,1,1),(3,4,3,4))\n```\n\n# 马氏距离\n有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为\n\n$$\nD(x) = \\sqrt{(X-\\mu )^TS^{-1}(X-\\mu)}\n$$\n而其中向量Xi与Xj之间的马氏距离定义为\n$$\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}\n$$\n 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：\n$$\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}\n$$\n也就是欧氏距离了。\n\n若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。\n\n马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。\n\n\n# 夹角余弦\n几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。\n\n## 二维空间向量的夹角余弦相似度\n在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：\n\n$$\n\\cos \\theta  = \\frac{x_{1}x_{2} + y_{1}y_{2}}{ \\sqrt{ x_{1}^2+x_{2}^2 }\\sqrt{ y_{1}^2+y_{2}^2 } }\n$$\npython 实现为\n```\ndef twoCos(a,b):\n    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )\n\n    return cos\nprint 'a,b 二维夹角余弦距离：',twoCos((1,1),(2,2))\n```\n\n\n## 多维空间向量的夹角余弦相似度\n两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦\n\n类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。\n$$\n\\cos \\theta  = \\frac{a \\cdot  b}{\\left | a \\right | \\left | b \\right |}\n$$\n即：\n$$\n\\cos \\theta  = \\frac{ \\sum_{k=1}^{n} x_{1k}x_{2k} }{ \\sqrt{ \\sum_{k=1}^{n}x_{1k}^2 }\\sqrt{ \\sum_{k=1}^{n} x_{2k}^2 } }\n$$\npython实现为\n```\ndef moreCos(a,b):\n    sum_fenzi = 0.0\n    sum_fenmu_1,sum_fenmu_2 = 0,0\n    for i in range(len(a)):\n        sum_fenzi += a[i]*b[i]\n        sum_fenmu_1 += a[i]**2 \n        sum_fenmu_2 += b[i]**2 \n\n    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )\nprint 'a,b 多维夹角余弦距离：',moreCos((1,1,1,1),(2,2,2,2))\n```\n\n夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。\n\n# 闵可夫斯基距离\n\n闵氏距离不是一种距离，而是一组距离的定义\n## 定义\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n$$\n\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p} \n$$\n\n其中p是一个变参数。\n\n当p=1时，就是曼哈顿距离\n\n当p=2时，就是欧氏距离\n\n当p→∞时，就是切比雪夫距离\n\n根据变参数的不同，闵氏距离可以表示一类的距离。\n\n## 闵氏距离的缺点\n闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。\n\n举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。\n\n简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。\n\n\n# 汉明距离\n\n## 定义\n两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。\n\n应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。\n\n## python 实现\n```\ndef hanmingDis(a,b):\n    sumnum = 0\n    for i in range(len(a)):\n        if a[i]!=b[i]:\n            sumnum += 1\n    return sumnum\n\nprint 'a,b 汉明距离：',hanmingDis((1,1,2,3),(2,2,1,3))\n```\n\n# 杰卡德距离 & 杰卡德相似系数\n## 杰卡德距离\n与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：\n$$\nJ_{\\delta} (A,B) = \\frac{| A \\bigcup B | - | A \\bigcap B |}{| A \\bigcup B |}\n$$\n杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。\n\npython 实现\n```\ndef jiekadeDis(a,b):\n    set_a = set(a)\n    set_b = set(b)\n    dis = float(len( (set_a | set_b) - (set_a & set_b) ) )/ len(set_a | set_b)\n    return dis\n\nprint 'a,b 杰卡德距离：', jiekadeDis((1,2,3),(2,3,4))\n```\n\n## 杰卡德相似系数\n两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。\n$$\nJ(A,B) = \\frac{| A \\bigcap B |}{| A \\bigcup B |}\n$$\n杰卡德相似系数是衡量两个集合的相似度一种指标。\n\npython 实现\n```\ndef jiekadeXSDis(a,b):\n    set_a = set(a)\n    set_b = set(b)\n    dis = float(len(set_a & set_b)  )/ len(set_a | set_b)\n    return dis\n\nprint 'a,b 杰卡德相似系数：', jiekadeXSDis((1,2,3),(2,3,4))\n```\n\n## 杰卡德相似系数与杰卡德距离的应用\n可将杰卡德相似系数用在衡量样本的相似度上。\n\n　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。\n\np ：样本A与B都是1的维度的个数\n\nq ：样本A是1，样本B是0的维度的个数\n\nr ：样本A是0，样本B是1的维度的个数\n\ns ：样本A与B都是0的维度的个数\n\n\n\n那么样本A与B的杰卡德相似系数可以表示为：\n\n这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。\n\n而样本A与B的杰卡德距离表示为：\n$$\nJ= \\frac{p}{p+q+r}\n$$\n\n# 相关系数 & 相关距离\n## 相关系数\n$$\n\\rho_{XY} = \\frac{Cov(X,Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}}=\\frac{ E( (X-EX) (Y-EY) ) }{ \\sqrt{D(X)} \\sqrt{D(Y)} }\n$$\n相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。\n\npython 实现\n相关系数可以利用numpy库中的corrcoef函数来计算\n例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。\n```\nfrom numpy import  *\na = array([[1, 1, 2, 2, 3],  \n       [2, 2, 3, 3, 5],  \n       [1, 4, 2, 2, 3]]) \n\nprint corrcoef(a)\n\n>>array([[ 1.        ,  0.97590007,  0.10482848],\n       [ 0.97590007,  1.        ,  0.17902872],\n       [ 0.10482848,  0.17902872,  1.        ]])\n\nprint corrcoef(a,rowvar=0)\n\n>>array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])\n```\n\n## 相关距离\n\n$$\nD_{xy} = 1 - \\rho _{XY}\n$$\n\npython 实现（基于相关系数）\n同样针对矩阵a\n```\n# 行之间的相关距离\nones(shape(corrcoef(a)),int) - corrcoef(a)\n\n>>array([[ 0.        ,  0.02409993,  0.89517152],\n       [ 0.02409993,  0.        ,  0.82097128],\n       [ 0.89517152,  0.82097128,  0.        ]])\n       \n       \n# 列之间的相关距离\nones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)\n\n>>array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])\n\n```\n\n\n# 信息熵\n\n信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。\n\n计算给定的样本集X的信息熵的公式：\n\n$$\nEntropy(X) = \\sum_{i=1}^{n} -p_{i} log_{2}p_{i} \n$$\n\n参数的含义：\n\nn：样本集X的分类数\n\npi：X中第i类元素出现的概率\n\n信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0\n\npython进行计算和实现可参考：\nhttp://blog.csdn.net/autoliuweijie/article/details/52244246\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["机器学习"],"categories":["技术篇"]},{"title":"一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","url":"/2017/04/16/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦/","content":"\n> 送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。\n\n<!--More-->\n\n\n![一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦。](http://img.blog.csdn.net/20170415205051154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n-----\n## **对《大话》的解读**\n\n有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。\n\n一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。\n\n至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。\n\n对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。\n\n------\n## **他好像一条狗呀！**\n\n-“那个人的样子好怪啊”\n-“我也看到了，他好像条狗啊”\n是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。\n\n至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。\n\n这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。\n\n那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。\n\n------\n## **那句意中人，满足了多少人的少女心**\n\n“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”\n“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”\n这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。\n\n进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。\n\n所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。\n\n从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。\n\n-----\n## **我猜中了开头，可是我猜不着这结局**\n紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。\n\n----\n## **《大话》把遗憾和难题抛给了时间**\n\n又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。 \n没有失去过，也永远不能明白得到的快乐。 \n附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。 \n先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。 \n转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。 \n只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 　 \n十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担\n\n---\n## **如今的你，何去何从**\n\n“如今的你，何去何从？”\n“对呀，何去何从”\n\n真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。\n\n而你呢？没有目标，你便是一个游荡的灵魂。\n\n---\n## **加长版加了什么**\n1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。\n\n2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝\n\n3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片”\n\n4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。\n\n5：2K画面的修复\n\n---\n<h2>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦\n\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["随手记"],"categories":["随手记"]},{"title":"2016年终总结-别了青春与流年，遇见下一个自己","url":"/2016/12/21/随手记/2016年终总结-别了青春与流年，遇见下一个自己/","content":"\n<font size=3>如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。</font>\n\n\n<!--More-->\n\n\n## **2016-时间是长了脚的妖怪，跑的飞快**\n\n&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次......\n&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。\n&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。  \n&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。\n\n![这里写图片描述](http://img.blog.csdn.net/20161220223409601)\n\n-----\n\n## **2016-剑未配好，已出江湖，来一场说走就走的北漂**\n&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。\n\n- 七月，别了流年\n&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。\n\n- 广联达\n&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。\n&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。\n&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。\n&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图\n![这里写图片描述](http://img.blog.csdn.net/20161220230511202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。\n\n- 昌平线，煎熬\n&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。\n&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。\n\n----\n## **2016-别了流年，是现在的我**\n&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。\n&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。\n&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。\n&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。\n&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。\n&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：http://blog.csdn.net/column/details/13159.html\n &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：http://blog.csdn.net/column/details/13079.html\n \n&nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。\n\n---\n## **2016-我在CSDN的收获**\n - 鲍大神\n&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条\"不归路\"，谢谢<a href=\"http://blog.csdn.net/baolibin528\">鲍大神</a>这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。\n\n- 梦姐姐\n&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。\n\n- 结识技术爱好者\n&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。\n\n&nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。\n\n----\n## **2016-开始commit我的github**\n&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。\n&nbsp;&nbsp;&nbsp;&nbsp;我的github：https://github.com/thinkgamer\n![这里写图片描述](http://img.blog.csdn.net/20161221000236962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n----\n## **2016-杂乱无章**\n&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。\n&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。\n\n----\n## **2017-下一个自己**\n&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。\n&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成：\n\n -  一个安卓APP和对应的Web \n -  小说《这夏未眠》\n - 发表社区划分论文\n - 深入学习Scala和Spark\n - 掌握一个深度学习框架（eg：Caffe）\n - 跟进研究Hadoop家族的最近版本，并形成文档\n - 换一台Mackbook Pro\n - 攒够100K+\n\n&nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["年终总结"],"categories":["随手记"]},{"title":"这夏未眠.前言","url":"/2016/09/17/夏未眠/这夏未眠-前言/","content":"\n青春在岁月中葬送了开始，用悲字调刻画的年轮在那片草地上画上了最后一圈，漫天飞舞的纸片如此刻房费的心一样苍白无力，尘埃落定，一切都已结束。你用对不起作结了那段美好时光，我用没关系掩饰心痛的泪水，转身离去，用空白再见。\n\n<!--More-->\n\n难以割舍的流河里，有你荡起的微笑，我回忆着，慢慢就流下了眼泪，此刻我想把记忆封存，我不想在若干年后拿起来温习时，只剩下一些虚无缥缈的梦幻，宁愿再看到一些文字而流下当初所认为的幸福的泪水，或许最无力的是如果，但我仍想说，如果时光循环到那年起点，我亦然会爱，因为夏未眠。\n\n\n\n没有人永远青春，却永远有人正在青春着，是呀，那夏，他们平行的爱情会以最美丽的姿态延后，永远会有人经历着，或许分手，或许继续在一起，但那早已与他们无关，他只希望她幸福，她只希望他幸福。但结果呢？两个人都没有幸福！\n\n我们没有权利去质疑在那个年龄他们的感情有没有权利，只要快乐，为什么不能在一起，他可以为了她放弃自己，只要她愿意，他可以放弃所拥有的一切，可是结果呢？谁又能料到，他毁了，她成功了，却没有一个人心安理得。\n\n夏天因为他们变得那么美，又因为他们变得那么悲。谁也不想，谁也不愿意去想，回忆总是短暂的，记忆却是痛的。\n\n最难熬的便是这痛苦的时段，或许每一次闭上眼，泪水便在打转，熬过一夏晴天，幻化成瓢泼的雨季。让心在哭泣中成长，茁壮了，坚强了，淡忘了！\n\n曾经萌动在稚嫩青春里的唯一一朵留恋，在这一个夏末彻底的枯萎了。\n\n看着窗外尘埃落满天，依稀记得已错过的昨天。时间是贼偷走一切，捉住了那只蝉，原来并不代表留住了整个夏天，爱我的与我爱的，I Miss You，But I Missed You！\n\n----\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>","tags":["这夏未眠"],"categories":["夏未眠"]},{"title":"这夏未眠.简介","url":"/2016/09/16/夏未眠/这夏未眠-简介/","content":"书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。\n\n两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。\n\n<!--More-->\n\n在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。\n\n可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。\n\n后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。\n\n----\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>","tags":["这夏未眠"],"categories":["夏未眠"]},{"title":"这夏未眠.序","url":"/2016/09/15/夏未眠/这夏未眠-序/","content":"这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。\n\n<!--More-->\n\n那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？\n\n想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。\n\n在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。\n\n----\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>","tags":["这夏未眠"],"categories":["夏未眠"]},{"title":"搜索引擎：MapReduce实战----倒排索引","url":"/2015/07/28/Hadoop/搜索引擎：MapReduce实战-倒排索引/","content":"> 倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。\n\n<!--More-->\n\n# 1.倒排索引简介\n\n有两种不同的反向索引形式：\n\n一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。\n一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。\n后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。\n\n\n举例：\n以英文为例，下面是要被索引的文本：\n```\nT0 = \"it is what it is\"\nT1 = \"what is it\"\nT2 = \"it is a banana\"\n```\n我们就能得到下面的反向文件索引：\n```\n\"a\":      {2}\n\"banana\": {2}\n\"is\":     {0, 1, 2}\n\"it\":     {0, 1, 2}\n\"what\":   {0, 1}\n```\n检索的条件\"what\", \"is\" 和 \"it\" 将对应这个集合：{0,1}∩{0,1,2}∩{0,1,2}={0,1}。\n\n对相同的文字，我们得到后面这些完全反向索引，有文档数量和当前查询的单词结果组成的的成对数据。 同样，文档数量和当前查询的单词结果都从零开始。\n\n所以，\"banana\": {(2, 3)} 就是说 “banana”在第三个文档里 (T2)，而且在第三个文档的位置是第四个单词(地址为 3)。\n```\n\"a\":      {(2, 2)}\n\"banana\": {(2, 3)}\n\"is\":     {(0, 1), (0, 4), (1, 1), (2, 1)}\n\"it\":     {(0, 0), (0, 3), (1, 2), (2, 0)}\n\"what\":   {(0, 2), (1, 0)}\n```\n如果我们执行短语搜索\"what is it\" 我们得到这个短语的全部单词各自的结果所在文档为文档0和文档1。但是这个短语检索的连续的条件仅仅在文档1得到。\n\n# 2.分析和设计\n## 1）Map过程\n\n首先使用默认的TextInputFormat类对输入文件进行处理，得到文本中每行的偏移量及其内容，Map过程首先必须分析输入的<key, value>对，得到倒排索引中需要的三个信息：单词、文档URI和词频，如图所示：\n![此处输入图片的描述][1]\n\n存在两个问题，第一：<key, value>对只能有两个值，在不使用Hadoop自定义数据类型的情况下，需要根据情况将其中的两个值合并成一个值，作为value或key值；\n\n第二，通过一个Reduce过程无法同时完成词频统计和生成文档列表，所以必须增加一个Combine过程完成词频统计\n```\n\n\npublic static class Map extends Mapper<Object,Text,Text,Text>{\n        private Text keyInfo = new Text();\n        private Text valueInfo = new Text();\n        private FileSplit split;  //存储所在文件的路径\n        public void map(Object key,Text value,Context context) throws IOException,\n\nInterruptedException{\n            split = (FileSplit)context.getInputSplit();     //获取当前任务分割的单词所在的文件路径\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            while(itr.hasMoreTokens()){\n                keyInfo.set(itr.nextToken()+\"+\"+split.getPath().toString());   //keyvalue是由单词和URI组成的\n                valueInfo.set(\"1\");                         \n\n                         //value值设置成1\n                context.write(keyInfo,valueInfo);\n            }\n        }\n    }\n```\n\n## （2）Combine过程\n将key值相同的value值累加，得到一个单词在文档中的词频，如图\n\n![此处输入图片的描述][2]\n\n```\n \tpublic static class Combiner extends Reducer<Text,Text,Text,Text>{\n        private Text info = new Text();\n        public void reduce(Text key,Iterable<Text>values,Context context) throws \nIOException, InterruptedException{\n            int sum = 0;\n            for(Text value:values){\n                sum += Integer.parseInt(value.toString());\n            }\n//            int index = key.toString().indexOf(\"+\");\n//            info.set(key.toString().substring(index+1)+\":\"+sum);    \n//            key.set(key.toString().substring(0,index));\n            String record = key.toString();\n            String[] str = record.split(\"[+]\");\n            info.set(str[1]+\":\"+sum);    \n            key.set(str[0]);\n            context.write(key,info);\n        }\n    }\n```\n\n## （3）Reduce过程\n\n讲过上述两个过程后，Reduce过程只需将相同key值的value值组合成倒排索引文件所需的格式即可，剩下的事情就可以直接交给MapReduce框架进行处理了\n\n![此处输入图片的描述][3]\n\n```\n\npublic static class Reduce extends Reducer<Text,Text,Text,Text>{\n        private Text result = new Text();\n        public void reduce(Text key,Iterable<Text>values,Context context) throws \n\nIOException, InterruptedException{\n            String value =new String();\n            for(Text value1:values){\n                value += value1.toString()+\" ; \";\n            }\n            result.set(value);\n            context.write(key,result);\n        }\n    }\n```\n\n完整代码如下：\n```\n\npackage ReverseIndex;\nimport java.io.*;\nimport java.util.StringTokenizer;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class ReverseIndex {\n\n    public static class Map extends Mapper<Object,Text,Text,Text>{\n        private Text keyInfo = new Text();\n        private Text valueInfo = new Text();\n        private FileSplit split;  //存储所在文件的路径\n        public void map(Object key,Text value,Context context) throws IOException,\n\nInterruptedException{\n            split = (FileSplit)context.getInputSplit();     //获取当前任务分割的单词所在的文件路径\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            while(itr.hasMoreTokens()){\n                keyInfo.set(itr.nextToken()+\"+\"+split.getPath().toString());   //keyvalue是由单词和URI组成的\n                valueInfo.set(\"1\");                         \n\n                         //value值设置成1\n                context.write(keyInfo,valueInfo);\n            }\n        }\n    }\n    public static class Combiner extends Reducer<Text,Text,Text,Text>{\n        private Text info = new Text();\n        public void reduce(Text key,Iterable<Text>values,Context context) throws \n\nIOException, InterruptedException{\n            int sum = 0;\n            for(Text value:values){\n                sum += Integer.parseInt(value.toString());\n            }\n\n//下面三行注释和紧接着四行功能一样，只不过实现方法不一样罢了\n\n//            int index = key.toString().indexOf(\"+\");\n//            info.set(key.toString().substring(index+1)+\":\"+sum);    \n//            key.set(key.toString().substring(0,index));\n\n//对传进来的key进行拆分，以+为界\n\n            String record = key.toString();\n            String[] str = record.split(\"[+]\");\n            info.set(str[1]+\":\"+sum);    \n            key.set(str[0]);\n            context.write(key,info);\n        }\n    }\n    public static class Reduce extends Reducer<Text,Text,Text,Text>{\n        private Text result = new Text();\n        public void reduce(Text key,Iterable<Text>values,Context context) throws \n\nIOException, InterruptedException{\n            String value =new String();\n            for(Text value1:values){\n                value += value1.toString()+\" ; \";\n            }\n            result.set(value);\n            context.write(key,result);\n        }\n    }\n    public static void main(String[] args) throws IOException, ClassNotFoundException,\n\nInterruptedException {\n        // TODO Auto-generated method stub\n        Job job = new Job();\n        job.setJarByClass(ReverseIndex.class);\n        \n        job.setNumReduceTasks(1);  //设置reduce的任务数量为1，平常的小测试不需要开辟太多的reduce任务进程\n        \n        job.setMapperClass(Map.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(Text.class);\n        \n        job.setCombinerClass(Combiner.class);\n        \n        job.setReducerClass(Reduce.class);\n        \n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        \n        FileInputFormat.addInputPath(job, new Path(\"/thinkgamer/input\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"/thinkgamer/output\"));\n        \n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n}\n```\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150806153515556?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150728102018022?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [3]: http://img.blog.csdn.net/20150728102039550?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Hadoop"],"categories":["技术篇"]},{"title":"MapReducer中的多次归约处理","url":"/2015/07/28/Hadoop/MapReducer中的多次归约处理/","content":"\n我们知道，MapReduce是分为Mapper任务和Reducer任务，Mapper任务的输出，通过网络传输到Reducer任务端，作为输入。\n\n在Reducer任务中，通常做的事情是对数据进行归约处理。既然数据来源是Mapper任务的输出，那么是否可以在Mapper端对数据进行归约处理，业务逻辑与Reducer端做的完全相同。处理后的数据再传送到Reducer端，再做一次归约。这样的好处是减少了网络传输的数量。\n<!--More-->\n\n可能有人疑惑几个问题：\n\n为什么需要在Mapper端进行归约处理？\n\n为什么可以在Mapper端进行归约处理？\n\n既然在Mapper端可以进行归约处理，为什么在Reducer端还要处理？\n\n\n\n回答第一个问题：因为在Mapper进行归约后，数据量变小了，这样再通过网络传输时，传输时间就变短了，减少了整个作业的运行时间。\n\n回答第二个问题：因为Reducer端接收的数据就是来自于Mapper端。我们在Mapper进行归约处理，无非就是把归约操作提前到Mapper端做而已。\n\n回答第三个问题：因为Mapper端的数据仅仅是本节点处理的数据，而Reducer端处理的数据是来自于多个Mapper任务的输出。因此在Mapper不能归约的数据，在Reducer端有可能归约处理。\n\n在Mapper进行归约的类称为Combiner。那么，怎么写Combiner哪？非常简单，就是我们自定义的Reducer类。那么，怎么用哪？更简单，见如下代码\n\n\njob.setCombineClass(Mapper.class)\n\n要注意的是，Combiner只在Mapper任务所在的节点运行，不会跨Mapper任务运行。Reduce端接收所有Mapper端的输出来作为输入。虽然两边的归约类是同一个，但是执行的位置完全不一样。\n\n并不是所有的归约工作都可以使用Combiner来做。比如求平均值就不能使用Combiner。因为对于平均数的归约算法不能多次调用。\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Hadoop"],"categories":["技术篇"]},{"title":"hadoop命令——hdfs","url":"/2015/07/11/Hadoop/hadoop命令——hdfs/","content":"\nhdfs是hadoop大体系下的分布式文件管理系统，是英文Hadoop Distributed File System的简写，其常用命令如下：\n\n<!--More-->\n\n# 一：fs命令（和Linux终端运行命令一致，也是hdfs最常用命令）\n\n![此处输入图片的描述][1]\n\n![此处输入图片的描述][2]\n\n# 二：其他相关命令\n\n1、hadoop 归档文件shell： hadoop archive -archiveName file.har -p /gyt/input /gyt/output (file.har为归档后的文件  /gyt/inut/为多个文件所在目录   /gyt/output/是归档后的输出目录)\n\n2、运行JAR程序包shell：hadoop jar /home/hadoop/hadoop-1.1.2/hadoop-examples-1.1.2.jar wordcount  /user/hadoop/input output（XXX.jar是程序目录，wordcount是程序入口，XXX/input是文件输入源，output是文件输出源）\n\n4、查看HDFS状态：hadoop dfsadmin -report比如有哪些datanode，每个datanode的情况\n\n5、离开安全模式：hadoop dfsadmin -safemode leave \n\n6、进入安全模式： hadoop dfsadmin -safemode enter\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n\n  [1]: http://img.blog.csdn.net/20150715110547708?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150715110719331?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Hadoop"],"categories":["技术篇"]},{"title":"Virtualbox虚拟Ubuntu系统与主机互ping","url":"/2015/07/08/Linux/虚拟Ubuntu系统与主机互ping/","content":"\n互ping的前提是主机和虚拟机的ip地址在同一波段【eg:主机为：192.168.1.10虚拟Linux：192.168.1.11】\n\n<!--More-->\n\n# 1、设置主机ip\n打开网络共享中心->更改适配器设置->以太网，修改其ip\n\n![此处输入图片的描述][1]\n\n在主机上运行CMD输入ipconfig显示如下\n\n![此处输入图片的描述][2]\n\n# 2、设置虚拟机ip\n打开终端以root身份运行\n执行 sudo gedit /etc/hosts   修改如下\n\n![此处输入图片的描述][3]\n\n执行 sudo gedit /etc/network/interfaces  修改如下\n\n![此处输入图片的描述][4]\n\n执行 sudo gedit /etc/resolv.conf  修改如下\n\n![此处输入图片的描述][5]\n\nnameserver后边紧跟的是主机的DNS，不同机器对应不同\n\n重启网络服务\n\nsudo /etc/init.d/networking restart\n\n关闭防火墙\n\nsudo ufw disable\n# 3、虚拟机网络连接设置\n\n小编亲测，若虚拟机网络格式设置错误的话是不会ping的通的，因此应该格外注意\n\n若主机连的是WiFi，则虚拟机设置->网络->启用网络连接（桥接网卡，Realtek  ......   Wireless Lan .......）\n\n若主机连的是有线网，则虚拟机设置->网络->启用网络连接（桥接网卡，Realtek  PCIe GBE Family ........）\n\n至此，已全部设置成功，主机与虚拟机之间便可以互ping了\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150708225332168?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150708225434716?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [3]: http://img.blog.csdn.net/20150708230418229?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [4]: http://img.blog.csdn.net/20150708230424484?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [5]: http://img.blog.csdn.net/20150708232308064?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Linux"],"categories":["技术篇"]},{"title":"linux-ifconfig命令配置ip地址","url":"/2015/07/08/Linux/linux-ifconfig命令配置ip地址/","content":"\nLinux下网卡命名规律：eth0，eth1。第一块以太网卡，第二块。\nlo为环回接口，它的IP地址固定为127.0.0.1，掩码8位。它代表你的机器本身。 \nifconfig 是查看网卡的信息 ，如果不加参数查看的是所有的网卡信息 \n<!--More-->\n\n![此处输入图片的描述][1]\n\n加上参数eth0的话是查看eth0网卡的信息 \n\n![此处输入图片的描述][2]\n\n网卡信息的一些解释： \n第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址） \n \n第二三行：网卡的IP地址、子网、掩码 \n \n第四行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）\nMTU:1500（最大传输单元）：1500字节   www.2cto.com  \n \n下面就是接收、发送数据包情况统计和发送接受数据字节数的统计信息。 \n \n配置网卡的IP地址 \n \nifconfig eth0 192.168.168.64 netmask 255.255.255.0  \n \n在eth0上配置上192.168.168.64 的IP地址及子网掩码。 \n \n配置网卡的硬件地址 \nifconfig eth0 hw ether xx：xx：xx：xx：xx：xx  \n \n禁用网卡eth0  \nifconfig eth0 down  \n \n启用网卡eth0   \nifconfig eth0 up  \n \n另外启动和禁用网卡还可以使用ifup | ifdown  eth0\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150708123431916?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150708123456712?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Linux"],"categories":["技术篇"]},{"title":"二、hadoop伪分布搭建","url":"/2015/07/07/Hadoop/二、hadoop伪分布搭建/","content":"本文主要介绍Hadoop的伪分布安装，基于Ubuntu14.04进行。\n\n<!--More-->\n\n# 环境                                              \n\n虚拟机：VirtualBox\nUbuntu:14.04\nhadoop:2.6\n\n# 安装                                             \n\n## 1、创建hadoop用户\n\nsudo useradd -m hadoop -s/bin/bash\n【Ubuntu终端复制粘贴快捷键】\n【在Ubuntu终端窗口中，复制粘贴的快捷键需要加上shift，即粘贴是 ctrl+shift+v。】\n使用如下命令修改密码，按提示输入两次密码 hadoop :\n\n\n```\nsudo passwd hadoop\n```\n可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题：\n```\nsudo adduser hadoop sudo\n```\n## 2、切换到hadoop用户下\n```\nsu hadoop\n```\n## 3、安装SSH server、配置SSH无密码登陆\n\n集群、单节点模式都需要用到SSH登陆（类似于远程登陆，你可以登录某台Linux电脑，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：\n```\nsudo apt-get install openssh-server\n```\n安装后，可以使用如下命令登陆本机：\n```\nssh localhost\n```\n此时会有提示(SSH首次登陆提示)，输入 yes 。然后按提示输入密码hadoop，这样就登陆到本机了。\n\n但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。\n\n首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：\n```\nexit                                   # 退出刚才的 ssh localhost\ncd ~/.ssh/                             # 若没有该目录，请先执行一次ssh localhost\nssh-keygen -t rsa                      # 会有提示，都按回车就可以\ncat id_rsa.pub >> authorized_keys     # 加入授权\n```\n此时再用 ssh localhost 命令，无需输入密码就可以直接登陆此时再用 ssh localhost 命令，无需输入密码就可以直接登陆\n\n![此处输入图片的描述][1]\n\n## 4、安装Java环境\nJava环境可选择 Oracle 的 JDK，或是 OpenJDK，按http://wiki.apache.org/hadoop/HadoopJavaVersions中说的，新版本在 OpenJDK 1.7 下是没问题的。为图方便，这边直接通过命令安装 OpenJDK 7。\n```\nsudo apt-get install openjdk-7-jre openjdk-7-jdk\n```\nOpenJDK 默认的安装位置为: /usr/lib/jvm/java-7-openjdk-amd64 (32位系统则是 /usr/lib/jvm/java-7-openjdk-i86 ，可通过命令dpkg -L openjdk-7-jdk查看到)。安装完后就可以使用了，可以用java -version 检查一下。\n\n接着需要配置一下 JAVA_HOME 环境变量，为方便，我们在 ~/.bashrc 中进行设置（扩展阅读:设置Linux环境变量的方法和区别）：\n```\nvi ~/.bashrc\n```\n在文件最前面添加如下单独一行（注意 = 号前后不能有空格），并保存：\n```\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n```\n如下图所示（该文件原本可能不存在，内容为空，这不影响）：\n\n![此处输入图片的描述][2]\n\n配置JAVA_HOME变量\n\n接着还需要让该环境变量生效，执行如下代码：\n```\nsource ~/.bashrc# 使变量设置生效\necho $JAVA_HOME# 检验是否设置正确\n```\n设置正确的话，会输出如下结果：\n\n![此处输入图片的描述][3]\n\n成功配置JAVA_HOME变量\n\n## 5、安装hadoop\n进入hadoop所在的目录将其解压到/usr/local/hadoop\n```\nsudo tar -zxvf ./hadoop-2.6.0.tar.gz -C /usr/local  # 解压到/usr/local中\n\ncd /usr/local/\nsudo mv ./hadoop-2.6.0/ ./hadoop            # 将文件夹名改为hadoop\nsudo chown -R hadoop:hadoop ./hadoop        # 修改文件权限\n```\nHadoop解压后即可使用。输入如下命令来检查 Hadoop \n是否可用，成功则会显示命令用法：\n```\ncd ./hadoop\n./bin/hadoop\n```\n\n## 6、hadoop伪分布配置\n\nHadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件core-site.xml 和hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。\n\n修改配置文件 core-site.xml (vim /usr/local/hadoop/etc/hadoop/core-site.xml)，将当中的\n```\n<configuration>\n</configuration>\n```\n修改为下面配置：\n```\n<configuration>\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>file:/usr/local/hadoop/tmp</value>\n        <description>Abase for other temporary directories.</description>\n    </property>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost:9000</value>\n    </property>\n</configuration>\n```\n同样的，修改配置文件 hdfs-site.xml：\n```\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n    <property>\n        <name>dfs.namenode.name.dir</name>\n        <value>file:/usr/local/hadoop/tmp/dfs/name</value>\n    </property>\n    <property>\n        <name>dfs.datanode.data.dir</name>\n        <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n    </property>\n</configuration>\n```\n修改配置文件yarn-site.xml\n```\n<configuration> \n     <property>  \n        <name>mapreduce.framework.name</name>  \n        <value>yarn</value> \n    </property>  \n    <property> \n        <name>yarn.nodemanager.aux-services</name>  \n        <value>mapreduce_shuffle</value>  \n   </property> \n</configuration> \n```\n配置完成后，执行 namenode 的格式\n```\nbin/hdfs namenode -format\n```\n成功的话，会看到successfully formatted 的提示，且倒数第5行的提示如下，Exitting with status 0 表示成功，若为Exitting with status 1 则是出错\n\n接着开启如下进程\n```\nsbin/start-dfs.sh\nsbin/start-yarn.sh\n```\n至此，所有的已经安装完事，且所有服务都已经启动\n\n# 验证\nhttp://127.0.0.1:8088\n\n![此处输入图片的描述][4]\n\nhttp://localhost:50070\n\n![此处输入图片的描述][5]\n![此处输入图片的描述][6]\n\n# 提示                           \n每次进入虚拟机系统时必须先进入hadoop用户下（su hadoop），才能开启服务，否则会报错\n参考文章：www.powerxing.com/install-hadoop/\n\nQQ交流：1923361654\n\nhadoop完全分布式部署参考：http://blog.csdn.net/gamer_gyt/article/details/51991893\n\nhadoop单机版部署参考：http://blog.csdn.net/gamer_gyt/article/details/46545303\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150707210848767?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150707210939550?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [3]: http://img.blog.csdn.net/20150707211039921?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [4]: http://img.blog.csdn.net/20150707220733097?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [5]: http://img.blog.csdn.net/20150708115937081?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [6]: http://img.blog.csdn.net/20150707220808406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Hadoop"],"categories":["技术篇"]},{"title":"MySQLdb的安装与使用","url":"/2015/07/05/Python/MySQLdb的安装与使用/","content":"\n安装已编译版本(此方法简便快捷): http://www.codegood.com/downloads\n\n根据自己系统下载，双击安装，搞定，然后import MySQLdb，查看是否成功\n<!--More-->\n\n# 安装\n \n我的，win7,32位，2.7版本\n\nMySQL-python-1.2.3.win-amd32-py2.7.exe\n\n# 使用\n```\n#!/usr/bin/python\n# encoding: utf-8\nimport time,MySQLdb\n# 打开数据库连接\ndb = MySQLdb.connect(\"localhost\",\"root\",\"root\",\"Python\" )\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n\n#删除表\nsql = \"drop table if exists thinkgamer\"\ncursor.execute(sql)\n\n#创建\nsql = \"create table if not exists thinkgamer(name varchar(128) primary key,created int(10))\"\ncursor.execute(sql)\n\n#写入\nsql = \"insert into thinkgamer(name,created) values(%s,%s)\"\nparam = (\"aaa\",int(time.time()))\nn = cursor.execute(sql,param)\nprint 'insert',n\n\n#写入多行\nsql = \"insert into thinkgamer(name,created) values(%s,%s)\"\nparam = ((\"bbb\",int(time.time())),(\"ccc\",33),(\"ddd\",44))\nn = cursor.executemany(sql,param)\nprint \"insertmany\",n\n\n#更新\nsql= \"update thinkgamer set name=%s where name='aaa'\"\nparam = (\"zzz\")\nn = cursor.execute(sql,param)\nprint \"updata\",n\n\n#查询\nn = cursor.execute(\"select * from thinkgamer\")\nfor row in cursor.fetchall():\n    print row\n    for r in row:\n        print r\n\n#删除\nsql = \"delete from thinkgamer where name =%s\"\nparam = (\"bbb\")\nn = cursor.execute(sql,param)\nprint \"delete\",n\n\n#查询\nn = cursor.execute(\"select * from thinkgamer\")\nprint cursor.fetchall()\n\ncursor.close()\n\n#提交\ndb.commit()\n#关闭\ndb.close()\n\n输出结果：\n\ninsert 1\ninsertmany 3\nupdata 1\n('zzz', 1436067892L)\nzzz\n1436067892\n('bbb', 1436067892L)\nbbb\n1436067892\n('ccc', 33L)\nccc\n33\n('ddd', 44L)\nddd\n44\ndelete 1\n(('zzz', 1436067892L), ('ccc', 33L), ('ddd', 44L))\n\n```\n\n更多详情请戳：[MySQLdb User's Guide][1]\n\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://mysql-python.sourceforge.net/MySQLdb.html","tags":["Python"],"categories":["技术篇"]},{"title":"VirtualBox共享文件夹设置及开机自动挂载","url":"/2015/06/28/Linux/VirtualBox共享文件夹设置及开机自动挂载/","content":"\n首先声明：本人的Vbox虚拟机里装的是Ubuntu，本机是windows\n<!--More-->\n## 1、用VirtualBox虚拟机的共享文件夹设置共享的本地文件（我的是设置的是本地E盘，java文件夹）\n\n![此处输入图片的描述][1]\n\n## 2、进入虚拟机Ubuntu系统，打开终端，用root用户操作（sudo -s回车输入密码）\n\n首先在虚拟机上创建一个共享目录         eg:mkdir /mnt/share\n\n实现挂载       mount -t vboxsf java /mnt/share    （java为本机windows上设置的共享文件夹） \n\n再次进入  /mnt/share  目录下就可以看到windows下java内的文件了\n\n\n## 3、实现开机自动挂载功能\n\nPS：网上查到了资料基本都是说在/etc/fstab 文件末添加一项\n\n /etc/fstab 文件末添加一项    \n```\nsharing /mnt/share vboxsf defaults 0 0   (或者sharing /mnt/share vboxsf rw,gid=100,uid=1000,auto 0 0）\n```\n注意！！！\n但我试了N遍，证明这是不好使的。\n\n\n\n正确的解决办法是：\n\n在文件 /etc/rc.local 中（用root用户）追加如下命令\n```\nmount -t vboxsf java /mnt/share\n```\n电脑关机在开机就好使了（小编亲测）\n\n 另外，在 VirtualBox 4.x 版本中，已有一个“自动挂载”功能，如下图所示：\n\n![此处输入图片的描述][2]\n\n再使用命令 mount实现挂载\n它自动把这些共享文件夹挂载到 /media/ ，目录下了，我想要说的是，这个挂载目录不是我想要的，\n所以我没采用VirtualBox的自动挂载功能（因为挂载目录自己不可控）。\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150628232546631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150628232610414?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Linux"],"categories":["技术篇"]},{"title":"VirtualBox导入XXXX.vdi时报错","url":"/2015/06/27/Linux/VirtualBox导入XXXX-vdi时报错/","content":"\nvirtualbox导入vdi文件时出现下面的问题：\n\n<!--More-->\n\n![此处输入图片的描述][1]\n\n解决方法：\n\nwindows+R，输入cmd,进入virtualbox的安装目录（或者在硬盘中直接进入virtualbox的安装目录，在任务栏里输入cmd），输入VBoxManage  internalcommands setvdiuuid D:\\path\\ubuntu.vdi\n\n 注意，在virtualbox4.0.4以上该命令改为\n\nVBoxManage internalcommands sethduuid D:\\path\\ubuntu.vdi\n\n然后重新导入即可。\n\n这是一个重新设置UUID（通用唯一识别码）号的命令，VirtualBox没有集成到GUI环境中，只能在命令行中使用。\n\n\n什么是UUID？    请点击UUID[请点击UUID][2]\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150627102144937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: https://baike.baidu.com/item/UUID?fr=aladdinfill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Linux"],"categories":["技术篇"]},{"title":"新浪明星日志热门推荐（java实现）","url":"/2015/06/19/Java/新浪明星日志热门推荐（java实现）/","content":"\n现在进行推荐的三步如下：\n\n<!--More-->\n\n# 1:利用数据的格式如下\n\n![此处输入图片的描述][1]\n\n# 2：编程语言采用的是Java，源代码如下：\n```\npackage top10;\n\nimport java.util.*;\nimport java.io.*;\n\npublic class top {\n\n    public static void top(String []one,String []two,String []three){\n        int []one1 = new int[15688];\n        for(int i =0;i<15687;i++)\n            {\n            one1[i]=0;\n            for(int j=i;j<15688;j++)\n                if(one[j]==one[i])\n                {\n                    one1[i]++;\n                }\n            }\n        for(int i =0;i<15688;i++)\n            for(int j=i;j<15688;j++)\n                if(one1[i]>one1[j])\n                {\n                    String temp1;\n                    temp1=one[i];\n                    one[i]=one[j];\n                    one[j]=temp1;\n                    \n                    String temp2;\n                    temp2=two[i];\n                    two[i]=two[j];\n                    two[j]=temp2;\n                    \n                    String temp3;\n                    temp3=three[i];\n                    three[i]=three[j];\n                    three[j]=temp3;\n                }\n        System.out.println(\"Top前十的结果为：\");\n        System.out.println(\"姓名\\t\"+\"\\t推荐博客地址\"+\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\"+\"推荐作者博客首地址\");\n        for(int k=1;k<=10;k++)\n        {\n            String str;\n                str = String.format(\"%s\\t\\t%s\\t\\t%s\", one[k],two[k],three[k]);\n            System.out.println(str);\n        }\n            \n    }\n    \n    public static void list() throws IOException{\n        FileReader in = new FileReader(\"title.txt\");\n        BufferedReader br = new BufferedReader(in);\n        String s1 = null;\n        String []one = new String[15688];\n        String []two = new String[15688];\n        String []three = new String[15688];\n        int i = 0,k=0,m=0,j = 1;\n        while((s1 = br.readLine()) != null) {\n            if(j%3==2)\n            {\n                two[k] = s1;\n                k++;\n            }\n            else if(j%3==0)\n            {\n                three[m] = s1;\n                m++;\n            }\n            else\n                {\n                one[i] = s1;        \n                i++;\n                }\n            j++;\n            }\n        br.close();\n        in.close();\n//        System.out.println(j);\n        top(one,two,three);\n    }\n    \n    public static void main(String[] args) throws IOException {\n        // TODO Auto-generated method stub\n        \n        System.out.println(\"+++++++++++++++++|||||||||||||||++++++++++++++++++\");\n        System.out.println(\"**************欢迎使用新浪明星博客推荐系统          ***************\");\n        System.out.println(\"**************    1、使用推荐功能                        ***************\");\n        System.out.println(\"**************    2、退出此系统,谢谢使用       ***************\");\n        System.out.println(\"+++++++++++++++++|||||||||||||||++++++++++++++++++\");\n        Scanner in = new Scanner(System.in);\n        while(true)\n        {\n            int i = in.nextInt();\n            {\n                switch(i)\n                {\n                case 1: list();break;\n                case 2:System.out.println(\"谢谢使用！！！\");break;\n                default:System.out.println(\"请重新输入！！！\");\n                }\n            }\n        }\n        \n    }\n}\n```\n# 3：运行的结果如图\n![此处输入图片的描述][2]\n\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://img.blog.csdn.net/20150628233105944?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center\n  [2]: http://img.blog.csdn.net/20150628233009834?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center","tags":["Java"],"categories":["技术篇"]},{"title":"一、Hadoop2.6.0单机模式配置","url":"/2015/06/18/Hadoop/一、Hadoop2-6-0单机模式配置/","content":"\n增加hadoop用户组，同时在该组里增加hadoop用户，后续在涉及到hadoop操作时，我们使用该用户。\n\n<!--More-->\n\n# 一、在Ubuntu下创建hadoop组和hadoop用户\n\n\n## 1、创建hadoop用户组\n![此处输入图片的描述][1]\n\n## 2、创建hadoop用户\n```\nsudo adduser -ingroup hadoop hadoop\n```\n\n回车后会提示输入新的UNIX密码，这是新建用户hadoop的密码，输入回车即可。\n\n如果不输入密码，回车后会重新提示输入密码，即密码不能为空。\n\n最后确认信息是否正确，如果没问题，输入 Y，回车即可。\n\n![此处输入图片的描述][2]\n##  3、为hadoop用户添加权限\n\n输入：sudo gedit /etc/sudoers\n回车，打开sudoers文件\n给hadoop用户赋予和root用户同样的权限\n\n![此处输入图片的描述][3]\n![此处输入图片的描述][4]\n\n# 二、用新增加的hadoop用户登录Ubuntu系统\n\n# 三、安装ssh\n```\nsudo apt-get install openssh-server\n```\n![此处输入图片的描述][5]\n\n安装完成后，启动服务\n```\nsudo /etc/init.d/ssh start\n```\n查看服务是否正确启动\n```\nps -e | grep ssh\n```\n![此处输入图片的描述][6]\n\n设置免密码登录，生成私钥和公钥\n```\nssh-keygen -t rsa -P \"\"\n```\n此时会在／home／hadoop/.ssh下生成两个文件：id_rsa和id_rsa.pub，前者为私钥，后者为公钥。\n\n下面我们将公钥追加到authorized_keys中，它用户保存所有允许以当前用户身份登录到ssh客户端用户的公钥内容。\n```\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n```\n![此处输入图片的描述][7]\n登录ssh\n```\nssh localhost\n```\n退出\n```\nexit\n```\n# 四、安装Java环境\n```\nsudo apt-get install openjdk-7-jdk\n```\n![此处输入图片的描述][8]\n\n查看安装结果，输入命令：java -version，结果如下表示安装成功。\n\n![此处输入图片的描述][9]\n\n# 五、安装hadoop2.4.0\n## 1、官网下载http://mirror.bit.edu.cn/apache/hadoop/common/\n## 2、安装\n解压\n```\nsudo tar xzf hadoop-2.4.0.tar.gz        \n```\n假如我们要把hadoop安装到/usr/local下\n拷贝到/usr/local/下，文件夹为hadoop\n```\nsudo mv hadoop-2.4.0 /usr/local/hadoop    \n```\n![此处输入图片的描述][10]\n\n赋予用户对该文件夹的读写权限\n```\nsudo chmod 774 /usr/local/hadoop\n```\n![此处输入图片的描述][11]\n\n## 3、配置\n### 1）配置~/.bashrc\n\n配置该文件前需要知道Java的安装路径，用来设置JAVA_HOME环境变量，可以使用下面命令行查看安装路径\n```\nupdate-alternatives - -config java\n```\n执行结果如下：\n\n![此处输入图片的描述][12]\n\n完整的路径为\n\n```\n/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n```\n\n我们只取前面的部分 /usr/lib/jvm/java-7-openjdk-amd64\n配置.bashrc文件\n```\nsudo gedit ~/.bashrc\n```\n该命令会打开该文件的编辑窗口，在文件末尾追加下面内容，然后保存，关闭编辑窗口。\n```\n#HADOOP VARIABLES START\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport HADOOP_INSTALL=/usr/local/hadoop\nexport PATH=$PATH:$HADOOP_INSTALL/bin\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\nexport YARN_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_INSTALL/lib\"\n#HADOOP VARIABLES END\n```\n最终结果如下图：\n\n![此处输入图片的描述][13]\n\n执行下面命，使添加的环境变量生效：\n```\nsource ~/.bashrc\n```\n\n### 2）编辑/usr/local/hadoop/etc/hadoop/hadoop-env.sh\n执行下面命令，打开该文件的编辑窗口\n```\nsudo gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n```\n找到JAVA_HOME变量，修改此变量如下\n```\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64    \n```\n修改hadoop-env.sh文件\n\n# 六、WordCount测试\n \n\n单机模式安装完成，下面通过执行hadoop自带实例WordCount验证是否安装成功：\n\n在/usr/local/hadoop路径下创建input文件夹：   \n```\nmkdir input\n(或 sudo mkdir /usr/local/hadoop/input)\n```\n拷贝README.txt到input：   \n```\ncp README.txt input\n```\n执行WordCount：\n```\nbin/hadoop jarshare/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.4.0-sources.jarorg.apache.hadoop.examples.WordCount input output\n```\n\n执行\n```\ncat output/*\n```\n查看字符统计结果\n\n至此，单机模式安装成功！\n\n\nhadoop伪分布部署参考：[点击打开链接][14]\n\nhadoop完全分布式部署参考：[点击打开链接][15]\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n\n  [1]: http://images.cnitblog.com/blog/12097/201406/181325537236804.png\n  [2]: http://images.cnitblog.com/blog/12097/201406/181325547071504.png\n  [3]: http://images.cnitblog.com/blog/12097/201406/181325553952348.png\n  [4]: http://images.cnitblog.com/blog/12097/201406/181325561927991.png\n  [5]: http://images.cnitblog.com/blog/12097/201406/181325569737863.png\n  [6]: http://images.cnitblog.com/blog/12097/201406/181325580364821.png\n  [7]: http://images.cnitblog.com/blog/12097/201406/181325593481023.png\n  [8]: http://images.cnitblog.com/blog/12097/201406/181326008641052.png\n  [9]: http://images.cnitblog.com/blog/12097/201406/181326019736724.png\n  [10]: http://images.cnitblog.com/blog/12097/201406/181326023644310.png\n  [11]: http://images.cnitblog.com/blog/12097/201406/181326027238353.png\n  [12]: http://images.cnitblog.com/blog/12097/201406/181326030041140.png\n  [13]: http://images.cnitblog.com/blog/12097/201406/181326035825782.png\n  [14]: http://blog.csdn.net/gamer_gyt/article/details/46793731\n  [15]: http://blog.csdn.net/gamer_gyt/article/details/51991893\n","tags":["Hadoop"],"categories":["技术篇"]},{"title":"Ubuntu下终端闪退","url":"/2015/03/22/Linux/Ubuntu下终端闪退/","content":"\n\n执行命令\n```\nsudo ls -al / | grep tmp\n```\n<!--More-->\n\n继续执行命令：\n```\nsudo chmod 1777 /temp\n```\n即可\n\n\n附：关于tmp的相关说明：\n\nhttp://www.ubuntu-tw.org/modules/newbb/viewtopic.php?viewmode=compact&topic_id=11904&forum=2\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Linux"],"categories":["技术篇"]},{"title":"Java文件的写入与读出","url":"/2015/03/17/Java/Java文件的写入与读出/","content":"\n> 由于要将爬虫的结果写到文件里，就自己晚上搜了一点资料，看了别人的博客，补充了一点文件的基础知识，现将其整理如下，供大家参考 Java文件的写入和读出有很多种方法我所介绍的主要是Read/Writer，OutputStream/InputStream\n\n<!--More-->\n\n# 一：Read/Writer\n由于Java本身可以导入许多包，在这里可以直接调用Java的io，语句是 import java.io；\n本人是将FileWriter和FileRead理解为一个类，分别定义了两个对象，FileWriter gyt = new FileWriter(\"Thinkgamer.txt\");\nFileReader out = new FileReader(\"Thinkgamer.txt\");\n使用gyt.write(str,int,int)将其写入到.txt格式的文件里，再用ch = out.read()将其输出。\n其完整代码如下\n```\npackage Thinkgamer;\n\nimport java.io.*;\n\npublic class cyan {\n\n    public static void main(String[] args) {\n        // TODO Auto-generated method stub\n        \n        String str = \"Thinkgamer QQ is 1923361654\";\n        try{\n            FileWriter gyt = new FileWriter(\"Thinkgamer.txt\");\n            gyt.write(str,0,str.length());\n            gyt.flush();\n            \n            \n            FileReader out = new FileReader(\"Thinkgamer.txt\");\n            int ch = 0;\n            while((ch = out.read())!=-1){\n            System.out.print((char)ch);\n            }\n        }\n        catch(Exception as){\n            as.printStackTrace();\n        }\n    }\n}\n```\n# 二：OutputStream/InputStream\n```\npackage Thinkgamer;\n\nimport java.io.*;\n\npublic class cyan {\n\n   public static void main(String[] args) {\n\n        String str = \"Thinkgamer QQ is 1923361654\";\n        try{\n        //    \n            OutputStream gyt = new FileOutputStream(\"Thinkgamer.txt\");\n            //    \n            OutputStreamWriter out = new OutputStreamWriter(gyt);\n     \n            OutputStreamWriter out = new OutputStreamWriter(new FileOutputStream(\"Thinkgamer.txt\"));\n     \n            FileInputStream in = new FileInputStream(\"Thinkgamer.txt\");\n     \n            out.write(str,0,str.length());\n            out.flush();\n     \n            for(int i =0;i < str.length();i++)\n            {\n                System.out.print((char)in.read());\n            }\n      }\n        catch(Exception ex){\n            ex.printStackTrace();\n        }\n    }\n }\n```\n\n特别注意：\n//在使用文件之后注意将文件关闭\n//关闭的语句是gyt.flush();\n//gyt为Read/Writer或OutputStream/InputStream所创建的一个对象\nOutputStream/InputStream的理解和Read/Writer方法差不多，在这里小编只将本人写的代码贴出来\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Java"],"categories":["技术篇"]},{"title":"Python中try...except...finally的理解","url":"/2015/01/05/Python/Python中try-except-finally的理解/","content":"\n首先我们打开一个不存在的文件：\n\n```\nfp = open(\"null.txt\",\"r\")  \n```\n然后提示报错如下：\n\n<!--More-->\n\n```\nIOErro Traceback (most recent call last)  \n<ipython-input-3-f70973547c7e> in <module>()  \n----> 1 fp = open(\"null.txt\",\"r\")  \n  \nIOError: [Errno 2] No such file or directory: 'null.txt'  \n```\n\n接着我们使用try...except...来执行这条语句\n\n```\nIn [4]: try:  \n   ...:     fp = open(\"null.txt\",\"r\")  \n   ...: except:  \n   ...:     print \"open error\"  \n   ...:       \nopen error  \n```\n\n\n接着我们再在后面加一条finally语句，所谓的filally就是最后要执行的，它不管你前边是否发送错误\n```\nIn [4]: try:  \n   ...:     fp =open('null.txt','r')  \n   ...: except:  \n   ...:     print 'opoen error'  \n   ...: finally:  \n   ...:     print 'end'  \n   ...:       \nopoen error  \nend  \n```\n    \n接着我们使用except打印出错误类型，然后观察错误的type\n\n```\nIn [5]: try:  \n   ...:     fp = open('null.txt','r')  \n   ...: except Exception,e:  \n   ...:     print e  \n   ...:     print type(e)  \n   ...:       \n[Errno 2] No such file or directory: 'null.txt'  \n<type 'exceptions.IOError'>  \n```\n我们可以看到except把错误捕捉到赋值给e，然后将其打印出，可以看到为IOError，所以在这里我们可以修改上边为：\n```\nIn [9]: try:  \n   ...:     fp = open('null.txt', 'r')  \n   ...: except IOError,e:  \n   ...:     print \"ioerror\"  \n   ...:       \nioerror  \n```\n当然这里如果不是IOError的话，上边except语句是捕捉不到的，例如：\n\n```\nIn [10]: try:  \n   ....:     a = 10/0  \n   ....: except IOError,e:  \n   ....:     print \"ioerror\"  \n   ....:       \n---------------------------------------------------------------------------  \nZeroDivisionError                         Traceback (most recent call last)  \n<ipython-input-10-e17eff88239d> in <module>()  \n      1 try:  \n----> 2     a = 10/0  \n      3 except IOError,e:  \n      4     print \"ioerror\"  \n      5   \n  \nZeroDivisionError: integer division or modulo by zero  \n```\n这时我们需要另外的错误类型进行捕获，例如：\n```\nIn [11]: try:  \n   ....:     a = 10/0  \n   ....: except IOError,e:  \n   ....:     print \"IOError\"  \n   ....: except ZeroDivisionError,e:  \n   ....:     print \"Zero Error\"  \n   ....:       \nZero Error  \n```\n可以看到第一个except并没有捕捉到错误，所以传给下一个except，当然如果所有的except都没有捕获到的话，程序就会抛出异常\n在这里我们不得不注意的是，python的错误类型其实都是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如：\n```\nIn [14]: try:  \n   ....:     a = 10 /0  \n   ....: except StandardError,e:  \n   ....:     print 'standardError'  \n   ....: except ZeroDivisionError,e:  \n   ....:     print 'zero error'  \n   ....:       \nstandardError  \n```\n第二个except永远也捕获不到ValueError，因为ZeroDivisionError是StandardError的子类，如果有，也被第一个except给捕获了。\n使用try...except...finally的另外一个好处是，可以跨越多层调用，例如：\n\n```\nIn [15]: def foo(m):  \n   ....:     return 10/int(m)  \n   ....:   \n  \nIn [16]: def goo():  \n   ....:     a = \"0\"  \n   ....:     foo(a)  \n   ....:       \n  \nIn [17]: def main():  \n   ....:     goo()  \n   ....:       \n  \nIn [18]: try:  \n   ....:     main()  \n   ....: except Exception,e:  \n   ....:     print e  \n   ....:       \ninteger division or modulo by zero  \n```\n在foo()函数中抛出的错误，然而在goo和main函数中都没有进行捕捉，在执行main函数时进行捕获，也可以捕获到\n\n除了try...except...finally之外还有python的logging模块，也可以进行错误调试，当然我们也可以使用raise抛出异常\n\n----\n\n<center>\n【技术服务】，详情点击查看：\n<a href=\"https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg\">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a>\n</center>\n\n----\n\n<center>\n<img src=\"https://img-blog.csdnimg.cn/20191108184219834.jpeg\">\n<br>\n扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！\n</center>\n\n\n----\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>\n\n<center><img src=\"https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70\" width=\"80%\"></center>","tags":["Python"],"categories":["技术篇"]},{"title":"逝去的夏天","url":"/2013/10/01/随手记/逝去的夏天/","content":"\n<center>\n那年的夏天，我们拎起书包 \n<br>\n无奈的走向，复习班的怀抱 \n<br>\n幻想一年后，大学生活的逍遥 \n<br>\n寂寥与苦恼，我们一起去熬 \n<br>\n\n<!--More-->\n\n<br><br>\n月光下，漫步操场，有人和心爱姑娘倾诉着衷肠 \n<br>\n听着歌，踏进流年，哼出那动人的歌谣在嘴边 \n<br>\n到夜晚，傻傻发呆，等待她回复美丽的笑颜\n<br>\n被窝里，谁流着泪，望着椭圆和磁场，画出无尽的茫然 \n<br><br>\n表示从下课到上课的铃声原来一直没变 \n<br>\n表示老班面庞带着囧样长的多荒唐 \n<br>\n看着钟摆晃荡  时光飞扬   还有可爱姑娘 \n<br>\n看着曾经传出的纸条 跨过几人几肩膀 \n<br><br>\n我们追逐梦想，奔向希望，一路走来跌跌又撞撞 \n<br>\n留下汗水 擦干眼泪向前一起闯 \n<br>\n考试和爱情的守望多么令人向往 \n<br>\n各种壮丽诗行和篇章从此不会再彷徨 \n<br><br>\n多年后，你若想念，我们一直都坚持的笑颜 \n<br>\n多年后，你若怀念，课堂上睡觉的瓜子脸 \n<br>\n都说吃他的菜喝他的汤钱照样会花光 \n<br>\n都说拍她的肩请她吃糖一笑为红颜 \n<br>\n都说一分一秒努力学习或者传纸条 \n<br>\n都说会把功劳全都记在流年的怀抱 \n<br>\n<br>\n\n我们即将高考，分别之后，奔波在中国不同角落 \n<br>\n瞥见短发女孩，还会以为是我心动那个谁 \n<br>\n蕾峰彩月金豆同桌，某某我等你 \n<br>\n也许彼此把彼此丢进记忆的流年 \n<br>\n但记得那排我们快乐的日子\n</center>\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["随手记"],"categories":["随手记"]},{"title":"十八岁，半夏锦年","url":"/2012/06/10/随手记/十八岁，半夏锦年/","content":">送走了我的青春，迎来我未知的迷惘，将来我会身在何方，我将永世不忘。 再见！\n\n<!--More-->\n\n<center>\n散落一地，流离 \n<br>\n回忆不起，青石板长巷的雨季\n<br>\n独自撑伞，没有你陪伴的故地 \n<br>\n簌簌樱花，零落成思绪 \n<br>\n\n\n<br>\n一场雨，让我离开这里 \n<br>\n出其不意，燕啄泥 \n<br>\n诉说那十八岁半夏锦年 \n<br>\n未完的结局 \n<br>\n<br>\n风萧萧，雨寒寒 \n<br>\n断了谁的琴弦，无语问寒蝉 \n<br>\n倚窗边，空白回忆里 艰难婉转… \n</center>\n\n\n----\n\n<center>\n<img src=\"http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\">\n</center>\n<center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center>\n","tags":["随手记"],"categories":["随手记"]}]