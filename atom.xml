<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>All In CTR、DL、ML、RL、NLP、KG</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2019-10-15T13:25:56.927Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>冷启动中的多避老虎机问题（Multi-Armed Bandit，MAB）</title>
    <link href="http://thinkgamer.cn/2019/10/15/RecSys/%E5%86%B7%E5%90%AF%E5%8A%A8%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%81%BF%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%88Multi-Armed%20Bandit%EF%BC%8CMAB%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2019/10/15/RecSys/冷启动中的多避老虎机问题（Multi-Armed Bandit，MAB）/</id>
    <published>2019-10-15T02:50:47.000Z</published>
    <updated>2019-10-15T13:25:56.927Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/102560272" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/102560272</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><p>推荐系统中有两个很重要的问题：EE问题和冷启动。在实际的场景中很好的解决这两个问题又很难，比如冷启动，我们可以基于热门、用户、第三方等信息进行半个性化的推荐，但很难去获得用户的真实兴趣分布。那么有没有一种算法可以很好的解决这个问题呢？答案就是：Bandit。</p><h3 id="Bandit算法与推荐系统"><a href="#Bandit算法与推荐系统" class="headerlink" title="Bandit算法与推荐系统"></a>Bandit算法与推荐系统</h3><p>在推荐系统领域里，有两个比较经典的问题常被人提起，一个是EE问题，另一个是用户冷启动问题。</p><p>EE问题又叫Exploit-Explore。</p><ul><li>Exploit表示的是对于用户已经确定的兴趣当然要迎合。</li><li>Explore表示的如果仅对用户进行兴趣投放，很快就会看厌，所以要不断的探索用户的新兴趣</li></ul><p>所以在进行物品推荐时，不仅要投其所好，还要进行适当的长尾物品挖掘。</p><p>用户冷启动问题，也就是面对新用户时，如何能够通过若干次实验，猜出用户的大致兴趣。</p><p>这两个问题本质上都是如何选择用户感兴趣的主题进行推荐，比较符合Bandit算法背后的MAB问题。</p><p>比如，用Bandit算法解决冷启动的大致思路如下：用分类或者Topic来表示每个用户兴趣，也就是MAB问题中的臂（Arm），我们可以通过几次试验，来刻画出新用户心目中对每个Topic的感兴趣概率。这里，如果用户对某个Topic感兴趣（提供了显式反馈或隐式反馈），就表示我们得到了收益，如果推给了它不感兴趣的Topic，推荐系统就表示很遗憾（regret）了。如此经历“选择-观察-更新-选择”的循环，理论上是越来越逼近用户真正感兴趣的Topic的。</p><h3 id="Bandit算法来源"><a href="#Bandit算法来源" class="headerlink" title="Bandit算法来源"></a>Bandit算法来源</h3><p>Bandit算法来源于历史悠久的赌博学，它要解决的问题是这样的：</p><p>一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, K-armed bandit problem, MAB）。</p><p>怎么解决这个问题呢？最好的办法是去试一试，不是盲目地试，而是有策略地快速试一试，这些策略就是Bandit算法。</p><p>这个多臂问题，推荐系统里很多问题都与它类似：</p><ul><li>假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。</li><li>假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？</li><li>我们的算法工程师又想出了新的模型，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？</li><li>如果只是推荐已知的用户感兴趣的物品，如何才能科学地冒险给他推荐一些新鲜的物品？</li></ul><p>Bandit算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？常见Bandit算法有哪些呢？往下看</p><h3 id="Thompson-sampling"><a href="#Thompson-sampling" class="headerlink" title="Thompson sampling"></a>Thompson sampling</h3><h4 id="1、Beta分布"><a href="#1、Beta分布" class="headerlink" title="1、Beta分布"></a>1、Beta分布</h4><p>Thompson Sampling是基于Beta分布进行的，所以首先看下什么是Beta分布？</p><p>Beta分布可以看作是一个概率的概率分布，当你不知道一个东西的具体概率是多少时，他可以给出所有概率出现的可能性。Beta是一个非固定的公式，其表示的是一组分布（这一点和距离计算中的闵可夫斯基距离类似）。</p><p>比如：</p><p>二项分布（抛n次硬币，正面出现k次的概率）</p><script type="math/tex; mode=display">P(S=k)=\binom{n}{k} p^k (1-p)^{n-k}</script><p>几何分布（抛硬币，第一次抛出正面所需的次数的概率）</p><script type="math/tex; mode=display">P(T=t)= (1-p)^{t-1} p</script><p>帕斯卡分布（抛硬币，第k次出现正面所需次数的概率）</p><script type="math/tex; mode=display">P(Y_k=t)=\binom{t-1}{k-1} p^{k-1} (1-p)^{t-k}p</script><p>去找一个统一的公式去描述这些分布，就是Beta分布：</p><script type="math/tex; mode=display">Beta(x| \alpha,\beta) = \frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^\beta</script><p>其中 $B(\alpha, \beta)$是标准化函数，他的作用是使总概率和为1，$\alpha, \beta$为形状参数，不同的参数对应的图像形状不同，他不但可以表示常见的二项分布、几何分布等，还有一个好处就是，不需要去关系某次实验结果服从什么分布，而是利用<br>$\alpha, \beta$的值就可以计算出我们想要的统计量。</p><p>常见的参数对应的图形为：</p><p><img src="https://img-blog.csdnimg.cn/20191015085055652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="Beta分布"></p><p>$Beta(\alpha, \beta)$的常见的统计量为：</p><ul><li>众数为：$\frac {\alpha-1}{\alpha + \beta -1}$</li><li>期望为：$\mu = E(x)= \frac {\alpha} {\alpha + \beta}$</li><li>方差为：$Var(x) = E(x - \mu)^2 = \frac { \alpha \beta } { (\alpha + \beta)^2 (\alpha + \beta +1) }$</li></ul><h4 id="2、Beta分布的例子"><a href="#2、Beta分布的例子" class="headerlink" title="2、Beta分布的例子"></a>2、Beta分布的例子</h4><p>网上资料中一个很常见的例子是棒球运动员的，这里进行借鉴。</p><p>棒球运动有一个指标是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。</p><p>现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。</p><p>对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。</p><p>接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取$\alpha$=81,$\beta$=219</p><p><img src="https://img-blog.csdnimg.cn/20191015090126646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="棒球运动员Beta分布例子"></p><p>之所以取这两个参数是因为：</p><ul><li>beta分布的均值是：$\frac{81} {81 + 219}=0.27$</li><li>从图中可以看到这个分布主要落在了(0.2,0.35)间，这是从经验中得出的合理的范围。</li></ul><blockquote><p>在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率。也就是说beta分布可以看作一个概率的概率分布。</p></blockquote><p>有了这样的初始值，随着运动的进行，其表达式可以表示为：</p><script type="math/tex; mode=display">Beta(\alpha_0 + hits , \beta_0 + misses)</script><p>其中 $\alpha_0, \beta_0$是一开始的参数，值为81，219。当击中一次球是 hits + 1，misses不变，当未击中时，hits不变，misses+1。这样就可以在每次击球后求其最近的平均水平了。</p><h4 id="3、Thompson-Smapling"><a href="#3、Thompson-Smapling" class="headerlink" title="3、Thompson Smapling"></a>3、Thompson Smapling</h4><p>Thompson sampling算法简单实用，简单介绍一下它的原理，要点如下：</p><ul><li>假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p。</li><li>我们不断地试验，去估计出一个置信度较高的“概率p的概率分布”就能近似解决这个问题了。</li><li>怎么能估计“概率p的概率分布”呢？ 答案是假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose。</li><li>每个臂都维护一个beta分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。</li><li>每次选择臂的方式是：计算每个臂现有的beta分布的平均水平，选择所有臂产生的随机数中最大的那个臂去摇。</li></ul><h4 id="4、TS的Python实现"><a href="#4、TS的Python实现" class="headerlink" title="4、TS的Python实现"></a>4、TS的Python实现</h4><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import numpy as <span class="built_in">np</span></span><br><span class="line">import <span class="built_in">random</span></span><br><span class="line"></span><br><span class="line">def ThompsonSampling(wins, trials):</span><br><span class="line">    pbeta = [<span class="number">0</span>] * N</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, len(trials)):</span><br><span class="line">        pbeta[i] = <span class="built_in">np</span>.<span class="built_in">random</span>.<span class="built_in">beta</span>(wins[i] + <span class="number">1</span>, trials[i] - wins[i] + <span class="number">1</span>)</span><br><span class="line">    choice = <span class="built_in">np</span>.argmax(pbeta)</span><br><span class="line">    trials[choice] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">random</span>.<span class="built_in">random</span>() &gt; <span class="number">0.5</span>:</span><br><span class="line">        wins[choice] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">T = <span class="number">10000</span>  # 实验次数</span><br><span class="line">N = <span class="number">10</span>  # 类别个数</span><br><span class="line"># 臂的选择总次数</span><br><span class="line">trials = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0</span>] * N )</span><br><span class="line"># 臂的收益</span><br><span class="line">wins = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0</span>] * N )</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, T):</span><br><span class="line">    ThompsonSampling(wins, trials)</span><br><span class="line"><span class="built_in">print</span>(trials)</span><br><span class="line"><span class="built_in">print</span>(wins)</span><br><span class="line"><span class="built_in">print</span>(wins/trials)</span><br></pre></td></tr></table></figure><h3 id="UCB"><a href="#UCB" class="headerlink" title="UCB"></a>UCB</h3><h4 id="1、UCB的原理"><a href="#1、UCB的原理" class="headerlink" title="1、UCB的原理"></a>1、UCB的原理</h4><p>UCB（Upper Confidence Bound，置信区间上界）可以理解为不确定性的程度，区间越宽，越不确定，反之就越确定，其表达式如下：</p><script type="math/tex; mode=display">score(i) = \frac {N_i}{T} + \sqrt{ \frac{2 ln T}{N_i}}</script><p>其中 Ni 表示第i个臂收益为 1 的次数，T表示选择的总次数</p><p>公式分为左右两部分，左侧（+左侧部分）表示的是候选臂i到目前为止的平均收益，反应的是它的效果。右侧（+右侧部分）叫做Bonus，本质上是均值的标准差，反应的是候选臂效果的不确定性，就是置信区间的上边界。</p><blockquote><p>统计学中的一些统计量表达的含义</p></blockquote><p>如果一个臂的收益很少，即Ni很小，那么他的不确定性就越大，在最后排序输出时就会有优势，bouns越大，候选臂的平均收益置信区间越宽，越不稳定，就需要更多的机会进行选择。反之如果平均收益很大，即+号左侧户数很大，在选择时也会有被选择的机会。</p><h4 id="2、UCB的Python实现"><a href="#2、UCB的Python实现" class="headerlink" title="2、UCB的Python实现"></a>2、UCB的Python实现</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 T = 1000 个用户，即总共进行1000次实现</span></span><br><span class="line">T = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 定义 N = 10 个标签，即 N 个 物品</span></span><br><span class="line">N = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保证结果可复现，设置随机数种子</span></span><br><span class="line">np.<span class="built_in">random</span>.seed(<span class="number">888</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个物品的累积点击率（理论概率）</span></span><br><span class="line">true_rewards = np.<span class="built_in">random</span>.uniform(low=<span class="number">0</span>, high=<span class="number">1</span>, size= N)</span><br><span class="line"><span class="comment"># true_rewards = np.array([0.5] * N)</span></span><br><span class="line"><span class="comment"># 每个物品的当前点击率</span></span><br><span class="line">now_rewards = np.zeros(N)</span><br><span class="line"><span class="comment"># 每个物品的点击次数</span></span><br><span class="line">chosen_count = np.zeros(N)</span><br><span class="line"></span><br><span class="line">total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算ucb的置信区间宽度</span></span><br><span class="line">def calculate_delta(T, <span class="keyword">item</span>):</span><br><span class="line">    <span class="keyword">if</span> chosen_count[<span class="keyword">item</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="literal">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="literal">return</span> np.<span class="built_in">sqrt</span>( <span class="number">2</span> * np.<span class="built_in">log</span>( T ) / chosen_count[<span class="keyword">item</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算UCB</span></span><br><span class="line">def ucb(t, N):</span><br><span class="line">    <span class="comment"># ucb得分</span></span><br><span class="line">    upper_bound_probs = [ now_rewards[<span class="keyword">item</span>] + calculate_delta(t,<span class="keyword">item</span>) <span class="keyword">for</span> <span class="keyword">item</span> <span class="keyword">in</span> range(N) ]</span><br><span class="line">    <span class="keyword">item</span> = np.argmax(upper_bound_probs)</span><br><span class="line">    <span class="comment"># 模拟伯努利收益</span></span><br><span class="line">    <span class="comment"># reward = sum(np.random.binomial(n =1, p = true_rewards[item], size=20000)==1 ) / 20000</span></span><br><span class="line">    reward = np.<span class="built_in">random</span>.binomial(n =<span class="number">1</span>, p = true_rewards[<span class="keyword">item</span>])</span><br><span class="line">    <span class="literal">return</span> <span class="keyword">item</span>, reward</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>,T+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 为第 t个用户推荐一个物品</span></span><br><span class="line">    <span class="keyword">item</span>, reward = ucb(t, N)</span><br><span class="line">    <span class="comment"># print("item is %s, reward is %s" % (item, reward))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 一共有多少用户接受了推荐的物品</span></span><br><span class="line">    total_reward += reward</span><br><span class="line">    chosen_count[<span class="keyword">item</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新物品的当前点击率</span></span><br><span class="line">    now_rewards[<span class="keyword">item</span>] =  ( now_rewards[<span class="keyword">item</span>] * (t<span class="number">-1</span>) + reward) /  t</span><br><span class="line">    <span class="comment"># print("更新后的物品点击率为：%s" % (now_rewards[item]))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出当前点击率 / 累积点击率</span></span><br><span class="line">    <span class="comment"># print("当前点击率为: %s" % now_rewards)</span></span><br><span class="line">    <span class="comment"># print("累积点击率为: %s" % true_rewards)</span></span><br><span class="line"></span><br><span class="line">    diff =  np.<span class="built_in">subtract</span>( true_rewards, now_rewards)</span><br><span class="line">    print(diff[<span class="number">0</span>])</span><br><span class="line">    print(total_reward)</span><br></pre></td></tr></table></figure><h4 id="3、UCB的推导"><a href="#3、UCB的推导" class="headerlink" title="3、UCB的推导"></a>3、UCB的推导</h4><p>观测 1：假设一个物品被推荐了k次，获取了k次反馈（点击 or 不点击），可以计算出物品被点击的平均概率</p><p>当k 接近于正无穷时，p’ 会接近于真实的物品被点击的概率</p><script type="math/tex; mode=display">p' = \frac {\sum reward_i}{k}</script><p>观测 2：现实中物品被点击的次数不可能达到无穷大，因此估计出的被点击的概率 p’ 和真实的点击的概率 p 总会存在一个差值 d，即：</p><script type="math/tex; mode=display">p'-d \leqslant p \leqslant p'+d</script><p>最后只需要解决差值 d 到底是怎么计算的？</p><p>首先介绍霍夫丁不等式（Chernoff-Hoeffding Bound），霍夫丁不等式假设reward_1, … , reward_n 是在[0,1]之间取值的独立同分布随机变量，用p’ 表示样本的均值，用p表示分布的均值，那么有：</p><script type="math/tex; mode=display">P\{|p'-p| \leqslant \delta \} \geqslant 1 - 2e^{-2n\delta ^2}</script><p>当 $\delta$ 取值为$\sqrt { 2In T /n}$ （其中 T 表示有物品被推荐的次数，n表示有物品被点击的次数），可以得到：</p><script type="math/tex; mode=display">P\{|p'-p| \leqslant \sqrt { \frac{2In T }{n}} \} \geqslant 1 - \frac{ 2 }{ T^4}</script><p>也就是说：</p><script type="math/tex; mode=display">p' - \sqrt { \frac{2In T }{n}} \leqslant p \leqslant p' + \sqrt { \frac{2In T }{n}}</script><p>是以 1 - 2/T^4 的概率成立的，<br>当T=2时，成立的概率为0.875<br>当T=3时，成立的概率为0.975<br>当T=4时，成立的概率为0.992<br>可以看出 $d =  \sqrt { \frac{2In T }{n}}$ 是一个不错的选择。</p><h3 id="Epsilon-Greedy"><a href="#Epsilon-Greedy" class="headerlink" title="Epsilon-Greedy"></a>Epsilon-Greedy</h3><h4 id="1、算法原理"><a href="#1、算法原理" class="headerlink" title="1、算法原理"></a>1、算法原理</h4><p>这是一个朴素的Bandit算法，有点类似模拟退火的思想：</p><ol><li>选一个（0,1）之间较小的数作为epsilon；</li><li>每次以概率epsilon做一件事：所有臂中随机选一个；</li><li>每次以概率1-epsilon 选择截止到当前，平均收益最大的那个臂。</li></ol><p>是不是简单粗暴？epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只选择收益最大的。</p><h4 id="2、Python实现"><a href="#2、Python实现" class="headerlink" title="2、Python实现"></a>2、Python实现</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedy</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, epsilon, counts, values)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.epsilon = epsilon</span><br><span class="line">        <span class="keyword">self</span>.counts = counts</span><br><span class="line">        <span class="keyword">self</span>.values = values</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(<span class="keyword">self</span>, n_arms)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.counts = [<span class="number">0</span> <span class="keyword">for</span> col <span class="keyword">in</span> range(n_arms)]</span><br><span class="line">        <span class="keyword">self</span>.values = [<span class="number">0</span>.<span class="number">0</span> <span class="keyword">for</span> col <span class="keyword">in</span> range(n_arms)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_arm</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> random.random() &gt; <span class="keyword">self</span>.<span class="symbol">epsilon:</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">self</span>.values.index( max(<span class="keyword">self</span>.values) )</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="comment"># 随机返回 self.values 中个的一个</span></span><br><span class="line">            <span class="keyword">return</span> random.randrange(len(<span class="keyword">self</span>.values))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reward</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> random.random() &gt; <span class="number">0</span>.<span class="number">5</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(<span class="keyword">self</span>, chosen_arm, reward)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.counts[chosen_arm] = <span class="keyword">self</span>.counts[chosen_arm] + <span class="number">1</span></span><br><span class="line">        n = <span class="keyword">self</span>.counts[chosen_arm]</span><br><span class="line"></span><br><span class="line">        value = <span class="keyword">self</span>.values[chosen_arm]</span><br><span class="line">        new_value = ((n - <span class="number">1</span>)  * value + reward ) / float(n)</span><br><span class="line">        <span class="keyword">self</span>.values[chosen_arm] = round(new_value,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">n_arms = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">algo = EpsilonGreedy(<span class="number">0</span>.<span class="number">1</span>, [], [])</span><br><span class="line"></span><br><span class="line">algo.initialize(n_arms)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>)<span class="symbol">:</span></span><br><span class="line">    chosen_arm = algo.select_arm()</span><br><span class="line">    reward = algo.reward()</span><br><span class="line">    algo.update(chosen_arm, reward)</span><br><span class="line"></span><br><span class="line">print(algo.counts)</span><br><span class="line">print(algo.values)</span><br></pre></td></tr></table></figure><h3 id="朴素Bandit算法"><a href="#朴素Bandit算法" class="headerlink" title="朴素Bandit算法"></a>朴素Bandit算法</h3><p>最朴素的Bandit算法就是：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。这个算法是人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。</p><p>Python实现比较简单，这里就不做演示了。</p><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/102560272&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐算法" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="冷启动，Bandit" scheme="http://thinkgamer.cn/tags/%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%8CBandit/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（四）之正则化</title>
    <link href="http://thinkgamer.cn/2019/09/27/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/09/27/TensorFlow/神经网络中的网络优化和正则化（四）之正则化/</id>
    <published>2019-09-27T00:15:10.000Z</published>
    <updated>2019-10-14T12:28:36.844Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍（如下所示），本篇为最后一篇主要介绍神经网络中的网络正则化。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化 </li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化</li></ul></li></ul><p>机器学习模型中的关键是泛化问题，即样本在真实数据集上的期望风险最小化，而在训练集上的经验风险最小化和期望风险并不一致。由于神经网络的拟合能力很强，其在训练集上的训练误差会降的很小，从而导致过拟合。</p><p><strong>正则化（Regularization）</strong>是一类通过限制模型复杂度，从而避免过拟合，提高模型泛化能力的一种方法，包括引入一些约束规则，增加先验，提前终止等。</p><p>在传统的机器学习模型中，提高模型泛化能力的主要方法是限制模型复杂度，比如$l_1,l_2$正则，但是在训练深层神经网络时，特别是在过度参数（OverParameterized）时，$l_1,l_2$正则化不如机器学习模型中效果明显，因此会引入其他的一些方法，比如：数据增强，提前终止，丢弃法，继承法等。</p><h3 id="l-1-l-2-正则"><a href="#l-1-l-2-正则" class="headerlink" title="$l_1,l_2$正则"></a>$l_1,l_2$正则</h3><p>$l_1,l_2$正则是机器学习中常用的正则化方法，通过约束参数的$l_1,l_2$范数来减少模型在训练数据上的过拟合现象。</p><p>通过引入$l_1,l_2$正则，优化问题变为：</p><script type="math/tex; mode=display">a\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta))+\lambda l_p(\theta)</script><p>$L$为损失函数，$N$为训练的样本数量，$f(.)$为待学习的神经网络，$\theta$为参数，$l_p$为$l_1,l_2$正则中的一个，$\lambda$为正则项系数。</p><p>带正则化的优化问题等价于下面带约束条件的优化问题：</p><script type="math/tex; mode=display">\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta))\\subject \, to \, l_p(\theta) \leq 1</script><p>下图给出了不同范数约束条件下的最优化问题示例：<br><img src="https://img-blog.csdnimg.cn/20190926205404238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="不同范数约束条件下的最优化问题示例"></p><p>上图中红线表示$l_p$范数，黑线表示$f(\theta)$的等高线（简单起见，这里用直线表示）</p><p>从上图最左侧图可以看出，$l_1$范数的约束条件往往会使最优解位于坐标轴上，从而使用最终的参数为稀疏向量，此外$l_1$范数在零点不可导，常用下式来代替：</p><script type="math/tex; mode=display">l_1(\theta) = \sum_{i} \sqrt{\theta_i ^2 + \epsilon }</script><p>其中$\epsilon$为一个非常小的常数。</p><p>一种折中的方法是<strong>弹性网络正则化（Elastic Net Regularization）</strong> ，同时加入$l_1, l_2$正则，如下：</p><script type="math/tex; mode=display">a\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta)_+\lambda_1 l_1(\theta) + \lambda_2 l_2(\theta)</script><p>其中$\lambda_1, \lambda_2$分别是正则化项的参数。</p><h3 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h3><p><strong>权重衰减（Weight Deacy）</strong> 也是一种有效的正则化方法，在每次调参时，引入一个衰减系数，表示式为：</p><script type="math/tex; mode=display">\theta_t \leftarrow (1-w)\theta_{t-1} - \alpha g_t</script><p>其中$g_t$为第t次更新时的梯度，$\alpha$为学习率，$w$为权重衰减系数，一般取值比较小，比如0.0005。在标准的随机梯度下降中，权重衰减和$l_2$正则达到的效果相同，因此，权重衰减在一些深度学习框架中用$l_2$正则来代替。但是在较为复杂的优化方法中，两者并不等价。</p><h3 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h3><p><strong>提前终止（early stop）</strong> 对于深层神经网络而言是一种简单有效的正则化方法，由于深层神经网络拟合能力很强，比较容易在训练集上过拟合，因此在实际操作时往往产出一个和训练集独立的验证集，并用在验证集上的错误来代表期望错误，当验证集上的错误不再下降时，停止迭代。</p><p>然而在实际操作中，验证集上的错误率变化曲线并不是一条平衡的曲线，很可能是先升高再降低，因此提前停止的具体停止标准需要根据实际任务上进行优化。</p><h3 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h3><p>当训练一个深层神经网络时，可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法称为 <strong>丢弃法（Dropout Method）</strong>。每次丢弃的神经元为随机的，对于每一个神经元都以一个概率p来判断要不要停留，对于每一个神经层 $y=f(Wx + b)$，我们可以引入一个丢弃函数$d(.)$使得$y=f(Wd(x)+b)$。丢弃函数的定义为：</p><script type="math/tex; mode=display">d(x) = \left\{\begin{matrix}m \odot x, When \, Train\\ px, When \,  Test\end{matrix}\right.</script><p>其中$m \in \{0,1\}^d$是丢弃掩码（dropout mask），通过以概率为p的贝努力分布随机生成，$p$可以通过一个验证集选取一个最优值，也可以设置为0.5， 这样对大部分网络和任务比较有效。在训练时，神经元的平均数量为原来的$p$倍，而在测试时，所有的神经元都可以是激活的，这会造成训练时和测试时的网络结构不一致，为了缓解这个问题，在测试时，需要将每一个神经元的输出乘以$p$，也相当于把不同的神经网络做了一个平均。</p><p>下图给出了一个网络经过dropout的示例。<br><img src="https://img-blog.csdnimg.cn/20190926170817842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="丢弃法示例"></p><p>一般来讲，对于隐藏层的神经元，丢弃率$p=0.5$时最好，这样当训练时有一半的神经元是丢弃的，随机生成的网络结构具有多样性。对于输入层的神经元，其丢弃率往往设置为更接近于1的数，使得输入变化不会太大，对输入层的神经元进行丢弃时，相当于给数据增加噪声，提高网络的鲁棒性。</p><p>丢弃法一般是针对神经元进行随机丢弃，但是也可以扩展到神经元之间的连接进行随机丢弃，或每一层进行随机丢弃。</p><p>丢弃法有两种解释：</p><p>（1）集成学习的解释<br>每做一次丢弃，相当于从原始的网络中采样得到一个子网络，如果一个神经网络有n个神经元，那么可以采样出$2^n$个子网络，每次训练都相当于是训练一个不同的子网络，这些子网络都共享最开始的参数。那么最终的网络可以看成是集成了指数级个不同风格的组合模型。</p><p>（2）贝叶斯学习的解释</p><p>丢弃法也可以解释为一个贝叶斯学习的近似，用$y=f(x,\theta)$表示一个要学习的网络，贝叶斯学习是假设参数$\theta$为随机向量，并且先验分布为$q(\theta)$，贝叶斯方法的预测为：</p><script type="math/tex; mode=display">E_{q(\theta)}[y] = \int_{q}f(x,\theta)q(\theta)d\theta \approx \frac{1}{M}\sum_{m=1}^{M}f(x, \theta_m)</script><p>其中$f(x, \theta_m)$为第m次应用丢弃方法后的网络，其参数$\theta_m$为全部参数$\theta$的一次采样。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>深层神经网络的训练需要大量的样本才能取得不错的效果，因为在数据量有限的情况下，可以通过 <strong>数据增强（Data Augmentation）</strong>来增加数据量，提高模型鲁棒性，避免过拟合。目前数据增强主要应用在图像数据上，在文本等其他类型的数据还没有太好的方法。</p><p>图像数据增强主要通过算法对图像进行转换，引入噪声方法增强数据的多样性，增强的方法主要有：</p><ul><li>转换（Rotation）：将图像按照顺时针或者逆时针方向随机旋转一定的角度；</li><li>翻转（Flip）：将图像沿水平或者垂直方向随机翻转一定的角度；</li><li>缩放（Zoom in/out）：将图像放大或者缩小一定的比例；</li><li>平移（Shift）：将图像按照水平或者垂直的方法平移一定步长；</li><li>加噪声（Noise）：加入随机噪声。</li></ul><h3 id="标签平滑"><a href="#标签平滑" class="headerlink" title="标签平滑"></a>标签平滑</h3><p>在数据增强中，可以通过给样本加入随机噪声来避免过拟合，同样也可以给样本的标签引入一定的噪声。假设在训练数据集中，有一些样本的标签是被错误标注的，那么最小化这些样本上的损失函数会导致过拟合。一种改善的正则化方法是<strong>标签平滑（label smothing）</strong>，即在输出标签中随机加入噪声，来避免模型过拟合。</p><p>一个样本$x$的标签一般用onehot向量表示，如下：</p><script type="math/tex; mode=display">y = [0,...,0,1,.....,1]^T</script><p>这种标签可以看作<strong>硬目标（hard targets）</strong>，如果使用softmax分类器并使用交叉熵损失函数，最小化损失函数会使得正确类和其他类权重差异很大。根据softmax函数的性质可以知道，如果要使得某一类的输出概率接近于1，其未归一化的得分要远大于其他类的得分，这样可能会导致其权重越来越大，并导致过拟合。i</p><p>此外如果标签是错误的，会导致严重的过拟合现象，为了改善这种情况，我们可以引入一个噪声会标签进行平滑，即假设样本以$\epsilon$的概率为其他类，平滑后的标签为：</p><script type="math/tex; mode=display">\tilde{y} =[ \frac{ \epsilon }{K-1} ,...,\frac{ \epsilon }{K-1} ,1- \epsilon,\frac{ \epsilon }{K-1},....,\frac{ \epsilon }{K-1}]^T</script><p>其中$K$为标签数量，这种标签可以看作是<strong>软目标（soft targets）</strong>。标签平滑可以避免模型的输出过拟合到硬目标上，并且通常不会降低其分类能力。</p><p>上边的标签平滑方法是给其他$K-1$个标签相同的概率$\frac{\epsilon}{K-1}$，没有考虑目标之间的相关性。一种更好的做法是按照类别相关性来赋予其他标签不同的概率，比如先训练另外一个更复杂的教师网络，并使用大网络的输出作为软目标进行训练学生网络，这种方法也称为知识精炼（Knowledge Distillation）。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>至此，神经网络中的网络优化和正则化（一）（二）（三）（四）篇已经完成，如下：</p><ul><li><a href="https://thinkgamer.blog.csdn.net/article/details/100996744" target="_blank" rel="external">神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/101026786" target="_blank" rel="external">神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/101033047" target="_blank" rel="external">神经网络中的网络优化和正则化（三）之超参数优化</a></li><li><a href="">神经网络中的网络优化和正则化（四）之正则化</a></li></ul><p>神经网络中的网络优化和正则化即是对立又统一的关系，一方面我们希望找到一个最优解使得模型误差最小，另一方面又不希望得到一个最优解，可能陷入过拟合。优化和正则化的目标是期望风险最小化。</p><p>目前在深层神经网络中泛化能力还没有很好的理论支持，在传统的机器学习上比较有效的$l_1,l_2$正则化在深层神经网络中作用也比较有限，而一些经验性的做法，比如随机梯度下降和提前终止，会更加有效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍（如下所示），本篇为最后一篇主要介绍神经网络中的网络正则化。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（三）之超参数优化</title>
    <link href="http://thinkgamer.cn/2019/09/25/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/09/25/TensorFlow/神经网络中的网络优化和正则化（三）之超参数优化/</id>
    <published>2019-09-25T12:53:25.000Z</published>
    <updated>2019-10-14T12:28:36.842Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>公众号标题：神经网络中的优化方法之学习率衰减和动态梯度方向</p></blockquote><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化</li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化 </li></ul></li></ul><hr><p>无论是神经网络还是机器学习都会存在很多的超参数，在神经网络中，常见的超参数有：</p><ul><li>网络结构：包括神经元之间的连接关系，层数，每层的神经元数量，激活函数的类型等</li><li>优化参数：包括优化方法，学习率，小批量样本数量</li><li>正则化系数</li></ul><p><strong>超参数优化（Hyperparamter Optimization）</strong> 主要存在两方面的困难：</p><ul><li>超参数优化是一个组合优化问题，无法像一般参数那样通过梯度优化的方法来求解，也没有一种通用的优化方法</li><li>评估一组超参数配置时间代价很高，从而导致一些优化算法（比如时间演化算法）在超参数优化中难以应用</li></ul><p>对于超参数的设置，一般有三种比较简单的优化方法，人工搜索，网格搜素，随机搜索</p><hr><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>网格搜索（grid search）是一种通过尝试所有超参数的组合来寻找一组合适的超参数组合的方法。如果参数是连续，可以将其离散化。比如“学习率”，我们可以根据经验选取几个值:$\alpha \in {0.01, 0.1, 0.5, 1.0}$。</p><p>一般而言，对于连续的超参数，不能采用等间隔的方式进行划分，需要根据超参数自身的特点进行离散化。</p><p>网格搜索根据不同的参数组合在测试集上的表现，选择一组最优的参数作为结果。</p><h3 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h3><p>不同超参数对模型的影响不同，有的超参数（比如正则项系数）对模型的影响有限，有的超参数（比如学习率）对模型的影响比较大，这时候采用网格搜索就会在影响不大的超参数上浪费时间。</p><p>一种在实践中比较有效的方法是对超参数进行随机组合（比如不太重要的参数进行随机抽取，重要的参数可以按照网格搜索的方式选择），选择表现最好的参数作为结果,这就是<strong>随机搜索（random search）</strong></p><blockquote><p>网格搜索和随机搜索没有利用超参数之间的相关性，即如果模型的超参数组合比较类似，其模型的性能表现也是比较接近的，这时候网格搜索和随机搜索就比较低效。下面介绍两种自适应的超参数优化方法：贝叶斯优化和动态资源分配。</p></blockquote><hr><h3 id="动态资源分配"><a href="#动态资源分配" class="headerlink" title="动态资源分配"></a>动态资源分配</h3><p>在超参数优化中，每组超参数配置的评估代价很高，如果我们可以在较早的阶段就估计出该组超参数效果就比较差，然后提前终止该组参数的测试，从而将更多的资源留给其他。这个问题可以归结为<strong>多臂赌博机问题</strong>的一个泛化问题，即<strong>最优臂问题（best-arm problem）</strong>，即在给定有限次数的情况下，如何获取最大收益。</p><p>动态资源分配的一种有效方法是<strong>逐层减半（successive halving）</strong>，将超参数优化看作是一种非随机的最优臂问题。该方法出自2015年的一篇论文，论文下载地址为：<a href="https://arxiv.org/pdf/1502.07943.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1502.07943.pdf</a></p><p>假设要尝试N组超参数配置，总共可利用的摇臂资源次数为B，我们可以通过$T= [log_2N]-1$轮逐次减半的方法来选取最优的配置，具体计算过程如下：</p><p><img src="https://img-blog.csdnimg.cn/20190919191933874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="逐次减半"></p><blockquote><p>在逐次减半方法中，N的设置十分重要，如果N越大，得到最佳配置的机会也越大，但每组配置分配到的资源就越少，这样早期的评估结果可能不准确，反之，如果N越小，每组超参数配置的评估就会越准确，但也有可能无法得到最优的参数配置。因此如何设置N是评估“利用-探索”的一个关键因素，一种改进的方法是：HyperBrand方法，通过尝试不同的N来寻找最优的参数配置。对应的论文下载地址为：<a href="https://openreview.net/pdf?id=ry18Ww5ee" target="_blank" rel="external">https://openreview.net/pdf?id=ry18Ww5ee</a></p></blockquote><hr><h3 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h3><h4 id="贝叶斯优化背后的思想"><a href="#贝叶斯优化背后的思想" class="headerlink" title="贝叶斯优化背后的思想"></a>贝叶斯优化背后的思想</h4><p>贝叶斯优化（Bayesian optimization）是一种自适应的超参数优化方法，根据当前已经试验的超参数组合，来预测下一个可能带来最大收益的组合。</p><blockquote><p>对于同一个算法来讲，不同的超参数组合其实是对应不同的模型，而贝叶斯优化可以帮助我们在众多模型中寻找性能最优的模型，虽然我们可以使用交叉验证的思想寻找更好的超参数组合，但是不知道需要多少样本才能从一系列候选模型中选择出最优的模型。这就是为什么贝叶斯优化能够减少计算任务加速优化过程的进程，同样贝叶斯优化不依赖于人为猜测需要样本量的多少，这种优化计算是基于随机性和概率分布得到的。<br><br><br>简单来说，当我们把第一条样本送到模型中的时候，模型会根据当前的样本点构建一条直接，当把第二天样本送到模型中的时候，模型将结合这两个点并从前面的线出发绘制一条修正的线，当输送第三个样本的时候，模型绘制的就是一条非线性曲线，当样本数据增加时，模型所结合的曲线就会变得更多，这就像统计学里的抽样定理，即我们从样本参数出发估计总体参数，且希望构建出的估计量与总体参数相合，无偏估计。<br><br><br>下图为非线性目标函数曲线图，对于给定的目标函数，在输送了所有的观察样本之后，它将搜寻到最大值，即寻找令目标函数最大的参数（arg max）。<br><img src="https://img-blog.csdnimg.cn/20190920122716823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="非线性目标函数曲线图"><br>我们的目标并不是使用尽可能多的样本去完全推断未知的目标函数，而是希望能求得使目标函数最大化的参数，所以我们将注意力从曲线上移开，当目标函数组合能提升曲线形成分布时，其就可以称为采集函数（Acquisition funtion），这就是<strong>贝叶斯优化背后的思想</strong>。（灰色区域部分参考：<a href="https://www.jiqizhixin.com/articles/2017-08-18-5）" target="_blank" rel="external">https://www.jiqizhixin.com/articles/2017-08-18-5）</a></p></blockquote><h4 id="时序模型优化"><a href="#时序模型优化" class="headerlink" title="时序模型优化"></a>时序模型优化</h4><p>一种常用的贝叶斯优化方法为时序模型优化（Sequential Model-Based Optimization，SMBD），假设超参数优化的函数f(x)服从高斯过程，则$p(f(x)|x)$为一个正态分布。贝叶斯优化过程是根据已有的N组实验结果$H={x_n,y_n}, n\in(1,N)$（$y_n$为$f(x_n)$的观测值）来建模高斯过程，并计算$f(x)$的后验分布$p(f(x)|x,H)$。</p><p>为了使得$p(f(x)|x,H)$接近其真实分布，就需要对样本空间进行足够多的采样，但是超参数优化中每一个样本的生成成本都很高，需要使用尽可能少的样本来使得$p_\theta(f(x)|x,H)$接近于真实分布。因此需要定义一个收益函数（Acquisition funtion）$\alpha (x, H)$来判断一个样本能否给建模$p_\theta(f(x)|x,H)$提供更多的收益。收益越大，其修正的高斯过程会越接近目标函数的真实分布。</p><p>收益函数的定义有很多方式，一个常用的是期望改善（Expected Improvement，EI）。假设$y^* = min \left \{  y_n, 1 \leq n \leq N \right \}$是当前已有样本中的最优值，期望改善函数为：</p><script type="math/tex; mode=display">EI(x, H) = \int_{-\infty }^{ +\infty } max (y^* - y, 0) p(y|x, H) dy</script><p>期望改善是定义一个样本$x$在当前模型$p(f(x)|x,H)$下，$f(x)$超过最好结果$y^*$的期望。除了期望改善函数之外，收益函数还有其他函数的定义，比如改善概率（Probability Of Improvement），高斯过程置信上界（GP Up Confidence Bound，GP-UCB）等。</p><p>时序模型优化过程如下所示：<br><img src="https://img-blog.csdnimg.cn/20190920150457198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="时序模型优化过程"></p><p>贝叶斯优化的缺点是高斯建模过程需要计算矩阵的逆，时间复杂度为$O(n^3)$，因此不能很好的处理高维过程，深层神经网络的参数一般比较多，需要更加高效的高斯过程建模，也有一些方法将时间复杂度从$O(n^3)$降到了$O(n)$。</p><blockquote><p>至此，超参数优化部分已经介绍完成，这里并没有对超参数优化进行实现，有很多Python库已经对其进行了封装，感兴趣的可以关注下，另外贝叶斯优化在日常实践中用的比较多但是不太好理解，可以多看几遍，对比一些文章什么看下理解下。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;公众号标题：神经网络中的优化方法之学习率衰减和动态梯度方向&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化</title>
    <link href="http://thinkgamer.cn/2019/09/22/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96:%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86:%E9%80%90%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/09/22/TensorFlow/神经网络中的网络优化和正则化（二）之参数初始化:数据预处理:逐层归一化/</id>
    <published>2019-09-22T13:32:55.000Z</published>
    <updated>2019-10-14T12:28:36.843Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化</li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化 </li></ul></li></ul><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><h4 id="对称权重现象"><a href="#对称权重现象" class="headerlink" title="对称权重现象"></a>对称权重现象</h4><p>在上一篇文章中我们提到神经网络中的参数学习是基于梯度下降的，而梯度下降需要赋予一个初始的参数，所以这个参数的初始化就显得特别重要。</p><p>在感知器和逻辑回归中，一般将参数初始化为0，但是在神经网络中如果把参数初始化为0，就会导致在第一次前向计算时，所有隐藏层神经元的激活值都相同，这样会导致深层神经元没有区分性，这种现象称为<strong>对称权重现象</strong></p><p>因此如果要高质量的训练一个网络，给参数选择一个合适的初始化区间是非常重要的，一般而言，参数初始化的区间应该根据神经元的性质进行差异化的设置，如果一个神经元的输入过多，权重就不要设置太大，以避免神经元的的输出过大（当激活函数为ReLU时）或者过饱和（激活函数为Sigmoid函数时）。<br>关于神经网络中的激活函数介绍可参考：</p><blockquote><p><a href="https://blog.csdn.net/Gamer_gyt/article/details/89440152" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/89440152</a></p></blockquote><p>常见的参数初始化方法包括以下两种。</p><h4 id="Gaussian初始化"><a href="#Gaussian初始化" class="headerlink" title="Gaussian初始化"></a>Gaussian初始化</h4><p>高斯初始化是最简单的初始化方法，参数服从一个固定均值和固定方差的高斯分布进行随机初始化。</p><p>初始化一个深度网络时，一个比较好的初始化方案是保持每个神经元输入的方差是一个常量，当一个神经元的输入连接数量为n时，可以考虑其输入连接权重以$N(0,\sqrt{\frac{1}{n}})$的高斯分布进行初始化，如果同时考虑神经元的输出连接数量为m时，可以按照$N(0,\sqrt{\frac{2}{m+n}})$进行高斯分布初始化。</p><h4 id="均匀分布初始化"><a href="#均匀分布初始化" class="headerlink" title="均匀分布初始化"></a>均匀分布初始化</h4><blockquote><p>均匀初始化是指在一个给定的区间[-r,r]内采用均匀分布来初始化参数，超参数r的设置也可以根据神经元的连接数量来进行自适应调整。</p></blockquote><p><strong>Xavier初始化方法</strong>是一种自动计算超参数r的方法，参数可以在[-r,r]之间采用均匀分布进行初始化。</p><p>如果神经元激活函数为logistic函数，对于第l-1层到第l层的权重参数区间可以设置为：</p><script type="math/tex; mode=display">r = \sqrt{ \frac{6}{ n^{l-1} + n^l}}</script><p>$n^l$ 表示第l层神经元的个数，$n^{l-1}$表示l-1层神经元的个数。</p><p>如果是tanh激活函数，权重参数区间可以设置为：</p><script type="math/tex; mode=display">r =4 \sqrt{ \frac{6}{ n^{l-1} + n^l}}</script><blockquote><p>在实际经验中，Xavier初始化方法用的比较多。</p></blockquote><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="为什么要进行数据预处理"><a href="#为什么要进行数据预处理" class="headerlink" title="为什么要进行数据预处理"></a>为什么要进行数据预处理</h4><p>一般情况下，在原始数据中，数据的维度往往不一致，比如在电商数据中，某个商品点击的次数往往要远大于购买的次数，即<strong>特征的分布范围差距很大</strong>，这样在一些使用余弦相似度计算的算法中，较大的特征值就会起到绝对作用，显然这样做是极其不合理的。同样在深度神经网络中，虽然可以通过参数的调整来自适应不同范围的输入，但是这样训练的效率也是很低的。</p><p>假设一个只有一层的网络 $y=tanh(w_1x_1 + w_2 x_2 +b)$，其中$x_1 \in [0,10], x_2 \in [0,1]$。因为激活函数 tanh的导数在[-2,2]之间是敏感的，其余的值域导数接近0，因此$w_1x_1 + w_2 x_2 +b$过大或者过小都会影响训练，为了提高训练效率，我们需要把$w_1x_1 + w_2 x_2 +b$限定在[-2,2]之间，因为$x_1,x_2$的取值范围，需要把$w_1$设置的小一些，比如在[-0.1, 0.1]之间，可以想象，如果数据维度比较多的话，我们需要精心的去设置每一个参数，<strong>但是如果把特征限定在一个范围内，比如[0,1]</strong>，我们就不需要太区别对待每一个参数。</p><p>除了参数初始化之外，不同特征取值范围差异比较大时也会影响梯度下降法的搜索效率，下图（图1-1）给出了数据归一化对梯度的影响，对比等高线图可以看出，归一化后，梯度的位置方向更加接近于最优梯度的方向。</p><p><img src="https://img-blog.csdnimg.cn/20190919111801183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="数据归一化对梯度的影响"></p><h4 id="数据预处理的方法"><a href="#数据预处理的方法" class="headerlink" title="数据预处理的方法"></a>数据预处理的方法</h4><p>关于原始数据归一化的方法有很多，可以参考《推荐系统开发实战》中第四章节部分内容，写的很全面，而且有对应的代码实现。该书的购买链接：</p><blockquote><p><a href="https://item.jd.com/12671716.html" target="_blank" rel="external">点击查看详情-京东链接</a></p></blockquote><h3 id="逐层归一化"><a href="#逐层归一化" class="headerlink" title="逐层归一化"></a>逐层归一化</h3><h4 id="深层神经网络中为什么要做逐层归一化"><a href="#深层神经网络中为什么要做逐层归一化" class="headerlink" title="深层神经网络中为什么要做逐层归一化"></a>深层神经网络中为什么要做逐层归一化</h4><p>在深层神经网络中，当前层的输入是上一层的输出，因此之前层参数的变化对后续层的影响比较大，就像一栋高楼，低层很小的变化就会影响到高层。</p><p>从机器学习的角度去看，如果某个神经网络层的输入参数发生了变化，那么其参数需要重新学习，这种现象叫做<strong>内部协变量偏移（Internal Covariate Shift）</strong>。</p><blockquote><p>这里补充下机器学习中的协变量偏移（Covariate Shift）。协变量是一个统计学的概念，是影响预测结果的统计变量。在机器学习中，协变量可以看作是输入，一般的机器学习都要求输入在训练集和测试集上的分布是相似的，如果不满足这个假设，在训练集上得到的模型在测试集上表现就会比较差。<br><img src="https://img-blog.csdnimg.cn/20190919130030578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="协变量偏移"></p></blockquote><p>为了解决内部协变量偏移问题，需要对神经网络层的每一层输入做归一化，下面介绍几种常见的方法：</p><ul><li>批量归一化</li><li>层归一化</li><li>其他方法</li></ul><h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>为了减少内部协变量偏移的影响，需要对神经网络每一层的净输入$z^l$进行归一化，相当于每一层都要做一次数据预处理，从而加快收敛速度，但是因为对每一层都进行操作，所以要求归一化的效率要很高，一般使用标准归一化，将净输入$z^l$的每一维都归一到标准正态分布，其公式如下：</p><script type="math/tex; mode=display">\hat{z}^l = \frac{ z^l - E[z^l] }{ \sqrt{var(z^l) + \epsilon } }</script><p>$E[z^l]，var(z^l)$表示在当前参数下$z^l$的每一维在整个训练集上的期望和方差，因为深度神经网络采用的是下批量的梯度下降优化方法，基于全部样本计算期望和方差是不可能的，因为通常采用小批量进行估计。给定一个包含K个样本的集合，第$l$层神经元的净输入$z^{(1,l)},….z^{(K,l)}$的均值和方差为：</p><script type="math/tex; mode=display">\mu _\beta = \frac{1 }{ K } \sum_{k=1}^{K } z^{(k,l)}\\\sigma^2 _\beta = \frac{1 }{ K }\sum_{ k=1}^{K} (z^{(k,l)} - \mu _\beta)^2</script><p>对净输入$z^l$的标准归一化会使其取值集中在0附近，这样当使用sigmoid激活函数时，这个取值空间刚好接近线性变换的空间，减弱了神经网络的非线性性质。因此为了不使归一化对网络产生影响，需要对其进行缩放和平移处理，公式如下：</p><script type="math/tex; mode=display">\hat{z}^l = \frac{ z^l - \mu _\beta }{ \sqrt{\sigma^2 _\beta+ \epsilon } } \odot \gamma  + \beta</script><p>其中$\gamma  , \beta$分别代表缩放和平移的向量。</p><blockquote><p>这里需要注意的是每次小批量样本的均值和方差是净输入$z^l$的函数，而不是常量因此在计算梯度时要考虑到均值和方差产生的影响，当训练完成时，用整个数据集上的均值和方差来代替每次小样本计算得到的均值和方差。在实际实践经验中，小批量样本的均值和方差也可以使用移动平均来计算。</p></blockquote><h4 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h4><p>批量归一化的操作对象是单一神经元，因此要求选择样本批量的时候，不能太小，否则难以计算单个神经元的统计信息，另外一个神经元的输入是动态变化的，比如循环神经网络，那么就无法应用批量归一化操作。</p><p><strong>层归一化（Layer Normalization）</strong> 是和批量归一化非常类似的方法，但层归一化的操作对象是某层全部神经元。</p><p>对于深层神经网络，第$l$层神经元的净输入为$z^l$，其均值和方差为：</p><script type="math/tex; mode=display">u^l = \frac{1}{n^l} \sum_{i=1}^{ n^l} z_i^l\\\sigma ^2_l = \frac{1}{n^l} \sum_{i=1}^{ n^l} (z_i^l - u^l )</script><p>其中$n^l$为第$l$层神经元的数量。则层归一化定义为：</p><script type="math/tex; mode=display">\hat{z^l} = \frac{z^l - u^l }{ \sqrt {\sigma^2 _l + \epsilon } } \odot \gamma  + \beta</script><p>其中$\gamma ,\beta$分别代表缩放和平移的向量，和$z^l$的维度相同。</p><p><strong>循环神经网络中的层归一化</strong>为：</p><script type="math/tex; mode=display">z_t = U h_{t-1} + W x_t\\h_t = f(\hat{z^l})</script><p>其中隐藏层为$h_t$，$x_t$为第$t$时刻的净输入，$U,W$为参数。</p><blockquote><p>在标准循环网络中，循环神经层的输入一般就随着时间慢慢变大或者变小，从而引起梯度爆炸或者梯度消失，而层归一化的神经网络可以有效的缓解这种状况。</p></blockquote><h4 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h4><p>除了上面介绍的两种归一化方法之外，还有一些其他的一些归一化方法，感兴趣的可以自行搜索查看。</p><ul><li>权重归一化(Weight Normalization)</li><li>局部响应归一化</li></ul><blockquote><p>至此，神经网络中的优化方法第二部分介绍完成，主要包好了三部分内容：参数初始化，数据预处理和逐层归一化。再下一篇将会重点介绍超参数优化的方法不仅适用于深度神经网络，也适用于一般的机器学习任务。如果你觉得不错，分享一下吧！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一篇包括&lt;ul&gt;
&lt;li&gt;网络优化和正则
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向</title>
    <link href="http://thinkgamer.cn/2019/09/22/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E5%92%8C%E5%8A%A8%E6%80%81%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91/"/>
    <id>http://thinkgamer.cn/2019/09/22/TensorFlow/神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向/</id>
    <published>2019-09-22T13:25:01.000Z</published>
    <updated>2019-10-14T12:28:36.841Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化</li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化 </li></ul></li></ul><hr><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>虽然神经网络有比较强的表达能力，但是应用神经网络到机器学习任务时仍存在一些问题，主要分为：</p><ol><li>网络优化<blockquote><p>神经网络模型是一个非凸函数，再加上神经网络中的梯度消失和梯度爆炸，很难进行优化，另外网络的参数比较多，且数据量比较大导致训练效率比较低。</p></blockquote></li><li>正则化<blockquote><p>神经网络拟合能力强，容易在训练集上产生过拟合，需要一些正则化的方法来提高网络的泛化能力。</p></blockquote></li></ol><p>从大量的实践经验看主要是从网络优化和正则化两个方面提高学习效率并得到一个好的网络模型。</p><p>在低维空间的非凸优化问题中主要是存在一些局部最优点，基于梯度下降优化算法会陷入局部最优点，因此低维空间的非凸优化的难点在于如何选择合适的参数和逃离局部最优点。</p><p>深层神经网络中参数较多，其是在高维空间的非凸优化问题中，和低维空间的非凸优化有些不同，其主要难点在于如何逃离鞍点（Saddle Point），鞍点的梯度为0，但是在一些维度上是最高点，在另一些维度上是最低点，如下图所示（图1-1）：<br><img src="https://img-blog.csdnimg.cn/2019091814412421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="鞍点示例"></p><p>在高维空间中，局部最优点要求在每一维度上都是最低点，这种概率很低，假设网络有1000<br>个参数，每一维上取得局部最优点的最小概率为p，则在整个参数空间中取得局部最优点的最小概率为$p^{1000}$，这种概率很小，也就是说在整个参数空间中，大部分梯度为0的点都是鞍点。</p><hr><h3 id="优化算法介绍"><a href="#优化算法介绍" class="headerlink" title="优化算法介绍"></a>优化算法介绍</h3><p>深层神经网络的参数学习主要是通过梯度下降算法寻找一组最小结构的风险参数，梯度下降分为：</p><ul><li>批量梯度下降</li><li>随机梯度下降</li><li>小批量梯度下降</li></ul><p>根据不同的数据量和参数量，可以选择一种合适的梯度下降优化算法，除了在收敛效果和效率上的区别，这三种梯度下降优化算法还存在一些共同问题（<strong>具体会在下一篇进行详细介绍</strong>）：</p><ul><li>如何初始化参数</li><li>预处理数据</li><li>如何选择合适的学习率，避免陷入局部最优</li></ul><p>在训练深层神经网络时，通常采用小批量梯度下降算法。令$f(x,\theta)$为一个深层神经网络，$\theta$为网络参数，使用小批量梯度优化算法时，每次选择K个训练样本$I_t =\left \{ (x^t,y^t)  \right \} , t \in (1,T)$，第t次迭代时损失函数关于$\theta$的偏导数为（公式1-1）：</p><script type="math/tex; mode=display">g_t(\theta ) = \frac{ 1 }{ K } \sum_{ (x^t,y^t) \in I_t} \frac{ \partial L(y^t,f(x^t, \theta)) }{ \partial \theta }</script><p>第t次更新的梯度$g’_t$定义为（公式1-2）：</p><script type="math/tex; mode=display">g_t'(\theta)= g_t(\theta_{t-1})</script><p>使用梯度下降来更新参数（公式1-3）：</p><script type="math/tex; mode=display">\theta_t = \theta_{t-1} - \alpha g'(\theta)</script><p>一般批量较小时，需要选择较小的学习率，否则模型不会收敛。下图（图1-2）给出了在Mnist数据集上批量大小对梯度的影响。从图1-2(a)可以看出，批量大小设置的越大，下降的越明显，并且下降的比较平滑，当选择批量的大小为1时，整体损失呈下降趋势，但是局部比较震荡。从图1-2(b)可以看出，如果按整个数据集上的迭代次数（Epoch）来看损失变化情况，则是批量样本数越小，下降效果越明显。</p><p><img src="https://img-blog.csdnimg.cn/20190918153354525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="批量的大小对梯度的影响"></p><p>为了更加有效的训练深层神经网络，在标准的小批量梯度下降算法中，经常使用一些改进方法加快优化速度，常见的改进方法有两种：</p><ul><li>学习率衰减</li><li>梯度方向优化</li></ul><blockquote><p>这些改进的优化方法也同样可以应用在批量梯度下降算法和随机梯度下降算法。</p></blockquote><hr><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>在梯度下降中，学习率的设置很重要，设置过大，则不会收敛，设置过小，则收敛太慢。从经验上看，学习率在一开始要设置的大些来保证收敛速度，在收敛到局部最优点附近时要小些来避免震荡，因此比较简单的学习率调整可以通过学习率衰减（Learning Rate Decay）的方式来实现。假设初始学习率为$\alpha_0$，第t次迭代的学习率为$a_t$，常用的衰减方式为按照迭代次数进行衰减，例如</p><ul><li>逆时衰减（公式1-4）</li></ul><script type="math/tex; mode=display">a_t = a_0 \frac{1 }{ 1 + \beta t}</script><ul><li>指数衰减（公式1-5）</li></ul><script type="math/tex; mode=display">a_t = a_0\beta^t</script><ul><li>自然指数衰减（公式1-6）</li></ul><script type="math/tex; mode=display">a_t = a_0 exp(-\beta * t)</script><p>其中$\beta$为衰减率，一般为0.96</p><h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>AdaGrad（Adaptive Gradient）算法是借鉴L2正则化的思想，每次迭代时自适应的调整每个参数的学习率。AdaGrad的参数更新公式为（公式1-7）：</p><script type="math/tex; mode=display">G_t = \sum_{t=1}^{T} g_t \odot g_t\\\bigtriangleup \theta_t = - \frac{\alpha }{ \sqrt{G_t + \epsilon  } } \odot g_t\\g'_t(\theta) = g_t(\theta_{t-1}) + \bigtriangleup \theta_t</script><p>其中$\alpha$为学习率，$\epsilon$是为了保证数据稳定性而设置的非常小的常数，一般取值是 $e^{-7}$到$e^{-10}$，这里的开平方，加，除运算都是按照元素进行的操作。</p><p>在AdaGrad算法中，如果某个参数的偏导数累积比较大，其学习率相对较小，相反，如果其偏导数累积比较大，其学习率相对较大。但是整体上随着迭代次数的增加，学习率逐渐减小。</p><p>AdaGrad算法的缺点是在经过一定次数的迭代后依然没有找到最优点，由于这时候的学习率已经很小了，就很难找到最优点。</p><h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>RMSProp是Geoff Hinton提出的一种自适应学习率的方法，可以在有些情况下避免AdaGrad的学习率单调递减以至于过早衰减的缺点。</p><p>RMSProp算法首先计算的是每次迭代速度$g_t$平方的指数衰减移动平均，如下所示（公式1-8）：</p><script type="math/tex; mode=display">G_t = \beta G_{t-1}  + (1-\beta) g_t \odot g_t = (1- \beta) \sum_{t=1}^{T} \beta ^{T-t} g_t \odot g_t</script><p>其中$\beta$为衰减率，一般取值为0.9，RMSProp算法参数更新公式为（公式1-9）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = - \frac{\alpha }{ \sqrt{G_t + \epsilon  } } \odot g_t\\g'_t(\theta) = g_t(\theta_{t-1}) + \bigtriangleup \theta_t</script><p>其中$\alpha$为学习率，通常为0.001。</p><blockquote><p>从公式1-8 可以看出，RMSProp和AdaGrad的区别在于$G_t$的计算由累积方式变成了指数衰减移动平均，在迭代过程中，每个参数的学习率并不是呈衰减趋势，即可以变大，也可以变小。</p></blockquote><h4 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h4><p>AdaDelta算法也是AdaGrad算法的一个改进，和RMSProp算法类似，AdaDelta算法通过梯度平方的指数衰减移动平均来调整学习率，除此之外，AdaDelta算法还引入了每次参数更新差$\bigtriangleup \theta$的平方的指数衰减移动平均。</p><p>第t次迭代时，每次参数更新差$\bigtriangleup \theta_t , 1&lt;t&lt;T-1$的指数衰减移动平均为（公式1-10）：</p><script type="math/tex; mode=display">\bigtriangleup X^2_{t-1} =\beta _1 \bigtriangleup X^2_{t-2} + (1 - \beta) \bigtriangleup \theta_{t-1} \odot \bigtriangleup \theta_{t-1}</script><p>其中$\beta_1$为衰减率，AdaDelta算法的参数更新差值为（公式1-11）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = - \frac{\sqrt {\bigtriangleup X^2_{t-1} + \epsilon }}{ \sqrt {G_t + \epsilon}} g_t</script><blockquote><p>其中$G_t$的计算方式和RMSProp算法一样。从公式1-11可以看出，AdaDelta算法将RMSProp算法中的初始学习率$\alpha$改为动态计算的$\sqrt {\bigtriangleup X^2_{t-1} + \epsilon }$，在一定程度上减缓了学习旅率的波动。</p></blockquote><hr><h3 id="梯度方向优化"><a href="#梯度方向优化" class="headerlink" title="梯度方向优化"></a>梯度方向优化</h3><p>除了调整学习率外，还可以使用最近一段时间内的平均梯度来代替当前时刻的梯度来作为参数的更新方向，从图1-2中可以看出，在小批量梯度下降中，如果每次选取样本数量比较小，损失就会呈现震荡的方式下降，有效的缓解梯度下降中的震荡的方式是通过用梯度的移动平均来代替每次的实际梯度。并提高优化速度，这就是<strong>动量法</strong>。</p><h4 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h4><p>动量法（Momentum Method）是用之前积累的动量来替代真正的梯度，每次替代的梯度可以看作是加速度。</p><p>在第t次迭代时，计算负梯度的“加权移动平均”作为参数的更新方向，如下所示（公式1-12）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = \rho \bigtriangleup \theta_{t-1}-\alpha g_t</script><p>其中$\rho$为动量因子，通常设置为0.9，$\alpha$为学习率。</p><blockquote><p>参数的实际更新值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内梯度方向不一致时，参数更新的幅度变小，相反，参数更新的幅度变大，起到加速的作用。</p><p>一般而言，在迭代初期，梯度的更新方向比较一致，动量法会起到加速作用，可以更快的起到加速的作用，可以更快的到达最优点，在迭代后期，梯度的更新方向不一致，在收敛时比较动荡，动量法会起到减速作用，增加稳定性。从某种程度来讲，当前梯度叠加上部分的上次梯度，一定程度上可以看作二次梯度。</p></blockquote><h4 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h4><p>Nesterov加速梯度（Nesterov Accelerated Gradient， NAG）也叫Nesterov动量法（Nesterov Momentum），是一种对动量法的改进。</p><p>在动量法中，实际的参数更新方向$\bigtriangleup \theta_t$为上一步的参数更新方向$\bigtriangleup \theta_{t-1}$和当前的梯度 $-g_t$的叠加，这样，$\bigtriangleup \theta_t$可以拆分为两步进行，先根据$\bigtriangleup \theta_{t-1}$更新一次得到参数$\tilde{\theta }$，再用$g_t$进行更新，如下所示（公式1-13）：</p><script type="math/tex; mode=display">\tilde{\theta } = \theta_{t-1} + \rho \bigtriangleup \theta_{t-1} \\\theta_t = \tilde{\theta } - \alpha g_t</script><p>其中$g_t$为点$\theta_{t-1}$上的梯度，所以第二步不太合理，更合理的更新方向为$\tilde{\theta }$上的梯度，这样合并后的更新方向为（公式1-14）：</p><script type="math/tex; mode=display"> \bigtriangleup \theta_t =  \rho \bigtriangleup \theta_{t-1} -\alpha g_t(\theta_{t-1} + \rho \bigtriangleup \theta_{t-1} )</script><p>其中$g_t(\theta_{t-1} + \rho \bigtriangleup \theta_{t-1} )$表示损失函数在$\tilde{\theta } = \theta_{t-1} + \rho \bigtriangleup \theta_{t-1}$上的偏导数。</p><p>下图（图1-3）给出了动量法和 Nesterov 加速梯度在参数更新时的比较：</p><p><img src="https://img-blog.csdnimg.cn/20190918192321930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="动量法和 Nesterov 加速梯度在参数更新时的比较"></p><h4 id="AdaM算法"><a href="#AdaM算法" class="headerlink" title="AdaM算法"></a>AdaM算法</h4><p>自适应动量估计算法（Adaptive Moment Estimation，Adam）可以看作是动量法和RMSprop的结合，不但使用动量作为参数更新，而且可以自适应调整学习率（公式1-15）。</p><script type="math/tex; mode=display">M_t = \beta _1M_{t-1} + (1-\beta _1)g_t\\ G_t = \beta _2 G_{t-1} + (1-\beta _2)g_t \odot g_t</script><p>其中$\beta_1 ，\beta_2$分别为两个移动平均的衰减率，通常取值：$\beta_1=0.9,\beta_2=0.99$。</p><p>$M_t$可以看作是梯度的均值（一阶矩），$G_t$可以看作是梯度的未减去均值的方差（二阶矩）。</p><p>假设$M_t =0,G_t=0$，那么在迭代初期，$M_t，G_t$的值会比真实的均值和方差要小，特别是当$\beta_1 ，\beta_2$都接近1时，偏差会很大，因此需要对偏差进行修正，如下所示（公式1-16）：</p><script type="math/tex; mode=display">\tilde{M_t} = \frac{M_t}{ 1 - \beta^t _1}\\\tilde{G_t} = \frac{G_t}{ 1 - \beta^t _2}</script><p>Adam算法的更新差值为（公式1-17）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = - \frac{\alpha }{\sqrt{ \tilde{G_t} + \varepsilon  }} \tilde{M_t}</script><p>其中学习率$\alpha$通常设置为0.001，并且也可以进行衰减，比如$a_t = \frac{a_0} { \sqrt{t}}$。</p><blockquote><p>Adam算法是RMSprop与动量法的结合，因此一种自然的Adam改进方法是引入Nesterov加速梯度，称为Nadam算法。</p></blockquote><h4 id="梯度截断"><a href="#梯度截断" class="headerlink" title="梯度截断"></a>梯度截断</h4><p>在深层神经网络或者循环网络中，除了梯度消失之外，梯度爆炸是影响学习效率的主要隐私，在基于梯度下降的优化过程中，如果梯度突然增大，用较大的梯度更新参数，反而会使结果远离最优点，为了避免这种情况，当梯度达到一定值的时候，要进行梯度截断（gradient clipping）。</p><p>梯度截断是一种比较简单的启发式方法，把梯度的模限定在一个范围内，当梯度的模大于或者小于某个区间时，就进行截断，一般截断的方式有以下几种：</p><ul><li>按值截断</li></ul><p>在第t次迭代时，梯度为$g_t$，给的一个区间[a,b]，如果梯度小于a时，令其为a，大于b时，令其为b。</p><ul><li>按模截断<br>将梯度的模截断到一个给定的截断阈值b。如果$||g_t||^2 \leq b$保持梯度不变，如果$||g_t||^2 &gt; b$，则$g_t= \frac{ b}{||g_t||} g_t$。</li></ul><p>截断阈值 b 是一个超参数,也可以根据一段时间内的平均梯度来自动调整。实验中发现,训练过程对阈值 b 并不十分敏感,通常一个小的阈值就可以得到很好的结果。</p><blockquote><p>在训练循环神经网络时，按模截断是避免梯度爆炸的有效方法。</p></blockquote><hr><h3 id="优化算法总结"><a href="#优化算法总结" class="headerlink" title="优化算法总结"></a>优化算法总结</h3><p>本文介绍了神经网络中的网络优化和正则化概述，以及网络优化中的加快网络优化的两种方法，这些方法大体分为两类：</p><ul><li>调整学习率，使得优化更稳定<blockquote><p>比如：AdaGrad，RMSprop，AdaDelta</p></blockquote></li><li>调整梯度方向，优化训练速度<blockquote><p>比如：动量法，Nesterov加速梯度，梯度截断</p></blockquote></li></ul><p>Adam则是RMSprop 和 动量法的结合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一篇包括&lt;ul&gt;
&lt;li&gt;网络优化和正则
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(3)-循环神经网络（下）篇</title>
    <link href="http://thinkgamer.cn/2019/09/18/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(3)-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89%E7%AF%87/"/>
    <id>http://thinkgamer.cn/2019/09/18/TensorFlow/常见的五种神经网络(3)-循环神经网络（下）篇/</id>
    <published>2019-09-18T00:14:54.000Z</published>
    <updated>2019-10-14T12:28:36.839Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/100943664</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>常见的五种神经网络系列第三篇，主要介绍循环神经网络，由于循环神经网络包含的内容过多，分为上中下三篇进行介绍，本文主要是循环神经网络（下）篇，主要介绍以下内容：</p><ul><li>长短期记忆网络（LSTM）</li><li>门控循环单元网络（GRU）</li><li>递归循环神经网络（RecNN）</li><li>图网络（GN）</li></ul><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><hr><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>长短期记忆（Long Short-Term Memory，LSTM）网络是循环神经网络的一个变体，可以有效的解决简单循环神经网络的梯度爆炸和梯度消失问题。</p><p>LSTM的改进包含两点：</p><ul><li>新的内部状态</li><li>门机制</li></ul><h4 id="新的内部状态"><a href="#新的内部状态" class="headerlink" title="新的内部状态"></a>新的内部状态</h4><p>LSTM网络引入一个新的内部状态（internal state）$c_t$专门进行线性的循环传递，同时（非线性）输出信息给隐藏层的外部状态$h_t$（公式3-1）。</p><script type="math/tex; mode=display">c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t}\\h_t = o_t \odot tanh(c_t)</script><p>其中 $f_t$，$i_t$，$o_t$为三个门来控制信息传递的路径，$\odot$为向量元素乘积，$c_{t-1}$为上一时刻的记忆单元，$\tilde{c_t}$是通过非线性函数得到的候选状态（公式3-2）:</p><script type="math/tex; mode=display">\tilde{c_t} = tanh( W_c x_t + U_c h_{t-1} + b_c )</script><p>在每个时刻t，LSTM网络的内部状态$c_t$记录了到当前时刻为止的历史信息。</p><h4 id="门机制"><a href="#门机制" class="headerlink" title="门机制"></a>门机制</h4><p>LSTM网络引入门机制来控制信息的传递， $f_t，i_t，o_t$分别为遗忘门，输入门，输出门。电路中门是0或1，表示关闭和开启，LSTM网络中的门是一种软门，取值在(0,1)，表示以一定比例的信息通过，其三个门的作用分别为：</p><ul><li>$f_t$：控制上一个时刻的内部状态 $c_{t-1}$需要遗忘多少信息 </li><li>$i_t$：控制当前时刻的候选状态$\tilde{c_t}$有多少信息需要保存</li><li>$o_t$：控制当前时刻的状态$c_t$有多少信息需要输出为$h_t$</li></ul><p>三个门的计算如下（公式3-3）：</p><script type="math/tex; mode=display">i_t=\sigma (W_i x_t+U_i h_{t-1} + b_i)\\f_t=\sigma (W_f x_t+U_f h_{t-1}+ b_f )\\o_t=\sigma (W_o x_t+U_o h_{t-1}+b_o)</script><p>其中$\sigma$为logsitic函数，其输出区间为(0,1)，$x_t$为当前输入，$h_{t-1}$为上一时刻的外部状态。 </p><p>下图（图3-1）给出了LSTM的循环单元结构，其计算分为三个过程：</p><ol><li>利用当前时刻的输入$x_t$和上一时刻的外部状态$h_{t-1}$计算出三个门和候选状态$\tilde{c_t}$</li><li>结合遗忘门$f_t$和输入门$i_t$来更新记忆单元$c_t$</li><li>结合输出门$o_t$将内部状态信息传递给外部状态$h_t$</li></ol><p><img src="https://img-blog.csdnimg.cn/20190917210304438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="LSTM循环单元结构"></p><p>通过LSTM循环单元，整个网络可以建立长距离的时序依赖关系，公式3-1～3-3可以简单的描述为（公式3-4）：</p><script type="math/tex; mode=display">\begin{bmatrix}\tilde{c_t}\\ o_t\\ i_t\\ f_t\end{bmatrix} = \begin{bmatrix}tanh\\ \sigma \\ \sigma \\ \sigma \end{bmatrix} (W\begin{bmatrix}x_t\\ h_{t-1}\end{bmatrix} + b)\\c_t = f_t \odot c_{t-1}+ i_t \odot \tilde{c_t}\\h_t = o_t  \odot tanh(c_t)</script><p>其中$x_t$为当前时刻的输入，$W$和$b$为网络参数。</p><blockquote><p>循环神经网络中的隐状态h存储了历史信息，可以看作是一种记忆（Memeory）。在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作一种短期记忆（Short-term Memeory）。在神经网络中，长期记忆（Long-term Memory）可以看作是网格参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆。而在LSTM网络中，记忆单元c可以在某个时刻捕捉到某个关键信息，并有能力将该信息保存一段时间，记忆单元c中保存的信息要远远长于隐状态h，但又远远短于长期记忆，因此被成为长短期记忆网络（Long Short-term Memory）。</p></blockquote><hr><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>门控单元（Gate Recurrent Unit，GRU）网络是一种比LSTM更加简单的循环神经网络。在LSTM中遗忘门和输入门是互补关系，比较冗余，GRU将遗忘门和输入门合并成一个门：更新门。同时GRU也不引入额外的记忆单元，直接在当前的状态$h_t$和上一个时刻的状态$h_{t-1}$之间引入线性依赖关系。</p><p>在GRU网络中，当前时刻的候选状态$\tilde{h_t}$为（公式3-5）：</p><script type="math/tex; mode=display">\tilde{h_t} = tanh( W_h x_h + U_h(r_t\odot h_{t-1}) + b_h )</script><blockquote><p>计算$\tilde{h_t}$时，选用tanh激活函数是因为其导数有比较大的值域，缓解梯度消失问题。</p></blockquote><p>其中$r_t \in [0,1]$ 为重置门（reset gate），用来控制候选状态$\tilde {h_t}$的计算是否依赖上一时刻的状态$h_{t-1}$，公式如下（公式3-6）：</p><script type="math/tex; mode=display">r_t = \sigma ( W_r x_t  + U_r h_{t-1} + b_r)</script><p>当 $r_t$为0 时，候选状态$\tilde{h_t}$只和当前输入$x_t$有关，和历史状态无关，当$r_t$为1时，候选状态$\tilde{h_t}$和当前输入$x_t$，历史状态$h_{t-1}$都有关，和简单循环网络一致。</p><p>GRU网络隐状态$h_t$的更新方式为（公式3-7）：</p><script type="math/tex; mode=display">h_t = z_t \odot h_{t-1}+ (1-z_t) \odot \tilde {h_t}</script><p>其中$z \in [0,1]$为更新门（update gate），用来控制当前状态需要从历史状态中保留多少信息（不经过非线性变换），以及需要从候选状态中获取多少信息。$z_t$公式如下（公式3-8）：</p><script type="math/tex; mode=display">z_t = \sigma (W_z x_t + U_z h_{t-1} + b_z)</script><ul><li>若$z_t=0$，当前状态$h_t$和历史状态$h_{t-1}$之间为非线性函数。</li><li>若$z_t=0，r=1$，GRU退化为简单循环网络</li><li>若$z_t=0，r=0$，当前状态$h_t$只和当前输入$x_t$有关，和历史状态$h_{t-1}$无关</li><li>若$z_t=1$，当前时刻状态$h_t=h_{t-1}$，和当前输入$x_t$无关</li></ul><p>GRU网络循环单元结构如下（图3-2）：</p><p><img src="https://img-blog.csdnimg.cn/20190917231241261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="GRU网络循环单元结构"></p><hr><h3 id="RecNN"><a href="#RecNN" class="headerlink" title="RecNN"></a>RecNN</h3><blockquote><p>如果将循环神经网络按时间展开，每个时刻的隐状态$h_t$看做是一个节点，那么这些节点构成一个链式结构，而链式结构是一种特殊的图结构，很容易将这种消息传递的思想扩展到任意的图结构上。</p></blockquote><p>递归神经网络（Recursive Neurnal Network，RecNN）是循环神经网络在有向无循环图上的控制，递归神经网络一般结构为树状的层次结构，如下图所示（图3-3）：</p><p><img src="https://img-blog.csdnimg.cn/20190917232047624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="递归神经网络"></p><p>以上图(a)为例，包含3个隐藏层$h_1,h_2,h_3$，其中$h_1$由两个输入$x_1,x_2$计算得到，$h_2$由两个输入$x_3,x_4$计算得到，$h_3$由两个隐藏层$h_1,h_2$计算得到。</p><p>对于一个节点$h_i$，它可以接受来自子节点集合$\pi_i$中所有节点的消息，并更新自己的状态，如下所示（公式3-9）：</p><script type="math/tex; mode=display">h_i = f(h_{\pi_i})</script><p>其中$h_{\pi_i}$表示$\pi_i$集合中所有节点状态的拼接，$f(.)$是一个和节点状态无关的非线性函数，可以为一个单层的前馈神经网络，比如图3-3(a)所表示的递归神经网络可以表示为（公式3-10）：</p><script type="math/tex; mode=display">h_1 = \sigma (W \begin{bmatrix}x_1\\ x_2\end{bmatrix}+ b) \\h_2 = \sigma (W \begin{bmatrix}x_3\\ x_4\end{bmatrix}+ b) \\h_3 = \sigma (W \begin{bmatrix}h_1\\ h_2\end{bmatrix}+ b)</script><p>其中$\sigma$表示非线性激活函数，W和b为可学习的参数，同样输出层y可以为一个分类器，比如（公式3-11）：</p><script type="math/tex; mode=display">h_3 = g (W' \begin{bmatrix}h_1\\ h_2\end{bmatrix}+ b')</script><p>其中$g(.)$为分类器，$W’$和$b’$为分类器的参数。当递归神经网络的结构退化为图3-3(b)时，就等价于简单神经循环网络。</p><p>递归神经网络主要用来建模自然语言句子的语义，给定一个句子的语法结构，可以使用递归神经网络来按照句法的组合关系来合成一个句子的语义，句子中每个短语成分可以分成一些子成分，即每个短语的语义可以由它的子成分语义组合而来，进而合成整句的语义。</p><p>同样也可以使用门机制来改进递归神经网络中的长距离依赖问题，比如树结构的长短期记忆模型就是将LSTM的思想应用到树结构的网络中，来实现更灵活的组合函数。</p><hr><h3 id="GN"><a href="#GN" class="headerlink" title="GN"></a>GN</h3><p>在实际应用中，很多数据是图结构的，比如知识图谱，社交网络，分子网络等。而前馈网络和反馈网络很难处理图结构的数据。</p><p><strong>图网络（Graph Network，GN）</strong>是将消息传递的思想扩展到图结构数据上的神经网络。</p><p>对于一个图结构$G(V,\varepsilon )$，其中$V$表示节点结合，$\varepsilon$表示边集合。每条边表示两个节点之间的依赖关系，节点之间的连接可以是有向的，也可以是无向的。图中每个节点v都用一组神经元来表示其状态$h^{(v)}$ ，初始状态可以为节点v的输入特征$x^{(v)}$，每个节点接受相邻节点的信息，来更新自己的状态，如下所示（公式3-12）：</p><script type="math/tex; mode=display">m^{(v)}_t = \sum_{u \in N(v)} f( h^{(v)}_{t-1},h^{(u)}_{t-1},e^{(u,v)} )\\h^{(v)}_t = g(h^{(v)}_{t-1},m^{(u)}_t)</script><p>其中$N(v)$表示节点v的邻居节点，$m^{(v)}_t$ 表示在t时刻节点v接受到的信息，$e^{(u,v)}$为边(v,u)上的特征。</p><p>公式3-12是一种同步更新方式，所有结构同时接受信息并更新自己的状态，而对于有向图来说，使用异步的更新方式会更有效率，比如循环神经网络或者递归神经网络，在整个图更新T次后，可以通过一个读出函数g(.)来得到整个网络的表示。</p><blockquote><p>至此，循环神经网络（上）（中）（下）篇已经介绍完毕。</p></blockquote><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100943664&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(3)-循环神经网络（中）篇</title>
    <link href="http://thinkgamer.cn/2019/09/11/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(3)-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%AD%EF%BC%89%E7%AF%87/"/>
    <id>http://thinkgamer.cn/2019/09/11/TensorFlow/常见的五种神经网络(3)-循环神经网络（中）篇/</id>
    <published>2019-09-10T17:16:12.000Z</published>
    <updated>2019-10-14T12:28:36.840Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/100709422" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/100709422</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>常见的五种神经网络系列第三种，主要介绍循环神经网络，分为上中下三篇进行介绍，本文主为（中）篇，涉及内容如下：</p><ul><li>循环神经网络中的参数学习</li><li>RNN中的长期依赖问题</li><li>常见的循环神经网络结构</li></ul><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><hr><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p>循环神经网络的参数可以通过梯度下降方法来学习。给定一个样本(x,y)，其中$x_{1:T}=(x_1, x_2, … ,x_T)$为长度是T的输入序列，其中$y_{1:T}=(y_1, y_2, … ,y_T)$是长度为T的标签序列，在每个时刻t，都有一个监督信息$y_t$，定义时刻t的损失函数为（公式1-1）：</p><script type="math/tex; mode=display">L_t = L(y_t, g(h_t))</script><p>其中$g(h_t)$为第t时刻的输出，L为可微分的损失函数，比如交叉熵，整个序列上的损失函数为（公式1-2）：</p><script type="math/tex; mode=display">L = \sum_{t=1}^{T} L_t</script><p>整个序列的损失函数L关于参数U的梯度为（公式1-3）：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial U} = \sum _{t=1}^{T}\frac{\partial L_t}{ \partial U }</script><p>即每个时刻的损失函数$L_t$对参数U的偏导数之和。</p><p>在循环神经网络中主要有两种计算梯度的方式：</p><ul><li>随时间反向传播算法（Backpropagation Through Time，BRTT）</li><li>实时循环学习（Real-Time Recurrent Learning，RTRL）</li></ul><h4 id="随时间反向传播算法"><a href="#随时间反向传播算法" class="headerlink" title="随时间反向传播算法"></a>随时间反向传播算法</h4><p>主要通过类似前馈神经网络的错误反向传播算法来进行计算梯度。随时间反向传播算法将循环神经网络看作是一个展开的多层前馈网络，其中“每一层”对应循环网络中的每个时刻，这样循环神经网络就可以按照前馈神经网络中的反向传播算法来计算梯度。与前馈神经网络不同的是，循环神经网络中各层的参数是共享的，因此参数的真实梯度是各个层的参数梯度之和。</p><p>先计算公式1-3中第t时刻损失对参数U的偏导数 $\frac {\partial L_t}{\partial U}$，参数U和每个时刻k的净输入$z_k = Uh_{k-1} + Wx_{k} + b$有关，因此第t个时刻损失函数$L_t$关于参数$U_ij$的梯度为（公式1-4）：</p><script type="math/tex; mode=display">\frac{\partial L_t}{ \partial U_{ij}} = \sum_{k=1}^{t} tr( ( \frac{\partial L_t}{ \partial z_k} )^T \frac{\partial^+ z_k}{ \partial U_{ij}}    )\\= \sum_{k=1}^{t}  ( \frac{\partial^+ z_k}{ \partial U_{ij}}    )^T  \frac{\partial L_t}{ \partial z_k}</script><p>其中$\frac{\partial^+ z_k}{ \partial U_{ij}}$ 表示“直接”偏导数，即公式$z_k = Uh_{k-1} + Wx_{k} + b$中保持$h_{k-1}$不变，对$U_{ij}$进行求偏导数，得到（公式1-5）：</p><script type="math/tex; mode=display"> \frac{\partial^+ z_k}{ \partial U_{ij}}  = \begin{bmatrix}0\\ ... \\ [h_{k-1}]_j \\ ... \\ 0\end{bmatrix} \triangleq I_i([h_{k-1}]_j)</script><p>其中$[h_{k-1}]_j$为第$k-1$时刻隐状态的第j维，$I_i(x)$除了第j行值为x，之外全为0的向量。</p><p>定义$\delta _{t,k} = \frac{\partial L_t}{ \partial z_k }$为第t时刻损失函数对第k时刻隐藏层神经元净输入$z_k$的导数，则（公式1-6）：</p><script type="math/tex; mode=display">\delta _{t,k} = \frac{\partial L_t}{ \partial z_k }\\= \frac{ \partial h_k }{ \partial z_k} \frac{\partial z_{k+1}}{ \partial h_k } \frac{ \partial L_t }{ \partial z_{k+1} }\\= diag(f'(z_k))U^T \delta _{t,k+1}</script><p>将（公式1-6） 和 （公式 1-5） 代入（公式1-4）得到（公式1-7）：</p><script type="math/tex; mode=display">\frac{\partial L_t}{ \partial U_{ij} } = \sum_{k=1}^{ t } [\delta _{t,k}]_i [h_{k-1}]_j</script><p>将（公式1-7）写成矩阵形式为（公式1-8）：</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial U } = \sum_{k=1}^{ t } \delta _{t,k} h^T_{k-1}</script><p>下图为随时间反向传播算法示例：</p><p><img src="https://img-blog.csdnimg.cn/20190910191047315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="随时间反向传播算法示例"></p><p>将（公式1-8）代入（公式1-3）得到整个序列的损失函数$L$关于参数$U$的梯度（公式1-9）:</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial U } = \sum_{t=1}^{ T}\sum_{k=1}^{ t } \delta _{t,k} h^T_{k-1}</script><p>同理可得到$L$关于参数$W$的梯度（公式1-10）：</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial W } = \sum_{t=1}^{ T}\sum_{k=1}^{ t } \delta _{t,k} x^T_k</script><p>$L$关于参数$b$的梯度（公式1-11）：</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial b } = \sum_{t=1}^{ T}\sum_{k=1}^{ t } \delta _{t,k}</script><blockquote><p>在 随时间反向传播算法中，参数的梯度需要在一个完整的“向前”计算和“向后”计算后才能得到并参数更新。</p></blockquote><h4 id="实时循环学习"><a href="#实时循环学习" class="headerlink" title="实时循环学习"></a>实时循环学习</h4><p>与随时间反向传播算法不同的是：实时循环学习（Real-Time Recurrent Learning）是通过前向传播的方式来计算梯度。</p><p>假设RNN中第 $t+1$时刻的状态$h_{t+1}$为（公式1-12）：</p><script type="math/tex; mode=display">h_{t+1} = f(z_{t+1}) = f(Uh_k + Wx_{k+1} + b)</script><p>其关于参数$U_{ij$的偏导数为（公式1-13）：</p><script type="math/tex; mode=display">\frac{ h_{t+1} }{ \partial U_{ij} } = \frac{ \partial h_{t+1} }{ \partial z_{t+1} } ( \frac{ \partial^+z_{t+1} }{ \partial  U_{ij}}   + U \frac{ \partial h_t}{ \partial U_{ij} } )\\= diag( f'(z_{t+1}) ) ( I_i ([h_t]_j)+ U \frac{ \partial h_t}{ \partial U_{ij} }  )\\=f'(z_{t+1}) \odot  ( I_i ([h_t]_j)+ U \frac{ \partial h_t}{ \partial U_{ij} }  )</script><p>其中$I_i(x)$为除了第i行之外元素全为0的向量。</p><p>RTRL自从第一个时刻开始，除了计算RNN的隐状态之外，还利用（公式1-13）依次前向计算偏导数$\frac{\partial h_1}{ \partial U_{ij}},\frac{\partial h_2}{ \partial U_{ij}},\frac{\partial h_3}{ \partial U_{ij}}…$</p><p>这样假设第t个时刻存在一个监督信息，其损失函数为$L_t$，就可以同时计算损失函数对$U_{ij}$的偏导数（公式1-14）：</p><script type="math/tex; mode=display">\frac{\partial L_t}{ \partial U_{ij}} =( \frac{\partial h_t}{ \partial U_{ij} } )^T \frac{\partial L_t}{ \partial h_t}</script><p>这样在第t个时刻就可以实时计算$L_t$关于参数U的梯度，并更新参数。参数W和b的梯度也可以按照上述方法进行计算。</p><blockquote><p>两种算法比较：RTRL算法和BPTT算法都是基于梯度求解参数，分别通过前向模式和反向模式应用链式法则来计算梯度。在RNN中一般输出维度要比输入维度少，因此BPTT算法的计算量会很小，但要保存计算过程中的梯度值，空间复杂度较高。RTRL算法不需要进行空间回传，比较适合用在在线学习或无限序列的任务中。</p></blockquote><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>在BRTT算法中，将（公式1-6）展开得到（公式1-15）：</p><script type="math/tex; mode=display">\delta _{t,k}=\prod_{i=k}^{t-1} ( diag('f(z_i ))U^T  )\delta _{t,t}</script><p>如果定义$\gamma \approx || diag(‘f(z_i ))U^T   ||$，则（公式1-16）：</p><script type="math/tex; mode=display">\delta _{t,k}=\gamma ^{t-k} \delta _{t,t}</script><p>若$\gamma &gt;1$，当$t-k \rightarrow +\infty$，$\gamma ^{t-k} \rightarrow +\infty$，会造成系统不稳定，称之为梯度爆炸（Gradient Exploding Problem），反之，若$\gamma &lt; 1$，当$t-k \rightarrow +\infty$，$\gamma ^{t-k} \rightarrow 0$，会出现和前馈神经网络类似的梯度消失问题（Gradient Vanishing Problem）。</p><blockquote><p>注意：在循环神经网络中，梯度消失指的是并不是说$\frac{ \partial L_t}{ \partial U}$的梯度消失了，而是$\frac{ \partial L_t}{ \partial h_k}$的梯度消失，当$t-k$很大时，即参数U的更新主要靠最近的几个状态来更新，长距离的状态对参数U没有影响。</p></blockquote><p>当循环神经网络中使用的激活函数是Logistic或者tanh的时候，由于其导数小于1，并且权重矩阵$||U||$也不会太大，因此，如果时间间隔t-k过大的话，也会出现梯度消失问题。所以一般采用 ReLU激活函数（关于激活函数的介绍可参考：<a href="https://blog.csdn.net/Gamer_gyt/article/details/89440152" target="_blank" rel="external">神经网络中的激活函数介绍</a>）。</p><p>虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸和梯度消失问题，实际上只能学习到短期的依赖关系，这样如果t时刻的输出$y_t$依赖于$t-k$时刻的输入$x_{t-k}$，当间隔k比较大时，简单神经网络很难建模这种长距离的依赖关系，称之为长期依赖问题（Long-Term Dependences Problem）。</p><p>改进措施：</p><ul><li>选取合适的参数</li><li>使用非饱和的激活函数</li></ul><p>循环网络的梯度爆炸问题比较容易解决，一般通过梯度截断和权重衰减来避免。而梯度消失很难解决，通常是对模型进行调优来解决。</p><h3 id="常见的循环神经网络结构"><a href="#常见的循环神经网络结构" class="headerlink" title="常见的循环神经网络结构"></a>常见的循环神经网络结构</h3><p>主要包含四种：</p><ul><li>N：N</li><li>1：N</li><li>N：1</li><li>N：M</li></ul><h4 id="N比N结构"><a href="#N比N结构" class="headerlink" title="N比N结构"></a>N比N结构</h4><p>N维输入对应N维输出，大致结构如下所示：</p><p><img src="https://img-blog.csdnimg.cn/20190910211158381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比N结构"></p><p>其常常用于处理以下问题：</p><ul><li>视频理解中获取视频每一帧标签，输入为视频解码后的图像，通过此结构，获取每一 帧的标签信息。这种场景一般用作视频理解的初期，对视频做初步的处理后， 后续可以基于这些标签信息进行语义分析，构建更为复杂的需求场景。</li><li>股票价格预测。基于历史的股票信息输入，预测下一时刻或者未来的股票走势信息。</li></ul><h4 id="1比N结构"><a href="#1比N结构" class="headerlink" title="1比N结构"></a>1比N结构</h4><p>一维输入，N维输出，大致结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190910211546198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络1比N结构"></p><p>还有一种结构是在同一信息在不同时刻输入到网络中，如下所示：<br><img src="https://img-blog.csdnimg.cn/20190910211800194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络1比N结构"></p><p>其常常用于处理以下问题：</p><ul><li>看图写描述 : 根据输入的 一张图 片，生成对这张图片的描述信息 </li><li>自动作曲 : 按照类别生成音乐 </li></ul><h4 id="N比1结构"><a href="#N比1结构" class="headerlink" title="N比1结构"></a>N比1结构</h4><p>N维输入，一维输出，大致结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190910211426311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比1结构"></p><p>其常常用于处理以下问题：</p><ul><li>视频理解中的获取视频每个场景的描述信息，或者获取整个影片的摘要信息。 </li><li>获取用户评价的情感信息，即根据用户的一句话的评论，来判断用户的喜好等情感信息 。</li></ul><h4 id="N比M结构"><a href="#N比M结构" class="headerlink" title="N比M结构"></a>N比M结构</h4><p>N维输入，M维输出，这种结构又被称为Encoder-Decoder模型，也可以称为Seq2Seq模型，这种模型的输入和输出可以不相等，该模型由两部分组成：编码部分和解码部分，大致结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190910212035184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比M结构"></p><p> c的前半部分循环神经网络为编码部分，称之为Endcoder, c可以是s3的直接输出，或者 是对s3输出做一定的变换，也可以对编码部分所有的s1、s2、s3进行变换得到，这样c中就包 含了对X1、 Xz、码的编码信息 。c的后半部分循环神经网络为解码部分，称之为Decoder。c作为之前的状态编码，作为初始值，输入到Decoder当中。 Decoder经过循环处理，最终将信息解码输出。</p><p> 除了上边所示的解码结构外，还有下图所示的结构：<br> <img src="https://img-blog.csdnimg.cn/20190910212711689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比M结构"></p><p> N比M的循环神经网络结构更具有普遍性，现实环境中有很多基于该结构落地的场景，他可以解决如下问题：</p><ul><li>机器翻译：将不同语言作为输入，输出为非输入语言的类型，这也是Encoder-Decoder的经典用法</li><li>文本摘要：输入一篇文章，输出这篇文章的摘要信息</li><li>语音识别：输入一段语音，输出这段语音信息的文字</li></ul><blockquote><p>至此，循环神经网络（中）篇已经介绍完了，在下篇中会展开介绍更多的内容，欢迎关注。</p></blockquote><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100709422&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(3)-循环神经网络（上）篇</title>
    <link href="http://thinkgamer.cn/2019/09/08/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(3)-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89%E7%AF%87/"/>
    <id>http://thinkgamer.cn/2019/09/08/TensorFlow/常见的五种神经网络(3)-循环神经网络（上）篇/</id>
    <published>2019-09-08T08:37:58.000Z</published>
    <updated>2019-10-14T12:28:36.838Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/100600661" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/100600661</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>常见的五种神经网络系列第三种，主要介绍循环神经网络，由于循环神经网络包含的内容过多，分为上中下三篇进行介绍，本文主要是循环神经网络（上）篇，主要介绍以下内容：</p><ul><li>循环神经网络概述</li><li>如何给神经网络增加记忆能力<ul><li>延时神经网络</li><li>有外部输入的非线性自回归模型</li><li>循环神经网络</li></ul></li><li>一般的循环神经网络<ul><li>单向循环神经网络</li><li>双向循环神经网络</li><li>深度循环神经网络</li></ul></li><li>循环神经网络应用到机器学习任务</li></ul><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在前馈神经网络中，信息在神经元之间的传递是单向，网络的输出只依赖于当前的输入，这样限制虽然使网络变得容易学习，但是却减弱了网络的表达能力。在很多现实任务中，网络的输出不仅和当前的输入有关，也和过去一段时间的输出相关，比如一个<strong>有限状态自动机</strong>不仅和当前的输入有关，也和当前的状态（上一步的输出）有关。如下图（图-1）</p><p><img src="https://img-blog.csdnimg.cn/2019090716181761.png" alt="有限状态自动机"></p><blockquote><p>有限状态自动机称为FSM（finite state machine）或者FSA（finite state automaton）</p></blockquote><p>此外，前馈神经网络难以处理时序数据，比如视频，语音，文本等。因为时序数据的长度是不固定的，而前馈神经网络要求输入和输出的维度是固定的。因此当处理这种复杂的时序数据时，就需要一种表达能力能强的模型。</p><p><strong>循环神经网络(Recurrent Neural Network,RNN)</strong> 是一类具有短期记忆能力的神经网络，在循环神经网络中，神经元不仅可以接受其他神经元的信息，还可以接受自身的信息，形成一个环路结构。RNN的参数学习可以通过随时间反向传播算法进行学习（下文会具体介绍），随时间反向传播算法按照时间的逆序将错误信息一步步的向前传递，当输入序列时间较长时，会存在梯度消失和梯度爆炸问题（也叫长期依赖问题），为了解决这个问题，人们对RNN进行了许多改进，其中最有效的是引入门控制，比如长短期记忆网络（LSTM）和门控循环单元网络（GRU），将在（下）篇进行介绍。</p><h3 id="如何给网络增加记忆能力"><a href="#如何给网络增加记忆能力" class="headerlink" title="如何给网络增加记忆能力"></a>如何给网络增加记忆能力</h3><p>上边提到前馈神经网络是一个静态网络，不能处理时序数据，那么可以通过以下三种方法给网络增加记忆能力：</p><ul><li>延时神经网络</li><li>有外部输入的非线性自回归模型</li><li>循环神经网络</li></ul><h4 id="延时神经网络"><a href="#延时神经网络" class="headerlink" title="延时神经网络"></a>延时神经网络</h4><p>一种简单的利用利用历史信息的方法是建立一个额外的延时单元，用来存储网络的历史信息（比如输入，输出，隐状态等），这其中比较有代表性的就是延时神经网络（TDNN，Time Delay Neural Network）。</p><p>延时神经网络是在前馈神经网络的非输出层都添加一个延时器，记录最近几次神经元的输出，在第t个时刻，第（l+1）层的神经元和第（l）层神经元的最近p次输出有关，即（公式-1）:</p><script type="math/tex; mode=display">h_t^{l+1} = f(h_t^l,h_{t-1}^l,....,h_{t-p+1}^l)</script><p>通过延时器，前馈神经网络就具有了短期记忆的能力。</p><h4 id="有外部输入的非线性自回归模型"><a href="#有外部输入的非线性自回归模型" class="headerlink" title="有外部输入的非线性自回归模型"></a>有外部输入的非线性自回归模型</h4><p><strong>自回归模型（Autoregressive Model）</strong> 是统计学中常用一类时间序列模型，用一个变量 $y_t$ 的历史信息来预测自己（公式-2）。</p><script type="math/tex; mode=display">y_t = w_0 + \sum_{i=1}^{p}w_p * y_{t-i} + \varepsilon_t</script><p>其中p为超参数，$w_p$ 为参数，$\varepsilon_t～N(0,\sigma ^2)$ 为第t个时刻的噪声，方差$\sigma^2$和时间t无关。</p><p>有外部输入的非线性自回归模型（Nonlinear Autoregressive Model）是自回归模型的扩展，在每个时刻t都有一个外部输入$x_t$，产出一个输出$y_t$，NART通过一个延时器来记录最近几次的外部输入和输出，第t个时刻的输出$y_t$为（公式-3）：</p><script type="math/tex; mode=display">y_t = f(x_t, x_{t-1},..,x_{t-p}, y_{t-1},y_{t-2},....,y_{t-q})</script><p>其中f(.)为非线性函数，可以是前馈神经网络，p和q为超参数。</p><h4 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h4><p>给定一个输入序列，$x_{1:T}=( x_1, x_2, … , x_T )$ 循环神经网络通过以下公式（公式-4）更新带反馈边的隐藏层的活性值$h_t$：</p><script type="math/tex; mode=display">h_t = (h_{t-1}, x_t)</script><p>循环神经网络示例如下（图-2）：<br><img src="https://img-blog.csdnimg.cn/20190907182513443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络示例"></p><h3 id="一般的循环神经网络"><a href="#一般的循环神经网络" class="headerlink" title="一般的循环神经网络"></a>一般的循环神经网络</h3><p>在（图-2）中展示了一个简单的循环神经网络，其整个结构分为3层：输入层，隐藏层和输出层。其中t时刻，隐藏层的状态$h_t$不仅与输入$x_t$有关，还与上一个时刻的隐藏层状态$h_{t-1}$有关。</p><p>由于隐藏层多了一个自身到自身的输入，因此该层被称为循环层，（图-2）所示的为一个简单循环神经网络。循环神经网络还有多种类型，基于循环的方向划分为：</p><ul><li>单向循环神经网络</li><li>双向循环神经网络</li></ul><p>基于循环的深度分为：</p><ul><li>循环神经网络</li><li>深度循环神经网络</li></ul><h4 id="单向循环神经网络"><a href="#单向循环神经网络" class="headerlink" title="单向循环神经网络"></a>单向循环神经网络</h4><p>（图-2）所示即为一个单向的循环神经网络，对其展开后的效果图如下（图-3）：<br><img src="https://img-blog.csdnimg.cn/2019090809043298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="单向循环神经网络"></p><p>上图可以理解为网络的输入通过时间往后传递，当前隐藏层的输出$h_t$取决于当前层的输入$x_t$和上一层的输出$h_{t-1}$，因此当前隐藏层的输出信息包含了之前时刻的信息，表达出对之前信息的记忆能力。单向循环神经网络表达如下（公式-5）：</p><script type="math/tex; mode=display">o_t = g(V*h_t)\\h_t = f(U*x_t + W*h_{t-1})</script><p>其中$o_t$为输出层的计算公式， $h_t$为隐藏层的计算公式，g(.) 和 f(.)为激活函数。值得说明的是在循环神经网络中U，V，W权重矩阵值每次循环都是一份，因此循环神经网络的每次循环步骤中，这些参数都是共享的，这也是循 环神经网络 的结构特征之一。</p><h4 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h4><p>在日常的信息推断中，当前信息不仅仅依赖之前的内容，也有可能会依赖后续的内容，比如英语的完形天空。这时候单向的循环神经网络就不能很好的处理，就需要 <strong> 双向循环神经网络(Bi-directional Recurrent Neural Network)</strong> 。</p><p>其主要思想是训练一个分别向前和分别向后的循环神经网络，表示完整的上下文信息，两个循环 网络对应同一个输出层，因此可以理解为两个循环神经网络的叠加，对应的输出结果根据两个神经网络输出状态计算获得，将双向循环神经网络按照时间序列结构展开，如下图所示（图-4）：<br><img src="https://img-blog.csdnimg.cn/20190908145624762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="双向循环神经网络"></p><p>从上图可以看出，隐藏层需要保留两部分，一部分为由前向后的正向传递$h_t$，一部分为由后向前的反向传递$h’_t$，最新的信息输出$o_t$。双向循环神经网络的表达公式如下（公式-6）：</p><script type="math/tex; mode=display">o_t = g(V*h_t + V'*h'_t)\\h_t = f(U*x_t + W*h_{t-1})\\h'_t = f(U'*x_t + W'*h'_{t-1})</script><h4 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h4><p>上边介绍的单向训练神经网络和双向循环神经网络都只有一个隐藏层，但是在实际应用中，为了增强表达能力，往往引入多个隐藏层，即深度循环神经网络，如下图所示（图-5）：<br><img src="https://img-blog.csdnimg.cn/20190908151530509.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="深度循环神经网络"></p><p>同样可以得到深度循环神经网络的表达式（公式-7）：</p><script type="math/tex; mode=display">o_t = g(V^{(i)}*h^{(i)}_t + V'^{(i)}*h'^{(i)}_t)\\h^{(i)}_t = f(U^{(i)}*h^{(i-1)}_t + W^{(i)}*h_{t-1})\\h'^{(i)}_t = f(U'^{(i)}*h'^{(i-1)}_t + W'^{(i)}*h'_{t+1})\\...\\h^{(1)}_t = f(U^{(1)} * h_t + W^{(1)}*h_{t-1})\\h'^{(1)}_t = f(U'^{(1)} * h_t + W'^{(1)}*h'_{t+1})\\</script><p>从上述公式可以看出，最终的输出依赖两个维 度的计算，横向上内部前后信息的 叠加，即按照时间的计算；纵向上是每一时刻的输入信息在 逐层之间的传递，即按照 空间结构的计算。</p><h3 id="循环神经网络应用到机器学习任务"><a href="#循环神经网络应用到机器学习任务" class="headerlink" title="循环神经网络应用到机器学习任务"></a>循环神经网络应用到机器学习任务</h3><p>循环神经网络可以应用到很多不同类型的机器学习任务，根据这些任务的特点，可以分为以下几种模式：</p><ul><li>序列到类别模式</li><li>同步的序列到序列模式</li><li>异步的序列到序列模式</li></ul><h4 id="序列到类别模式"><a href="#序列到类别模式" class="headerlink" title="序列到类别模式"></a>序列到类别模式</h4><p>主要应用在序列数据的分类问题，其输入为序列，输出为类别。比如在文本分类中，输入为单词序列，输出为文本的类别。</p><p>假设一个样本 $x_{1:T}=(x_1, x_2, … , x_T)$为一个长度为T的序列，输出为类别 $y \in (1, …, C)$，可以将样本x按不同的时刻输入到循环神经网络中，并得到不同时刻的隐含状态$h_t$，可以将 $h_t$ 看作是整个序列的最终表示，并输入给分类器 g(.) 进行分类，如下所示（公式-8）：</p><script type="math/tex; mode=display">\hat{y} = g(h_T)</script><p>其中g(.) 为简单的线性分类器（比如LR）或者复杂的分类器（前馈神经网络）。</p><p>除了将最后时刻的状态作为序列表示之外，我们还可以对整个序列的状态进行平均，并用整个状态的最终平均作为整个序列的表示（公式-9）：</p><script type="math/tex; mode=display">\hat{y} = g( \frac{1}{T} \sum_{t=1}^{T} h_t )</script><p>公式-8 和公式-9 分别对应下图（图-6）的（a）和（b）:<br><img src="https://img-blog.csdnimg.cn/20190908154219364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="序列到类别模式"></p><h4 id="同步的序列到序列模式"><a href="#同步的序列到序列模式" class="headerlink" title="同步的序列到序列模式"></a>同步的序列到序列模式</h4><p>主要用于序列标注（Sequence Labeling）任务，即每一时刻都有输入和输出，输入序列和输出序列的长度相同。比如词性标注（Part-of-Speech Tagging）中，每一个单词都需要标注其对应的词性标签。</p><p>假设一个样本 $x_{1:T}=(x_1, x_2, … , x_T)$为一个长度为T的序列，输出为序列 $y_{1:T}=(y_1, y_2, … , y_T)$，可以将样本x按不同的时刻输入到循环神经网络中，并得到不同时刻的隐含状态$h_t$，每个时刻的 $h_t$ 代表了当前时刻和历史的信息，并输入给分类器 g(.) 进行分类，得到当前的标签 $\hat{y}_t$，如下所示（公式-8）：</p><script type="math/tex; mode=display">\hat{y} = g(h_T), \forall_t \in [1,T]</script><p><img src="https://img-blog.csdnimg.cn/20190908154735645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="同步的序列到序列模式"></p><h4 id="异步的序列到序列模式"><a href="#异步的序列到序列模式" class="headerlink" title="异步的序列到序列模式"></a>异步的序列到序列模式</h4><p>异步的序列到序列模式也成为编码器-解码器，输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度，比如在机器翻译中，输入为源语音的单词序列，输出为目标语言的单词序列。</p><p>假设输入为一个长度为T的序列 $x_{1:T}=(x_1, x_2, … , x_T)$，输出为长度为M的序列$y_{1:M}=(x_1, x_2, … , x_M)$，经常通过先编码后解码的形式实现。</p><p>先将样本x按不同时刻输入到一个循环神经网络（编码器）中，并得到其编码$h_T$，然后再使用另外一个循环神经网络（解码器）中，得到输出序列$\hat {y}_{1:M}$。为了建立输出序列之间的依赖关系，在解码器中通常使用非线性的自回归模型。如下所示（公式-9）：</p><script type="math/tex; mode=display">h_t = f_1(h_{t-1},x_t), \forall_t \in [1,T]\\h_{T+t} = f_2(h_{T+t-1},x_t), \forall_t \in [1,M]\\\hat{y} _t = g(h_{T+t}), \forall_t \in [1,M]</script><p>其中 $f_1(.)$，$f_2(.)$分别为用作编码器和解码器的循环神经网络，g(.)为分类器，$\hat{y}_t$ 为输出预测$\hat{y}_t$的表示。</p><p><img src="https://img-blog.csdnimg.cn/20190908160323234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><blockquote><p>至此，循环神经网络（上）篇已经介绍完了，在（中）篇和下篇中会展开介绍更多的内容，欢迎关注。</p></blockquote><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100600661&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(2)-卷积神经网络</title>
    <link href="http://thinkgamer.cn/2019/09/05/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(2)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://thinkgamer.cn/2019/09/05/TensorFlow/常见的五种神经网络(2)-卷积神经网络/</id>
    <published>2019-09-05T10:05:24.000Z</published>
    <updated>2019-10-14T12:28:36.837Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/2019090518010966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><hr><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><hr><blockquote><p>卷积神经网络（Convolutional Neural Network）是一种具有局部连接，权重共享等特性的深层前馈神经网络。一般是由卷积层，汇聚层，全连接层交叉堆叠而成，使用反向传播算法进行训练。其有三个结构上的特征：局部连接，权重共享以及汇聚。这些特征使得卷积神经网络具有一定程度上的平移，缩放和旋转不变性。较前馈神经网络而言，其参数更少。</p></blockquote><p>卷积神经网络目前主要应用在图像和视频分析的各种任务上，比如图像分类，人脸识别，物体识别，图像分割等，其准确率也远远超过了其他的人工神经网络。近年来，卷积神经网络也应用到自然语言处理和推荐系统等领域。</p><h1 id="卷积的概念"><a href="#卷积的概念" class="headerlink" title="卷积的概念"></a>卷积的概念</h1><p>卷积（Convolution）也叫摺积，是分析数学中一种重要的运算。在信号处理或者图像处理中，会经常使用一维或二维卷积。</p><h2 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h2><p>一维卷积经常用在信号处理上，用来计算信号的累积。假设一个信号发生器每个时刻t发生一个信号 $x_t$，其信号衰减率维$w_k$，即在$k-1$时刻后，信息变为原来的$w_k$倍，假设$w_1 = 1, w_2=1/2,w_3=1/4$那么在t时刻收到的信号$y_t$为当前时刻产生的信息之前时刻产生的延迟信息的叠加(公式1.1)。</p><script type="math/tex; mode=display">y_t = 1 * x_t + 1/2 * x_{t-1} + 1/4 * x_{t-2}\\=w_1 * x_t + w_2 * x_{t-1} + w_3 * x{t-2}\\= \sum_{k=1}^{3} w_k * x_{t-k+1}</script><p>我们把$w_1, w_2, ….$称为滤波器（Filter）或者卷积核（Convolution Kernel）。假设滤波器长度为m，它和一个信号序列$x_1,x_2,…$的卷积为(公式1.2)：</p><script type="math/tex; mode=display">\sum_{k=1}^{m} w_k * x_{t-k+1}</script><p>信号序列x和滤波器w的卷积定义为(公式1.3)：</p><script type="math/tex; mode=display">y = w \otimes  x</script><p>一维卷积示例如下：<br><img src="https://img-blog.csdnimg.cn/20190904075135300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="一维卷积神经网络"></p><h2 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h2><p>卷积也常用在图像处理中，因为图像是一个二维结构，需要对一维卷积进行扩展。给定一个图像$X \in R^{M<em>N}$和滤波器$W \in R^{m</em>n}$，一般$m &lt;&lt; M, n &lt;&lt;N$，其卷积为(公式1.4)：</p><script type="math/tex; mode=display">y_{ij}=\sum_{u=1}^{M}\sum_{v=1}^{N} w_{uv} * x_{i-u+1,j-v+1}</script><p>二维卷积示例如下：<br><img src="https://img-blog.csdnimg.cn/20190904080806425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="二维卷积示例"></p><blockquote><p>注意：上图中的展示的卷积核（3*3矩阵）和二维结构数据相乘时需要逆时针旋转180度！对照着卷积公式可以理解。</p></blockquote><h2 id="互相关"><a href="#互相关" class="headerlink" title="互相关"></a>互相关</h2><p>在计算卷积过程中，需要进行卷积核翻转，在具体实现上一般会以互相关操作来代替卷积，从而会减少一些不必要的操作或者开销。互相关是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。给定一个图像$X \in R^{M<em>N}$和卷积核$W \in R^{m</em>n}$，他们的互相关为(公式1.5)：</p><script type="math/tex; mode=display">y_{ij}=\sum_{u=1}^{M}\sum_{v=1}^{N} w_{uv} * x_{i+u-1,j+v-1}</script><p>和公式1.4相比，互相关和卷积的区别在于是否对卷积核进行翻转，因此互相关也称为不翻转卷积。</p><p>在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行核翻转与其特征抽取能力无关。特别是当卷积核是可学习的参数时，卷积和互相关是等价的，因此为了实现方便，通常使用互相关来代替卷积。事实上很多深度学习工具中卷积操作都是用互相关来代替的。</p><p>公式1.5可以表示为：</p><script type="math/tex; mode=display">Y = W \otimes X</script><hr><h1 id="常见的卷积核及特征"><a href="#常见的卷积核及特征" class="headerlink" title="常见的卷积核及特征"></a>常见的卷积核及特征</h1><h2 id="常见的卷积核"><a href="#常见的卷积核" class="headerlink" title="常见的卷积核"></a>常见的卷积核</h2><ol><li>对图像无任何影响的卷积核<script type="math/tex; mode=display">\begin{bmatrix}0  & 0 & 0 \\ 0 &  1 & 0 \\ 0 & 0  & 0\end{bmatrix}</script></li><li>对图像进行锐化的滤波器<script type="math/tex; mode=display">\begin{bmatrix}-1  & -1 & -1 \\ -1 &  9 & -1 \\ -1 & -1  & -1\end{bmatrix}</script></li><li>浮雕滤波器<script type="math/tex; mode=display">\begin{bmatrix}-1  & -1 & 0 \\ -1 &  0 & 1 \\ 0 & 1  & 1\end{bmatrix}</script></li><li><p>均值模糊滤波器</p><script type="math/tex; mode=display">\begin{bmatrix}0  & 0.2 & 0 \\ 0.2 &  0.2 & 0.2 \\ 0 & 0.2  & 0\end{bmatrix}</script><blockquote><p>均值模糊是对像素点周围的像素进行均值化处理，将上下左右及当前像素点分文5份，然后进行平均，每份占0.2，即对当前像素点周围的点进行均值化处理。</p></blockquote></li><li><p>高斯模糊滤波器</p><blockquote><p>均值模糊是一种简单的模糊处理方式，但是会现实模糊不够平滑，而高斯模糊可以很好的处理，因此高斯模糊经常用于图像的降噪处理上，尤其是在边缘检测之前，进行高斯模糊，可以移除细节带来的影响。</p></blockquote></li></ol><ul><li>一维高斯模糊<script type="math/tex; mode=display">G(x)=\frac{1}{ \sqrt{2 \pi \sigma ^2}} e^{( -\frac{x^2}{2\sigma ^2} )}</script></li><li>二维高斯模糊<script type="math/tex; mode=display">G(x)=\frac{1}{ \sqrt{2 \pi \sigma ^2}} e^{( -\frac{x^2+y^2}{2\sigma ^2} )}</script></li></ul><h2 id="卷积核的特征"><a href="#卷积核的特征" class="headerlink" title="卷积核的特征"></a>卷积核的特征</h2><blockquote><p>这里的滤波器就是卷积核</p></blockquote><ul><li>当滤波器矩阵中的值相加为0甚至更小时，被滤波器处理之后的图像相对会比原始图像暗，值越小越暗</li><li>当滤波器矩阵中的值相加和为1时，被滤波器处理之后的图像与原始图像的亮度相比几乎一致</li><li>当滤波器矩阵中的值相加和大于1时，被滤波器处理之后的图像相对会比原始图像的亮度更亮</li></ul><hr><h1 id="卷积的变种"><a href="#卷积的变种" class="headerlink" title="卷积的变种"></a>卷积的变种</h1><p>在卷积的标准定义基础上，还可以引入滤波器的滑动步长和零填充来增加卷积的多样性，可以更加灵活的提取特征。</p><ul><li>滤波器的步长（Stride）是指滤波器在滑动时的时间间隔</li><li>零填充（Zero Padding）是在输入向量两端进行补零</li></ul><p>下图展示为步长为2和零填充的示例：<br><img src="https://img-blog.csdnimg.cn/20190904081641651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="步长为2和零填充示例"></p><p>假设卷积层的输入神经元个数为n，卷积大小为m，步长为s，输入神经元两端各补p各零，那么该卷积对应的神经元数量为：(n+2p-m)/s + 1。</p><p>一般的卷积分为以下三种：</p><ul><li>窄卷积（Narrow Convolution）：步长s=1，两端不补零即p=0，卷积后输出长度为：n-m + 1</li><li>宽卷积（Wide Convolution）：步长s=1，两端补零p=m-1，卷积后输出长度为：n+m-1</li><li>等宽卷积（Equal-Width Convolution）：步长s = 1,两端补零p = (m −1)/2,卷积后输出长度 n。</li></ul><hr><h1 id="卷积的数学性质"><a href="#卷积的数学性质" class="headerlink" title="卷积的数学性质"></a>卷积的数学性质</h1><p>卷积有很多比较好的数学性质，这里主要介绍一些二维的数学性质，同样针对一维卷积也同样适用。</p><h2 id="交换性"><a href="#交换性" class="headerlink" title="交换性"></a>交换性</h2><p>如果不限制两个卷积的长度，卷积是具有交换性的。即 $x \otimes  y = y \otimes  x$，当输入信息和卷积核有固定长度时，他们的宽卷积依然具有交换性。对于两维图像$X \in R^{M<em>N}$和卷积核$W \in R^{m</em>n}$，对图像X的两个维度进行零填充，两端各补m-1和n-1个零，得到全填充（Full Padding）的图像$\tilde{X} \in R^{(M+2m-2)(N+2n-2)}$。图像X和卷积核W的宽卷积（Wide Convolution）定义为：$W \tilde{\otimes } X \triangleq X \tilde{\otimes } W$<br>，其中$\tilde{\otimes }$为宽卷积操作。宽卷积具有交换性，即：$W \tilde{\otimes } X = X \tilde{\otimes } W$</p><h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><p>假设$Y = W \otimes X$，其中$X \in R^{M<em>N}$，$W \in R^{m</em>n}$，$Y \in R^{(M-m+1)*(N-n+1)}$，函数$f(Y) \in R$为一个标量函数，则(公式1.6)</p><script type="math/tex; mode=display">\frac{\partial f(Y)}{\partial w_{uv}} = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} \frac{\partial f(Y)}{\partial y_{ij}} \frac{\partial y_{ij}}{\partial w_{uv}}\\= \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}  \frac{\partial f(Y)}{\partial y_{ij}}x_{ {i+u-1},{j+v-1}}  \\= \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}  \frac{\partial f(Y)}{\partial y_{ij}}x_{ {u+i-1},{v+j-1}}</script><p>从公式1.6可以看出，f(Y)关于W的偏导数为X和$\frac{\partial f(Y)}{\partial Y}$的卷积（公式1.7）</p><script type="math/tex; mode=display">\frac{\partial f(Y)}{\partial W} = \frac{\partial f(Y)}{ \partial Y } \otimes X</script><p>同理得到（公式1.8）：</p><script type="math/tex; mode=display">\frac{\partial f(Y)}{\partial x_{st}} = \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1} \frac{\partial f(Y)}{\partial y_{ij}} \frac{\partial y_{ij}}{\partial x_{st}}\\= \sum_{i=1}^{M-m+1}\sum_{j=1}^{N-n+1}  \frac{\partial f(Y)}{\partial y_{ij}}w_{ {s-i+1},{t-j+1}}</script><p>其中当$(s-i+1) &lt; 1$，或$(s-i+1)&gt;m$，或$(t-j+1) <1$，或$(t-j+1)>n$，或$w_{s-i+1,t-j+1}=0$时，即相当于对W进行了 p=(M-m,N-n)的零填充。</1$，或$(t-j+1)></p><p>从公式1.8可以看出，f(Y)关于X的偏导数为W和$\frac{\partial f(Y)}{ \partial Y }$，公式1.8中的卷积是真正的卷积而不是互相关，为了一致性，我们用互相关的卷积，即(公式1.9)：</p><script type="math/tex; mode=display">\frac{\partial f(Y)}{\partial X} = rot180(\frac{\partial f(Y)}{\partial X}) \tilde{\otimes }W=rot180(W) \tilde{\otimes }\frac{\partial f(Y)}{\partial X}</script><p>其中rot180(.)表示旋转180度。</p><hr><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络一般由卷积层，汇聚层和全连接层构成。</p><h2 id="用卷积代替全连接"><a href="#用卷积代替全连接" class="headerlink" title="用卷积代替全连接"></a>用卷积代替全连接</h2><p>在全连接前馈神经网络中，如果第$l$层有$n^l$个神经元，第$l-1$层有$n^{l-1}$个神经元，连接边就有$n^l * n^{l-1}$也就是权重参数有这么多个，当m和n都很大时，权重矩阵的参数会非常多，训练的效率会非常低。</p><p>如果用卷积代替全连接，第$l$层的净输入$z^l$与$l-1$层活性值$a^{l-1}$和滤波器$w^l \in R^m$的卷积，即$z^l = w^l * a^{l-1} + b^l$,其中滤波器$w^l$<br>为可学习的权重向量，$b^l \in R^{l-1}$为可学习的偏置。</p><p>根据卷积的定义，卷积层有两个很重要的性质：</p><ul><li>局部连接：在卷积层(假设是第$l$层)中的每一个神经元都只和下一层(第$l − 1$层)中某个局部窗口内的神经元相连,构成一个局部连接网络。</li><li>全局共享：作为参数的滤波器 $w^l$，对于第 $l$层的所有的神经元都是相同的。</li></ul><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积层的作用是提取一个局部区域的特征，不同大小的卷积相当于不同的特征提取器。上文介绍的卷积和神经元都是一维的，但卷积神经网络主要是针对图像处理而言的，而图像通常是二维的，为了充分利用图像的局部特征，通常将神经元组织为三维结构的神经层，其大小 M <em> 宽度 W </em> 深度 D，即D个M*N的特征映射组成。</p><p>对于输入层的而言，特征映射就是图像本身，如果是灰色图像，则深度为1，如果为彩色图像（分别是RGB三个通道的颜色特征映射），则深度为3。</p><h2 id="汇聚层"><a href="#汇聚层" class="headerlink" title="汇聚层"></a>汇聚层</h2><p>汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是进行特征选择，降低特征数量，从而减少参数数量。</p><p>卷积层虽然可以明显减少网络中的连接数量，但是特征映射中的神经元个数并未显著减少。如果后边接一个分类器的话，分类器的输入维数依然很高，很容易出现过拟合。因此有了汇聚层的产生，在卷积后边加一个汇聚层，从而降低特征维数，避免过拟合。</p><p>假设汇聚层的输入特征映射组为$X \in R^{M <em> N </em> D}$，对于其中每一个映射$X^d$，将其划分为很多区域$R^d_{m,n}$，1 &lt;= m &lt;= M’，1&lt;= n &lt;= N’，这些区域可以重叠，也可以不重叠。汇聚（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括。常见的汇聚方式有两种：</p><ul><li>最大汇聚（Maximum Pooling）：一个区域内所有神经元的最大值</li><li>平均汇聚（Mean Pooling）：一个区域内所有神经元的平均值</li></ul><p>典型的汇聚层是将每个特征映射划分为2<em>2大小的不重叠区域，然后使用最大汇聚的方式进行下采样。汇聚层也可以看作是一个特殊的卷积层，卷积核大小为m </em> m，步长为s * s，卷积核为 max函数或者mean函数。过大的采样区域会急剧减少神经元的数量，会造成过多的信息损失。</p><p>下图所示为最大汇聚示例：<br><img src="https://img-blog.csdnimg.cn/20190905133224389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="最大汇聚实例"></p><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>在全连接层中，将最后一层的卷积输出展开，并将当前层的每个节点与下一层的另一个节点连接起来。全连接层只是人工神经网络的另一种说法，如下图所示，全连接层中的操作与一般神经网络中的操作完全相同。</p><p><img src="https://img-blog.csdnimg.cn/20190904171044639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="image"></p><p>对于output layer中的的每个神经元，其表达式可以记做为(公式1.10)：</p><script type="math/tex; mode=display">y = \sigma (\sum_{i=1}^{m} w_i ^T x_i + b)</script><p>如果outptu有多个神经元，最终可以通过softmax进行最终类别的判断。</p><hr><h1 id="典型的卷积网络结构"><a href="#典型的卷积网络结构" class="headerlink" title="典型的卷积网络结构"></a>典型的卷积网络结构</h1><p>一个典型的卷积网络是有卷积层，汇聚层，全连接层交叉堆叠而成。目前常用的卷积神经网络结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/2019090413573373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="卷积神经网络"></p><p>卷积块是由M个卷积层b个汇聚层（M通常在2～之间，b为0或1），一个卷积网络中可以堆叠N个连续的卷积块，然后再接着K个全连接层（N的取值空间比较大，一般是1～100或者更大，K通常为0～2）。</p><p>目前整个网络倾向于使用更小的卷积核（比如1<em>1或者3</em>3）以及更深的结构（比如层数大于50），此外，卷积操作的灵活性越来越大，汇聚层的作用变得越来越小，因此目前流行的卷积网络中，汇聚层的比例也在逐渐降低，倾向于全连接网络。</p><hr><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>在卷积神经网络中，参数为卷积核中的权重和偏置，和全连接前馈神经网络一样，使用误差反向传播算法来进行参数学习。梯度主要通过每一层的误差项$\delta$进行反向传播，并进一步计算每一层的梯度。在卷积神经网络中主要有两种功能不同的网络层：卷积层和汇聚层。而参数为卷积核中权重和偏置，因此只需要计算卷积层中参数梯度。</p><p>不失一般性，对第$l$层卷积层,第$l-1$层的输入特征映射为$X^{(l-1)} \in R^{M<em>N</em>D}$，通过卷积计算得到第$l$层净输入为$Z^{(l)}\in  R^{M’<em>N’</em>P}$，第$l$层的第p(1&lt;= p &lt;= P)个特征净输入为(公式1.11)</p><script type="math/tex; mode=display">Z^{(l,p)} = \sum_{d=1}^{D} W^{(l,p,d)} \otimes X^{(l-1,d)}+ b^{(l,p)}</script><p>其中$W^{(l,p,d)} ,b^{(l,p)}$为卷积核以及偏置。第$l$层共有P * D 个卷积和P个偏置，可以分别使用链式法则计算其梯度。</p><p>根据公式1.7 和 1.11，损失函数关于第$l$层的卷积核$W^{(l,p,d)}$的偏导数为为(公式1.12)：</p><script type="math/tex; mode=display">\frac{\partial L (Y,\hat{Y})}{ \partial W^{(l,p,d)} } = \frac{\partial L (Y,\hat{Y})}{ \partial Z^{(l,p)} } \otimes X^{(l-1,d)}=\delta ^{(l,p)} \otimes X^{(l-1,d)}</script><p>其中为(公式1.13)</p><script type="math/tex; mode=display">\delta ^{(l,p)} = \frac{\partial L (Y,\hat{Y})}{ \partial Z^{(l,p)} }</script><p>为损失函数关于第$l$层的第p个特征映射净输入$Z^{(l,p)}$的偏导数。</p><p>同理可得，损失函数关于第$l$层的第p个偏置$b^{(l,p)}$的偏导数为为(公式1.14)：</p><script type="math/tex; mode=display">\frac{ \partial L (Y,\hat{Y}) }{ \partial b^{(l,p)} } = \sum_{i,j} [\delta ^{(l,p)}]_{i,j}</script><p>卷积网络中，每层参数的梯度依赖其所在层的误差项$\delta ^{(l,p)}$</p><h2 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h2><p>卷积层和汇聚层的误差项计算不同。</p><h3 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层"></a>卷积层</h3><p>当$l+1$层为卷积层时，假设特征映射净输入(公式1.15)</p><script type="math/tex; mode=display">Z^{(l+1,p)} = \sum_{d=1}^{D} W^{(l+1,p,d)} \otimes X^{(l,d)} + b^{(l+1,p)}</script><p>其中$W^{(l+1,p,d)},b^{(l+1,p)}$为第$l$层的卷积核和偏置。第$l+1$层共有 P *D 个卷积核和P个偏置。</p><p>第 $l$层的第 $d$个特征映射的误差项$\delta ^{(l,d)}$的具体推导过程如下(公式1.16):</p><script type="math/tex; mode=display">\delta ^{(l,d)} \triangleq  \frac{\partial L (Y,\hat{Y})}{ \partial Z^{(l,d)} }\\=\frac{\partial X^{(l,d)} } { \partial Z^{(l,d)}} \cdot \frac{\partial L (Y,\hat{Y})}{ \partial X^{(l,d)} } \\= f'_l (Z^{(l,p)})  \odot \sum_{p=1}^{P}( rot180(W^{(l+1,p,d)} ) \tilde{\otimes  } \frac{\partial L(Y,\hat{Y})}{ \partial Z^{(+1,p)}})\\=  f'_l (Z^{(l,p)})  \odot \sum_{p=1}^{P}(rot180(W^{(l+1,p,d)} ) \tilde{\otimes  }\delta ^{(l+1,p)})</script><p>其中$\tilde{\otimes}$表示宽卷积。</p><h3 id="汇聚层-1"><a href="#汇聚层-1" class="headerlink" title="汇聚层"></a>汇聚层</h3><p>当第$l+1$层为汇聚层时, 因为汇聚层是下采样操作, $l+1$层的每个神经元的误差项 $\delta$对应于第$l$层的相应特征映射的一个区域。$l$层的第$p$个特征映射中的每个神经元都有一条边和$l+1$层的第$p$个特征映射中的一个神经元相连。</p><p>根据链式法则,第$l$层的一个特征映射的误差项$\delta ^{(l,p)}$，只需要将 $l+1$层对应特征映射的误差项$\delta ^{(l+1,p)}$进行上采样操作(和第 $l$层的大小一样) ,再和 $l$层特征映射的激活值偏导数逐元素相乘,就得到了 $\delta ^{ (l,p)}$</p><p>第 $l$层的第$p$个特征映射的误差项$\delta ^{(l,p)}$的具体推导过程如下(公式1.17)：</p><script type="math/tex; mode=display">\delta ^{(l,p)} \triangleq  \frac{\partial L (Y,\hat{Y})}{ \partial Z^{(l,p)} }\\=\frac{\partial X^{(l,p)} } { \partial Z^{(l,p)}} \cdot \frac{\partial Z^{(l+1,p)} } { \partial X^{(l,p)}} \cdot \frac{\partial L (Y,\hat{Y})}{ \partial Z^{(l+1,p)} }\\ = f'_l (Z^{(l,p)})  \odot up(\delta ^{(l+1,p)})</script><p>其中$f’_l$为第l层使用的激活函数导数，up为上采样函数(upsampling)，与汇聚层中使用的下采样函数刚好相反，如果下采样是最大汇聚（max pooling），误差项$\delta ^{(l+1,p)}$中每个值都会传递到上一层对应区域中的最大值所对应的神经元，该区域中其他位置的神经元的误差都设为0，如果下采样是平均汇聚(mean pooling) ,误差项 $\delta ^{(l+1,p)}$中每个值会被平均分配到上一层对应区域中的所有神经元上。</p><hr><h1 id="几种典型的卷积神经网络"><a href="#几种典型的卷积神经网络" class="headerlink" title="几种典型的卷积神经网络"></a>几种典型的卷积神经网络</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>LeNet-5 虽然提出的时间比较早（LeCun et al., 1998），但是一个非常成功的卷积神经网络模型，90年代在许多银行进行使用，用来识别手写数字，其网络结构如下：<br><img src="https://img-blog.csdnimg.cn/20190904141123853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="LeNet-5"></p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>AlexNet是第一个现代深度卷积网络模型，其首次使用了现在深度卷积网络的一些技巧，比如GPU并行训练，采用ReLU作为非线性激活函数，使用DropOut防止过拟合，使用数据增强来提高模型准确率。AlexNet获得了2012年ImageNet图像分类比赛的冠军，其网络结构如下：<br><img src="https://img-blog.csdnimg.cn/20190904141502612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="AlexNet"></p><h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><p>在卷积网络中，如何定义一个卷积的大小是一个十分关键的问题，在Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模块， Inception网络是由多个inception模块和汇聚层堆叠而成。</p><p>Inception模块同时使用1<em>1，3</em>3，5<em>5等大小不同的卷积核，并将得到的特征映射在深度上拼接（堆叠）起来作为输出特征映射。下图给出了v1版本的inception模块结构图，采用了4组平行的特征抽取方式，分别为1</em>1，3<em>3，5</em>5的卷积和3<em>3的最大汇聚，同时为了提高计算效率，减少参数数量，inception模块在进行3</em>3，5<em>5的卷积之前，3</em>3的最大汇聚之后，进行一次1*1的卷积来减少特征映射的深度。<br><img src="https://img-blog.csdnimg.cn/20190904142716385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="v1版本的inception模块"></p><p>Inception网络最早的v1版本就是非常著名的GoogleNet，获得了2014年ImageNet图像分类竞赛的冠军。其结构图如下所示：<br><img src="https://img-blog.csdnimg.cn/20190904142927193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="GoogleLeNet"></p><p>当然Inception网络有多个改进版本，比如Inception-v3网络，Inception-ResNet v2网络和改进版的Inception-v4模型。</p><h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>残差网络（Residual Network，ResNet）是通过给非先行的卷积层增加直连边的方式来提高信息的传播效率。</p><p>假设在一个深度网络中，我们期望一个非线性单元$f(x,\theta)$去逼近一个目标函数为h(x)。如果将目标函数拆分为两部分：恒等函数（Identity）和残差函数（Reside Function）h(x)-x。</p><script type="math/tex; mode=display">h(x) = \underset{IdentityFunc}{\underbrace{x}} +( \underset{ResidueFunc}{\underbrace{h(x)-x}})</script><p>根据通用近似定理，一个由神经网络构成的非线性单元有足够的能力来近似逼近原始目标函数或残差函数，但实际中后者更容易血虚。因此原来的优化问题可以转化为：让非线性单元$f(x,\theta)$去近似残差h(x)-x,并用$f(x,\theta) +x$去逼近h(x)。</p><p>下图给出了一个典型的残差单元示例，残差单元由多个级联的（等长）卷积层和一个跨层的直连边组成，再经过ReLU激活后得到输出。残差网络就是将很多个残差单元串联起来构成的一个非常深的网络。<br><img src="https://img-blog.csdnimg.cn/20190904144622317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="残差单元示例"></p><hr><h1 id="其他卷积方式"><a href="#其他卷积方式" class="headerlink" title="其他卷积方式"></a>其他卷积方式</h1><h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2><p>我们一般通过卷积操作来实战高维特征到低维特征的转换，但在一些任务中需要把低维特征映射到高维特征，并且希望通过卷积操作来实现。</p><p>卷积操作可以通过仿射变换的形式。假设一个5维的向量x，经过大小为3的卷积核w=[w1,w2,w2]^T来进行卷积，得到3维向量z，卷积操作可以写为：</p><p><img src="https://img-blog.csdnimg.cn/20190904154849239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="5维向量x与大小为3的卷积核进行卷积"></p><p>其中C是一个稀疏矩阵，其非零元素来自于卷积核w中的元素。如果实现3维向量z到5维向量x的映射，可以通过仿射矩阵转置来实现。</p><p><img src="https://img-blog.csdnimg.cn/20190904155151517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="仿射矩阵转置"></p><p>其中rot180(.)表示旋转180度。</p><blockquote><p>我们将低维特征映射到高维特征的卷积操作称之为转置卷积（Transposed Convolution），也叫反转卷积（Deconvolution）。</p></blockquote><h2 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h2><p>对于一个卷积层，如果希望增加输出单元的感受野，一般可以通过三种方式实现：</p><ul><li>增加卷积核的大小</li><li>增加层数</li><li>在卷积之前进行汇聚操作</li></ul><p>前两种会增加参数数量，最后一种会丢失一些信息。</p><p>空洞卷积（Atrous Convolution）也成为膨胀卷积（Dilated Convolution），是一种不增加参数数量，同时增加输出单元感受野的一种方法。</p><p>空洞卷积通过给卷积核插入“空洞”来变相的增加其大小，如果在卷积核的每两个元素之间插入d-1个空洞，卷积核的有效大小维：<br>$m’=m+ (m-1) * (d-1)$<br>其中d称为膨胀率（Dilation Rate）。当d=1时卷积核维普通的卷积核。<br><img src="https://img-blog.csdnimg.cn/20190904160154440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="不同膨胀率的卷积核"></p><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2019090518010966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文】RecSys18-序列推荐模型TransFM(Translation-based Factorization Machines for Sequential Recommendation)</title>
    <link href="http://thinkgamer.cn/2019/08/31/%E8%AE%BA%E6%96%87/%E3%80%90%E8%AE%BA%E6%96%87%E3%80%91RecSys18-%E5%BA%8F%E5%88%97%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8BTransFM(Translation-based%20Factorization%20Machines%20for%20Sequential%20Recommendation)/"/>
    <id>http://thinkgamer.cn/2019/08/31/论文/【论文】RecSys18-序列推荐模型TransFM(Translation-based Factorization Machines for Sequential Recommendation)/</id>
    <published>2019-08-31T01:14:39.000Z</published>
    <updated>2019-10-14T12:28:36.845Z</updated>
    
    <content type="html"><![CDATA[<p>序列推荐模型 Translation-based Recommendation，参考：<a href="https://thinkgamer.blog.csdn.net/article/details/100129827" target="_blank" rel="external">点击阅读</a></p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>论文是由Rajiv Pasricha和Julian McAuley两位大佬提出的发表在RecSys18 上的，是TransRec和FM的结合版本（论文下载地址：<a href="https://cseweb.ucsd.edu/~jmcauley/pdfs/recsys18a.pdf）。在下面会简单介绍TransRec和FM。" target="_blank" rel="external">https://cseweb.ucsd.edu/~jmcauley/pdfs/recsys18a.pdf）。在下面会简单介绍TransRec和FM。</a></p><p>对于电商网站（如亚马逊），媒体网站（如Netflix，Youtube）等而言，推荐系统是其中至关重要的一环。传统的推荐方法尝试对用户和物品的全局交互进行建模。例如矩阵分解和其派生模型，虽然能够有效的捕获到用户的偏好，但是未考虑到时序特征，其忽略了用户的最近交互行为，提供了一个静态的推荐列表。</p><p>序列推荐的目的是基于用户的历史行为序列去预测用户将来的行为。Julian McAuley作为主要作者的另一篇论文（Translation-based Recommendation）提出了“翻译”空间的概念，将物品作为一个点嵌入到“翻译”空间内，用户的序列行为则作为一个翻译向量存在于该空间，然后通过距离计算便根据用户u的当前行为物品i，预测其接下来可能有行为的物品，具体可参考：<a href="https://mp.weixin.qq.com/s/YovZKGd2BDqnpW5BBGLA-A。TransRec的主要思路如下图所示：" target="_blank" rel="external">https://mp.weixin.qq.com/s/YovZKGd2BDqnpW5BBGLA-A。TransRec的主要思路如下图所示：</a></p><center><img src="https://img-blog.csdnimg.cn/20190831085905828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="500px"></center><p>本论文中提出了TransFM，其结合了FM和TransRec的思想，将其应用在序列推荐中，这样做的好处是使用简单的模型对复杂的交互之间进行建模并能取得不错的效果。</p><blockquote><p>FM能够对任意的实值特征向量进行操作，并通过参数分解对特征之间的高阶交互进行建模。他可以应用在一般的预测任务里，并可以通过特征替换，取代常见的推荐算法模型。</p></blockquote><p>TransFM的主要思路如下图所示：</p><center><img src="https://img-blog.csdnimg.cn/20190831085920392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="500px"></center><p>TransFM是对所有观察到的行为之间可能的交互进行建模，对于每一个特征i，模型学习到两部分：一个低维的embedding向量$\overrightarrow{v_i}$和一个翻译向量$\overrightarrow{v_i’}$</p><p>特征之间的交互强度使用平方欧几里德距离来进行计算，在上图中，展示了user，item，time的embedding特征和翻译向量，交互行为之间的权重由起始点和结束点之间的平方欧几里德距离进行计算。与FM一样，TransFM可以在参数和特征纬度的线性时间内进行计算，从而有效的实现大规模数据集的计算。</p><h1 id="相关研究"><a href="#相关研究" class="headerlink" title="相关研究"></a>相关研究</h1><h2 id="序列推荐"><a href="#序列推荐" class="headerlink" title="序列推荐"></a>序列推荐</h2><p>已经存在了许多基于MC（马尔可夫链，Markov Chains）的序列推荐模型，比如FPMC（Factorized Personalized Markov Chains），使用独立分解矩阵对三阶交互行为进行分解，继而来模拟成对的相互作用。PRME使用欧几里德距离替换内积对用户-物品之间的交互行为进行建模。TransRec同样也是一个序列推荐模型，通过共享物品的embedding向量空间，将用户行为转化为翻译向量，其计算公式如下：</p><center><img src="https://img-blog.csdnimg.cn/20190831085937428.png" width="200px"></center>这些对于给定的用户历史行为序列十分有效，但是在不改变模型结构的前提下，并不能捕获时间，地理和其他的上下文特征。## 因子分解机FM对于任意的机器学习任务来讲是一个通用的学习框架，他模型任意任意特征之间的二阶交互，并很容易扩招到更高阶，每个特征的交互通过参数之间的内积来权衡。其公式如下（这里讨论的是FM的二阶形式）：<center><img src="https://img-blog.csdnimg.cn/20190831085947201.png" width="200px"></center>通过选择合适的损失函数，FM可以应用在任意的分类，回归或者排序任务中，在这篇文章里主要是针对隐式反馈结合BPR算法框架去优化预测的结果。## 混合推荐混合推荐结合了协同和conetnt-based，目的在于提升效果并且为行为很较少的用户提供有效的选择，在一定程度上缓解了用户冷启动。这里可以利用的潜在的信息包括：时间特征，地理特征，社交特征等。最近的一些关于混合推荐的工作结合了图像特征，或者是使用深度学习自动生成有用的内容特征。虽然这些方法都取得了不错的表现，但依赖于专门的模型和技术。相比之间，论文里提出的TransFM是一种更广义的办法，可以对任意的特征向量和预测任务进行操作，通过适当的特征工程，TransFM模型可以结合时间，地理，人口统计和其他内容特征，而无需更改模型本身结构。# TransFM模型## 问题定义<center><img src="https://img-blog.csdnimg.cn/20190831085958369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="500px"></center><p>TransFM使用平方欧几里德距离替换FM中的内积计算，并用embedding 向量和翻译向量之和表示特征v_i的向量，其公式如下：</p><center><img src="https://img-blog.csdnimg.cn/20190831090026658.png" width="300px"></center>其中距离计算方式为：<center><img src="https://img-blog.csdnimg.cn/20190831090035517.png" width="230px"></center><p>使用平方欧几里德距离替换内积的好处是：提高模型的泛化能力，更有效的捕获embedding之间的传递性。比如(a,b)，(b,c)之间有很高的交互权重，那么(a,c)之间的相关性也会更强。</p><p>下图展示了TransFM和其他几种算法的预测方法，从中可以看出PRME学习的是两个用户的embedding向量之间的距离，FM学习的是任意特征与相应参数之间的内积，TransRec学习的是物品的embedding向量和用户行为的翻译序列，TransFM学习的是每个特征的embedding向量和翻译向量，使用平方欧几里德距离去度量特征之间的交互。</p><center><img src="https://img-blog.csdnimg.cn/20190831090058665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="500px"></center>## 模型计算FM是可以将计算复杂度降低到nk的，同样TransFM也可以降低其计算负责度。首先：<center><img src="https://img-blog.csdnimg.cn/20190831090139230.png" width="300px"></center><p>其次进行化简得：</p><center><img src="https://img-blog.csdnimg.cn/20190831090151717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><p>上面的第一个总和可以分成六个单独项，每一项又可以继续进行化简：</p><center><img src="https://img-blog.csdnimg.cn/20190831090204314.png" width="300px"></center><p>假设输入的特征是n维，隐向量长度为k，那么时间复杂度就是O(nk)，而不是O(n^2k)。</p><h2 id="参数优化"><a href="#参数优化" class="headerlink" title="参数优化"></a>参数优化</h2><p>模型使用S-BPR（Sequential Bayesian Personalized Ranking）进行优化，其优化方式如下：</p><center><img src="https://img-blog.csdnimg.cn/20190831090239744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="350px"></center>其中Ω(Θ)为L2正则。## 实践和推断作者等人在TensorFlow中对TransFM进行了实现，用的是mini-batch gradient descent 和 Adam进行模型的训练（adam对于有大量参数且稀疏的数据集上表现良好）。作者这里也罢代码进行了开源，包括数据集，已经不同算法实现实现对比，其地址为：https://github.com/rpasricha/TransFM# 实验作者结合了一些算法在亚马逊和谷歌数据集上进行测试，其中评价的指标是AUC，效果如下：<center><img src="https://img-blog.csdnimg.cn/20190831090454914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="500px"></center><center><img src="https://img-blog.csdnimg.cn/20190831090503108.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="350px"></center><p>上边的Table 3是指从Amazon选取top5 品类 ，从Google Local 中选取6个城市作为实验依据。</p><h1 id="FM模型和其他模型的融合"><a href="#FM模型和其他模型的融合" class="headerlink" title="FM模型和其他模型的融合"></a>FM模型和其他模型的融合</h1><p>PRME（Personalized Ranking Metric Embedding）</p><center><img src="https://img-blog.csdnimg.cn/2019083109063261.png" width="300px"></center><blockquote><p>和TransFM对比的不同在于TransFM中i的向量是embedding向量和translation向量和，而这里没有translation向量。实时证明TransFM效果要好很多。</p></blockquote><p>HRM（Hierarchical Representation Model ）</p><center><img src="https://img-blog.csdnimg.cn/20190831090640757.png" width="300px"></center><p>对比的实验结果如下：</p><center><img src="https://img-blog.csdnimg.cn/20190831090647827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="500px"></center><h1 id="我的总结"><a href="#我的总结" class="headerlink" title="我的总结"></a>我的总结</h1><ol><li>TransFM结合了TransRec 和 FM和优势，在大量，稀疏的数据集上取得了不错的效果。</li><li>在参数和特征纬度下，计算时间线性增大（nk）</li><li>改变FM中的内积计算方式，使用平方欧几里德距离，提高了模型的泛化能力，和样本特征之间的传递性</li><li>在不改变模型结构的前提下，可以轻易将时间，地域或者其他内容特征加入到模型中</li><li>数据集拆分时避免了从整体数据集中的随机拆分，而是按照时间先后的顺序进行拆分。保证了一定的时间连续性，很多论文中划分训练集和测试集时都是这样做的，在工业界中模型的训练和评估大部分也是这样做的。</li><li>根据经验将参数限定在一个范围内，根据网格搜索法寻找最佳参数</li><li>实验对比的丰富性，使结论更具有说服力</li></ol><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;序列推荐模型 Translation-based Recommendation，参考：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100129827&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击阅读&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="论文" scheme="http://thinkgamer.cn/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="TransFM" scheme="http://thinkgamer.cn/tags/TransFM/"/>
    
  </entry>
  
  <entry>
    <title>【论文】RecSys17-序列推荐模型 Translation-based Recommendation</title>
    <link href="http://thinkgamer.cn/2019/08/29/%E8%AE%BA%E6%96%87/%E3%80%90%E8%AE%BA%E6%96%87%E3%80%91RecSys17-%E5%BA%8F%E5%88%97%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%20Translation-based%20Recommendation/"/>
    <id>http://thinkgamer.cn/2019/08/29/论文/【论文】RecSys17-序列推荐模型 Translation-based Recommendation/</id>
    <published>2019-08-29T00:19:12.000Z</published>
    <updated>2019-10-14T06:42:35.690Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>序列推荐模型 TransFM（Translation-based Factorization Machines for Sequential Recommendation）参考：<a href="https://thinkgamer.blog.csdn.net/article/details/100168818" target="_blank" rel="external">点击阅读</a></p></blockquote><a id="more"></a><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>这篇论文是由 Ruining He，Wang-Cheng Kang和Julian McAuley三位大佬提出的，在2017年的ACM推荐系统会议（RecSys’17）上获得了最佳论文奖（在大佬主页可以下载该论文中涉及的代码和数据集，可惜代码是C++写的，不懂C++的童鞋挑战性很大～）</p><ul><li>第一作者的Ruining He主页为<a href="https://sites.google.com/view/ruining-he/" target="_blank" rel="external">https://sites.google.com/view/ruining-he/</a></li><li>RecSys历届最佳论文地址：<a href="https://recsys.acm.org/best-papers/" target="_blank" rel="external">https://recsys.acm.org/best-papers/</a></li><li>本论文下载地址：<a href="https://arxiv.org/pdf/1707.02410.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1707.02410.pdf</a></li></ul><p>论文的两个研究点：</p><ul><li>用户的序列推荐（用户在浏览了一些items之后给他推荐物品j）</li><li>物品到物品的推荐（用户购买了一个牛仔裤，给他推荐一个衬衫）</li></ul><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>对用户和物品以及物品和物品之间的关系进行建模是设计一个成功推荐系统的核心。一种经典的做法是预测用户行为序列（或者是下一个物品的推荐），其挑战在于对用户，用户历史行为物品和用户接下来有行为的物品之间的三阶交互关系进行建模。现有的方法是对这些高阶的交互分解为成对的关系组合，通过不同的模型去对用户的偏好（用户和物品的交互）和序列匹配（物品和物品的交互）进行建模。</p><p>比如MF（Matrix Factorization，矩阵分解）只对用户和物品之间的交互行为进行建模；MC（Markov Chain，马尔可夫链）只对用交互过程中的物品对进行建模，通常通过对转移矩阵进行分解提高其泛化能力。</p><p>对于序列推荐，研究者提出了可扩展的张量分解方法，比如FPMC（Factorized Personalized Markov Chains），FPMC通过两个成对的交互关系来模拟u，i，j之间的三阶交互关系，其实这就是MF和MC的结合。对于提升FPMC有两个方向的研究思路，一个思路是在个性化度量嵌入方法用欧几里德距离替换FPMC中的内积，其中度量假设尤其是三角不等式使模型的泛化性更好，然而，这些研究工作采用的仍然是对用户偏好和序列的连续性分别建模的框架，由于这两部分本身存在关系，因此这样做是存在一定的问题的。另一个思路是利用平均/最大池化等操作去聚合用户u和前一项i的向量表示，然后再测量它们与下一个项j的相似度，这种思路虽然部分解决了两个组件之间的相互依赖问题，但很难解释而且不能从度量embedding向量中获得收益。</p><p>为了解决上述存在的问题，提出了Translation-based Recommendation模型，具体解释往下看。</p><hr><h1 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h1><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>涉及的相关字符含义如下图所示：</p><center><img src="https://img-blog.csdnimg.cn/20190829080844419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>TransRec的主要思路如下图：</p><center><img src="https://img-blog.csdnimg.cn/20190829080909516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><p>物品作为一个点被嵌入到翻译空间内，用户的序列行为则作为一个翻译向量存在于该空间，然后，通过个性化翻译操作捕获前面提到的三阶交互，其基本思路就是用户的翻译向量和上一个行为物品的翻译向量之和，确定下一个有行为的物品j，如下所示</p><script type="math/tex; mode=display">\underset{ \gamma _i }{\rightarrow} + \underset{t_u}{\rightarrow}  \approx  \underset{ \gamma _j }{\rightarrow}</script><p>其中距离的计算可以采用L1-distance或者L2-distance。<br>由于生产环境中数据的稀疏性，很难为每个用户学习到一个向量，因此添加了一个全局翻译向量来初始化所有的用户，这样也能够有效的缓解用户冷启动。如下所示</p><script type="math/tex; mode=display">\underset{ T_u }{\rightarrow} = \underset{t}{\rightarrow}  + \underset{ t_u }{\rightarrow}</script><p>在一个生产系统中，往往会存在头部物品（即热门物品），这些物品的流行度很高，那么如果仅仅采用简单的距离计算的话，那些头部物品就很可能出现在每个用户的推荐结果中。因此在计算时增加了一个偏置项beta_j来对热门物品进行降权。最终对于给定的用户u和之前的行为物品i可以由以下的计算公式为其推荐物品j。</p><center><img src="https://img-blog.csdnimg.cn/20190829080953218.png" width="200px"></center>物品j的热度越高，则beta_j越小，这样当两个物品计算出来的距离一致时，倾向于推荐那些热度小的物品，这样也能够在一定程度上提高推荐物品的多样性。***这里有一点需要注意的是：为了避免“维度诅咒”问题，将r_j限定在整个翻译空间的一个子集上，例如一个单位球体的空间范围。***对于给定过的用户和历史行为序列，模型的目标是对集合中的物品进行排序，这里采用的是pairwise方法的S-BPR（Sequential Bayesian Personalized Ranking）。其优化的公式如下：<center><img src="https://img-blog.csdnimg.cn/20190829081100913.png" width="300px"></center>其中j是真实的下一个交互的物品，j'是除j之外的集合中的任意一个物品。omega为L2正则项。## 参数学习物品i和用户对应的全局翻译向量随机初始化为单元向量，每个物品的偏置向量和每个用户的翻译向量初始化为0。目标函数通过随机梯度上升进行优化，随机从集合中抽取用户u，正例j和负例j'，通过下面的计算公式进行迭代：<center><img src="https://img-blog.csdnimg.cn/20190829081139397.png" width="300px"></center>其中ε为学习率，λ为正则项参数。重复该公式，直到收敛或者效果达到最优或者达到最大迭代次数。## 最近邻查找在测试时，可以通过最近邻搜索进行推荐，一个小的挑战就是物品的偏差。这里分为两部分去解决这个问题。第一使用：$$\beta _j ' \leftarrow  \beta _j - max_{k\in I}\beta _k$$ 表示$\beta _j$对偏置项进行转换，不会改变计算结果的排序。第二使用L2范数计算$$\overrightarrow{\gamma _j}' = (\overrightarrow{\gamma _j}';\sqrt{-\beta _j'})$$或者使用L1范数计算$$\overrightarrow{\gamma _j}' = (\overrightarrow{\gamma _j}';-\beta _j')$$实验证明L2范数效果更好。对于给定的用户u和物品i，在整个向量空间内为其计算寻找最近的$\overrightarrow{\gamma _j}'$---# 实验为了充分验证TransRec的优势，使用了大量的公开数据集，如下图所示：<center><img src="https://img-blog.csdnimg.cn/20190829081637879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><p>在算法选择上以PopRec为baseline，BPR-MF，FMC，FPMC，PRME，HRM作为对比算法模型，在上表中的数据集上对比实验如下：</p><center><img src="https://img-blog.csdnimg.cn/2019082908164614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><p>在进行实验寻找最佳参数时，使用的是网格搜索法，使用随机提督上升优化模型的学习率为0.05，正则项参数的测试范围是：{0, 0.001, 0.01, 0.1, 1}。在TransRec中尝试了L2-ball 和 L2-sphere计算距离，L2-ball的效果更好一些。</p><p>在最开始我们提到本论文主要有两点<br>用户的序列推荐（用户在浏览了一些items之后给他推荐物品j）<br>物品到物品的推荐（用户购买了一个牛仔裤，给他推荐一个衬衫）</p><p>在TransRec中，通过删除个性化向量部分，TransRec可以直接进行物品到物品的推荐，这和知识图谱中的推荐有点相似，因为需要对不同项之间的关系进行建模。其中实现的结果类似于下图这样。</p><center><img src="https://img-blog.csdnimg.cn/20190829081659946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><p>同样也对该部分进行了单独的实验，采用的数据集，对比的算法和实验的结论如下：</p><center><img src="https://img-blog.csdnimg.cn/20190829081707975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="400px"></center><hr><h1 id="我的总结"><a href="#我的总结" class="headerlink" title="我的总结"></a>我的总结</h1><ol><li>文章中介绍了TransRec的优势<br>（1）只用一个模型来模拟用户，物品之间的三阶交互<br>（2）可以从隐式假设度量中获益<br>（3）轻易的解决数据量大的问题</li><li>论文中不仅提出了使用一个模型来对用户物品之间的三阶关系进行建模，还借鉴知识图谱中的思想提出了物品道物品之间的推荐。</li><li>文中实验时将数据集拆分成了三部分，训练集，验证集，和测试集。其比例为8:1:1。</li><li>数据集拆分时避免了从整体数据集中的随机拆分，而是按照时间先后的顺序进行拆分。保证了一定的时间连续性，这一点值得借鉴。</li><li>在寻找最佳参数时使用的是网格搜索法。</li><li>用户的翻译向量采用了全局翻译向量和个性化的翻译向量之和。一定程度上解决了用户的冷启动。</li><li>样本偏置项，减小物品本身热度对模型的影响。</li><li>为了避免“维度诅咒”问题，将样本限定在整体样本空间的一个子集上。</li><li>基于TransRec的思路，作者又提出了和FM的结合，其论文是Translation-based Factorization Machines for Sequential Recommendation，接下来会对其进行介绍。</li></ol><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;序列推荐模型 TransFM（Translation-based Factorization Machines for Sequential Recommendation）参考：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100168818&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击阅读&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="论文" scheme="http://thinkgamer.cn/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="TransRec" scheme="http://thinkgamer.cn/tags/TransRec/"/>
    
  </entry>
  
  <entry>
    <title>Spark排序算法系列之ALS模型实现</title>
    <link href="http://thinkgamer.cn/2019/08/13/RecSys/Spark%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/Spark%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B9%8BALS%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/"/>
    <id>http://thinkgamer.cn/2019/08/13/RecSys/Spark排序算法/Spark排序算法系列之ALS模型实现/</id>
    <published>2019-08-13T06:27:45.000Z</published>
    <updated>2019-10-14T06:42:35.663Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在上一篇文章中介绍了ALS算法的原理（<a href="https://blog.csdn.net/Gamer_gyt/article/details/98897829" target="_blank" rel="external">点击阅读</a>），在这篇文章中主要介绍一下ALS算法在Spark中的实现。</p></blockquote><a id="more"></a><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>协同过滤(Collaborative Filtering)在推荐系统中应用的非常广，该算法的目标是去填充用户-物品评分矩阵中的缺失值，即未评分。该算法的Spark的ML包和MLlib包中均有实现。</p><p>其中涉及的参数如下：</p><ul><li>numBlocks：数据分区的数目，默认为10</li><li>rank：隐向量的长度，默认是10（m <em> n =&gt; m </em> k - k * n）</li><li>maxIter：最大迭代次数，默认为10</li><li>regParam：正则化参数系数，默认为1.0</li><li>implicitPrefs：控制使用显式反馈还是隐式反馈，默认是false即显式反馈。</li><li>alpha：隐式反馈时的置信度参数，默认为1.0</li><li>nonnegative：是否对最小二乘使用非负约束，默认为false</li></ul><h3 id="隐式反馈与显式反馈"><a href="#隐式反馈与显式反馈" class="headerlink" title="隐式反馈与显式反馈"></a>隐式反馈与显式反馈</h3><p>基于矩阵分解的协同过滤标准方法将用户-物品矩阵中的rate视为用户对项目给出的显式偏好，例如：用户对电影进行评分。</p><p>在许多实际的用例中，通常只能获取隐式反馈数据（例如：观看，点击，购买，喜欢，分享等）。spark.ml中用于处理此类数据的方法取自Collaborative Filtering for Implicit Feedback Datasets。本质上，这种方法不是试图直接对评级矩阵进行建模，而是将数据视为表示用户操作观察强度的数字（例如点击次数或某人花在观看电影上的累积持续时间）。然后，这些数字与观察到的用户偏好的置信水平相关，而不是与项目的明确评级相关。然后，该模型试图找到可用于预测用户对项目的预期偏好的潜在因素。</p><h3 id="正则化参数"><a href="#正则化参数" class="headerlink" title="正则化参数"></a>正则化参数</h3><p>通过用户-物品的评分矩阵中用户的评分物品数和物品收到的评分个数来作为正则项，解决最小二乘更新过程中的问题。 这种方法被命名为“ALS-WR”，可以参考论文： Collaborative Filtering for Implicit Feedback Datasets。它减小来regParam对数据集规模的依赖，因此我们可以将从采样子集中学习的最佳参数应用于完整数据集，并获得较好的结果。</p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>Spark ALS算法支持输出item 或者user的隐向量，据此我们可以计算出用户或者物品的相似度，继而进行排序得到用户或者item的top N相似user或者item。这样在数据进行召回时便可以进行召回了。</p><p>比如根据用户用行为的物品召回，当用户浏览了若干了item时，便将这些item相似的item加入到召回池中，进行rank排序。</p><h3 id="ML中的ALS实现"><a href="#ML中的ALS实现" class="headerlink" title="ML中的ALS实现"></a>ML中的ALS实现</h3><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">object ALSML &#123;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">def main(args</span>: Array[String]): Unit = &#123;</span><br><span class="line">        val spark = SparkSession<span class="variable">.builder</span>()<span class="variable">.master</span>("local[5]")<span class="variable">.appName</span>("ALSML")<span class="variable">.enableHiveSupport</span>()<span class="variable">.getOrCreate</span>()</span><br><span class="line">        Logger<span class="variable">.getRootLogger</span><span class="variable">.setLevel</span>(Level<span class="variable">.WARN</span>)</span><br><span class="line"></span><br><span class="line">        val input = "data/sample_movielens_ratings<span class="variable">.txt</span>"</span><br><span class="line">        val model_param = "maxIters:10,rank:5,numBlocks:10,regParam:0.01,alpha:0.618,userCol:userId,itemCol:movieId,rateCol:rating,implicitPrefs:true"</span><br><span class="line">        val output_model = "model/als_ml"</span><br><span class="line">        // 训练模型 找到合适的参数</span><br><span class="line">        runBasedML(spark,input,model_param,output_model)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def runBasedML(spark: SparkSession, input: String, param: String,output_model_path: String) = &#123;</span><br><span class="line">        import spark<span class="variable">.sqlContext</span><span class="variable">.implicits</span><span class="variable">._</span></span><br><span class="line">        val ratings = spark<span class="variable">.read</span><span class="variable">.textFile</span>(input)<span class="variable">.map</span>(parseRating)<span class="variable">.toDF</span>()</span><br><span class="line">        val Array(training, test) = ratings<span class="variable">.randomSplit</span>(Array(0.8, 0.2))</span><br><span class="line"></span><br><span class="line">        println("创建并训练ALS模型 ...")</span><br><span class="line">        val als = ALSMLUtil<span class="variable">.createModel</span>(param)</span><br><span class="line">        val model = als<span class="variable">.fit</span>(training)</span><br><span class="line">        println("模型的效果评估 ...")</span><br><span class="line">        ALSMLUtil<span class="variable">.evaluateModel</span>(model, test)</span><br><span class="line"></span><br><span class="line">        println("为用户进行item推荐 ...")</span><br><span class="line">        model<span class="variable">.recommendForAllUsers</span>(10)<span class="variable">.show</span>(10)</span><br><span class="line"></span><br><span class="line">        println("为指定用户进行top N item推荐 ...")</span><br><span class="line">        val users = ratings<span class="variable">.select</span>(als<span class="variable">.getUserCol</span>)<span class="variable">.distinct</span>()<span class="variable">.limit</span>(3)</span><br><span class="line">        model<span class="variable">.recommendForUserSubset</span>(users,10)<span class="variable">.show</span>(10)</span><br><span class="line"></span><br><span class="line">        println("为item进行用户推荐 ...")</span><br><span class="line">        model<span class="variable">.recommendForAllItems</span>(10)<span class="variable">.show</span>(10)</span><br><span class="line">        println("为指定的item进行top N 用户推荐 ...")</span><br><span class="line">        val movies = ratings<span class="variable">.select</span>(als<span class="variable">.getItemCol</span>)<span class="variable">.distinct</span>()<span class="variable">.limit</span>(3)</span><br><span class="line">        model<span class="variable">.recommendForItemSubset</span>(movies, 10)<span class="variable">.show</span>(10)</span><br><span class="line"></span><br><span class="line">        println("输出隐向量 ...")</span><br><span class="line">        model<span class="variable">.itemFactors</span><span class="variable">.rdd</span><span class="variable">.map</span>(f =&gt; (f<span class="variable">.get</span>(0), f<span class="variable">.getList</span>(1)<span class="variable">.toArray</span><span class="variable">.mkString</span>(",")))<span class="variable">.take</span>(10)<span class="variable">.foreach</span>(println)</span><br><span class="line"></span><br><span class="line">        println("保存与加载模型 ...")</span><br><span class="line">        model<span class="variable">.write</span><span class="variable">.overwrite</span>()<span class="variable">.save</span>(output_model_path)</span><br><span class="line">        val newModel = ALSModel<span class="variable">.load</span>(output_model_path)</span><br><span class="line">        newModel<span class="variable">.itemFactors</span><span class="variable">.rdd</span><span class="variable">.map</span>(f =&gt; (f<span class="variable">.get</span>(0), f<span class="variable">.getList</span>(1)<span class="variable">.toArray</span><span class="variable">.mkString</span>(",")))<span class="variable">.take</span>(10)<span class="variable">.foreach</span>(println)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def parseRating(str: String): Rating = &#123;</span><br><span class="line">        val fields = str<span class="variable">.split</span>("::")</span><br><span class="line">        assert(fields<span class="variable">.size</span> == 4)</span><br><span class="line">        Rating(fields(0)<span class="variable">.toInt</span>, fields(1)<span class="variable">.toInt</span>, fields(2)<span class="variable">.toFloat</span>, fields(3)<span class="variable">.toLong</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="MLlib中的ALS实现"><a href="#MLlib中的ALS实现" class="headerlink" title="MLlib中的ALS实现"></a>MLlib中的ALS实现</h3><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object ALSMLlib &#123;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">def main(args</span>: Array[String]): Unit = &#123;</span><br><span class="line">        val spark = SparkSession<span class="variable">.builder</span>()<span class="variable">.master</span>("local[5]")<span class="variable">.appName</span>("ALS")<span class="variable">.enableHiveSupport</span>()<span class="variable">.getOrCreate</span>()</span><br><span class="line">        Logger<span class="variable">.getRootLogger</span><span class="variable">.setLevel</span>(Level<span class="variable">.WARN</span>)</span><br><span class="line"></span><br><span class="line">        val input = "data/sample_movielens_ratings<span class="variable">.txt</span>"</span><br><span class="line">        val model_param = "maxIters:10,rank:5,numBlocks:10,regParam:0.01,alpha:0.618,implicitPrefs:true"</span><br><span class="line">        val output_model_path = "model/als_ml"</span><br><span class="line"></span><br><span class="line">        run(spark, input, model_param, output_model_path)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def run(spark: SparkSession, input: String, model_param: String, output_model_path: String): Unit = &#123;</span><br><span class="line">        println("加载数据 ...")</span><br><span class="line">        val ratings = spark<span class="variable">.sparkContext</span><span class="variable">.textFile</span>(input)</span><br><span class="line">            <span class="variable">.map</span>(_<span class="variable">.split</span>("::")<span class="variable">.slice</span>(0,3) match &#123; case Array(userId, movieId, rating)  =&gt;</span><br><span class="line">                Rating(userId<span class="variable">.toString</span><span class="variable">.toInt</span>, movieId<span class="variable">.toString</span><span class="variable">.toInt</span>, rating<span class="variable">.toString</span><span class="variable">.toDouble</span>)</span><br><span class="line">            &#125;)</span><br><span class="line">        println("训练模型 ...")</span><br><span class="line">        val param = new ALSMLlibParam()</span><br><span class="line">        param<span class="variable">.parseString</span>(model_param)</span><br><span class="line">        val model = ALS<span class="variable">.train</span>(ratings,param<span class="variable">.getRank</span>, param<span class="variable">.getMaxIters</span>,param<span class="variable">.getAlpha</span>,param<span class="variable">.getNumBlocks</span>)</span><br><span class="line"></span><br><span class="line">        println("评估模型 ...")</span><br><span class="line">        val usersProducts = ratings<span class="variable">.map</span> &#123; case Rating(user, product, rate) =&gt; (user, product) &#125;</span><br><span class="line">        val predictions = model<span class="variable">.predict</span>(usersProducts)<span class="variable">.map</span>&#123; case Rating(user, product, rate) =&gt; ((user,product),rate)&#125;</span><br><span class="line">        val rateAndPre = ratings<span class="variable">.map</span> &#123; case Rating(user, product, rate) =&gt; ((user, product), rate) &#125;<span class="variable">.join</span>(predictions)</span><br><span class="line">        val MSE = rateAndPre<span class="variable">.map</span> &#123; case ((user, product), (r1, r2)) =&gt;</span><br><span class="line">            val err = (r1 - r2)</span><br><span class="line">            err * err</span><br><span class="line">        &#125;<span class="variable">.mean</span>()</span><br><span class="line">        println("Mean Squared Error = " + MSE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        println(s"用户（2）对 物品（2）的预测评分为：$&#123;model<span class="variable">.predict</span>(2,2)&#125;")</span><br><span class="line">        println("用户纬度的特征向量为：")</span><br><span class="line">        model<span class="variable">.userFeatures</span><span class="variable">.map</span>(f =&gt; (f<span class="variable">._</span>1,f<span class="variable">._</span>2<span class="variable">.mkString</span>(",")))<span class="variable">.take</span>(10)<span class="variable">.foreach</span>(println)</span><br><span class="line">        println("物品纬度的特征向量为：")</span><br><span class="line">        model<span class="variable">.productFeatures</span><span class="variable">.map</span>(f =&gt; (f<span class="variable">._</span>1,f<span class="variable">._</span>2<span class="variable">.mkString</span>(",")))<span class="variable">.take</span>(10)<span class="variable">.foreach</span>(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.scheduler</span><span class="selector-class">.DAGSchedulerEventProcessLoop</span><span class="selector-class">.onReceive</span>(DAGScheduler<span class="selector-class">.scala</span>:<span class="number">1821</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.scheduler</span><span class="selector-class">.DAGSchedulerEventProcessLoop</span><span class="selector-class">.onReceive</span>(DAGScheduler<span class="selector-class">.scala</span>:<span class="number">1810</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.util</span><span class="selector-class">.EventLoop</span>$<span class="variable">$anon</span>$<span class="number">1</span>.run(EventLoop<span class="selector-class">.scala</span>:<span class="number">48</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.scheduler</span><span class="selector-class">.DAGScheduler</span><span class="selector-class">.runJob</span>(DAGScheduler<span class="selector-class">.scala</span>:<span class="number">642</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.SparkContext</span><span class="selector-class">.runJob</span>(SparkContext<span class="selector-class">.scala</span>:<span class="number">2034</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.SparkContext</span><span class="selector-class">.runJob</span>(SparkContext<span class="selector-class">.scala</span>:<span class="number">2055</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.SparkContext</span><span class="selector-class">.runJob</span>(SparkContext<span class="selector-class">.scala</span>:<span class="number">2074</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>$<span class="variable">$anonfun</span><span class="variable">$take</span>$<span class="number">1</span>.apply(RDD<span class="selector-class">.scala</span>:<span class="number">1364</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDDOperationScope</span>$.withScope(RDDOperationScope<span class="selector-class">.scala</span>:<span class="number">151</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDDOperationScope</span>$.withScope(RDDOperationScope<span class="selector-class">.scala</span>:<span class="number">112</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span><span class="selector-class">.withScope</span>(RDD<span class="selector-class">.scala</span>:<span class="number">363</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span><span class="selector-class">.take</span>(RDD<span class="selector-class">.scala</span>:<span class="number">1337</span>)</span><br><span class="line">at com<span class="selector-class">.kk</span><span class="selector-class">.recommend</span><span class="selector-class">.tools</span><span class="selector-class">.model</span><span class="selector-class">.ALSBasedMLUtil</span>$.evaluateModel(ALSBasedMLUtil<span class="selector-class">.scala</span>:<span class="number">51</span>)</span><br><span class="line">at com<span class="selector-class">.kk</span><span class="selector-class">.recommend</span><span class="selector-class">.topic</span><span class="selector-class">.follow</span><span class="selector-class">.ItemCFV2</span>$.testBasedML(ItemCFV2<span class="selector-class">.scala</span>:<span class="number">104</span>)</span><br><span class="line">at com<span class="selector-class">.kk</span><span class="selector-class">.recommend</span><span class="selector-class">.topic</span><span class="selector-class">.follow</span><span class="selector-class">.ItemCFV2</span>$.main(ItemCFV2<span class="selector-class">.scala</span>:<span class="number">40</span>)</span><br><span class="line">at com<span class="selector-class">.kk</span><span class="selector-class">.recommend</span><span class="selector-class">.topic</span><span class="selector-class">.follow</span><span class="selector-class">.ItemCFV2</span><span class="selector-class">.main</span>(ItemCFV2.scala)</span><br><span class="line">at sun<span class="selector-class">.reflect</span><span class="selector-class">.NativeMethodAccessorImpl</span><span class="selector-class">.invoke0</span>(Native Method)</span><br><span class="line">at sun<span class="selector-class">.reflect</span><span class="selector-class">.NativeMethodAccessorImpl</span><span class="selector-class">.invoke</span>(NativeMethodAccessorImpl<span class="selector-class">.java</span>:<span class="number">62</span>)</span><br><span class="line">at sun<span class="selector-class">.reflect</span><span class="selector-class">.DelegatingMethodAccessorImpl</span><span class="selector-class">.invoke</span>(DelegatingMethodAccessorImpl<span class="selector-class">.java</span>:<span class="number">43</span>)</span><br><span class="line">at java<span class="selector-class">.lang</span><span class="selector-class">.reflect</span><span class="selector-class">.Method</span><span class="selector-class">.invoke</span>(Method<span class="selector-class">.java</span>:<span class="number">498</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.deploy</span><span class="selector-class">.yarn</span><span class="selector-class">.ApplicationMaster</span>$<span class="variable">$anon</span>$<span class="number">2</span>.run(ApplicationMaster<span class="selector-class">.scala</span>:<span class="number">688</span>)</span><br><span class="line">Caused by: java<span class="selector-class">.lang</span><span class="selector-class">.ArrayStoreException</span>: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.catalyst</span><span class="selector-class">.expressions</span><span class="selector-class">.GenericRowWithSchema</span></span><br><span class="line">at scala<span class="selector-class">.runtime</span><span class="selector-class">.ScalaRunTime</span>$.array_update(ScalaRunTime<span class="selector-class">.scala</span>:<span class="number">90</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.IndexedSeqOptimized</span><span class="variable">$class</span>.copyToArray(IndexedSeqOptimized<span class="selector-class">.scala</span>:<span class="number">180</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.mutable</span><span class="selector-class">.WrappedArray</span><span class="selector-class">.copyToArray</span>(WrappedArray<span class="selector-class">.scala</span>:<span class="number">35</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.TraversableOnce</span><span class="variable">$class</span>.copyToArray(TraversableOnce<span class="selector-class">.scala</span>:<span class="number">278</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.AbstractTraversable</span><span class="selector-class">.copyToArray</span>(Traversable<span class="selector-class">.scala</span>:<span class="number">104</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.TraversableOnce</span><span class="variable">$class</span>.toArray(TraversableOnce<span class="selector-class">.scala</span>:<span class="number">286</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.mutable</span><span class="selector-class">.WrappedArray</span><span class="selector-class">.toArray</span>(WrappedArray<span class="selector-class">.scala</span>:<span class="number">73</span>)</span><br><span class="line">at com<span class="selector-class">.kk</span><span class="selector-class">.recommend</span><span class="selector-class">.tools</span><span class="selector-class">.model</span><span class="selector-class">.ALSBasedMLUtil</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.apply(ALSBasedMLUtil<span class="selector-class">.scala</span>:<span class="number">48</span>)</span><br><span class="line">at com<span class="selector-class">.kk</span><span class="selector-class">.recommend</span><span class="selector-class">.tools</span><span class="selector-class">.model</span><span class="selector-class">.ALSBasedMLUtil</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.apply(ALSBasedMLUtil<span class="selector-class">.scala</span>:<span class="number">46</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.Iterator</span>$<span class="variable">$anon</span>$<span class="number">12</span>.nextCur(Iterator<span class="selector-class">.scala</span>:<span class="number">434</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.Iterator</span>$<span class="variable">$anon</span>$<span class="number">12</span>.hasNext(Iterator<span class="selector-class">.scala</span>:<span class="number">440</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.Iterator</span>$<span class="variable">$anon</span>$<span class="number">10</span>.hasNext(Iterator<span class="selector-class">.scala</span>:<span class="number">389</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.Iterator</span><span class="variable">$class</span>.foreach(Iterator<span class="selector-class">.scala</span>:<span class="number">893</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.AbstractIterator</span><span class="selector-class">.foreach</span>(Iterator<span class="selector-class">.scala</span>:<span class="number">1336</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.generic</span><span class="selector-class">.Growable</span><span class="variable">$class</span>.<span class="variable">$plus</span><span class="variable">$plus</span><span class="variable">$eq</span>(Growable<span class="selector-class">.scala</span>:<span class="number">59</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.mutable</span><span class="selector-class">.ArrayBuffer</span>.<span class="variable">$plus</span><span class="variable">$plus</span><span class="variable">$eq</span>(ArrayBuffer<span class="selector-class">.scala</span>:<span class="number">104</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.mutable</span><span class="selector-class">.ArrayBuffer</span>.<span class="variable">$plus</span><span class="variable">$plus</span><span class="variable">$eq</span>(ArrayBuffer<span class="selector-class">.scala</span>:<span class="number">48</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.TraversableOnce</span><span class="variable">$class</span>.to(TraversableOnce<span class="selector-class">.scala</span>:<span class="number">310</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.AbstractIterator</span><span class="selector-class">.to</span>(Iterator<span class="selector-class">.scala</span>:<span class="number">1336</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.TraversableOnce</span><span class="variable">$class</span>.toBuffer(TraversableOnce<span class="selector-class">.scala</span>:<span class="number">302</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.AbstractIterator</span><span class="selector-class">.toBuffer</span>(Iterator<span class="selector-class">.scala</span>:<span class="number">1336</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.TraversableOnce</span><span class="variable">$class</span>.toArray(TraversableOnce<span class="selector-class">.scala</span>:<span class="number">289</span>)</span><br><span class="line">at scala<span class="selector-class">.collection</span><span class="selector-class">.AbstractIterator</span><span class="selector-class">.toArray</span>(Iterator<span class="selector-class">.scala</span>:<span class="number">1336</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>$<span class="variable">$anonfun</span><span class="variable">$take</span>$<span class="number">1</span>$<span class="variable">$anonfun</span>$<span class="number">28</span>.apply(RDD<span class="selector-class">.scala</span>:<span class="number">1364</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>$<span class="variable">$anonfun</span><span class="variable">$take</span>$<span class="number">1</span>$<span class="variable">$anonfun</span>$<span class="number">28</span>.apply(RDD<span class="selector-class">.scala</span>:<span class="number">1364</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.SparkContext</span>$<span class="variable">$anonfun</span><span class="variable">$runJob</span>$<span class="number">5</span>.apply(SparkContext<span class="selector-class">.scala</span>:<span class="number">2074</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.SparkContext</span>$<span class="variable">$anonfun</span><span class="variable">$runJob</span>$<span class="number">5</span>.apply(SparkContext<span class="selector-class">.scala</span>:<span class="number">2074</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.scheduler</span><span class="selector-class">.ResultTask</span><span class="selector-class">.runTask</span>(ResultTask<span class="selector-class">.scala</span>:<span class="number">87</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.scheduler</span><span class="selector-class">.Task</span><span class="selector-class">.run</span>(Task<span class="selector-class">.scala</span>:<span class="number">109</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.executor</span><span class="selector-class">.Executor</span><span class="variable">$TaskRunner</span>.run(Executor<span class="selector-class">.scala</span>:<span class="number">381</span>)</span><br><span class="line">at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.ThreadPoolExecutor</span><span class="selector-class">.runWorker</span>(ThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">1149</span>)</span><br><span class="line">at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.run(ThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">624</span>)</span><br><span class="line">at java<span class="selector-class">.lang</span><span class="selector-class">.Thread</span><span class="selector-class">.run</span>(Thread<span class="selector-class">.java</span>:<span class="number">748</span>)</span><br></pre></td></tr></table></figure><p>结局办法：</p><p>解决办法：<a href="https://stackoverflow.com/questions/32727518/genericrowwithschema-exception-in-casting-arraybuffer-to-hashset-in-dataframe-to" target="_blank" rel="external">点击阅读</a></p><p>打印的Schema信息：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- <span class="string">userId:</span> integer (nullable = <span class="literal">false</span>)</span><br><span class="line"> |-- <span class="string">recommendations:</span> array (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">element:</span> struct (containsNull = <span class="literal">true</span>)</span><br><span class="line"> |    |    |-- <span class="string">topicId:</span> integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |    |-- <span class="string">rating:</span> <span class="keyword">float</span> (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></p><blockquote><p>在row 中的这些列取得时候，要根据类型取，简单的像String，Seq[Double] 这种类型就可以直接取出来，但是像 Seq[(Double,Double)] 这种类型直接取得花就会丢失schema信息，虽然值能取到，但是schema信息丢了，在dataFrame中操作的时候就会抛错</p></blockquote><p>ALS测试结果数据的格式如下：</p><div class="table-container"><table><thead><tr><th></th><th>userId</th><th>recommendations</th></tr></thead><tbody><tr><td>  148</td><td>[[1972, 0.0334868…</td></tr></tbody></table></div><p>原始的写法是：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">result.select(<span class="string">"userId"</span>, <span class="string">"recommendations"</span>)</span><br><span class="line">        .filter(<span class="function"><span class="params">row</span> =&gt;</span> !(row.isNullAt(<span class="number">0</span>) || row.isNullAt(<span class="number">1</span>)))</span><br><span class="line">        .rdd.flatMap( <span class="function"><span class="params">l</span>=&gt;</span>&#123;</span><br><span class="line">            val uid = l.get(<span class="number">0</span>).toString</span><br><span class="line">            val itemList = l.getAs[mutable.WrappedArray[(Int,Double)]](<span class="string">"recommendations"</span>)</span><br><span class="line">            <span class="keyword">for</span>(item&lt;- itemList) <span class="keyword">yield</span> (uid, item._1.toString)</span><br><span class="line">        &#125;)</span><br></pre></td></tr></table></figure><p>修改后为：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">result.select(<span class="string">"userId"</span>, <span class="string">"recommendations"</span>)</span><br><span class="line">        .filter(<span class="function"><span class="params">row</span> =&gt;</span> !(row.isNullAt(<span class="number">0</span>) || row.isNullAt(<span class="number">1</span>)))</span><br><span class="line">        .rdd.flatMap( <span class="function"><span class="params">l</span>=&gt;</span>&#123;</span><br><span class="line">            val uid = l.get(<span class="number">0</span>).toString</span><br><span class="line">            val itemList= l.getAs[Seq[Row]](<span class="number">1</span>).map(<span class="function"><span class="params">x</span>=&gt;</span>&#123;(x.getInt(<span class="number">0</span>),x.getFloat(<span class="number">1</span>))&#125;)</span><br><span class="line">            <span class="keyword">for</span>(item&lt;- itemList) <span class="keyword">yield</span> (uid, item._1.toString)</span><br><span class="line">            &#125;)</span><br></pre></td></tr></table></figure></p><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在上一篇文章中介绍了ALS算法的原理（&lt;a href=&quot;https://blog.csdn.net/Gamer_gyt/article/details/98897829&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击阅读&lt;/a&gt;），在这篇文章中主要介绍一下ALS算法在Spark中的实现。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="CTR" scheme="http://thinkgamer.cn/tags/CTR/"/>
    
  </entry>
  
  <entry>
    <title>基于协同的ALS算法原理介绍与实现</title>
    <link href="http://thinkgamer.cn/2019/08/08/RecSys/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E7%9A%84ALS%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <id>http://thinkgamer.cn/2019/08/08/RecSys/推荐算法/基于协同的ALS算法原理介绍与实现/</id>
    <published>2019-08-08T15:41:20.000Z</published>
    <updated>2019-10-14T06:42:35.664Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ALS也是一种协同算法，其全称是交替最小二乘法（Alternating Least Squares），由于简单高效，已被广泛应用在推荐场景中，目前已经被集成到Spark MLlib和ML库中，在下一篇文章会对其使用方式进行详细介绍，本篇文章主要介绍ALS的底层算法原理。</p></blockquote><a id="more"></a><h3 id="最小二乘法（Least-Squares）"><a href="#最小二乘法（Least-Squares）" class="headerlink" title="最小二乘法（Least Squares）"></a>最小二乘法（Least Squares）</h3><p>在介绍ALS算法之前，先来了解LS，即最小二乘法。LS算法是ALS的基础，是一种数学优化技术，也是一种常用的机器学习算法，他通过最小化误差平方和寻找数据的最佳匹配，利用最小二乘法寻找最优的未知数据，保证求的数据与已知的数据误差最小。</p><p>LS也被用于拟合曲线，比如所熟悉的线性模型。下面以简单的线性一元线性回归模型说明最小二乘法。<br>假设我们有一组数据{(x1,y1),(x2,y2),(x3,y3)…}其符合线性回归，假设其符合的函数为如下：</p><script type="math/tex; mode=display">y = w_0 + w_1 x</script><p>我们使用一个平方差函数来表达参数的好坏，平方差函数如下：</p><script type="math/tex; mode=display">L_n = (y_n - f(x;w_0,w_1))^2</script><p>其中f(.) 表示我们假设的线性回归函数。显然Ln越小越好，Ln越小表示误差越小。假设有N个样本，则N个样本的平均平方差为：</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_{n=1}^{N} (y_n - f(x;w_0,w_1))^2</script><p>L越小表示参数w越精确，而这里最关键的就是寻找到最合适的w0和w1，则此时的数学表达式为：</p><script type="math/tex; mode=display">\underset{w_0,w_1}{arg \ min}  \frac{1}{N} \sum_{n=1}^{N} (y_n - f(x;w_0,w_1))^2</script><p>将先行回归函数代入到最小二乘损失函数中，得到的结果为：</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_{n=1}^{N} (y_n - w_0 - w_1 x_n)^2\\\frac{1}{N} \sum_{n=1}^{N} (w_1 ^2x_n^2 + 2w_1x_n(w_0 - y_n) + w_0^2 - 2w_0y_n + y_n^2)</script><p>L函数取得最小值时，w0和w1的一阶偏导数一定是0（因为误差平方和是一个大于等于0的数，是没有最大值的，所以取得最小值时，一阶偏导数一定为0）。因为对L函数分别求偏导，使其等于0，并对w0和w1求解，即可。</p><h3 id="交替最小二乘法（Alternating-Least-Squares）"><a href="#交替最小二乘法（Alternating-Least-Squares）" class="headerlink" title="交替最小二乘法（Alternating Least Squares）"></a>交替最小二乘法（Alternating Least Squares）</h3><p>ALS算法本质上是基于物品的协同，近年来，基于模型的推荐算法ALS(交替最小二乘)在Netflix成功应用并取得显著效果提升，ALS使用机器学习算法建立用户和物品间的相互作用模型，进而去预测新项。</p><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>用户对物品的打分行为可以用一个矩阵（R）来表示：<br><img src="https://img-blog.csdnimg.cn/20190808233244547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>矩阵中的打分值 r_ij表示用户 u_i 对物品 v_j 的打分，其中”?”表示用户没有打分，这也就是要通过机器学习的方法去预测这个打分值，从而达到推荐的目的。</p><h4 id="模型抽象"><a href="#模型抽象" class="headerlink" title="模型抽象"></a>模型抽象</h4><p>根绝协同过滤的思想，R矩阵的行向量对应每个用户U，列向量对应每个物品V。ALS的核心思想是：将用户和物品都投射到k维空间，也就是说假设有k个隐向量特征，至于这个k个隐向量是什么不用关系（可能是标签，年龄，性别等），将每个用户和每个物品都用k维的向量来表示，把他们的内积近似为打分值，这样便可以得到近似的评分。</p><script type="math/tex; mode=display">R \approx UV^T</script><p>其中：</p><ul><li>R为打分矩阵（m*n，m表示用户个数，n表示物品个数）</li><li>U表示用户对隐含特征的偏好矩阵（m*k）</li><li>V表示物品对隐含特征的归属矩阵（n*K）</li></ul><p>上述模型的参数就是U和V，求得U和V之后，就可以近似的得到用户对未评分物品的评分。</p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>求上述公式中的U和V，就需要一个代价函数来衡量参数的拟合程度。用户对物品的行为分为显式行为和隐式行为，两种不同类型的行为下，对应的代价函数也是不一样的。</p><blockquote><p>关于显式行为和隐式行为的介绍可以餐考 我的《推荐系统开发实战》一书。</p></blockquote><p><strong>显式反馈代价函数</strong><br>如果用户对物品有明确的评分行为，那么可以对比重构出来的评分矩阵和实际的评分矩阵，便可得到误差。由于用户对物品的评分却失很多，仅以有评分行为的物品去计算误差。下面是显式反馈的代价函数。</p><script type="math/tex; mode=display">J(U,V) = \sum_{i}^{m} \sum_{j}^{n}[(r_{ij} - u_iv_j^T) ^2 + \lambda ( ||u_i||^2 + ||v_j||^2 ) ]</script><p>其中：λ 为正则项系数</p><h4 id="隐式反馈代价函数"><a href="#隐式反馈代价函数" class="headerlink" title="隐式反馈代价函数"></a>隐式反馈代价函数</h4><p><strong><em>隐式反馈对应的ALS算法即：ALS-WR（Alternating Least Squares With Weighted-λ -regularization）</em></strong></p><p>很多情况下，用户并没有明确反馈对物品的偏好，需要通过用户的相关行为去推测其对物品的偏好，比如在电商网站中，用户是否点击物品，点击的话在一定程度上表示喜欢，未点击的话可能是不喜欢，也可能是没有看到该物品。这种形式下的反馈就被称为隐式反馈。即矩阵R为隐式反馈矩阵，引入变量p_ij表示用户u_i对物品v_j的置信度，如果隐式反馈大于0，置信度为，反之置信度为0。</p><script type="math/tex; mode=display">p_{ij} = \left\{\begin{matrix}1  & r_{ij} >0 \\ 0 & r_{ij} =0\end{matrix}\right.</script><p>上文也提到了，隐式反馈为0，不代表用户完全不喜欢，也可能是用户没有看到该物品。另外用户点击一个物品，也不代表是喜欢他，可能是误点，所以需要一个信任等级来显示用户喜欢某个物品，一般情况下，r_ij越大(用户行为的次数)，越能暗示用户喜欢某个物品，因此引入变量c_ij，来衡量p_ij的信任度。</p><script type="math/tex; mode=display">c_{ij} = 1 + \alpha r_{ij}</script><p>α 为置信度系数，那么代价函数变为如下形式：</p><script type="math/tex; mode=display">J(U,V) = \sum_{i}^{m} \sum_{j}^{n}[c_{ij}(p_{ij} - u_iv_j^T) ^2 + \lambda ( ||u_i||^2 + ||v_j||^2 ) ]</script><h4 id="算法求解"><a href="#算法求解" class="headerlink" title="算法求解"></a>算法求解</h4><p>无论是隐式代价函数求解还是显式代价函数求解，他们都是凸函数，而且变量耦合在一起，常规的梯度下降算法不能求解。但是先固定U求V，再固定V求U，如此迭代下去，问题就可以解决了。</p><script type="math/tex; mode=display">U^0 \rightarrow V^1 \rightarrow U^1 \rightarrow V^2 \rightarrow U^2 ....</script><p>固定一个变量，求另外一个变量，用什么方法求解呢？梯度下降？可以，但是比较麻烦。这其实是一个最小二乘的问题，由于一般隐含的特征k不会太大，可以直接当做是正规方程去解决。如此的交替的使用最小二乘去求解，所以名字就叫做交替最小二乘法。</p><p><strong>显氏反馈求解</strong><br>固定V求解U，对公式进行求导化简，可得：</p><script type="math/tex; mode=display">U ^T = \left( V^T V + \lambda I \right)^{-1} V^T R^T</script><p>同理，固定U求解V，对公式进行求导化简，可得：</p><script type="math/tex; mode=display">V ^T = \left( U^T U + \lambda I \right)^{-1} U^T R</script><p><strong>隐式反馈求解</strong><br>固定V求解U，对公式进行求导化简，可得：</p><script type="math/tex; mode=display">U ^T = \left( V^T C_v V + \lambda I \right)^{-1} V^T C_v R^T</script><p>同理，固定U求解V，对公式进行求导化简，可得：</p><script type="math/tex; mode=display">V ^T = \left( U^T C_u U + \lambda I \right)^{-1} U^T C_u R</script><hr><h3 id="面试点"><a href="#面试点" class="headerlink" title="面试点"></a>面试点</h3><ol><li><p>最小二乘法英文名字是什么？解释及其对应的数学原理</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Least</span> Squares</span><br><span class="line">参考上文</span><br></pre></td></tr></table></figure></li><li><p>ALS全称是什么？为什么叫交替最小二乘法？</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Alternaing</span> Least Squares</span><br><span class="line">参考上文</span><br></pre></td></tr></table></figure></li><li><p>隐式反馈和显氏反馈的区别？两种形式下ALS的代价函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">《推荐系统开发实战》中有对其的介绍</span><br><span class="line"> 两种形式下的代价函数参考上文</span><br></pre></td></tr></table></figure></li><li><p>代价函数中的正则项及其含义？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">参考上文</span><br></pre></td></tr></table></figure></li><li><p>过拟合和欠拟合的含义？在ALS中什么情况会出现过拟合和欠拟合？对应的解决办法？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">欠拟合定义：拟合的函数与训练集误差较大，</span><br><span class="line">过拟合定义：拟合的函数与训练集完美匹配（误差很小）</span><br><span class="line">合适拟合定义：拟合的函数与训练集误差较小</span><br><span class="line"></span><br><span class="line">欠拟合出现原因：数据规模太小，特征太多，正则化项系数较小</span><br><span class="line">过拟合出现原因：数据特征太少，正则化项系数较大</span><br><span class="line"></span><br><span class="line">欠拟合解决办法：增大数据规模、减小数据特征数（维数）、增大正则化系数λ</span><br><span class="line">过拟合解决办法：增多数据特征数、添加高次多项式特征、减小正则化系数λ</span><br></pre></td></tr></table></figure></li><li><p>Spark实现ALS可调节的参数有哪些？分别表示什么含义？</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">maxIters <span class="comment">// 最大迭代次数，默认10 </span></span><br><span class="line"><span class="built_in">rank</span> <span class="comment">// 隐向量的长度，默认是10，一般远小于m，n</span></span><br><span class="line">numBlocks <span class="comment">// 数据分区的个数，默认是10</span></span><br><span class="line">regParam <span class="comment">// ALS中的正则化参数，默认是1.0</span></span><br><span class="line">alpha <span class="comment">// ALS隐氏反馈变量的参数，置信度系数，默认是1.0</span></span><br><span class="line">userCol <span class="comment">// 用户列名</span></span><br><span class="line">itemCol <span class="comment">// item列名</span></span><br><span class="line">rateCol <span class="comment">// 评分列名</span></span><br><span class="line">implicitPrefs <span class="comment">// 显氏反馈 还是 隐氏反馈，默认false，意味显氏反馈</span></span><br></pre></td></tr></table></figure></li></ol><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><blockquote><p>【搜索与推荐Wiki】专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ALS也是一种协同算法，其全称是交替最小二乘法（Alternating Least Squares），由于简单高效，已被广泛应用在推荐场景中，目前已经被集成到Spark MLlib和ML库中，在下一篇文章会对其使用方式进行详细介绍，本篇文章主要介绍ALS的底层算法原理。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐算法" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="ALS" scheme="http://thinkgamer.cn/tags/ALS/"/>
    
  </entry>
  
  <entry>
    <title>【技术分享】2019全球人工智能技术峰会PDF资料拿走不谢</title>
    <link href="http://thinkgamer.cn/2019/08/08/Share/%E3%80%90%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E3%80%912019%E5%85%A8%E7%90%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%B3%B0%E4%BC%9APDF%E8%B5%84%E6%96%99%E6%8B%BF%E8%B5%B0%E4%B8%8D%E8%B0%A2/"/>
    <id>http://thinkgamer.cn/2019/08/08/Share/【技术分享】2019全球人工智能技术峰会PDF资料拿走不谢/</id>
    <published>2019-08-08T04:58:29.000Z</published>
    <updated>2019-10-14T06:42:35.672Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>2019 全球人工智能技术峰会PDF资料免费分享，资料内容涵盖各个方面，全部都是一线互联网公司的产业实践。</p></blockquote><a id="more"></a><h3 id="工业实践"><a href="#工业实践" class="headerlink" title="工业实践"></a>工业实践</h3><ul><li>「百度」源于产业实践的开源深度学习平台飞浆（PaddlePaddle）</li><li>「易观」如何建设大数据中台（从0到1建设大数据中台）</li><li>「华为」云边协同，重新定义AI</li></ul><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><ul><li>「网易云」AI算法在音乐推荐中的应用</li><li>「VIPKID」在线教育行业中视频理解的应用</li><li>「美团点评」美团外卖商业变现实践<h3 id="搜索推荐"><a href="#搜索推荐" class="headerlink" title="搜索推荐"></a>搜索推荐</h3></li><li>「荔枝」荔枝UGC推荐探索与实践</li><li>「金山」推荐系统在剑网3推栏项目中的落地</li></ul><h3 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h3><ul><li>「瑞士再保险」知识图谱构建（数据，算法与架构）</li><li>「美团点评」基于知识图谱的问答在O2O智能交 互场景中的应用和演进</li><li>「中科院」基于知识图谱的问答关键技术—从答案到自然答案</li></ul><h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><ul><li>「阿里巴巴」语音对话机器人在阿里小蜜中的相关技术探索</li><li>「追一科技」深度学习在企业智能交互中的应用</li><li>「贝壳找房」4D看房：写稿机器人与VR的美丽邂逅</li></ul><h3 id="智能安防"><a href="#智能安防" class="headerlink" title="智能安防"></a>智能安防</h3><ul><li>「扇贝」A Short Intro: 无处不在的对抗样本攻防</li><li>「友邦安达」AI边缘计算的崛起</li></ul><h3 id="智能金融"><a href="#智能金融" class="headerlink" title="智能金融"></a>智能金融</h3><ul><li>「旷视」旷视AI 在金融风控领域的运用</li><li>「MinTech」MinTech的金融科技 实践与探索</li><li>「百信银行」机器阅读在智能银行中的应用深度剖析与实践</li></ul><h3 id="智慧零售"><a href="#智慧零售" class="headerlink" title="智慧零售"></a>智慧零售</h3><ul><li>「WakeData」唤醒沉睡的数据</li><li>「MobTech」基于创新算法的半监督的 lookalike效果营销</li><li>「京东」新零售时代的智慧中台</li></ul><h3 id="智慧城市"><a href="#智慧城市" class="headerlink" title="智慧城市"></a>智慧城市</h3><ul><li>「哈罗出行」科技推动出行进化-密密织就的出行智能</li></ul><h3 id="智能商业"><a href="#智能商业" class="headerlink" title="智能商业"></a>智能商业</h3><ul><li>「苏宁」苏宁物流 智能决策系统建设与应用</li><li>「贝锐科技」三生万物理论的实践进化</li><li>「科大讯飞」一场客服与AI的融合之旅</li></ul><h3 id="IT架构"><a href="#IT架构" class="headerlink" title="IT架构"></a>IT架构</h3><ul><li>「威佩网络」人工智能和大数据系统在电子竞技数据处理平台中的应用</li><li>「快狗打车」快狗打车智能调度系统架构演进</li><li>「国美」国美Redis集群-国美千亿级Redis集群架构变迁的思考</li></ul><h3 id="AIOps"><a href="#AIOps" class="headerlink" title="AIOps"></a>AIOps</h3><ul><li>「宜信」分布式主动感知在智能运维中的实践</li><li>「F5」无探针实时应用大数据采集引擎最佳实践和AIOps实现</li><li>「日志易」海量日志分析与智能运维</li></ul><h3 id="智能企业赋能"><a href="#智能企业赋能" class="headerlink" title="智能企业赋能"></a>智能企业赋能</h3><ul><li>「百度」企业赋能AI 服务生活 DuerOS的技能服务开发</li><li>「蘑菇街」基于图像技术构建蘑菇街时尚目的地</li><li>「51Talk」人工智能赋能教育-人工智能如何助力K12在线英语</li></ul><hr><p>以上资料，私聊微信获取</p><center><img src="https://img-blog.csdnimg.cn/20190808125509913.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="200px"></center><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="200px"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><center><font color="red">注：《推荐系统开发实战》已经在京东上线，感兴趣的朋友可以进行关注！</font></center><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="40%"></center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;2019 全球人工智能技术峰会PDF资料免费分享，资料内容涵盖各个方面，全部都是一线互联网公司的产业实践。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="技术分享" scheme="http://thinkgamer.cn/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>重庆保时捷女司机揭露了丑恶世道，但终要相信有善有正义有张小敬这样的长安不良帅</title>
    <link href="http://thinkgamer.cn/2019/08/04/%E9%9A%8F%E6%89%8B%E8%AE%B0/%E9%87%8D%E5%BA%86%E4%BF%9D%E6%97%B6%E6%8D%B7%E5%A5%B3%E5%8F%B8%E6%9C%BA%E6%8F%AD%E9%9C%B2%E4%BA%86%E4%B8%91%E6%81%B6%E4%B8%96%E9%81%93%EF%BC%8C%E4%BD%86%E7%BB%88%E8%A6%81%E7%9B%B8%E4%BF%A1%E6%9C%89%E5%96%84%E6%9C%89%E6%AD%A3%E4%B9%89%E6%9C%89%E5%BC%A0%E5%B0%8F%E6%95%AC%E8%BF%99%E6%A0%B7%E7%9A%84%E9%95%BF%E5%AE%89%E4%B8%8D%E8%89%AF%E5%B8%85/"/>
    <id>http://thinkgamer.cn/2019/08/04/随手记/重庆保时捷女司机揭露了丑恶世道，但终要相信有善有正义有张小敬这样的长安不良帅/</id>
    <published>2019-08-04T02:52:43.000Z</published>
    <updated>2019-10-14T06:42:35.694Z</updated>
    
    <content type="html"><![CDATA[<p>世道很好，只是被藏的很好，只有走在那些青石板上，我才能看到石砖墙瓦的流芳百年</p><p>世道很坏，只是很少的存在，只有遇见那些孤立无援，我才能看到人心冷暖的肆无忌惮</p><a id="more"></a><blockquote><p>这是我第一次写对社会事件的看法，当然不是为了蹭热点，而是这件事真的是触动了我。因为最近沉迷于关于西藏的记录片，真心感受到了藏民的质朴无华和亲切友善，然而这个开着保时捷的重庆女司机却再次刷新了我的三观。我以为那些不被人看到的丑恶会被岁月淡化，时光只会记录到存于这世界的美好，然而我失望了…</p><p>PS：最近看的两部藏区纪录片是《冈仁波齐》《阿拉姜色》</p></blockquote><p>7月30日上午，重庆一位驾驶保时捷的女子在斑马线违章掉头时与另一辆车的男司机发生口角。这件事本身没什么，同样作为一名司机，我觉得双方互相道个歉，然后故事就大结局了！不会像《长安十二时辰》中龙波能不能把花萼楼炸掉那样牵动着我的心，时时不能大结局。</p><video src="https://v.qq.com/x/page/y090600y5uh.html" controls="controls"></video><p>然而，女司机的一巴掌直接把她扇到了微博热搜，扇成了人民茶余饭后的谈资，扇出了她的往往种种违章，扇醒了沉迷于和平安乐的普罗大众，扇惊了她身后的派出所所长丈夫。</p><p>再来看下女司机，16年购入保时捷至今，三年中共有29条交通违法记录，违法行为包括闯红灯、乱停车、违反禁止标线、驾驶时拨打接听手持电话等。曾经在郊区无意间闯了一个红灯，被扣了3分+200人民币！说实话对于这钱，这分我挺悔恨的，毕竟我们的驾驶证只有12分，我们也都是平凡人！但是她却不一样，29条违章记录之后依旧在道路上横行霸道？难道人家的本本跟我的不太一样！</p><p>随着事件的发酵，这个江湖人称“月姐”的保时捷女司机被挖出了更多的“不良形象”，头批大波浪，戴白帽，身着喇叭裤，吊带小背心，黑色墨镜，嘴刁中南海，黑道感十足，简直就像黑道中的大嫂。再加上那个标志性的用食指指人的动作，说实话，遇见这种人，我会离她方圆十里开外！</p><p>关于这件事的处理结果还没有公布，不过她的几句霸语倒惊醒了我：</p><ul><li>“我出了名的飙车！红灯从来都是闯，没人敢把我怎么样。”</li><li>“我一个电话，所有记录都可以删除。”</li><li>“信不信，我分分钟找人弄你全家。”</li></ul><center><img src="https://img-blog.csdnimg.cn/20190804105026263.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></center>## 世道是怎样的败落，竟没人敢把你怎么样？“这世间的种种繁荣，都只不过是展现在你眼前的假象”，这句话说的有点过了，但是这世道上的一些黑暗是超乎我们想象的。某些拿着国家津贴的管员不作为，贪污受贿，包养小三，当被抓时的那种悔恨懊恼是你触碰法律底线最后的救赎吗某些身穿白大褂原本该济世救人的医生，却也受不了“灰色收入”的诱惑，而丢掉了该有的职业操守某些卖房时说的比唱的都好听的开发商，在收完定金和首付款之后，迟迟不能交房，最后变成烂尾楼这种事情在社会上屡见不鲜，但有法律这条约束线，还能起到警戒和惩罚的作用，我们能做的就是不管他人，约束好自己，不触碰法律的底线，要始终相信那些触犯法律的人终会得到制裁！## 这世上谁可以命令除他之外的所有人，是大唐时高高在上的圣人吗？权利大概是这个世界上最让人崇尚的东西，也是最让人畏惧的东西，没钱的怕有钱的，有钱的怕有权的，有权的天不怕地不怕，就怕权利更大的。不晓得这女司机是何方神圣，但这种通过“权利”可以办成一些平凡人 办不成的事的现象是显然存在的，这就像一个潜规则，只不过我们都是受害者！只要你背靠大树，那你就可以享受枝繁叶茂带来的阴凉。《长安十二时辰》中背靠永王的熊火帮不是烧杀抢掠，无恶不作吗！同样的一些其他潜规则，比如，谁有钱有权更霸道，谁就更有理，这是强者的逻辑。再比如，忍一时风平浪静，退一步海阔天空，这是弱者的借口。有句话说得好：人善被人欺，马善被人骑，柿子专挑软的捏。有些人之所以肆无忌惮、横行霸道，就是看准了老实人心地善良，不敢反抗。## 你的那句“弄死你全家”让我心有余悸。生老病死是人之常情，但欲加之罪何患无词。弱者生存的阶级里最害怕的就是强者的践踏。是你的错，你先动的手，他还了一下，就放话要弄死他全家！说实话真的怕了，能说出这句话的人，肯定处于金字塔的上层，这样她才有这样的底气，如此肆无忌惮。如今的法制社会，我们应该捍卫的是法律，在遇到事情时，拿起法律的武器来保护自己。而那个女司机则是靠着“法律执行人”的丈夫“弄死你全家”。想想也挺可笑的，士兵手里的剑是刺杀敌人的，医生手里的刀是济世救人的，官员手里的权利是维护治安的，而你则是来刷新我的三观的！## 那个扇了女司机的男司机以后会怎么样？在这个世界上，有人打心底里认为，只要有钱，便可以为所欲为，同样因为没钱，这世上的很多弱者在自己受了委屈之后，都会选择默不作声，任人宰割。即使是事件中的男司机，一时忍无可忍的，动手还击。可事后冷静下来，还是第一时间道歉了事。尽管错不在他，但我们也能体会到他内心的无奈。弱小的力量怎么能去和金钱，势力抗衡！《长安十二时辰》中有这样一个情节交代，永王（右相想要扶持的王子）因为为西域的小勃律使修建一个驿馆，在花萼楼上领功。被太子揭了老底，原来搞的暴力拆迁。而闻染的父亲闻无忌，虽为烽燧堡战役的幸存者，但依旧不能抵抗这场所谓的暴力拆迁，他的协商和争论只是让他白白送了姓名！这件事件的男司机代表的不正是我们这些平凡的人，堂堂的七尺男儿，为了妻儿家人，在事发之后也不得不低下头，进行了道歉！谁叫对方有钱优有势，而自己却是个普通之人呢！## 那个开保时捷的女司机后来怎么样了？重庆市公安局已经成立了调查组，彻查相关情况，依法处理，重视群众反应。<center><img src="https://img-blog.csdnimg.cn/20190805175924660.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></center><p>政府的态度已经很明显了，但那只是政府的官方态度，具体后事如何，暂时是未知的。不过这让我想起来了最近热播的《长安十二时辰》。</p><p>其中发生的事件是：龙波携众人和阙乐霍多，对花萼楼麒麟臂进行偷梁换柱，意图送圣人西去，而“大案牍术”选择的张小敬对其穷追不舍，彻查此案。</p><p>事件的起因是：安西铁军第八团在烽燧堡巡查之际，遭遇敌兵，于是死守城墙，但是援军迟迟不来，最后得知援军早已撤离，而援军不至的缘由是当时的兵部尚书林九郎并未下令进行支援，于是敌意升级到了大唐的圣人。</p><p>不过圣人在这里是做了个冤大头为什么这么说呢？因为当时的底层官员上报的是大唐边境平静，并无敌军来犯，如果作为林九郎下令增兵烽燧堡，这无疑是在打圣人的脸，是拆穿大唐边境一片平和的谎言。那么圣人必会迁怒于他，他的右相之路恐怕就没那么顺利了吧…</p><p>曾经表面“繁荣”的大唐不和现在的我们一样，看似和平安定，岁月静好，实则波涛汹涌，多少不为人知的“黑暗料理”一个个在显露出来，接下来我们就静等重庆女司机的处理结果吧！</p><h2 id="该有的光明会来，因为有张小敬这样的长安不良帅"><a href="#该有的光明会来，因为有张小敬这样的长安不良帅" class="headerlink" title="该有的光明会来，因为有张小敬这样的长安不良帅"></a>该有的光明会来，因为有张小敬这样的长安不良帅</h2><center><img src="https://img-blog.csdnimg.cn/20190804105136566.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></center>一个好人，什么时候开始变坏？从他觉得孤立无援的那一刻起；一个老实人，什么时候开始冷漠，从他看透人心冷暖的那一秒始。可如果人人都如此，那社会上的好人就越来越少，坏人就越来越多。最后必然陷入整体崩溃中。所以，不要把这个世界让给那些坏人，也不要让好人在黑暗里孤独的抗争。十年饮冰难凉热血，面对丑恶，忍耐不是美德，愤怒才是。而愤怒，绝不是一两个人的事情，而是所有有正义感的人共同的事业。说到底，这世界并不完美，犹如表象繁荣的大唐，但却依旧值得我们去奋斗，就像张小敬一样，不退！！！你的善良，你的坚持，终会发光！---<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="200px"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><center><font color="red">注：《推荐系统开发实战》已经在京东上线，感兴趣的朋友可以进行关注！</font></center><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="40%"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;世道很好，只是被藏的很好，只有走在那些青石板上，我才能看到石砖墙瓦的流芳百年&lt;/p&gt;
&lt;p&gt;世道很坏，只是很少的存在，只有遇见那些孤立无援，我才能看到人心冷暖的肆无忌惮&lt;/p&gt;
    
    </summary>
    
      <category term="随手记" scheme="http://thinkgamer.cn/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="随手记" scheme="http://thinkgamer.cn/tags/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>基于协同的Slope One算法原理介绍和实现</title>
    <link href="http://thinkgamer.cn/2019/08/02/RecSys/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E7%9A%84Slope%20One%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%9E%E7%8E%B0/"/>
    <id>http://thinkgamer.cn/2019/08/02/RecSys/推荐算法/基于协同的Slope One算法原理介绍和实现/</id>
    <published>2019-08-02T08:45:53.000Z</published>
    <updated>2019-10-14T06:42:35.665Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>该篇文章主要介绍Slope One算法。Slope One 算法是由 Daniel Lemire 教授在 2005 年提出的一个 Item-Based 的协同过滤推荐算法。和其它类似算法相比, 它的最大优点在于算法很简单, 易于实现, 执行效率高, 同时推荐的准确性相对较高。 </p></blockquote><a id="more"></a><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/51346159" target="_blank" rel="external">协同过滤算法理解和Python实现</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/51684716" target="_blank" rel="external">基于标签的推荐算法</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/51694250" target="_blank" rel="external">基于图的推荐算法</a></li></ul><h1 id="经典的ItemCF的问题"><a href="#经典的ItemCF的问题" class="headerlink" title="经典的ItemCF的问题"></a>经典的ItemCF的问题</h1><p>经典的基于物品推荐，相似度矩阵计算无法实时更新，整个过程都是离线计算的，而且还有另一个问题，相似度计算时没有考虑相似度的置信问题。例如，两个物品，他们都被同一个用户喜欢了，且只被这一个用户喜欢了，那么余弦相似度计算的结果是 1，这个 1 在最后汇总计算推荐分数时，对结果的影响却最大。</p><p>Slope One 算法针对这些问题有很好的改进。不过 Slope One 算法专门针对评分矩阵，不适用于行为矩阵。</p><h1 id="Slope-One算法过程"><a href="#Slope-One算法过程" class="headerlink" title="Slope One算法过程"></a>Slope One算法过程</h1><p>Slope One 算法是基于不同物品之间的评分差的线性算法，预测用户对物品评分的个性化算法。</p><p>Slope算法主要分为3步</p><ol><li>计算物品之间的评分差的均值，记为物品间的评分偏差 (两物品同时被评分)<script type="math/tex; mode=display">R(i,j) = \frac{ \sum_{ u \in N(i)\bigcap N(j) } (r_{ui} - r_{uj}) }{ | N(i) \bigcap N(j) | }</script><blockquote><p>( r_ui - r_uj ) 表示评分的差,这里需要注意的是j相对i的评分偏差是 r_ui - r_uj ，如果是i相对j的评分偏差则是 r_uj - r _ui,两 者是互为相反数的关系。</p></blockquote></li></ol><p>其中：</p><ul><li>r_ui ：用户u对物品i的评分</li><li>r_uj ：用户u对物品j的评分</li><li>N(i) ：物品i评过分的用户</li><li>N(j) ：物品j评过分的用户</li><li>N(i) 交 N(j) ：表示同时对物品i 和物品j评过分的用户数。</li></ul><ol><li>根据物品间的评分偏差和用户的历史评分，预测用户对未评分的物品的评分。<script type="math/tex; mode=display">p_{uj} = \frac{ \sum_{i \in N(u)} |N(i) \bigcap N(j) |(r_{ui} - R(i,j))  }{ \sum_{i \in N(u)}|N(i) \bigcap N(j)| }</script>其中：</li></ol><ul><li>N(u) ：用户u评过分的物品</li></ul><ol><li>将预测评分进行排序，取Top N对应的物品推荐给用户</li></ol><h1 id="实例说明"><a href="#实例说明" class="headerlink" title="实例说明"></a>实例说明</h1><p>例如现在有一份评分数据，表示用户对电影的评分：</p><div class="table-container"><table><thead><tr><th>-</th><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr></thead><tbody><tr><td>U1</td><td>2</td><td>3</td><td>3</td><td>4</td><td></td></tr><tr><td>U2</td><td></td><td>4</td><td>2</td><td>3</td><td>3</td></tr><tr><td>U3</td><td>4</td><td>2</td><td>3</td><td></td><td>2</td></tr><tr><td>U4</td><td>3</td><td></td><td>5</td><td>4</td><td>3</td></tr></tbody></table></div><p>现在我们来预测预测每个用户对未评分电影的评分。</p><p>Step1: 计算物品之间的评分偏差，以U1为例：</p><script type="math/tex; mode=display">R(a,b) = \frac{ (2-3) + (4-2) }{ 2 } = 0.5</script><script type="math/tex; mode=display">R(a,c) = \frac{ (2-3) + (4-3) +(3-5) }{ 3 } = -0.67</script><script type="math/tex; mode=display">R(a,d) = \frac{ (2-4) + (3-4) }{ 2 } = -1.5</script><script type="math/tex; mode=display">R(a,e) = \frac{ (4-2) + (3-3) }{ 2 } = 1</script><p>同理可以计算出电影b，c，d，e与其他电影的评分偏差。</p><p>Step2: 计算用户对未评分物品的可能评分（为了方便计算，这里以U2为例）</p><p>由上表可知，用户U2 对电影a没有评分，这里计算用户U2对电影a的评分。</p><script type="math/tex; mode=display">p_{u_2,a} = \frac{2 *  (4-0.5) +3 * (2-(-0.67)) + 2 * (3-(-1.5) ) + 2 * (3-1))    }{ 2+3+2+2}  = 3.11</script><p>Step3: 评分排序</p><p>由于给定样例中，U2只对a没有评过分，所以这里不需要进行排序，正常的话，按分数进行倒排就行。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>这里采用Python实现，在实现过程中并没有考虑算法的复杂度问题。</p><p>加载数据<br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def loadData(self):</span><br><span class="line">    user_rate = &#123;</span><br><span class="line">        <span class="string">"U1"</span>: &#123;<span class="string">"a"</span>: <span class="number">2</span>, <span class="string">"b"</span>: <span class="number">3</span>, <span class="string">"c"</span>: <span class="number">3</span>, <span class="string">"d"</span>: <span class="number">4</span>&#125;,</span><br><span class="line">        <span class="string">"U2"</span>: &#123;<span class="string">"b"</span>: <span class="number">4</span>, <span class="string">"c"</span>: <span class="number">2</span>, <span class="string">"d"</span>: <span class="number">3</span>, <span class="string">"e"</span>: <span class="number">3</span>&#125;,</span><br><span class="line">        <span class="string">"U3"</span>: &#123;<span class="string">"a"</span>: <span class="number">4</span>, <span class="string">"b"</span>: <span class="number">2</span>, <span class="string">"c"</span>: <span class="number">3</span>, <span class="string">"e"</span>: <span class="number">2</span>&#125;,</span><br><span class="line">        <span class="string">"U4"</span>: &#123;<span class="string">"a"</span>: <span class="number">3</span>, <span class="string">"c"</span>: <span class="number">5</span>, <span class="string">"d"</span>: <span class="number">4</span>, <span class="string">"e"</span>: <span class="number">3</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    item_rate = &#123;</span><br><span class="line">        <span class="string">"a"</span>: &#123;<span class="string">"U1"</span>: <span class="number">2</span>, <span class="string">"U3"</span>: <span class="number">4</span>, <span class="string">"U4"</span>: <span class="number">3</span>&#125;,</span><br><span class="line">        <span class="string">"b"</span>: &#123;<span class="string">"U1"</span>: <span class="number">3</span>, <span class="string">"U2"</span>: <span class="number">4</span>, <span class="string">"U3"</span>: <span class="number">2</span>&#125;,</span><br><span class="line">        <span class="string">"c"</span>: &#123;<span class="string">"U1"</span>: <span class="number">3</span>, <span class="string">"U2"</span>: <span class="number">2</span>, <span class="string">"U3"</span>: <span class="number">3</span>, <span class="string">"U4"</span>: <span class="number">5</span>&#125;,</span><br><span class="line">        <span class="string">"d"</span>: &#123;<span class="string">"U1"</span>: <span class="number">4</span>, <span class="string">"U2"</span>: <span class="number">3</span>, <span class="string">"U4"</span>: <span class="number">4</span>&#125;,</span><br><span class="line">        <span class="string">"e"</span>: &#123;<span class="string">"U2"</span>: <span class="number">3</span>, <span class="string">"U3"</span>: <span class="number">2</span>, <span class="string">"U4"</span>: <span class="number">3</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> user_rate,item_rate</span><br></pre></td></tr></table></figure></p><p>计算物品之间的评分偏差<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def cal_item_avg_diff(self):</span><br><span class="line">    avgs_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> item1 <span class="keyword">in</span> self.item_rate.keys():</span><br><span class="line">        <span class="keyword">for</span> item2 <span class="keyword">in</span> self.item_rate.keys():</span><br><span class="line">            avg = 0.0</span><br><span class="line">            user_count = 0</span><br><span class="line">            <span class="keyword">if</span> item1 != item2:</span><br><span class="line">                <span class="keyword">for</span><span class="built_in"> user </span><span class="keyword">in</span> self.user_rate.keys():</span><br><span class="line">                    user_rate = self.user_rate[user]</span><br><span class="line">                    <span class="keyword">if</span> item1 <span class="keyword">in</span> user_rate.keys() <span class="keyword">and</span> item2 <span class="keyword">in</span> user_rate.keys():</span><br><span class="line">                        user_count += 1</span><br><span class="line">                        avg += user_rate[item1] - user_rate[item2]</span><br><span class="line">                avg = avg / user_count</span><br><span class="line">            avgs_dict.setdefault(item1,&#123;&#125;)</span><br><span class="line">            avgs_dict[item1][item2] = avg</span><br><span class="line">    return avgs_dict</span><br></pre></td></tr></table></figure></p><p>计算预估评分<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">item_both_rate_user</span><span class="params">(<span class="keyword">self</span>, item1, item2)</span></span><span class="symbol">:</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> <span class="keyword">self</span>.user_rate.keys()<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> item1 <span class="keyword">in</span> <span class="keyword">self</span>.user_rate[user].keys() <span class="keyword">and</span> item2 <span class="keyword">in</span> <span class="keyword">self</span>.user_rate[user].keys()<span class="symbol">:</span></span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(<span class="keyword">self</span>, user, item, avgs_dict)</span></span><span class="symbol">:</span></span><br><span class="line">    total = <span class="number">0</span>.<span class="number">0</span> <span class="comment"># 分子</span></span><br><span class="line">    count = <span class="number">0</span>   <span class="comment"># 分母</span></span><br><span class="line">    <span class="keyword">for</span> item1 <span class="keyword">in</span> <span class="keyword">self</span>.user_rate[user].keys()<span class="symbol">:</span></span><br><span class="line">        num = <span class="keyword">self</span>.item_both_rate_user(item, item1)</span><br><span class="line">        count += num</span><br><span class="line">        total += num * (<span class="keyword">self</span>.user_rate[user][item1] - avgs_dict[item][item1])</span><br><span class="line">    <span class="keyword">return</span> total/count</span><br></pre></td></tr></table></figure></p><p>主函数调用<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="attr">slope</span> = SlopeOne()</span><br><span class="line">    <span class="attr">avgs_dict</span> = slope.cal_item_avg_diff()</span><br><span class="line">    <span class="attr">result</span> = slope.predict(<span class="string">"U2"</span>, <span class="string">"a"</span>, avgs_dict)</span><br><span class="line">    print(<span class="string">"U2 对 a的预测评分为: %s"</span> % result)</span><br></pre></td></tr></table></figure></p><p>打印结果为：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">U2</span> 对 <span class="selector-tag">a</span>的预测评分为: 3<span class="selector-class">.111111111111111</span></span><br></pre></td></tr></table></figure></p><p>和上边我们计算的结果一致。</p><p>完整代码在：<a href="https://github.com/Thinkgamer/Machine-Learning-With-Python/tree/master/Recommend" target="_blank" rel="external">https://github.com/Thinkgamer/Machine-Learning-With-Python/tree/master/Recommend</a></p><h1 id="Slope-One的应用场景"><a href="#Slope-One的应用场景" class="headerlink" title="Slope One的应用场景"></a>Slope One的应用场景</h1><p>该算法适用于物品更新不频繁，数量相对较稳定并且物品数目明显小于用户数的场景。比较依赖用户的用户行为日志和物品偏好的相关内容。 </p><p>其优点： </p><ul><li>算法简单，易于实现，执行效率高； </li><li>可以发现用户潜在的兴趣爱好； </li></ul><p>其缺点： </p><ul><li>依赖用户行为，存在冷启动问题和稀疏性问题。</li></ul><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="200px"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><center><font color="red">注：《推荐系统开发实战》已经在京东上线，感兴趣的朋友可以进行关注！</font></center><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="40%"></center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;该篇文章主要介绍Slope One算法。Slope One 算法是由 Daniel Lemire 教授在 2005 年提出的一个 Item-Based 的协同过滤推荐算法。和其它类似算法相比, 它的最大优点在于算法很简单, 易于实现, 执行效率高, 同时推荐的准确性相对较高。 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐算法" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="Slope One" scheme="http://thinkgamer.cn/tags/Slope-One/"/>
    
  </entry>
  
  <entry>
    <title>【技术分享】你想知道的网易云音乐推荐架构解析，都在这里！</title>
    <link href="http://thinkgamer.cn/2019/08/01/Share/%E3%80%90%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E3%80%91%E4%BD%A0%E6%83%B3%E7%9F%A5%E9%81%93%E7%9A%84%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E6%8E%A8%E8%8D%90%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%EF%BC%81/"/>
    <id>http://thinkgamer.cn/2019/08/01/Share/【技术分享】你想知道的网易云音乐推荐架构解析，都在这里！/</id>
    <published>2019-07-31T17:17:24.000Z</published>
    <updated>2019-10-14T06:42:35.672Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文选自网易云音乐推荐算法负责人-肖强前辈在全球人工智能峰会上的分享，主要介绍了三方面：关于网易云音乐，AI算法在音乐推荐中的应用和AI场景下的音乐思考。这里拿来分享给大家，并加上自己的理解，希望对大家有所帮助。</p></blockquote><a id="more"></a><blockquote><p>首先说明我是网易云音乐的深度用户，目前级别LV9，每天都会去听日推。喜欢网易云音乐的原因不仅是友好的用户交互设计，而且还是因为在网易云音乐中能看到一个个陌生的故事。虽不知这些故事是真是假，但总会找到一些共鸣，在这里像是一个心灵的寄托一样。我相信大多数人和我一样，喜欢在敲代码的时候戴上耳机，来一个燥一点的音乐，所有的事情与我无关，我只想专心写代码，哈哈，开篇小小的题外话，下面进入正题。</p></blockquote><p>关于本文的PDF，可关注公号 合作=&gt;私信 获取小编微信，私聊获取！</p><p>本文将从三个方面介绍AI算法在网易云音乐推荐中的应用：</p><ul><li>关于网易云音乐</li><li>AI算法在音乐推荐中的应用</li><li>音乐场景下的AI思考</li></ul><hr><h1 id="关于网易云音乐"><a href="#关于网易云音乐" class="headerlink" title="关于网易云音乐"></a>关于网易云音乐</h1><blockquote><p>关于网易云音乐的介绍就不用多说了，相信大家都知道这个产品。但是这个产品里边也有很多其他的业务，如下这些。当然推荐也会在各个业务线进行应用，最大化的提高用户体验。</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjlOWUpEU3laT0hGUkZERGJ1WGdWajdLdnBtWkNXaGY3S3Rvem5kZlQ1T3ZyT0tQMUdpY0tBalEvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnBqYjhlUFhSakVEekV3SHFSUXBjSnB6NkROVnBFc01ZYzNSYWFHMlhYblNMaWIybDB4TjB3UFEvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjdYNnlYdlQySjk0OGROcXAzbUZNTHo1S3Bnd0d2ZUhobWsxaWFMZ0pGMWJLb3VQbW5FQkloMncvNjQwP3d4X2ZtdD1wbmc"> </center><h1 id="AI算法在音乐推荐中的应用"><a href="#AI算法在音乐推荐中的应用" class="headerlink" title="AI算法在音乐推荐中的应用"></a>AI算法在音乐推荐中的应用</h1><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjR5ZTFNaWFxckt2d1lBVGp0SUQ2ZlJKREFRQWljeGpFV3h3VHZkSGljUkk1WFFic2xpY3h1Y2xYZ1EvNjQwP3d4X2ZtdD1wbmc"> </center><blockquote><p>除了上图中介绍的三个场景以外，推荐在云音乐还有其他很多的应用，比如推荐的MV/视频，推荐的电台，推荐的Look直播等。</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjVMemc4MFg5RERWa053RVZpY01lUWNkc3dXRzJFT25QOXl2WXRBZldJbXlJdjNQUW9Rbk1WTVEvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkt2U1htcGJ3c3JzWlE1SVNpYUZnUWhpY3hGNFg0bjB0amhBaFpMZXlpYm5EbnhpYWo3SGlicHRmcEtnLzY0MD93eF9mbXQ9cG5n"> </center><blockquote><p>几乎所有的推荐系统或者业务系统的分析和底层数据支持都离不开用户的行为日志，在对基础的日志进行ETL处理之后，进行数仓的存储，画像模型，特征工程等均基于这个日志来进行构建。</p></blockquote><p><br></p><blockquote><p>在此之上这是推荐系统的召回模块，然后进行粗排，精排和展示。这是所有推荐系统的通用架构，然后各个公司，各个部门会在此基础上进行适合各自业务的开发和精细化应用，以适应具体的需求，继而最大化的发挥推荐系统的价值。</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbll5WnBPaWFoU1JqWWhLRHVXdkYzVVNpY0pSbGlieUxIQ1E2TnJtSEdpYWhpYW41YVdvWGlhcWFybWt6US82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblpFS3IzMUJ3TnVpYzhrd0hOS0FHUzNscnFVaWNhREtLS3ZiQVB4WmxWQld3SEJMWnZ5eWgyek1nLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnhKaWFXbmw3V3hkWXRHOWt4aWNJMk85RzMyQ0NmVzk1SzQyQXFaOTN4dWt2aWNJMHdaSlJGWXVzQS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnRSaGY4a2lhcjZqSXVYNmFjTWxCWG5tMUlUMEVFYXhzTlFNRE1WWUgwV2Fub29OOG80NjUyU0EvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjc1dXNibmJFeUJJdlZZZkdodE41aWFzbnYwN05IVGI1YnRUNjRwWThpY29xRFFiV1NJN3hIbzNRLzY0MD93eF9mbXQ9cG5n"> </center><blockquote><p>不同的业务场景下，推荐的侧重点和实现方法是不一样的。音乐推荐，视频推荐，商品推荐，新闻推荐等，这些各自有各自的特点，音乐本身的复杂性，就要要求系统能够更好的理解音乐，网易云音乐则主要从NLP和视频，图像层面去理解音乐。</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkhPZWV6dVRaMmRxb2Uwa0swbTdweGE1VWRqdTE5VkJsUGljaWI3R2ljQ2ljUWdkeFQ0TFB4a21pYlFBLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbjZ4UFNHY25qemVUdURhSzlyTGlhY2hKc2FicE5mTXRKbTBtRXd2MkFaZHYzTjV6SndKYzU3T3cvNjQwP3d4X2ZtdD1wbmc"> </center><blockquote><p>传统的CF算法应用的是余弦相似度或者Jaccard距离进行计算，云音乐这里则对其相似度计算方法进行了优化，其计算公式如上图所示，与传统相比，效果提升显著。PS：CF即协同过滤，虽然传统，但是在推荐中扮演的角色至关重要。</p><p>接下来主要介绍下云音乐中的排序模型，其经历了线性模型，树模型，FTRL，深度学习模型，深度时序模型。PS：不过在这里小编要提醒的，做排序模型要一步一个脚印，一步一步来，我们不可能在建设推荐系统之时，直接上深度学习，因为只有经历过每个阶段的排序模型，我们才能更好的去理解业务，去提升业务。</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbmZSMjdmYXg0dThRMkFtcjNLTk9OMVc3QWREdHQ5UmhpYlh4QjZFZU1ya1pjemlhSjBoNXNMWEh3LzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkgwMU9vQmpVWUtwNHl4NkZWd200djBOS1NWN1c2U0FNMWNXdGlhODVBbGMxRFlpYmVTTGlhTU9aQS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbmZOT1FzTEgxR0poTUE1NWliN2xpYzNKWlZaSjlxV2J6bUtWN3Q2NEk3VFdTbzVEbXJMUFEwZ1l3LzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbk1rVWR5cHRHV201b0x6NjJHckhFbldFY2liM2F2VEVSN1d1SjQzYU12bXdsNXNFdjBUVUdVbHcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblpqSXdNeWo0TDRHbVVnZkpDcURrODFLaWJmd2ljVUtyNVFkaWJBVUtqc21nQTlIVGtpYlJONXVQMXcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnhpY2tDaWF3ODM2c3pLZ3IzVFJrU2VxZFU2VkhYQnk4aG9vZ3M3ZFpZaWF4R0w2eTQ1c25obzZpYUEvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbm11aWNRS3d3ZU96UnV4aWNONGd0OWtudHRUZE5BSmtsbVVtZlJQbE1oMXZxSFI4OTRVaDhyeGJBLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnpvUE1oamxxcW9XTXpMVmtiTXk1NGljR1VobFQzcWptVHI4cmpjYkk3ZUluaFRxbVpsamtzc3cvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnZBdTh6S1dpYzBiTmZGZWRmeGx3cUNpY0hwSkFQalpnUmpIU2ZsNUpCRmFnTXEwdVdTODVpY3ZmQS82NDA_d3hfZm10PXBuZw"> </center><h1 id="音乐场景下的AI思考"><a href="#音乐场景下的AI思考" class="headerlink" title="音乐场景下的AI思考"></a>音乐场景下的AI思考</h1><blockquote><p>当某种算法在推荐中发挥的价值很难再进行提升时，我们需要进行的是业务的深层次的思考和对更加有效或者先进的算法进行探索。</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbmlhbE1WdFhZMzgzUkNqUkx0MTZGSGJHOWgzaldtTEtHWXJQY3hQZ1Z4QWw5QzJnMzRtRWhZd1EvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnhJb2pQSmgyWFhKWWh1TVBVemNuRjM3dDlmQnNnWGszaG5VaHlNeGt5UEhSS3c0ZnhDRlhnZy82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblp4alJLWkN0aGFxc0dKQnEyb1BJSnFTUmNtMzB5RndIMXpvN3FyZWh3czFtWFFEWDNXRkRLQS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbkdYaWJkQjlUVm9KaWE3OEdoSWxTdGlhSkpxeFBVOXFjcDNscko2VHc5UzNOQ2JpY2liU1pCUkJ1S1d3LzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbklQaWNpYWZLWnVtemVBcVFma0o4dnZwcVpiTzVNOGp1Q2w5YXdlUzJ0UnRyTHNQNzhEbEptaWM1US82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjblRpY2xTZWt0ejNKY29pYUdLWkhPRGliRzFJaWNIbnEyQlBHWDYzSTFSYXZZd2FoaWFaZzJKZHJtSzNBLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNqaENmc1J6NEJjdWh5Y3JpYloxRTFjbnZGSlBCUDhSME1SWGhydjUya3FtYUk3NEZLWHRKcUYzZEVKeFFhMm12d0lRUmdmc3BQaGVlUS82NDA_d3hfZm10PXBuZw"> </center><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><p><font color="red">注：《推荐系统开发实战》是小编的最新出版的技术图书，已经在京东，当当上线，感兴趣的朋友可以进行购买阅读！</font></p><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="50%"></center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文选自网易云音乐推荐算法负责人-肖强前辈在全球人工智能峰会上的分享，主要介绍了三方面：关于网易云音乐，AI算法在音乐推荐中的应用和AI场景下的音乐思考。这里拿来分享给大家，并加上自己的理解，希望对大家有所帮助。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="技术分享" scheme="http://thinkgamer.cn/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>【技术分享】美团外卖的商业变现的技术思考和实践</title>
    <link href="http://thinkgamer.cn/2019/08/01/Share/%E3%80%90%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E3%80%91%E7%BE%8E%E5%9B%A2%E5%A4%96%E5%8D%96%E7%9A%84%E5%95%86%E4%B8%9A%E5%8F%98%E7%8E%B0%E7%9A%84%E6%8A%80%E6%9C%AF%E6%80%9D%E8%80%83%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>http://thinkgamer.cn/2019/08/01/Share/【技术分享】美团外卖的商业变现的技术思考和实践/</id>
    <published>2019-07-31T16:55:31.000Z</published>
    <updated>2019-10-14T06:42:35.673Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文选自美团-王永康前辈在全球人工智能峰会上的分享，主要介绍了四方面：业务介绍，平台侧收入优化，商家侧转化优化和用户侧体验优化。这里拿来分享给大家，并加上自己的理解，希望对大家有所帮助。</p></blockquote><a id="more"></a><p>本文将从四个方面介绍美团外卖商业变现实践：</p><ul><li>业务介绍</li><li>平台侧：收入优化</li><li>商家侧：转化优化</li><li>用户侧：体验优化</li></ul><h1 id="业务介绍"><a href="#业务介绍" class="headerlink" title="业务介绍"></a>业务介绍</h1><p>首先介绍了美团外卖的业务情况，其包含了外卖商家360w，用户数3亿+，日活跃骑手数60w，覆盖城市2500+。其次介绍了外卖的业务形态，包含：</p><ul><li>展示广告</li><li>搜索广告</li><li>feed流广告</li><li>消息push广告</li></ul><p>其广告的转化形式由曝光到点击再到下单，则为一条有效的转化。其中涉及的名次含义为：</p><ul><li>CPT：Cost Per Time（成本/时间），即按时长计费广告。按时长计费是包时段包位置投放广告的一种形式。</li><li>GD：Guarentee Delivery（保证交货），保证递送的广告，即保量广告，按展示量定价。</li><li>CPM：Cost Per Mille（成本/千次），千次展示成本，即按展示付费。</li><li>CPC：Cost Per Click（每次点击成本），每次点击成本，按点击付费，如关键词广告。</li><li>CPA：Cost Per Action（每次行为成本），CPA计价方式是指按广告投放实际效果，即按回应的有效问卷或定单来计费，而不限广告投放量。</li></ul><p>另外补充两个名次含义：</p><ul><li>oCPC：optimization Cost Per Click（每次点击优化成本），目标转化成本，仍按点击付费。</li><li>刷次：用户在APP信息流界面，手指每次下滑刷新，叫做一次刷次。</li></ul><p>同时介绍了计算广告的核心——最佳匹配。​</p><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmVaTEZwVGI1aWFMc1NPaWJYU25ITTMxMHdFY21YbmJpY1F2VTRkZTc5eHZZZlFlZUxSd3NuMDJwZy82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlh2MFl5NTZWUXhVZHFoN0c5c0o1WTVOZjB1Ynh4TEhYUE1MNjVITWRVMVNpYmlhVnVsQWpPcHFnLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlJJN1lHeTJnaWN2bUxmbUo2bjBPUDJKTDBjVFdjV1JYWkpRb1JRc3E1ZWh0eVhHNkxZb2ljcnlRLzY0MD93eF9mbXQ9cG5n"> </center><h1 id="平台侧：收入优化"><a href="#平台侧：收入优化" class="headerlink" title="平台侧：收入优化"></a>平台侧：收入优化</h1><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlc2V3hpYldBbDVGWE13Yk9KUFBlSWU3ajNsTlJvcnNDRFhtcTJPckFESnhCZkFDa0tydzVmcFEvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlhYM1IxY203U1FxOG54RDI2ME9xV2lhaDJCak93R3pONjQyM3hqTXdCaWJVcDdVdWd3NUtPTVBRLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlZZM2ljVjZpYWxuTGV5eDFYR0xQMXA2Y3ZpYWx5YXZrWVM0U1NXSW1QVUduVEV6aWJBcGFjMGhLdHcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRjFkbWN5akM0MFdySjI3MjlITTFPSkQyZ1p5d1prWW5PQlo5ZVRUdUhuOWlhRzl2bGpaMmEzYXcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmdEa210OW9lMzY1WktaRjI2RUd2Q0VwVDhBNUxrV0VweGlhdkk5ODg1SkVTeVZsc3ZadWgyNHcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnB0elhHMUdqcWlicGliaGZhejQ5SUl1YmE1aWNlcGVIdHl6cWFjY1NsbHlWZjczMFZJM0tVTkdNQS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmhEeUFTZVpFWVRERFlpY2VFR3BlaHJpYnpvbDhqUG9UcEFPaWF0OGljVFc4RUxIQjc0Yng4YVpSblEvNjQwP3d4X2ZtdD1wbmc"> </center><blockquote><p>需要注意的是这里提到了一个使用Side Information框架解决冷启动的方法，感兴趣的读者可以自己研究下什么是side information</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRjNTc2tmTjFiV21jeUNlZXV6NGZQTHVQQkk2Nm82aWM1OGNxczZnV0xINUhhRmdQOGEzWjVxdUEvNjQwP3d4X2ZtdD1wbmc"> </center><blockquote><p>这里基于用户的行为序列数据，可以使用wode2vec，LSTM，VGG，Inception进行Embedding序列的生成</p></blockquote><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRk93NFRUNkROWEZLVjZoVVlRY3IxcldFWHVxeTNpYmM2VlR6UUFhc0VSa1RDR2liWlVnbmJkS3VnLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlB5d1VtazA3YXlBTlhaa2N3NVBFbjNJUnp3c0drYXo0TVFiRWZGRlJRcWh3T2lja085UlFXVHcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRkhyVUNRaEtxcWREbDV1cXAzcFhxRHNaaGRQUGlhM2ZzSklrV04wa2lhTXUwb3JuRUtMYXhsTzFBLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRkRYMmh1S1JBNVFvdnBUMkJlbnByZGFpY2ZxYVJFRUk2aWJ6bzVJSjFhNW1naWNiT0xuTEVhSEZnUS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnBISFU1Z3dyWGF6cG1DbXRwVHJja3Q5TTlXbk1Ub0JXaWFtQVJ2MHcxamJmdEtOR0t2c2lhczFRLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRkZjeG5RU1liOWxCUTVnOWJpYmZ2eHJmTVh0aWFTYXR3ZnFITHpLUkVZcThHMjh5OHdCR0gzaEhnLzY0MD93eF9mbXQ9cG5n"> </center><h1 id="商家侧：转化优化"><a href="#商家侧：转化优化" class="headerlink" title="商家侧：转化优化"></a>商家侧：转化优化</h1><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnI2QWNiT1ZpYlo2VUxNQk1FeE15TzVkNmhndWliRWxUYlBLMFQzNkxqMFFIbTgzOVNQNVJxaWJLUS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRklGTmJoS0dQVTBnOER2dmNsbVdEZTVIMEVpYThHRmN0eVJsNzRUQ3lwdVI1N0ljQXN3aWFZRFVBLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnZMSlBGWFNpYnB6bkQ5aWNkUHhEbGpQSVhVUmliMGRyTzl6a0xrVkVQSXd1V1NncEhTU0tVaWJqNUEvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmFSNEVJSUpVZFFoVHozY05ZaWNTdHNRcWY4VThLY0xZMVIwZ29iZFhqWHFmTno1NkhpY0ZOdHlRLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmRDVEhJbXEzb3JqeGljb2REbnBFd2liUVNadU5TcjZ1bWZYWU9kaDNrTjlpYW5JVmpsWmhrWnpIUS82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRnE4emlhRUloV0Q4aWJhWThibE5pYlh2YUNPWjhQUTNra09ZUk5UUHROaWNnZjJvWkNWMW5xRVpGVHcvNjQwP3d4X2ZtdD1wbmc"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmIxTkFzZUswV29udVFqc1JYWFZpYmpveVBuelBkdk1lMXBxT3ZyTXhWUVNlc28xZmU2aGJ1MGcvNjQwP3d4X2ZtdD1qcGVn"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRjRENTBCUWZNYzRmTjV0R2FtSGZMWU0yVXhVdGlhYVgyNXFXY1pwcWljeUFBRVQ0TElmbkZqN3N3LzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRktpYWdNdWUxUzlkOHlpYUF3YnZRaEgxaWFTTzNyZFRjdVRhbndIM1BhT0I2YUZ3OThwbXNLU3c3US82NDA_d3hfZm10PXBuZw"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRno4bURaOWw0Rk5BeVdWeExldWxCYk50SDJQbURnT0dyNzJjc2x2TExCMWlia2lidUJ6VERyUkRRLzY0MD93eF9mbXQ9cG5n"> </center><h1 id="用户侧：体验优化"><a href="#用户侧：体验优化" class="headerlink" title="用户侧：体验优化"></a>用户侧：体验优化</h1><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRmljVVU1bW1vaFhzVEhCZDFHWG1BZ055NGtpYWlhbHdpYllkamJJM2d6Ulg0VTRQWXdmN2diaWFPWlJ3LzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRklORzJTM2t3R0w1UUV1aWFMbno5dk5sdklhY0J5bmNMVENGZ3NEbVp5N2lhclA1RDBnQ284YXZnLzY0MD93eF9mbXQ9cG5n"> </center><center><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8wMmtpY0VXc0luaWNnNXhKNVpXOUJnUmljUkJlZmZ4bzhmRlh4SjZwWFhkR2taS2E3ZFN6cWdJRnhCMHR6M1cyakhOUU1WT2VoTGxaSWZpYmlib2txYTF6dTN3LzY0MD93eF9mbXQ9cG5n"> </center><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><p><font color="red">注：《推荐系统开发实战》是小编的最新出版的技术图书，已经在京东，当当上线，感兴趣的朋友可以进行购买阅读！</font></p><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="50%"></center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文选自美团-王永康前辈在全球人工智能峰会上的分享，主要介绍了四方面：业务介绍，平台侧收入优化，商家侧转化优化和用户侧体验优化。这里拿来分享给大家，并加上自己的理解，希望对大家有所帮助。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="技术分享" scheme="http://thinkgamer.cn/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Spark MLlib 之 数据类型与大规模数据集的相似度计算原理探索</title>
    <link href="http://thinkgamer.cn/2019/07/29/Spark/Spark%20MLlib%20%E4%B9%8B%20%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86%E6%8E%A2%E7%B4%A2/"/>
    <id>http://thinkgamer.cn/2019/07/29/Spark/Spark MLlib 之 数据类型与大规模数据集的相似度计算原理探索/</id>
    <published>2019-07-29T07:41:15.000Z</published>
    <updated>2019-10-14T06:42:35.674Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文出自「xingoo」在原文的基础上加以小编自己的理解形成的学习笔记，希望对读者有帮助。原文出自：<a href="https://www.cnblogs.com/xing901022/p/9296882.html" target="_blank" rel="external">Spark MLlib 之 大规模数据集的相似度计算原理探索</a></p></blockquote><a id="more"></a><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>最近小编在做的是计算两两用户的粉丝重合度，根据粉丝重合度去评估两个用户之间的相似度，根据条件进行过滤之后大概有3000个用户，但每个用户的粉丝量参差不齐，有上百万的，有几千的，这样在去构建笛卡尔积的时候，进行粉丝数据关联，得到的用户集就会特别大，spark运行的时候就会很慢，而且会出现很严重的数据倾斜。这个时候了解到了spark支持的数据类型，看到了CoordinateMatrix，然后深究其原理，便看到了这篇文章，经过整理形成了此文。</p><h1 id="Spark支持的数据类型"><a href="#Spark支持的数据类型" class="headerlink" title="Spark支持的数据类型"></a>Spark支持的数据类型</h1><p>官方文档地址：<a href="https://spark.apache.org/docs/latest/mllib-data-types.html" target="_blank" rel="external">https://spark.apache.org/docs/latest/mllib-data-types.html</a></p><h2 id="1-Local-Vector（本地向量）"><a href="#1-Local-Vector（本地向量）" class="headerlink" title="1.Local Vector（本地向量）"></a>1.Local Vector（本地向量）</h2><p>本地向量是从0开始的下标和double类型的数据组成，存储在本地机器上，所以称为Local Vector。它支持两种形式：</p><ul><li>Dense （密集的向量）</li><li>Sparse （稀疏的向量）</li></ul><p>比如一个向量[1.0,0.0,3.0]，用Dense表示为：[1.0,0.0,3.0]，用Sparse表示为：(3,[0,2],[1.0,3.0])，其中3为向量的长度，[0,2]表示元素[1.0,3.0]的位置，可见sparse形式下0.0是不存储的。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.mllib</span><span class="selector-class">.linalg</span><span class="selector-class">.Vectors</span></span><br><span class="line"></span><br><span class="line">val denseVector = Vectors.dense(<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">3.0</span>)</span><br><span class="line">val sparseVector1 = Vectors.sparse(<span class="number">3</span>,Array(<span class="number">0</span>,<span class="number">2</span>),Array(<span class="number">1.0</span>,<span class="number">3.0</span>))</span><br><span class="line">val sparseVector2 = Vectors.sparse(<span class="number">3</span>,Seq((<span class="number">0</span>,<span class="number">1.0</span>),(<span class="number">2</span>,<span class="number">3.0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"DenseVector is : $denseVector"</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"DenseVector to Sparse is : $&#123;denseVector.toSparse&#125;"</span>)</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"sparseVector1 is : $sparseVector1"</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"sparseVector1 to Dense is : $&#123;sparseVector1.toDense&#125;"</span>)</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"sparseVector2 is : $sparseVector2"</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"sparseVector2 to Dense is : $&#123;sparseVector2.toDense&#125;"</span>)</span></span></span><br></pre></td></tr></table></figure><p>输出为：<br><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DenseVector <span class="keyword">is</span> : <span class="comment">[1.0,0.0,3.0]</span></span><br><span class="line">DenseVector to Sparse <span class="keyword">is</span> : (3,<span class="comment">[0,2]</span>,<span class="comment">[1.0,3.0]</span>)</span><br><span class="line"></span><br><span class="line">sparseVector1 <span class="keyword">is</span> : (3,<span class="comment">[0,2]</span>,<span class="comment">[1.0,3.0]</span>)</span><br><span class="line">sparseVector1 to Dense <span class="keyword">is</span> : <span class="comment">[1.0,0.0,3.0]</span></span><br><span class="line"></span><br><span class="line">sparseVector2 <span class="keyword">is</span> : (3,<span class="comment">[0,2]</span>,<span class="comment">[1.0,3.0]</span>)</span><br><span class="line">sparseVector2 to Dense <span class="keyword">is</span> : <span class="comment">[1.0,0.0,3.0]</span></span><br></pre></td></tr></table></figure></p><h2 id="2-Labeled-point-带标签的点"><a href="#2-Labeled-point-带标签的点" class="headerlink" title="2. Labeled point(带标签的点)"></a>2. Labeled point(带标签的点)</h2><p>labeled point由本地向量组成，既可以是dense向量，也可以是sparse向量。在mllib中常用于监督类算法，使用double类型来保存该类型的数据，因为也可以用于回归和分类算法。例如二分类，label可以是0（负例）或1（正例），对于多分类，label可以是0，1，2…</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.mllib</span><span class="selector-class">.linalg</span><span class="selector-class">.Vectors</span></span><br><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.mllib</span><span class="selector-class">.regression</span><span class="selector-class">.LabeledPoint</span></span><br><span class="line"></span><br><span class="line">val pos = LabeledPoint(<span class="number">1.0</span>, Vectors.dense(<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">3.0</span>))</span><br><span class="line">val neg = LabeledPoint(<span class="number">0.0</span>, Vectors.sparse(<span class="number">3</span>, Array(<span class="number">0</span>, <span class="number">2</span>), Array(<span class="number">1.0</span>, <span class="number">3.0</span>)))</span><br></pre></td></tr></table></figure><p><strong>sparse data</strong></p><p>稀疏数据存储是非常普遍的现象，mllib支持读取libsvm格式的数据，其数据格式如下：<br><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label index<span class="number">1</span>:<span class="keyword">value</span><span class="number">1</span>,index<span class="number">2</span>:<span class="keyword">value</span><span class="number">2</span> ...</span><br></pre></td></tr></table></figure></p><p>其读取方式包括：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.mllib</span><span class="selector-class">.util</span><span class="selector-class">.MLUtils</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// method 1</span></span><br><span class="line">spark<span class="selector-class">.read</span><span class="selector-class">.format</span>(<span class="string">"libsvm"</span>) .load(<span class="string">"libsvm data path"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// method 2</span></span><br><span class="line">MLUtils.loadLibSVMFile(spark<span class="selector-class">.sparkContext</span>, <span class="string">"libsvm data path"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="3-Local-Matrix（本地矩阵）"><a href="#3-Local-Matrix（本地矩阵）" class="headerlink" title="3. Local Matrix（本地矩阵）"></a>3. Local Matrix（本地矩阵）</h2><p>local matrix由行下标，列索引和double类型的值组成，存储在本地机器上，mllib支持密集矩阵和稀疏矩阵，其存储是按照列进行存储的。</p><p>例如下面的为密集矩阵:</p><center><img src="https://img-blog.csdnimg.cn/20190722205304310.gif" width="100px"></center><p>通过数组存储的形式为： [1.0, 3.0, 5.0, 2.0, 4.0, 6.0]，矩阵大小为[3，2]</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))</span></span><br><span class="line">val denseMatrix = Matrices.dense(<span class="number">3</span>,<span class="number">2</span>, Array(<span class="number">1.0</span>,<span class="number">3.0</span>,<span class="number">5.0</span>,<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">6.0</span>))</span><br><span class="line">println(s<span class="string">"denseMatrix is : $denseMatrix"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))</span></span><br><span class="line">val sparseMatrix = Matrices.sparse(<span class="number">3</span>,<span class="number">2</span>, Array(<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>),Array(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>),Array(<span class="number">9</span>,<span class="number">6</span>,<span class="number">8</span>))</span><br><span class="line">println(s<span class="string">"sparseMatrix is : $sparseMatrix"</span>)</span><br></pre></td></tr></table></figure><p>注：稀疏矩阵解释，首先指定矩阵是3行2列，Array(0, 1, 3)是指，第0个非零元素在第一列，第一第二个非零元素在第二列。</p><p>Array(0, 2, 1)是指，第一个非零元素在第0行，第二个非零元素在第2行，第三个非零元素在第1行。</p><p>此处设计比较好，假设100个元素分两列，不需要把每个元素所在列都标出来，只需要记录3个数字即可。Array(9, 6, 8)表示按顺序存储非零元素.</p><hr><p>Array(0,1,3)比较难理解，可以参考以下文章：</p><ul><li><a href="https://www.cnblogs.com/lyy-blog/p/9288701.html" target="_blank" rel="external">https://www.cnblogs.com/lyy-blog/p/9288701.html</a></li><li><a href="https://www.tuicool.com/articles/A3emmqi" target="_blank" rel="external">https://www.tuicool.com/articles/A3emmqi</a></li></ul><h2 id="4-Distributed-Matrix（分布式矩阵）"><a href="#4-Distributed-Matrix（分布式矩阵）" class="headerlink" title="4. Distributed Matrix（分布式矩阵）"></a>4. Distributed Matrix（分布式矩阵）</h2><p>一个分布式矩阵由下标和double类型的数据组成，不过分布式的矩阵的下标不是int类型，而是long类型，数据保存在一个或多个rdd中，选择一个正确的格式去存储分布式矩阵是非常重要的。分布式矩阵转换成不同的格式需要一个全局的shuffle(global shuffle)，而全局shuffle的代价会非常高。到目前为止，Spark MLlib中已经实现了三种分布式矩阵。</p><p>最基本的分布式矩阵是RowMatrix，它是一个行式的分布式矩阵，没有行索引。比如一系列特征向量的集合。RowMatrix由一个RDD代表所有的行，每一行是一个本地向量。假设一个RowMatrix的列数不是特别巨大，那么一个简单的本地向量能够与driver进行联系，并且数据可以在单个节点上保存或使用。IndexedRowMatrix与RowMatrix类似但是有行索引，行索引可以用来区分行并且进行连接等操作。CoordinateMatrix是一个以协同列表（coordinate list)格式存储数据的分布式矩阵，数据以RDD形式存储。</p><p>注意：因为我们需要缓存矩阵的大小，所以分布式矩阵的RDDs格式是需要确定的，使用非确定RDDs的话会报错。</p><h3 id="Row-Matrix"><a href="#Row-Matrix" class="headerlink" title="Row Matrix"></a>Row Matrix</h3><p>RowMatrix它是一个行式的分布式矩阵，没有行索引。比如一系列特征向量的集合。RowMatrix由一个RDD代表所有的行，每一行是一个本地向量。因为每一行代表一个本地向量，所以它的列数被限制在Integer.max的范围内，在实际应用中不会太大。</p><p>一个RowMatrix可以由一个RDD[Vector]的实例创建。因此我们可以计算统计信息或者进行分解。QR分解（QR decomposition）是A=QR，其中Q是一个矩阵，R是一个上三角矩阵。对sigular value decomposition(SVD和principal component analysis（PCA）,可以去参考降维的部分。<br>　　<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Row Matrix</span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(<span class="string">"Row Matrix ..."</span>)</span></span></span><br><span class="line">val arr = Array(Vectors.dense(<span class="number">1</span>,<span class="number">0</span>),Vectors.dense(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">val rows = spark<span class="selector-class">.sparkContext</span><span class="selector-class">.parallelize</span>(arr)</span><br><span class="line">val mat: RowMatrix = new RowMatrix(rows)</span><br><span class="line">val m = mat.numRows()</span><br><span class="line">val n = mat.numCols()</span><br><span class="line">val qrResult = mat.tallSkinnyQR(true)</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"m is: $m，n is $n，\nqrResult is :"</span>)</span></span></span><br><span class="line">qrResult<span class="selector-class">.Q</span><span class="selector-class">.rows</span><span class="selector-class">.foreach</span>(println)</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">()</span></span></span><br><span class="line">qrResult<span class="selector-class">.R</span><span class="selector-class">.rowIter</span><span class="selector-class">.foreach</span>(println)</span><br></pre></td></tr></table></figure></p><p>输出为：<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Row Matrix ...</span><br><span class="line">m <span class="keyword">is</span>: <span class="number">2</span>，n <span class="keyword">is</span> <span class="number">2</span>，</span><br><span class="line">qrResult <span class="keyword">is</span> :</span><br><span class="line">[<span class="meta">1.0,0.0</span>]</span><br><span class="line">[<span class="meta">0.0,1.0</span>]</span><br><span class="line"></span><br><span class="line">[<span class="meta">1.0,0.0</span>]</span><br><span class="line">[<span class="meta">0.0,1.0</span>]</span><br></pre></td></tr></table></figure></p><h3 id="IndexedRowMatrix"><a href="#IndexedRowMatrix" class="headerlink" title="IndexedRowMatrix"></a>IndexedRowMatrix</h3><p>IndexedRowMatrix与RowMatrix类似，但是它有行索引。由一个行索引RDD表示，索引每一行由一个long型行索引和一个本地向量组成。 </p><p>一个IndexedRowMatrix可以由RDD[IndexedRow]的实例来生成，IndexedRow是一个（Long, Vector)的封装。去掉行索引，IndexedRowMatrix能够转换成RowMatrix。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// IndexedRowMatrix</span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(<span class="string">"Indexed Row Matrix ..."</span>)</span></span></span><br><span class="line">val arr2 = Array(</span><br><span class="line">        IndexedRow(<span class="number">0</span>,Vectors.dense(<span class="number">1</span>,<span class="number">0</span>)),</span><br><span class="line">        IndexedRow(<span class="number">1</span>,Vectors.dense(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">val rows2: RDD[IndexedRow] = spark<span class="selector-class">.sparkContext</span><span class="selector-class">.parallelize</span>(arr2)</span><br><span class="line">val mat2 = new IndexedRowMatrix(rows2)</span><br><span class="line">val m2 = mat2.numRows()</span><br><span class="line">val n2 = mat2.numCols()</span><br><span class="line"><span class="comment">// 去掉行索引，转换成RowMatrix</span></span><br><span class="line">val qrResult2 = mat2.toRowMatrix().tallSkinnyQR(true)</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"m2 is: $m2，n2 is $n2，\nqrResult2 is :"</span>)</span></span></span><br><span class="line">qrResult2<span class="selector-class">.Q</span><span class="selector-class">.rows</span><span class="selector-class">.foreach</span>(println)</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">()</span></span></span><br><span class="line">qrResult2<span class="selector-class">.R</span><span class="selector-class">.rowIter</span><span class="selector-class">.foreach</span>(println)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Indexed Row Matrix ...</span><br><span class="line">m2 <span class="keyword">is</span>: <span class="number">2</span>，n2 <span class="keyword">is</span> <span class="number">2</span>，</span><br><span class="line">qrResult2 <span class="keyword">is</span> :</span><br><span class="line">[<span class="meta">1.0,0.0</span>]</span><br><span class="line">[<span class="meta">0.0,1.0</span>]</span><br><span class="line"></span><br><span class="line">[<span class="meta">1.0,0.0</span>]</span><br><span class="line">[<span class="meta">0.0,1.0</span>]</span><br></pre></td></tr></table></figure><h3 id="CoordinateMatrix"><a href="#CoordinateMatrix" class="headerlink" title="CoordinateMatrix"></a>CoordinateMatrix</h3><p>CoordinateMatrix是一个分布式矩阵，其实体集合是一个RDD，每一个是一个三元组(i:Long, j:Long, value:Double）。其中i是行索引，j是列索引，value是实体的值。当矩阵的维度很大并且是稀疏矩阵时，才使用CoordinateMatrix。 </p><p>一个CoordinateMatrix可以通过一个RDD[MatrixEntry]的实例来创建，MatrixEntry是一个(Long, Long, Double)的封装。CoordinateMatrix可以通过调用toIndexedRowMatrix转换成一个IndexedRowMatrix。CoordinateMatrix的其他降维方法暂时还不支持（Spark-1.6.2)。 　　</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CoordinateMatrix</span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(<span class="string">"Coordinate Matrix ..."</span>)</span></span></span><br><span class="line">val arr3 = Array(</span><br><span class="line">    MatrixEntry(<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>),</span><br><span class="line">    MatrixEntry(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">val entries = spark<span class="selector-class">.sparkContext</span><span class="selector-class">.parallelize</span>(arr3)</span><br><span class="line">val mat3 = new CoordinateMatrix(entries)</span><br><span class="line">val m3 = mat.numRows()</span><br><span class="line">val n3 = mat.numCols()</span><br><span class="line">val qrResult3 = mat3.toIndexedRowMatrix().toRowMatrix().tallSkinnyQR(true)</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"m3 is: $m3，n3 is $n3，\nqrResult3 is :"</span>)</span></span></span><br><span class="line">qrResult3<span class="selector-class">.Q</span><span class="selector-class">.rows</span><span class="selector-class">.foreach</span>(println)</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">()</span></span></span><br><span class="line">qrResult3<span class="selector-class">.R</span><span class="selector-class">.rowIter</span><span class="selector-class">.foreach</span>(println)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Coordinate Matrix ...</span><br><span class="line">m3 <span class="keyword">is</span>: <span class="number">2</span>，n3 <span class="keyword">is</span> <span class="number">2</span>，</span><br><span class="line">rowMat3 <span class="keyword">is</span> :</span><br><span class="line">[<span class="meta">1.0,0.0</span>]</span><br><span class="line">[<span class="meta">0.0,1.0</span>]</span><br><span class="line"></span><br><span class="line">[<span class="meta">1.0,0.0</span>]</span><br><span class="line">[<span class="meta">0.0,1.0</span>]</span><br></pre></td></tr></table></figure><h3 id="BlockMatrix"><a href="#BlockMatrix" class="headerlink" title="BlockMatrix"></a>BlockMatrix</h3><p>一个BlockMatrix是一个分布式的矩阵，由一个MatrixBlocks的RDD组成。MatrixBlock是一个三元组((Int, Int), Matrix),其中(Int, Int)是block的索引，Matrix是一个在指定位置上的维度为rowsPerBlock * colsPerBlock的子矩阵。BlockMatrix支持与另一个BlockMatrix对象的add和multiply操作。BlockMatrix提供了一个帮助方法validate，这个方法可以用于检测该`BlockMatrix·是否正确。</p><p>可以通过IndexedRowMatrix或者CoordinateMatrix调用toBlockMatrix快速得到BlockMatrix对象。默认情况下toBlockMatrix方法会得到一个1024 x 1024的BlockMatrix。使用时可以通过手动传递维度值来设置维度，toBlockMatrix(rowsPerBlock, colsPerBlock)。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockMatrix</span></span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(<span class="string">"Block Matrix ..."</span>)</span></span></span><br><span class="line">val arr4 = Array(</span><br><span class="line">    MatrixEntry(<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>),</span><br><span class="line">    MatrixEntry(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">val entries4: RDD[MatrixEntry] = spark<span class="selector-class">.sparkContext</span><span class="selector-class">.parallelize</span>(arr4)</span><br><span class="line">val coordMat: CoordinateMatrix = new CoordinateMatrix(entries4)</span><br><span class="line">val matA: BlockMatrix = coordMat.toBlockMatrix().cache()</span><br><span class="line"><span class="comment">// 检测BlockMatrix格式是否正确，错误的话会抛出异常，正确的话无其他影响</span></span><br><span class="line">matA.validate()</span><br><span class="line">matA<span class="selector-class">.blocks</span><span class="selector-class">.foreach</span>(println)</span><br><span class="line">val m4 = matA.numRowBlocks</span><br><span class="line">val n4 = matA.numColBlocks</span><br><span class="line"><span class="function"><span class="title">println</span><span class="params">(s<span class="string">"m4 is: $m4，n4 is $n4"</span>)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算A^T * A.</span></span><br><span class="line">val ata = matA<span class="selector-class">.transpose</span><span class="selector-class">.multiply</span>(matA)</span><br><span class="line">ata<span class="selector-class">.blocks</span><span class="selector-class">.foreach</span>(println)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">Block</span> <span class="selector-tag">Matrix</span> ...</span><br><span class="line">((0,0),2 <span class="selector-tag">x</span> 2 <span class="selector-tag">CSCMatrix</span></span><br><span class="line">(0,0) 1<span class="selector-class">.0</span></span><br><span class="line">(1,1) 1<span class="selector-class">.0</span>)</span><br><span class="line"><span class="selector-tag">m4</span> <span class="selector-tag">is</span>: 1，<span class="selector-tag">n4</span> <span class="selector-tag">is</span> 1</span><br><span class="line">((0,0),1<span class="selector-class">.0</span>  0<span class="selector-class">.0</span>  </span><br><span class="line">0<span class="selector-class">.0</span>  1<span class="selector-class">.0</span>  )</span><br></pre></td></tr></table></figure><h1 id="相似度计算原理探索"><a href="#相似度计算原理探索" class="headerlink" title="相似度计算原理探索"></a>相似度计算原理探索</h1><blockquote><p>无论是ICF基于物品的协同过滤、UCF基于用户的协同过滤、基于内容的推荐，最基本的环节都是计算相似度。如果样本特征维度很高或者<user, item,="" score="">的维度很大，都会导致无法直接计算。设想一下100w*100w的二维矩阵，计算相似度怎么算？</user,></p><p>在spark中RowMatrix提供了一种并行计算相似度的思路，下面就来看看其中的奥妙吧！</p></blockquote><h2 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h2><p>相似度有很多种，每一种适合的场景都不太一样。比如：</p><ul><li>欧氏距离，在几何中最简单的计算方法</li><li>夹角余弦，通过方向计算相似度，通常在用户对商品评分、NLP等场景使用</li><li>杰卡德距离，在不考虑每一样的具体值时使用</li><li>皮尔森系数，与夹角余弦类似，但是可以去中心化。比如评分时，有人倾向于打高分，有人倾向于打低分，他们的最后效果在皮尔森中是一样的</li><li>曼哈顿距离，一般在路径规划、地图类中常用，比如A*算法中使用曼哈顿来作为每一步代价值的一部分（F=G+H, G是从当前点移动到下一个点的距离，H是距离目标点的距离，这个H就可以用曼哈顿距离表示）</li></ul><center><img src="https://img-blog.csdnimg.cn/20190722195328417.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"> </center><p>上面两个向量(x1,y1)和(x2,y2)计算夹角的余弦值就是两个向量方向的相似度，其公式为：</p><script type="math/tex; mode=display">cos(\theta )=\frac { a\cdot b }{ ||a||\ast ||b|| } \\ =\quad \frac { { x }_{ 1 }\ast { x }_{ 2 }\quad +\quad { y }_{ 1 }\ast y_{ 2 } }{ \sqrt { { x }_{ 1 }^{ 2 }+{ y }_{ 1 }^{ 2 } } \ast \sqrt { { x }_{ 2 }^{ 2 }+{ y }_{ 2 }^{ 2 } }  }</script><p>其中，||a||表示a的模，即每一项的平方和再开方。</p><h2 id="公式拆解"><a href="#公式拆解" class="headerlink" title="公式拆解"></a>公式拆解</h2><p>那么如果向量不只是两维，而是n维呢？比如有两个向量：</p><script type="math/tex; mode=display">第一个向量：({x}_{1}, {x}_{2}, {x}_{3}, ..., {x}_{n})\\第二个向量：({y}_{1}, {y}_{2}, {y}_{3}, ..., {y}_{n})</script><p>他们的相似度计算方法套用上面的公式为：</p><script type="math/tex; mode=display">cos(\theta )\quad =\quad \frac { \sum _{ i=1 }^{ n }{ ({ x }_{ i }\ast { y }_{ i }) }  }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \ast \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } \\ =\quad \frac { { x }_{ 1 }\ast { y }_{ 1 }+{ x }_{ 2 }\ast { y }_{ 2 }+...+{ x }_{ n }\ast { y }_{ n } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \ast \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } \\ =\quad \frac { { x }_{ 1 }\ast { y }_{ 1 } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \ast \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +\frac { { x }_{ 2 }\ast { y }_{ 2 } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \ast \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +...+\frac { { x }_{ n }\ast { y }_{ n } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  } \ast \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } \\ =\quad \frac { { x }_{ 1 } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }  } \ast \frac { { y }_{ 1 } }{ \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +\frac { { x }_{ 2 } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }  } \ast \frac { { y }_{ 2 } }{ \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  } +...+\frac { { x }_{ n } }{ \sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }  } \ast \frac { { y }_{ n } }{ \sqrt { \sum _{ i=1 }^{ n }{ y_{ i }^{ 2 } }  }  }</script><p>通过上面的公式就可以发现，夹角余弦可以拆解成每一项与另一项对应位置的乘积x1∗y1，再除以每个向量自己的</p><script type="math/tex; mode=display">\sqrt { \sum _{ i=1 }^{ n }{ { x }_{ i }^{ 2 } }  }</script><p>就可以了。</p><h2 id="矩阵并行"><a href="#矩阵并行" class="headerlink" title="矩阵并行"></a>矩阵并行</h2><p>画个图看看，首先创建下面的矩阵：</p><center><img src="https://img-blog.csdnimg.cn/20190722195942186.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></center><p>注意，矩阵里面都是一列代表一个向量….上面是创建矩阵时的三元组，如果在spark中想要创建matrix，可以这样：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">val df = spark.createDataFrame(Seq(</span><br><span class="line">      (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="number">0</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2.0</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2.0</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="number">1</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3.0</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3.0</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="number">2</span>, <span class="number">3.0</span>),</span><br><span class="line">      (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="number">3</span>, <span class="number">1.0</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="number">3</span>, <span class="number">4.0</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">val matrix = new CoordinateMatrix(df.map(row =&gt; MatrixEntry(row.getAs[Integer](<span class="number">0</span>).toLong, row.getAs[Integer](<span class="number">1</span>).toLong, row.getAs[Double](<span class="number">2</span>))).toJavaRDD)</span><br></pre></td></tr></table></figure><p>然后计算每一个向量的normL2，即平方和开根号。</p><center><img src="https://img-blog.csdnimg.cn/20190722200134362.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></center>以第一个和第二个向量计算为例，第一个向量为(1,1,1,1)，第二个向量为(2,2,1,1)，每一项除以对应的normL2，得到后面的两个向量：$$0.5*0.63+0.5*0.63+0.5*0.31+0.5*0.31 \approx 0.94$$两个向量最终的相似度为0.94。那么在Spark如何快速并行处理呢？通过上面的例子，可以看到两个向量的相似度，需要把每一维度乘积后相加，但是一个向量一般都是跨RDD保存的，所以可以先计算所有向量的第一维，得出结果$$(向量1的第1维，向量2的第1维，value)\\(向量1的第2维，向量2的第2维，value)\\...\\(向量1的第n维，向量2的第n维，value)\\(向量1的第1维，向量3的第1维，value)\\..\\(向量1的第n维，向量3的第n维，value)\\$$最后对做一次reduceByKey累加结果即可.....## 阅读源码首先创建dataframe形成matrix：<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.linalg.distributed.&#123;<span class="attribute">CoordinateMatrix, MatrixEntry&#125;</span></span><br><span class="line"><span class="attribute">import org.apache.spark.sql.SparkSession</span></span><br><span class="line"><span class="attribute"></span></span><br><span class="line"><span class="attribute">object MatrixSimTest &#123;</span></span><br><span class="line"><span class="attribute">  def main(args</span>: Array[String]): Unit = &#123;</span><br><span class="line">    // 创建dataframe，转换成matrix</span><br><span class="line">    val spark = SparkSession<span class="variable">.builder</span>()<span class="variable">.master</span>("local[*]")<span class="variable">.appName</span>("sim")<span class="variable">.getOrCreate</span>()</span><br><span class="line">    spark<span class="variable">.sparkContext</span><span class="variable">.setLogLevel</span>("WARN")</span><br><span class="line"></span><br><span class="line">    import spark<span class="variable">.implicits</span><span class="variable">._</span></span><br><span class="line"></span><br><span class="line">    val df = spark<span class="variable">.createDataFrame</span>(Seq(</span><br><span class="line">      (0, 0, 1.0),</span><br><span class="line">      (1, 0, 1.0),</span><br><span class="line">      (2, 0, 1.0),</span><br><span class="line">      (3, 0, 1.0),</span><br><span class="line">      (0, 1, 2.0),</span><br><span class="line">      (1, 1, 2.0),</span><br><span class="line">      (2, 1, 1.0),</span><br><span class="line">      (3, 1, 1.0),</span><br><span class="line">      (0, 2, 3.0),</span><br><span class="line">      (1, 2, 3.0),</span><br><span class="line">      (2, 2, 3.0),</span><br><span class="line">      (0, 3, 1.0),</span><br><span class="line">      (1, 3, 1.0),</span><br><span class="line">      (3, 3, 4.0)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    val matrix = new CoordinateMatrix(df<span class="variable">.map</span>(row =&gt; MatrixEntry(row<span class="variable">.getAs</span>[Integer](0)<span class="variable">.toLong</span>, row<span class="variable">.getAs</span>[Integer](1)<span class="variable">.toLong</span>, row<span class="variable">.getAs</span>[Double](2)))<span class="variable">.toJavaRDD</span>)</span><br><span class="line">    // 调用sim方法</span><br><span class="line">    val x = matrix<span class="variable">.toRowMatrix</span>()<span class="variable">.columnSimilarities</span>()</span><br><span class="line">    // 得到相似度结果</span><br><span class="line">    x<span class="variable">.entries</span><span class="variable">.collect</span>()<span class="variable">.foreach</span>(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>得到的结果为：<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MatrixEntry(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0.7071067811865476</span>)</span><br><span class="line">MatrixEntry(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0.8660254037844386</span>)</span><br><span class="line">MatrixEntry(<span class="number">2</span>,<span class="number">3</span>,<span class="number">0.2721655269759087</span>)</span><br><span class="line">MatrixEntry(<span class="number">0</span>,<span class="number">1</span>,<span class="number">0.9486832980505139</span>)</span><br><span class="line">MatrixEntry(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0.9128709291752768</span>)</span><br><span class="line">MatrixEntry(<span class="number">1</span>,<span class="number">3</span>,<span class="number">0.596284793999944</span>)</span><br></pre></td></tr></table></figure>直接进入columnSimilarities方法看看是怎么个流程吧！<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">columnSimilarities</span><span class="params">()</span></span>: CoordinateMatrix = &#123;</span><br><span class="line">  columnSimilarities(<span class="number">0</span>.<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>内部调用了带阈值的相似度方法，这里的阈值是指相似度小于该值时，输出结果时，会自动过滤掉。<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def columnSimilarities(threshold: <span class="keyword">Double</span>): CoordinateMatrix = &#123;</span><br><span class="line">  //检查参数...</span><br><span class="line"></span><br><span class="line">  val <span class="built_in">gamma</span> = <span class="keyword">if</span> (threshold &lt; <span class="number">1e-6</span>) &#123;</span><br><span class="line">    <span class="keyword">Double</span>.PositiveInfinity</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="number">10</span> * math.<span class="built_in">log</span>(numCols()) / threshold</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> columnSimilaritiesDIMSUM(computeColumnSummaryStatistics().normL2.toArray, <span class="built_in">gamma</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>这里的gamma用于采样，具体的做法咱们来继续看源码。然后看一下computeColumnSummaryStatistics().normL2.toArray这个方法：<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def computeColumnSummaryStatistics(): MultivariateStatisticalSummary = &#123;</span><br><span class="line">  val summary = rows.treeAggregate(<span class="keyword">new</span> MultivariateOnlineSummarizer)(</span><br><span class="line">    (aggregator, data) =&gt; aggregator.add(data),</span><br><span class="line">    (aggregator1, aggregator2) =&gt; aggregator1.merge(aggregator2))</span><br><span class="line">  updateNumRows(summary.count)</span><br><span class="line">  summary</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>之前有介绍这个treeAggregate是一种带“预reduce”的map-reduce，返回的summary，里面帮我们统计了每一个向量的很多指标，比如<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">currMean    为 每一个向量的平均值</span><br><span class="line">currM2      为 每个向量的每一维的平方和</span><br><span class="line">currL1      为 每个向量的绝对值的和</span><br><span class="line">currMax     为 每个向量的最大值</span><br><span class="line">currMin     为 每个向量的最小值</span><br><span class="line">nnz         为 每个向量的非<span class="number">0</span>个数</span><br></pre></td></tr></table></figure>这里我们只需要currM2，它是每个向量的平方和。summary调用的normL2方法：<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">override def normL2: Vector = &#123;</span><br><span class="line">  require(totalWeightSum &gt; 0, s"Nothing has been added to this summarizer.")</span><br><span class="line"></span><br><span class="line">  val realMagnitude = Array.ofDim[<span class="string">Double</span>](<span class="link">n</span>)</span><br><span class="line"></span><br><span class="line">  var i = 0</span><br><span class="line">  val len = currM2.length</span><br><span class="line">  while (i <span class="xml"><span class="tag">&lt; <span class="attr">len</span>) &#123;</span></span></span><br><span class="line"><span class="xml">    realMagnitude(i) = math.sqrt(currM2(i))</span></span><br><span class="line"><span class="xml">    i += 1</span></span><br><span class="line"><span class="xml">  &#125;</span></span><br><span class="line"><span class="xml">  Vectors.dense(realMagnitude)</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>上面这步就是对平方和开个根号，这样就求出来了每个向量的分母部分。下面就是最关键的地方了：<figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">private[mllib] def columnSimilaritiesDIMSUM(</span><br><span class="line">      colMags: <span class="type">Array</span>[<span class="type">Double</span>],</span><br><span class="line">      gamma: <span class="type">Double</span>): <span class="type">CoordinateMatrix</span> = &#123;</span><br><span class="line">    <span class="comment">// 一些参数校验</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对gamma进行开方</span></span><br><span class="line">    <span class="meta">val</span> sg = math.sqrt(gamma) <span class="comment">// sqrt(gamma) used many times</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里把前面算的平方根的值设置一个默认值，因为如果为0，除0会报异常，所以设置为1</span></span><br><span class="line">    <span class="meta">val</span> colMagsCorrected = colMags.map(x =&gt; <span class="keyword">if</span> (x == <span class="number">0</span>) <span class="number">1.0</span> <span class="keyword">else</span> x)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把抽样概率数组 和 平方根数组进行广播</span></span><br><span class="line">    <span class="meta">val</span> sc = rows.context</span><br><span class="line">    <span class="meta">val</span> pBV = sc.broadcast(colMagsCorrected.map(c =&gt; sg / c))</span><br><span class="line">    <span class="meta">val</span> qBV = sc.broadcast(colMagsCorrected.map(c =&gt; math.min(sg, c)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历每一行，计算每个向量该维的乘积，形成三元组</span></span><br><span class="line">    <span class="meta">val</span> sims = rows.mapPartitionsWithIndex &#123; (indx, iter) =&gt;</span><br><span class="line">      <span class="meta">val</span> p = pBV.value</span><br><span class="line">      <span class="meta">val</span> q = qBV.value</span><br><span class="line">      <span class="comment">// 获得随机值</span></span><br><span class="line">      <span class="meta">val</span> rand = <span class="function"><span class="keyword">new</span> <span class="title">XORShiftRandom</span>(indx)</span></span><br><span class="line"><span class="function">      <span class="title">val</span> <span class="title">scaled</span> = <span class="title">new</span> <span class="title">Array</span>[<span class="title">Double</span>](p.size)</span></span><br><span class="line"><span class="function">      <span class="title">iter</span>.<span class="title">flatMap</span> &#123; <span class="title">row</span> =&gt;</span></span><br><span class="line">        row <span class="keyword">match</span> &#123;</span><br><span class="line">          case <span class="type">SparseVector</span>(size, indices, values) =&gt;</span><br><span class="line">            <span class="comment">// 如果是稀疏向量，遍历向量的每一维，除以平方根</span></span><br><span class="line">            <span class="meta">val</span> nnz = indices.size</span><br><span class="line">            <span class="keyword">var</span> k = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> (k &lt; nnz) &#123;</span><br><span class="line">              scaled(k) = values(k) / q(indices(k))</span><br><span class="line">              k += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 遍历向量数组，计算每一个数值与其他数值的乘机。</span></span><br><span class="line">            <span class="comment">// 比如向量(1, 2, 0 ,1)</span></span><br><span class="line">            <span class="comment">// 得到的结果为 (0,1,value)(0,3,value)(2,3,value)</span></span><br><span class="line">            <span class="type">Iterator</span>.tabulate (nnz) &#123; k =&gt;</span><br><span class="line">              <span class="meta">val</span> buf = <span class="function"><span class="keyword">new</span> <span class="title">ListBuffer</span>[((<span class="type">Int</span>, <span class="type">Int</span>), <span class="title">Double</span>)]()</span></span><br><span class="line"><span class="function">              <span class="title">val</span> <span class="title">i</span> = <span class="title">indices</span>(k)</span></span><br><span class="line"><span class="function">              <span class="title">val</span> <span class="title">iVal</span> = <span class="title">scaled</span>(k)</span></span><br><span class="line"><span class="function">              <span class="comment">// 判断当前列是否符合采样范围，如果小于采样值，就忽略</span></span></span><br><span class="line"><span class="function">              <span class="title">if</span> (iVal != <span class="number">0</span> &amp;&amp; rand.nextDouble() &lt; <span class="title">p</span>(i)) &#123;</span></span><br><span class="line"><span class="function">                <span class="title">var</span> <span class="title">l</span> = <span class="title">k</span> + 1</span></span><br><span class="line"><span class="function">                <span class="title">while</span> (l &lt; nnz) &#123;</span></span><br><span class="line"><span class="function">                  <span class="title">val</span> <span class="title">j</span> = <span class="title">indices</span>(l)</span></span><br><span class="line"><span class="function">                  <span class="title">val</span> <span class="title">jVal</span> = <span class="title">scaled</span>(l)</span></span><br><span class="line"><span class="function">                  <span class="title">if</span> (jVal != <span class="number">0</span> &amp;&amp; rand.nextDouble() &lt; <span class="title">p</span>(j)) &#123;</span></span><br><span class="line"><span class="function">                    <span class="comment">// 计算每一维与其他维的值</span></span></span><br><span class="line"><span class="function">                    <span class="title">buf</span> += (((i, j), <span class="title">iVal</span> * <span class="title">jVal</span>))</span></span><br><span class="line"><span class="function">                  &#125;</span></span><br><span class="line"><span class="function">                  <span class="title">l</span> += 1</span></span><br><span class="line"><span class="function">                &#125;</span></span><br><span class="line"><span class="function">              &#125;</span></span><br><span class="line"><span class="function">              <span class="title">buf</span></span></span><br><span class="line"><span class="function">            &#125;.<span class="title">flatten</span></span></span><br><span class="line"><span class="function">          <span class="title">case</span> <span class="title">DenseVector</span>(values) =&gt;</span></span><br><span class="line">            <span class="comment">// 跟稀疏同理</span></span><br><span class="line">            <span class="meta">val</span> n = values.size</span><br><span class="line">            <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> (i &lt; n) &#123;</span><br><span class="line">              scaled(i) = values(i) / q(i)</span><br><span class="line">              i += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">Iterator</span>.tabulate (n) &#123; i =&gt;</span><br><span class="line">              <span class="meta">val</span> buf = <span class="function"><span class="keyword">new</span> <span class="title">ListBuffer</span>[((<span class="type">Int</span>, <span class="type">Int</span>), <span class="title">Double</span>)]()</span></span><br><span class="line"><span class="function">              <span class="title">val</span> <span class="title">iVal</span> = <span class="title">scaled</span>(i)</span></span><br><span class="line"><span class="function">              <span class="title">if</span> (iVal != <span class="number">0</span> &amp;&amp; rand.nextDouble() &lt; <span class="title">p</span>(i)) &#123;</span></span><br><span class="line"><span class="function">                <span class="title">var</span> <span class="title">j</span> = <span class="title">i</span> + 1</span></span><br><span class="line"><span class="function">                <span class="title">while</span> (j &lt; n) &#123;</span></span><br><span class="line"><span class="function">                  <span class="title">val</span> <span class="title">jVal</span> = <span class="title">scaled</span>(j)</span></span><br><span class="line"><span class="function">                  <span class="title">if</span> (jVal != <span class="number">0</span> &amp;&amp; rand.nextDouble() &lt; <span class="title">p</span>(j)) &#123;</span></span><br><span class="line"><span class="function">                    <span class="title">buf</span> += (((i, j), <span class="title">iVal</span> * <span class="title">jVal</span>))</span></span><br><span class="line"><span class="function">                  &#125;</span></span><br><span class="line"><span class="function">                  <span class="title">j</span> += 1</span></span><br><span class="line"><span class="function">                &#125;</span></span><br><span class="line"><span class="function">              &#125;</span></span><br><span class="line"><span class="function">              <span class="title">buf</span></span></span><br><span class="line"><span class="function">            &#125;.<span class="title">flatten</span></span></span><br><span class="line"><span class="function">        &#125;</span></span><br><span class="line"><span class="function">      &#125;</span></span><br><span class="line"><span class="function">    <span class="comment">// 最后再执行一个reduceBykey，累加所有的值，就是i和j的相似度</span></span></span><br><span class="line"><span class="function">    &#125;.<span class="title">reduceByKey</span>(_ + _).<span class="title">map</span> &#123; <span class="title">case</span> ((i, j), <span class="title">sim</span>) =&gt;</span></span><br><span class="line">      <span class="type">MatrixEntry</span>(i.toLong, j.toLong, sim)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">new</span> <span class="title">CoordinateMatrix</span>(sims, numCols(), <span class="title">numCols</span>())</span></span><br><span class="line"><span class="function">  &#125;</span></span><br></pre></td></tr></table></figure>这样把所有向量的平方和广播后，每一行都可以在不同的节点并行处理了。总结来说，Spark提供的这个计算相似度的方法有两点优势：- 通过拆解公式，使得每一行独立计算，加快速度- 提供采样方案，以采样方式抽样固定的特征维度计算相似度不过杰卡德目前并不能使用这种方法来计算，因为杰卡德中间有一项需要对向量求dot，这种方式就不适合了；如果杰卡德想要快速计算，可以去参考LSH局部敏感哈希算法，这里就不详细说明了。---<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><p><font color="red">注：《推荐系统开发实战》是小编的最新出版的技术图书，已经在京东，当当上线，感兴趣的朋友可以进行购买阅读！</font></p><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="50%"></center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文出自「xingoo」在原文的基础上加以小编自己的理解形成的学习笔记，希望对读者有帮助。原文出自：&lt;a href=&quot;https://www.cnblogs.com/xing901022/p/9296882.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spark MLlib 之 大规模数据集的相似度计算原理探索&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>《推荐系统开发实战》之业内推荐系统架构介绍</title>
    <link href="http://thinkgamer.cn/2019/07/20/RecSys/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/%E3%80%8A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98%E3%80%8B%E4%B9%8B%E4%B8%9A%E5%86%85%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2019/07/20/RecSys/推荐系统开发实战/《推荐系统开发实战》之业内推荐系统架构介绍/</id>
    <published>2019-07-20T00:38:45.000Z</published>
    <updated>2019-10-14T06:42:35.667Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不管是电商网站，还是新闻资讯类网站，推荐系统都扮演着十分重要的角色。一个优秀的推荐系统能够推荐出让人满意的物品，但这不仅是推荐算法的功劳，整个推荐架构所扮演的角色也举足轻重。</p></blockquote><a id="more"></a><hr><p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/96265282" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/96265282</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki<br>个人网站：<a href="http://thinkgamer.github.io" target="_blank" rel="external">http://thinkgamer.github.io</a></p><hr><p>学术界往往更加关注推荐算法的各项评估指标。从基本的协同过滤到点击率预估算法，从深度学习到强化学习，学术界都始终走在最前列。一个推荐算法从出现到在业界得到广泛应用是一个长期的过程，因为在实际的生产系统中，首先需要保证的是稳定、实时地向用户提供推荐服务，在这个前提下才能追求推荐系统的效果。</p><p>在生产系统中，不管是用户维度、物品维度还是用户和物品的交互维度，数据都是极其丰富的，学术界对算法的使用方法不能照搬到工业界。当一个用户访问推荐模块时，系统不可能针对该用户对所有的物品进行排序，那么推荐系统是怎么解决的呢？对应的商品众多，如何决定将哪些商品展示给用户？对于排序好的商品，如何合理地展示给用户？</p><h1 id="架构介绍"><a href="#架构介绍" class="headerlink" title="架构介绍"></a>架构介绍</h1><p>图14-1所示是业界推荐系统通用架构图，主要包括：底层基础数据、数据加工存储、召回内容、计算排序、过滤和展示、业务应用。<br>底层基础数据是推荐系统的基石，只有数据量足够多，才能从中挖掘出更多有价值的信息，进而更好地为推荐系统服务。底层基础数据包括用户和物品本身数据、用户行为数据、用户系统上报数据等。</p><center><img src="https://img-blog.csdnimg.cn/20190717080622698.png" width="500px"></center><p>图14-2～图14-4所示为用户本身数据、物品本身数据和用户行为数据。</p><center><img src="https://img-blog.csdnimg.cn/20190717080738525.png" width="300px"></center><center><img src="https://img-blog.csdnimg.cn/20190717080746725.png" width="300px"></center><center><img src="https://img-blog.csdnimg.cn/20190717080753359.png" width="300px"></center><p>得到底层基础数据之后，就要对数据进行加工处理和分析了，如结合用户属性信息和行为信息构建用户画像，结合物品属性信息和用户对物品的行为信息构建物品画像。基于用户对物品的行为数据构建特征工程，同时进行相关的数据分析。<br>数据在处理之后存储到相应的位置（业务推荐系统使用的数据一般存储在redis中），供推荐系统实时调用。</p><h1 id="召回内容"><a href="#召回内容" class="headerlink" title="召回内容"></a>召回内容</h1><p>电商网站、内容网站、视频网站中数据量很大，并不能直接把所有的物品数据全部输送到推荐系统进行排序，那么如何对物品进行筛选就成了很关键的问题。<br>第4章中介绍了一些常用的数据挖掘算法和应用场景，在进行物品召回时可以基于一些常用的机器学习算法构建用户偏好模型、用户兴趣模型、物品相似模型、物品互补模型等。在进行内容召回时，只召回和用户有偏好关系、和用户有直接关联、和用户有直接关系的相关物品，输入排序模型，进行打分排序。<br>例如，在某新闻类网站中，根据用户对新闻的相关行为信息构建用户对新闻标签的兴趣模型，在为用户推荐时就可以推荐用户偏好标签下的新闻数据，如图14-5所示。</p><center><img src="https://img-blog.csdnimg.cn/20190717080944181.png" width="500px"></center><p>在物品召回过程中，重点是如何构建合适的用户偏好模型，只有保证偏好模型的准确性才能确保用户召回物品的准确性。</p><h1 id="计算排序"><a href="#计算排序" class="headerlink" title="计算排序"></a>计算排序</h1><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>“数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。</p><p>特征工程在机器学习中起着非常重要的作用，一般认为包括特征构建、特征提取、特征选择三部分。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是：</p><ul><li>特征提取强调通过特征转换的方式得到一组具有明显物理意义或统计意义的特征；</li><li>特征选择是从特征集合中挑选一组具有明显物理意义或统计意义的特征子集。</li></ul><p>两者都能帮助减少特征维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征对于模型构建的重要性。<br>特征工程的标准化流程主要分为以下几步：<br>（1）基于业务理解，找到对因变量有影响的所有自变量，即特征。<br>（2）评估特征的可用性、覆盖率、准确率等。<br>（3）特征处理：包括特征清洗、特征预处理（特征预处理可参考第4章“数据预处理”部分）、特征选择。<br>（4）特征监控：特征对算法模型的影响很大，微小的浮动都会带来模型效果的很大波动，因此做好重要特征的监控可防止特征异常变动带来线上事故。</p><h2 id="特征分类"><a href="#特征分类" class="headerlink" title="特征分类"></a>特征分类</h2><p>在工业界的推荐系统中，典型的特征主要分为以下四类。</p><ul><li>相关性特征：评估内容的属性与用户是否匹配。显性的匹配包括关键词匹配、分类匹配、来源匹配、主题匹配等。</li><li>环境特征：包括地理位置、时间。这些既是偏差特征，又能以此构建一些匹配特征。</li><li>热度特征：包括全局热度、分类热度、主题热度及关键词热度等。内容热度信息在大的推荐系统中特别是在用户冷启动时非常有效。</li><li>协同特征：可以在一定程度上帮助解决所谓算法越推越窄的问题。协同特征并不考虑用户已有历史，而是通过用户行为分析不同用户间的相似性，如点击相似、兴趣分类相似、主题相似、兴趣词相似，甚至向量相似，从而扩展模型的探索能力。<h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2>在得到召回的物品之后，就要考虑如何对这些物品进行正确的排序。目前业界在机器学习领域最普遍的做法是将排序推荐模型作为二分类模型来训练，即在构造样本集的过程中对应的标签为0或1（未点击或点击）。常用的排序算法包括但不局限于GBDT、LR、XGBoost等，当然也有很多把GBDT和LR结合起来使用的，但是模型融合后的效果在不同的业务场景中带来的提升并不是很大。</li></ul><h1 id="物品过滤和展示"><a href="#物品过滤和展示" class="headerlink" title="物品过滤和展示"></a>物品过滤和展示</h1><blockquote><p>过滤和展示直接影响用户体验，因此在做推荐系统时一定要注意相关的过滤和展示规则。</p></blockquote><h2 id="物品过滤"><a href="#物品过滤" class="headerlink" title="物品过滤"></a>物品过滤</h2><p>经常会听到人们说“电商网站经常给我推荐我已经买过的东西”。其实在做推荐系统的过程中会有相关的过滤规则，在电商推荐系统中，最常用的过滤规则是：用户购买过滤，即在进行商品召回时过滤掉用户过去一段时间内已经购买过的商品和相似商品。例如，用户昨天买了一个机械键盘，今天的推荐系统就不会再给该用户推荐机械键盘了。<br>同时也会有一些其他过滤规则如：</p><ul><li>项目指定的一些敏感词汇或敏感商品等过滤。</li><li>刷单商品过滤。</li><li>曝光商品过滤（有时会认为那些曝光过的商品是用户不感兴趣的，即看到了没有进行点击）。</li><li>无货商品过滤。</li></ul><p>至于为什么推荐系统会给用户推荐已经购买过的商品，是因为在用户购买该商品之后，又对该类型的商品产生了新的行为，所以推荐系统会再次进行推荐。</p><h2 id="物品展示"><a href="#物品展示" class="headerlink" title="物品展示"></a>物品展示</h2><p>展示即用户看到的推荐结果。不同类型的推荐系统中展示的规则不一样，但基本原则是：品类隔离展示，即同类型的商品不能出现在相邻的位置。例如推荐系统返回的推荐结果集中有两个手机，这两个手机就不能在相邻的位置展示。</p><p>有的推荐系统会要求第一屏内不能出现同类型的商品，如推荐系统给用户的第一屏展示了8个商品，那么这8个商品中就不能出现同类型的商品（如不能出现两个手机）。</p><h1 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h1><p>无论是推荐架构最开始的召回内容、计算排序，还是最后的过滤和展示，每次新上一个方案之后都要进行效果统计，生产系统中最常用的效果评估方法就是ABTest，更多关于ABTest的使用介绍可以参考第10章。<br>在生产系统中，进行ABtest之后，往往会将不好的方案下线，保留效果更好的一方，同时也会不断上线新的召回、排序特征等，迭代优化模型，提升线上效果。</p><hr><center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【搜索与推荐Wiki】 </center><hr><p><font color="red">注：《推荐系统开发实战》是小编近期要上的一本图书，预计本月（7月末）可在京东，当当上线，感兴趣的朋友可以进行关注！</font></p><center><img src="https://img-blog.csdnimg.cn/20190708234949217.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="50%"></center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;不管是电商网站，还是新闻资讯类网站，推荐系统都扮演着十分重要的角色。一个优秀的推荐系统能够推荐出让人满意的物品，但这不仅是推荐算法的功劳，整个推荐架构所扮演的角色也举足轻重。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐系统开发实战" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
</feed>
