<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>All In CTR、DL、ML、RL、NLP、KG</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2020-01-10T02:16:01.796Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【技术服务】和【商务合作】</title>
    <link href="http://thinkgamer.cn/2066/06/06/%E9%9A%8F%E7%BC%98/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2066/06/06/随缘/商务合作介绍/</id>
    <published>2066-06-05T22:06:06.000Z</published>
    <updated>2020-01-10T02:16:01.796Z</updated>
    
    <content type="html"><![CDATA[<hr><center><br><br><font size="8" style="font-weight:bold; color: green;">WelCome To “Thinkgamer 小站”</font><br><br></center><hr><p>全网唯一ID：Thinkgamer,个人微信公众号”搜索与推荐Wiki“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，数据挖掘，数据分析，全站开发，大数据。</p><h3 id="About"><a href="#About" class="headerlink" title="About"></a>About</h3><p>CyanScikit科技团队成立于2019年，由一群热爱技术，追求极致的Coders和Managers组成！</p><h3 id="Leader"><a href="#Leader" class="headerlink" title="Leader"></a>Leader</h3><p>全球唯一ID：Thinkgamer。《推荐系统开发实战》作者，CSDN博客技术专家，原Top电商算法工程师。擅长领域：</p><ul><li>推荐系统</li><li>数据开发/分析/挖掘</li></ul><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><center>    <img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center><ul><li>技术咨询、学习路线定制</li><li>推荐系统、技术培训</li><li>数据存储（方案/规范等）、数据开发、数据分析、数据挖掘</li><li>数据可视化、小程序/H5、全栈开发</li><li>爬虫、广告接入</li></ul><h3 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h3><p>使用技术去更好的服务于客户！</p><h3 id="Team"><a href="#Team" class="headerlink" title="Team"></a>Team</h3><center>    <img src="https://img-blog.csdnimg.cn/20191105121227125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center><h3 id="Contact-Me"><a href="#Contact-Me" class="headerlink" title="Contact Me"></a>Contact Me</h3><center>    <img src="https://img-blog.csdnimg.cn/20191105121227125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center><center>    <img src="https://img-blog.csdnimg.cn/20191105121446131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;center&gt;
&lt;br&gt;&lt;br&gt;
&lt;font size=&quot;8&quot; style=&quot;font-weight:bold; color: green;&quot;&gt;WelCome To “Thinkgamer 小站”&lt;/font&gt;
&lt;br&gt;&lt;br&gt;
&lt;/center&gt;

&lt;hr&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="随手记" scheme="http://thinkgamer.cn/tags/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之14、推荐中心.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B14%E3%80%81%E6%8E%A8%E8%8D%90%E4%B8%AD%E5%BF%83/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之14、推荐中心/</id>
    <published>2019-12-05T13:21:34.000Z</published>
    <updated>2020-01-10T02:35:51.843Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的文章中，我们实现了召回和排序，接下来将进入推荐逻辑处理阶段，通常称为推荐中心，推荐中心负责接收应用系统的推荐请求，读取召回和排序的结果并进行调整，最后返回给应用系统。推荐中心的调用流程如下所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-84442ee3d9e5a3ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="推荐接口设计"><a href="#推荐接口设计" class="headerlink" title="推荐接口设计"></a>推荐接口设计</h1><p>通常推荐接口包括 Feed 流推荐和相似文章推荐</p><ul><li>Feed 流推荐：根据用户偏好，获取推荐文章列表（这里的时间戳用于区分是刷新推荐列表还是查看历史推荐列表）<br>参数：用户 ID，频道 ID，推荐文章数量，请求推荐的时间戳<br>结果：曝光参数，每篇文章的行为埋点参数，上一条推荐的时间戳</li><li>相似文章推荐：当用户浏览某文章时，获取该文章的相似文章列表<br>参数：文章 ID，推荐文章数量<br>结果：文章 ID 列表</li></ul><p>行为埋点参数：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"param"</span>: '&#123;<span class="attr">"action"</span>: <span class="string">"exposure"</span>, <span class="attr">"userId"</span>: <span class="number">1</span>, <span class="attr">"articleId"</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],  <span class="attr">"algorithmCombine"</span>: <span class="string">"c1"</span>&#125;',</span><br><span class="line">    <span class="attr">"recommends"</span>: [</span><br><span class="line">        &#123;<span class="attr">"article_id"</span>: <span class="number">1</span>, <span class="attr">"param"</span>: &#123;<span class="attr">"click"</span>: <span class="string">"&#123;"</span>action<span class="string">": "</span>click<span class="string">", "</span>userId<span class="string">": "</span><span class="number">1</span><span class="string">", "</span>articleId<span class="string">": 1, "</span>algorithmCombine<span class="string">": 'c1'&#125;"</span>, <span class="attr">"collect"</span>: <span class="string">"..."</span>, <span class="attr">"share"</span>: <span class="string">"..."</span>,<span class="attr">"read"</span>:<span class="string">"..."</span>&#125;&#125;,</span><br><span class="line">        &#123;<span class="attr">"article_id"</span>: <span class="number">2</span>, <span class="attr">"param"</span>: &#123;<span class="attr">"click"</span>: <span class="string">"..."</span>, <span class="attr">"collect"</span>: <span class="string">"..."</span>, <span class="attr">"share"</span>: <span class="string">"..."</span>, <span class="attr">"read"</span>:<span class="string">"..."</span>&#125;&#125;,</span><br><span class="line">        &#123;<span class="attr">"article_id"</span>: <span class="number">3</span>, <span class="attr">"param"</span>: &#123;<span class="attr">"click"</span>: <span class="string">"..."</span>, <span class="attr">"collect"</span>: <span class="string">"..."</span>, <span class="attr">"share"</span>: <span class="string">"..."</span>, <span class="attr">"read"</span>:<span class="string">"..."</span>&#125;&#125;,</span><br><span class="line">        &#123;<span class="attr">"article_id"</span>: <span class="number">4</span>, <span class="attr">"param"</span>: &#123;<span class="attr">"click"</span>: <span class="string">"..."</span>, <span class="attr">"collect"</span>: <span class="string">"..."</span>, <span class="attr">"share"</span>: <span class="string">"..."</span>, <span class="attr">"read"</span>:<span class="string">"..."</span>&#125;&#125;</span><br><span class="line">    ]</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="number">1546391572</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里接口采用 gRPC 框架，在 user_reco.proto 文件中定义 Protobuf 序列化协议，其中定义了 Feed 流推荐接口：<code>rpc user_recommend(User) returns (Track) {}</code> 和相似文章接口：<code>rpc article_recommend(Article) returns(Similar) {}</code><br><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">syntax = <span class="string">"proto3"</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> user_id = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">int32</span> channel_id = <span class="number">2</span>;</span><br><span class="line">    <span class="built_in">int32</span> article_num = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">int64</span> time_stamp = <span class="number">4</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// int32 ---&gt; int64 article_id</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Article</span> </span>&#123;</span><br><span class="line">    <span class="built_in">int64</span> article_id = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">int32</span> article_num = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">param2</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> click = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">string</span> collect = <span class="number">2</span>;</span><br><span class="line">    <span class="built_in">string</span> share = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">string</span> read = <span class="number">4</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">param1</span> </span>&#123;</span><br><span class="line">    <span class="built_in">int64</span> article_id = <span class="number">1</span>;</span><br><span class="line">    param2 params = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Track</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> exposure = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">repeated</span> param1 recommends = <span class="number">2</span>;</span><br><span class="line">    <span class="built_in">int64</span> time_stamp = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Similar</span> </span>&#123;</span><br><span class="line">    <span class="keyword">repeated</span> <span class="built_in">int64</span> article_id = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">UserRecommend</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">rpc</span> user_recommend(User) <span class="keyword">returns</span> (Track) &#123;&#125;</span></span><br><span class="line"><span class="function">    <span class="keyword">rpc</span> article_recommend(Article) <span class="keyword">returns</span>(Similar) &#123;&#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure></p><p>接着，通过如下命令生成服务端文件 user_reco_pb2.py 和客户端文件 user_reco_pb2_grpc.py<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m grpc_tools.protoc -I. <span class="attribute">--python_out</span>=. <span class="attribute">--grpc_python_out</span>=. user_reco.proto</span><br></pre></td></tr></table></figure></p><p>定义参数解析类，用于解析推荐请求的参数，包括用户 ID、频道 ID、文章数量、请求时间戳以及算法名称<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class Temp(object):</span><br><span class="line">    <span class="attr">user_id</span> = -<span class="number">10</span></span><br><span class="line">    <span class="attr">channel_id</span> = -<span class="number">10</span></span><br><span class="line">    <span class="attr">article_num</span> = -<span class="number">10</span></span><br><span class="line">    <span class="attr">time_stamp</span> = -<span class="number">10</span></span><br><span class="line">    <span class="attr">algo</span> = <span class="string">""</span></span><br></pre></td></tr></table></figure></p><p>定义封装埋点参数方法，其中参数 <code>res</code> 为推荐结果，参数 <code>temp</code> 为用户请求参数，将推荐结果封装为在 user_reco.proto 文件中定义的 Track 结构，其中携带了文章对埋点参数，包括了事件名称、算法名称以及时间等等，方便后面解析用户对文章对行为信息<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_track</span><span class="params">(res, temp)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    封装埋点参数</span></span><br><span class="line"><span class="string">    :param res: 推荐文章id列表</span></span><br><span class="line"><span class="string">    :param temp: rpc参数</span></span><br><span class="line"><span class="string">    :return: 埋点参数</span></span><br><span class="line"><span class="string">        文章列表参数</span></span><br><span class="line"><span class="string">        单文章参数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 添加埋点参数</span></span><br><span class="line">    track = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 准备曝光参数</span></span><br><span class="line">    <span class="comment"># 全部字符串形式提供，在hive端不会解析问题</span></span><br><span class="line">    _exposure = &#123;<span class="string">"action"</span>: <span class="string">"exposure"</span>, <span class="string">"userId"</span>: temp.user_id, <span class="string">"articleId"</span>: json.dumps(res),</span><br><span class="line">                 <span class="string">"algorithmCombine"</span>: temp.algo&#125;</span><br><span class="line"></span><br><span class="line">    track[<span class="string">'param'</span>] = json.dumps(_exposure)</span><br><span class="line">    track[<span class="string">'recommends'</span>] = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 准备其它点击参数</span></span><br><span class="line">    <span class="keyword">for</span> _id <span class="keyword">in</span> res:</span><br><span class="line">        <span class="comment"># 构造字典</span></span><br><span class="line">        _dic = &#123;&#125;</span><br><span class="line">        _dic[<span class="string">'article_id'</span>] = _id</span><br><span class="line">        _dic[<span class="string">'param'</span>] = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备click参数</span></span><br><span class="line">        _p = &#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: temp.user_id, <span class="string">"articleId"</span>: str(_id),</span><br><span class="line">              <span class="string">"algorithmCombine"</span>: temp.algo&#125;</span><br><span class="line"></span><br><span class="line">        _dic[<span class="string">'param'</span>][<span class="string">'click'</span>] = json.dumps(_p)</span><br><span class="line">        <span class="comment"># 准备collect参数</span></span><br><span class="line">        _p[<span class="string">"action"</span>] = <span class="string">'collect'</span></span><br><span class="line">        _dic[<span class="string">'param'</span>][<span class="string">'collect'</span>] = json.dumps(_p)</span><br><span class="line">        <span class="comment"># 准备share参数</span></span><br><span class="line">        _p[<span class="string">"action"</span>] = <span class="string">'share'</span></span><br><span class="line">        _dic[<span class="string">'param'</span>][<span class="string">'share'</span>] = json.dumps(_p)</span><br><span class="line">        <span class="comment"># 准备detentionTime参数</span></span><br><span class="line">        _p[<span class="string">"action"</span>] = <span class="string">'read'</span></span><br><span class="line">        _dic[<span class="string">'param'</span>][<span class="string">'read'</span>] = json.dumps(_p)</span><br><span class="line"></span><br><span class="line">        track[<span class="string">'recommends'</span>].append(_dic)</span><br><span class="line"></span><br><span class="line">    track[<span class="string">'timestamp'</span>] = temp.time_stamp</span><br><span class="line">    <span class="keyword">return</span> track</span><br></pre></td></tr></table></figure></p><h1 id="AB-Test-流量切分"><a href="#AB-Test-流量切分" class="headerlink" title="AB Test 流量切分"></a>AB Test 流量切分</h1><p>由于推荐算法和策略是需要不断改进和完善等，所以 ABTest 也是推荐系统不可或缺的功能。可以根据用户 ID 将流量切分为多个桶（Bucket），每个桶对应一种排序策略，桶内流量将使用相应的策略进行排序，使用 ID 进行流量切分能够保证用户体验的一致性。通常 ABTest 过程如下所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-a422277ff8f14b37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>通过定义 AB Test 参数，可以实现为不同的用户使用不同的推荐算法策略，其中 <code>COMBINE</code> 为融合方式，<code>RECALL</code> 为召回方式，<code>SORT</code> 为排序方式，<code>CHANNEL</code> 为频道数量，<code>BYPASS</code> 为分桶设置，<code>sort_dict</code> 为不同的排序服务对象。可以看到 Algo-1 使用 LR 进行排序，而 Algo-2 使用 Wide&amp;Deep 进行排序<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="comment"># ABTest参数信息</span></span><br><span class="line">param = namedtuple(<span class="string">'RecommendAlgorithm'</span>, [<span class="string">'COMBINE'</span>,</span><br><span class="line">                                          <span class="string">'RECALL'</span>,</span><br><span class="line">                                          <span class="string">'SORT'</span>,</span><br><span class="line">                                          <span class="string">'CHANNEL'</span>,</span><br><span class="line">                                          <span class="string">'BYPASS'</span>]</span><br><span class="line">                   )</span><br><span class="line"></span><br><span class="line">RAParam = param(</span><br><span class="line">    COMBINE=&#123;</span><br><span class="line">        <span class="string">'Algo-1'</span>: (<span class="number">1</span>, [<span class="number">100</span>, <span class="number">101</span>, <span class="number">102</span>, <span class="number">103</span>, <span class="number">104</span>], [<span class="number">200</span>]),  <span class="comment"># 首页推荐，所有召回结果读取+LR排序</span></span><br><span class="line">        <span class="string">'Algo-2'</span>: (<span class="number">2</span>, [<span class="number">100</span>, <span class="number">101</span>, <span class="number">102</span>, <span class="number">103</span>, <span class="number">104</span>], [<span class="number">201</span>])  <span class="comment"># 首页推荐，所有召回结果读取 排序</span></span><br><span class="line">    &#125;,</span><br><span class="line">    RECALL=&#123;</span><br><span class="line">        <span class="number">100</span>: (<span class="string">'cb_recall'</span>, <span class="string">'als'</span>),  <span class="comment"># 离线模型ALS召回，recall:user:1115629498121 column=als:18</span></span><br><span class="line">        <span class="number">101</span>: (<span class="string">'cb_recall'</span>, <span class="string">'content'</span>),  <span class="comment"># 离线word2vec的画像内容召回 'recall:user:5', 'content:1'</span></span><br><span class="line">        <span class="number">102</span>: (<span class="string">'cb_recall'</span>, <span class="string">'online'</span>),  <span class="comment"># 在线word2vec的画像召回 'recall:user:1', 'online:1'</span></span><br><span class="line">        <span class="number">103</span>: <span class="string">'new_article'</span>,  <span class="comment"># 新文章召回 redis当中    ch:18:new</span></span><br><span class="line">        <span class="number">104</span>: <span class="string">'popular_article'</span>,  <span class="comment"># 基于用户协同召回结果 ch:18:hot</span></span><br><span class="line">        <span class="number">105</span>: (<span class="string">'article_similar'</span>, <span class="string">'similar'</span>)  <span class="comment"># 文章相似推荐结果 '1' 'similar:2'</span></span><br><span class="line">    &#125;,</span><br><span class="line">    SORT=&#123;</span><br><span class="line">        <span class="number">200</span>: <span class="string">'LR'</span>,</span><br><span class="line">        <span class="number">201</span>: <span class="string">'WDL‘</span></span><br><span class="line"><span class="string">    &#125;,</span></span><br><span class="line"><span class="string">    CHANNEL=25,</span></span><br><span class="line"><span class="string">    BYPASS=[</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                "Bucket": ['</span><span class="number">0</span><span class="string">', '</span><span class="number">1</span><span class="string">', '</span><span class="number">2</span><span class="string">', '</span><span class="number">3</span><span class="string">', '</span><span class="number">4</span><span class="string">', '</span><span class="number">5</span><span class="string">', '</span><span class="number">6</span><span class="string">', '</span><span class="number">7</span><span class="string">', '</span><span class="number">8</span><span class="string">', '</span><span class="number">9</span><span class="string">', '</span>a<span class="string">', '</span><span class="string">b', '</span>c<span class="string">', '</span>d<span class="string">'],</span></span><br><span class="line"><span class="string">                "Strategy": "Algo-1"</span></span><br><span class="line"><span class="string">            &#125;,</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                "BeginBucket": ['</span>e<span class="string">', '</span><span class="string">f'],</span></span><br><span class="line"><span class="string">                "Strategy": "Algo-2"</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sort_dict = <span class="subst">&#123;</span></span></span><br><span class="line"><span class="string"><span class="subst">    <span class="string">"LR"</span>: lr_sort_service,</span></span></span><br><span class="line"><span class="string"><span class="subst">    <span class="string">"WDL"</span>: wdl_sort_service</span></span></span><br><span class="line"><span class="string"><span class="subst">&#125;</span></span></span><br></pre></td></tr></table></figure></p><p>流量切分，将用户 ID 进行哈希，然后取哈希结果的第一个字符，将包含该字符的策略桶所对应的算法编号赋值到此用户请求参数的 <code>algo</code> 属性中，后面将调用该编号对应的算法策略为此用户计算推荐数据<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line">from setting.default import DefaultConfig, RAParam</span><br><span class="line"></span><br><span class="line"><span class="section"># 进行分桶实现分流，制定不同的实验策略</span></span><br><span class="line">bucket = hashlib.md5(user_id.encode()).hexdigest()[:1]</span><br><span class="line">if bucket in RAParam.BYPASS[<span class="string">0</span>][<span class="symbol">'Bucket'</span>]:</span><br><span class="line"><span class="code">    temp.algo = RAParam.BYPASS[0]['Strategy']</span></span><br><span class="line">else:</span><br><span class="line"><span class="code">    temp.algo = RAParam.BYPASS[1]['Strategy']</span></span><br></pre></td></tr></table></figure></p><h1 id="推荐中心逻辑"><a href="#推荐中心逻辑" class="headerlink" title="推荐中心逻辑"></a>推荐中心逻辑</h1><p>推荐中心逻辑主要包括：</p><ul><li>接收应用系统发送的推荐请求，解析请求参数</li><li>进行 ABTest 分流，为用户分配推荐策略</li><li>根据分配的算法调用召回服务和排序服务，读取推荐结果</li><li>根据业务进行调整，如过滤、补足、合并信息等</li><li>封装埋点参数，返回推荐结果</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12790782-dd571ececce3d264.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>首先，在 Hbase 中创建历史推荐结果表 history_recommend，用于存储用户历史推荐结果<br><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'history_recommend'</span>, &#123;<span class="function"><span class="params">NAME</span>=&gt;</span><span class="string">'channel'</span>, <span class="function"><span class="params">TTL</span>=&gt;</span><span class="number">7776000</span>, <span class="function"><span class="params">VERSIONS</span>=&gt;</span><span class="number">999999</span>&#125;   <span class="number">86400</span></span><br><span class="line"># 每次指定一个时间戳,可以达到不同版本的效果</span><br><span class="line">put <span class="string">'history_recommend'</span>, <span class="string">'reco:his:1'</span>, <span class="string">'channel:18'</span>, [<span class="number">17283</span>, <span class="number">140357</span>, <span class="number">14668</span>, <span class="number">15182</span>, <span class="number">17999</span>, <span class="number">13648</span>, <span class="number">12884</span>,<span class="number">18135</span>]</span><br></pre></td></tr></table></figure></p><p>继续在 Hbase 中创建待推荐结果表 wait_recommend，用于存储经过多路召回并且排序之后的待推荐结果，当 wait_recommend 没有数据时，才再次调用排序服务计算出新的待推荐结果并写入到 wait_recommend，所以不需设置多个版本。注意该表与 cb_recall 的区别，cb_recall 存储的是还未经排序的召回结果。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">create</span> <span class="string">'wait_recommend'</span>, <span class="string">'channel'</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">put</span> <span class="string">'wait_recommend'</span>, <span class="string">'reco:1'</span>, <span class="string">'channel:18'</span>, [<span class="number">17283</span>, <span class="number">140357</span>, <span class="number">14668</span>, <span class="number">15182</span>, <span class="number">17999</span>, <span class="number">13648</span>, <span class="number">12884</span>,<span class="number">18135</span>]</span><br><span class="line"><span class="built_in">put</span> <span class="string">'wait_recommend'</span>, <span class="string">'reco:1'</span>, <span class="string">'channel:0'</span>, [<span class="number">17283</span>, <span class="number">140357</span>, <span class="number">14668</span>, <span class="number">15182</span>, <span class="number">17999</span>, <span class="number">13648</span>, <span class="number">12884</span>, <span class="number">17302</span>, <span class="number">13846</span>]</span><br></pre></td></tr></table></figure></p><p>用户获取 Feed 流推荐数据时，如果用户向下滑动，发出的是刷新推荐列表的请求，需要传入当前时间作为请求时间戳参数，该请求时间戳必然大于 Hbase 历史推荐结果表中的请求时间戳，那么程序将获取新的推荐列表，并返回 Hbase 历史推荐结果表中最近一次推荐的请求时间戳，用于查询历史推荐结果；如果用户向上滑动，发出的是查看历史推荐结果的请求，需要传入前面刷新推荐列表时返回的最近一次推荐的请求时间戳，该请求时间戳必然小于等于 Hbase 历史推荐结果中最近一次推荐的时间戳，那么程序将获取小于等于该请求时间戳的最近一次历史推荐结果，并返回小于该推荐结果最近一次推荐的时间戳，也就是上一次推荐的时间戳，下面是具体实现。</p><p>在获取推荐列表时，首先获取用户的历史数据库中最近一次时间戳 <code>last_stamp</code>，没有则将 <code>last_stamp</code> 置为 0<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    last_stamp = self.hbu.get_table_row(<span class="string">'history_recommend'</span>,</span><br><span class="line">                                        <span class="string">'reco:his:&#123;&#125;'</span>.format(temp.user_id).encode(),</span><br><span class="line">                                        <span class="string">'channel:&#123;&#125;'</span>.format(temp.channel_id).encode(),</span><br><span class="line">                                        include_timestamp=<span class="keyword">True</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    last_stamp = <span class="number">0</span></span><br></pre></td></tr></table></figure></p><ul><li>如果用户请求的时间戳小于历史推荐结果中最近一次请求的时间戳 <code>last_stamp</code>，那么该请求为用户获取历史推荐结果<br>1.如果没有历史推荐结果，则返回时间戳 <code>0</code> 以及空列表 <code>[]</code><br>2.如果历史推荐结果只有一条，则返回这一条历史推荐结果并返回时间戳 <code>0</code>，表示已经没有历史推荐结果（APP 可以显示已经没有历史推荐记录了）<br>3.如果历史推荐结果有多条，则返回历史推荐结果中第一条推荐结果（最近一次），然后返回历史推荐结果中第二条推荐结果的时间戳</li></ul><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> temp.time_stamp &lt; last_stamp:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        row = self.hbu.get_table_cells(<span class="string">'history_recommend'</span>,</span><br><span class="line">                                       <span class="string">'reco:his:&#123;&#125;'</span>.format(temp.user_id).encode(),</span><br><span class="line">                                       <span class="string">'channel:&#123;&#125;'</span>.format(temp.channel_id).encode(),</span><br><span class="line">                                       timestamp=temp.time_stamp + <span class="number">1</span>,</span><br><span class="line">                                       include_timestamp=True)</span><br><span class="line">    except Exception <span class="keyword">as</span> <span class="keyword">e</span>:</span><br><span class="line">        row = []</span><br><span class="line">        <span class="keyword">res</span> = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> not ro<span class="variable">w:</span></span><br><span class="line">        temp.time_stamp = <span class="number">0</span></span><br><span class="line">        <span class="keyword">res</span> = []</span><br><span class="line">    elif <span class="built_in">len</span>(row) == <span class="number">1</span> <span class="built_in">and</span> row[<span class="number">0</span>][<span class="number">1</span>] == temp.time_stamp:</span><br><span class="line">        <span class="keyword">res</span> = <span class="built_in">eval</span>(row[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        temp.time_stamp = <span class="number">0</span></span><br><span class="line">    elif <span class="built_in">len</span>(row) &gt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">res</span> = <span class="built_in">eval</span>(row[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        temp.time_stamp = <span class="keyword">int</span>(row[<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">res</span> = <span class="keyword">list</span>(<span class="keyword">map</span>(<span class="keyword">int</span>, <span class="keyword">res</span>))</span><br><span class="line">    # 封装推荐结果</span><br><span class="line">    track = add_track(<span class="keyword">res</span>, temp)</span><br><span class="line">    # 曝光参数设置为空</span><br><span class="line">    track[<span class="string">'param'</span>] = <span class="string">''</span></span><br></pre></td></tr></table></figure><p>（注意：这里将用户请求的时间戳 +1，因为 Hbase 只能获取小于该时间戳的历史推荐结果）</p><ul><li>如果用户请求的时间戳大于 Hbase 历史推荐结果中最近一次请求的时间戳 <code>last_stamp</code>，那么该请求为用户刷新推荐列表，需要读取推荐结果并返回。如果结果为空，需要调用 <code>user_reco_list()</code> 方法，再次计算推荐结果，再返回。<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> temp.time_stamp &gt; last_stamp:</span><br><span class="line">    <span class="comment"># 获取缓存</span></span><br><span class="line">    <span class="attr">res</span> = redis_cache.get_reco_from_cache(temp, self.hbu)</span><br><span class="line">    <span class="comment"># 如果结果为空，需要再次计算推荐结果 进行召回+排序，同时写入到hbase待推荐结果列表</span></span><br><span class="line">    <span class="keyword">if</span> not res:</span><br><span class="line">        <span class="attr">res</span> = self.user_reco_list(temp)</span><br><span class="line"></span><br><span class="line">    temp.<span class="attr">time_stamp</span> = int(last_stamp)</span><br><span class="line">    <span class="attr">track</span> = add_track(res, temp)</span><br></pre></td></tr></table></figure></li></ul><p>定义 <code>user_reco_list()</code> 方法，首先要读取多路召回结果，根据为用户分配的算法策略，读取相应路径的召回结果，并进行重后合并<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">reco_set = []</span><br><span class="line"><span class="comment"># (1, [100, 101, 102, 103, 104], [])</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">number</span> <span class="keyword">in</span> RAParam.COMBINE[temp.algo][<span class="number">1</span>]:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">number</span> == <span class="number">103</span>:</span><br><span class="line">        <span class="title">_res</span> = self.recall_service.read_redis_new_article(temp.channel_id)</span><br><span class="line">        reco_set = list(<span class="built_in">set</span>(reco_set).<span class="built_in">union</span>(<span class="built_in">set</span>(<span class="title">_res</span>)))</span><br><span class="line">    elif <span class="built_in">number</span> == <span class="number">104</span>:</span><br><span class="line">        <span class="title">_res</span> = self.recall_service.read_redis_hot_article(temp.channel_id)</span><br><span class="line">        reco_set = list(<span class="built_in">set</span>(reco_set).<span class="built_in">union</span>(<span class="built_in">set</span>(<span class="title">_res</span>)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 100, 101, 102召回结果读取</span></span><br><span class="line">        <span class="title">_res</span> = self.recall_service.read_hbase_recall(RAParam.RECALL[<span class="built_in">number</span>][<span class="number">0</span>],</span><br><span class="line">                                                     <span class="string">'recall:user:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                                                     <span class="string">'&#123;&#125;:&#123;&#125;'</span>.<span class="built_in">format</span>(RAParam.RECALL[<span class="built_in">number</span>][<span class="number">1</span>],</span><br><span class="line">                                                                    temp.channel_id).encode())</span><br><span class="line">        reco_set = list(<span class="built_in">set</span>(reco_set).<span class="built_in">union</span>(<span class="built_in">set</span>(<span class="title">_res</span>)))</span><br></pre></td></tr></table></figure></p><p>接着，过滤当前该请求频道的历史推荐结果，如果不是 0 频道还需过滤 0 频道的历史推荐结果<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">history_list = []</span><br><span class="line">data = self.hbu.get_table_cells(<span class="string">'history_recommend'</span>,</span><br><span class="line">                                <span class="string">'reco:his:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                                <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="built_in">in</span> data:</span><br><span class="line">    history_list = <span class="built_in">list</span>(<span class="built_in">set</span>(history_list).union(<span class="built_in">set</span>(eval(_))))</span><br><span class="line"></span><br><span class="line">data = self.hbu.get_table_cells(<span class="string">'history_recommend'</span>,</span><br><span class="line">                                <span class="string">'reco:his:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                                <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(<span class="number">0</span>).encode())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="built_in">in</span> data:</span><br><span class="line">    history_list = <span class="built_in">list</span>(<span class="built_in">set</span>(history_list).union(<span class="built_in">set</span>(eval(_))))</span><br><span class="line"></span><br><span class="line">reco_set = <span class="built_in">list</span>(<span class="built_in">set</span>(reco_set).difference(<span class="built_in">set</span>(history_list)))</span><br></pre></td></tr></table></figure></p><p>最后，根据分配的算法策略，调用排序服务，将分数最高的 N 个推荐结果返回，并写入历史推荐结果表，如果还有剩余的排序结果，将其余写入待推荐结果表<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用指定模型对召回结果进行排序</span></span><br><span class="line"><span class="comment"># temp.user_id， reco_set</span></span><br><span class="line"><span class="title">_sort</span>_num = RAParam.COMBINE[temp.algo][<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 'LR'</span></span><br><span class="line">reco_set = sort_dict[RAParam.SORT[<span class="title">_sort</span>_num]](reco_set, temp, self.hbu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> reco_set:</span><br><span class="line">    <span class="literal">return</span> reco_set</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果reco_set小于用户需要推荐的文章</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(reco_set) &lt;= temp.article_num:</span><br><span class="line">        res = reco_set</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 大于要推荐的文章结果</span></span><br><span class="line">        res = reco_set[:temp.article_num]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将剩下的文章列表写入待推荐的结果</span></span><br><span class="line">        self.hbu.get_table_put(<span class="string">'wait_recommend'</span>,</span><br><span class="line">                               <span class="string">'reco:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                               <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode(),</span><br><span class="line">                               str(reco_set[temp.article_num:]).encode(),</span><br><span class="line">                               timestamp=temp.time_stamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接写入历史记录当中，表示这次又成功推荐一次</span></span><br><span class="line">    self.hbu.get_table_put(<span class="string">'history_recommend'</span>,</span><br><span class="line">                           <span class="string">'reco:his:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                           <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode(),</span><br><span class="line">                           str(res).encode(),</span><br><span class="line">                           timestamp=temp.time_stamp)</span><br><span class="line"></span><br><span class="line">    <span class="literal">return</span> res</span><br></pre></td></tr></table></figure></p><p>到这里，推荐中心的基本逻辑已经结束。下面是读取多路召回结果的实现细节：通过指定列族，读取基于模型、离线内容以及在线的召回结果，并删除 cb_recall 的召回结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_hbase_recall_data</span><span class="params">(self, table_name, key_format, column_format)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    读取cb_recall当中的推荐数据</span></span><br><span class="line"><span class="string">    读取的时候可以选择列族进行读取als, online, content</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    recall_list = []</span><br><span class="line">    data = self.hbu.get_table_cells(table_name, key_format, column_format)</span><br><span class="line">    <span class="comment"># data是多个版本的推荐结果[[],[],[],]</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> data:</span><br><span class="line">        recall_list = list(set(recall_list).union(set(eval(_))))</span><br><span class="line">    self.hbu.get_table_delete(table_name, key_format, column_format)</span><br><span class="line">    <span class="keyword">return</span> recall_list</span><br></pre></td></tr></table></figure></p><p>读取 redis 中的新文章<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_redis_new_article</span><span class="params">(self, channel_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    读取新文章召回结果</span></span><br><span class="line"><span class="string">    :param channel_id: 提供频道</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    _key = <span class="string">"ch:&#123;&#125;:new"</span>.format(channel_id)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = self.client.zrevrange(_key, <span class="number">0</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        res = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> list(map(int, res))</span><br></pre></td></tr></table></figure></p><p>读取 redis 中的热门文章，并选取热度最高的前 K 个文章<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def read_redis_hot_article(self, channel_id):</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    读取热门文章召回结果</span><br><span class="line">    :param channel_id: 提供频道</span><br><span class="line">    :<span class="keyword">return</span>:</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    _key = <span class="string">"ch:&#123;&#125;:hot"</span>.format(channel_id)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">res</span> = self.client.zrevrange(_key, <span class="number">0</span>, -<span class="number">1</span>)</span><br><span class="line">    except Exception <span class="keyword">as</span> <span class="keyword">e</span>:</span><br><span class="line"></span><br><span class="line">    # 由于每个频道的热门文章有很多，因为 保留文章点击次数</span><br><span class="line">    <span class="keyword">res</span> = <span class="keyword">list</span>(<span class="keyword">map</span>(<span class="keyword">int</span>, <span class="keyword">res</span>))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="keyword">res</span>) &gt; self.hot_num:</span><br><span class="line">        <span class="keyword">res</span> = <span class="keyword">res</span>[:self.hot_num]</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">res</span></span><br></pre></td></tr></table></figure></p><p>读取相似文章<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_hbase_article_similar</span><span class="params">(self, table_name, key_format, article_num)</span>:</span></span><br><span class="line">    <span class="string">"""获取文章相似结果</span></span><br><span class="line"><span class="string">    :param article_id: 文章id</span></span><br><span class="line"><span class="string">    :param article_num: 文章数量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        _dic = self.hbu.get_table_row(table_name, key_format)</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        _srt = sorted(_dic.items(), key=<span class="keyword">lambda</span> obj: obj[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> len(_srt) &gt; article_num:</span><br><span class="line">            _srt = _srt[:article_num]</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> _srt:</span><br><span class="line">            res.append(int(_[<span class="number">0</span>].decode().split(<span class="string">':'</span>)[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        res = []</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure></p><h1 id="使用缓存策略"><a href="#使用缓存策略" class="headerlink" title="使用缓存策略"></a>使用缓存策略</h1><ul><li>如果 redis 缓存中存在数据，就直接从 redis 缓存中获取推荐结果</li><li>如果 redis 缓存为空而 Hbase 的待推荐结果表 wait_recommend 不为空，则从 wait_recommend 中获取推荐结果，并将一定数量的待推荐结果放入 redis 缓存中</li><li>若 redis 和 wait_recommend 都为空，则需读取召回结果并进行排序，将排序结果写入 Hbase 的待推荐结果表 wait_recommend 中及 redis 中</li></ul><p>（每次读取的推荐结果都要将其写入 Hbase 的历史推荐结果表 history_recommend 中）</p><p>读取 redis 缓存<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取redis对应的键</span></span><br><span class="line">key = 'reco:&#123;&#125;:&#123;&#125;:art'.format(temp.user_id, temp.channel_id)</span><br><span class="line"><span class="comment"># 读取，删除，返回结果</span></span><br><span class="line">pl = cache_client.pipeline()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取redis数据</span></span><br><span class="line">res = cache_client.zrevrange(key, 0, temp.article_num - 1)</span><br><span class="line">if res:</span><br><span class="line">    <span class="comment"># 手动删除读取出来的缓存结果</span></span><br><span class="line">    pl.zrem(key, *res)</span><br></pre></td></tr></table></figure></p><p>如果 redis 缓存为空<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 删除键</span></span><br><span class="line">    cache_client.<span class="built_in">delete</span>(key)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 从wait_recommend中读取</span></span><br><span class="line">        wait_cache = eval(hbu.get_table_row(<span class="string">'wait_recommend'</span>,</span><br><span class="line">                                            <span class="string">'reco:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                                            <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode()))</span><br><span class="line">    except Exception <span class="keyword">as</span> e:</span><br><span class="line">        wait_cache = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果为空则直接返回空</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> wait_cache:</span><br><span class="line">        <span class="literal">return</span> wait_cache</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果wait_recommend中有数据</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(wait_cache) &gt; <span class="number">100</span>:</span><br><span class="line">        cache_redis = wait_cache[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前100个数据放入redis</span></span><br><span class="line">        pl.zadd(key, dict(zip(cache_redis, range(<span class="built_in">len</span>(cache_redis)))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 100个后面的数据，在放回wait_recommend</span></span><br><span class="line">        hbu.get_table_put(<span class="string">'wait_recommend'</span>,</span><br><span class="line">                          <span class="string">'reco:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                          <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode(),</span><br><span class="line">                          str(wait_cache[<span class="number">100</span>:]).encode())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 清空wait_recommend数据</span></span><br><span class="line">        hbu.get_table_put(<span class="string">'wait_recommend'</span>,</span><br><span class="line">                          <span class="string">'reco:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                          <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode(),</span><br><span class="line">                          str([]).encode())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 所有不足100个数据，放入redis</span></span><br><span class="line">        pl.zadd(key, dict(zip(wait_cache, range(<span class="built_in">len</span>(wait_cache)))))</span><br><span class="line"></span><br><span class="line">    res = cache_client.zrange(key, <span class="number">0</span>, temp.article_num - <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>最后，在 Supervisor 中配置 gRPC 实时推荐程序<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[program:online]</span></span><br><span class="line"><span class="attr">environment</span>=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python ,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python</span><br><span class="line"><span class="attr">command</span>=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/reco_sys/abtest/routing.py</span><br><span class="line"><span class="attr">directory</span>=/root/toutiao_project/reco_sys/abtest</span><br><span class="line"><span class="attr">user</span>=root</span><br><span class="line"><span class="attr">autorestart</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">redirect_stderr</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">stdout_logfile</span>=/root/logs/recommendsuper.log</span><br><span class="line"><span class="attr">loglevel</span>=info</span><br><span class="line"><span class="attr">stopsignal</span>=KILL</span><br><span class="line"><span class="attr">stopasgroup</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">killasgroup</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h1 id="想说的话"><a href="#想说的话" class="headerlink" title="想说的话"></a>想说的话</h1><p>文章推荐系统系列到此就完结啦～ 撒花 🎉🎉🎉<br>若有疏漏的地方，欢迎各位多多指正，感谢关注，love &amp; peace. 🙏🙏🙏</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在前面的文章中，我们实现了召回和排序，接下来将进入推荐逻辑处理阶段，通常称为推荐中心，推荐中心负责接收应用系统的推荐请求，读取召回和排序的结果并进行调整，最后返回给应用系统。推荐中心的调用流程如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-ima
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之13、基于Wide&amp;Deep模型的在线排序.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B13%E3%80%81%E5%9F%BA%E4%BA%8EWide&amp;Deep%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9C%A8%E7%BA%BF%E6%8E%92%E5%BA%8F/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之13、基于Wide&amp;Deep模型的在线排序/</id>
    <published>2019-12-05T13:21:33.000Z</published>
    <updated>2020-01-10T02:34:54.872Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/12790782-66263ecac4ac1688.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上图是 Wide&amp;Deep 模型的网络结构，深度学习可以通过嵌入（Embedding）表达出更精准的用户兴趣及物品特征，不仅能减少人工特征工程的工作量，还能提高模型的泛化能力，使得用户行为预估更加准确。Wide&amp;Deep 模型适合高维稀疏特征的推荐场景，兼有稀疏特征的可解释性和深模型的泛化能力。通常将类别特征做 Embedding 学习，再将 Embedding 稠密特征输入深模型中。Wide 部分的输入特征包括：类别特征和离散化的数值特征，Deep部分的输入特征包括：数值特征和 Embedding 后的类别特征。其中，Wide 部分使用 FTRL + L1；Deep 部分使用 AdaGrad，并且两侧是一起联合进行训练的。</p><h1 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h1><p>TensorFlow 实现了很多深度模型，其中就包括 Wide&amp;Deep，API 接口为 <code>tf.estimator.DNNLinearCombinedClassifier</code>，我们可以直接使用。在上篇文章中已经实现了将训练数据写入 TFRecord 文件，在这里可以直接读取<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@staticmethod</span><br><span class="line">def read_ctr_records():</span><br><span class="line">    <span class="attr">dataset</span> = tf.data.TFRecordDataset([<span class="string">"./train_ctr_201905.tfrecords"</span>])</span><br><span class="line">    <span class="attr">dataset</span> = dataset.<span class="built_in">map</span>(parse_tfrecords)</span><br><span class="line">    <span class="attr">dataset</span> = dataset.shuffle(<span class="attr">buffer_size=10000)</span></span><br><span class="line">    <span class="attr">dataset</span> = dataset.repeat(<span class="number">10000</span>)</span><br><span class="line">    return dataset.make_one_shot_iterator().get_next()</span><br></pre></td></tr></table></figure></p><p>解析每个样本，将 TFRecord 中序列化的 feature 列，解析成 channel_id (1), article_vector (100), user_weights (10), article_weights (10)<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def parse_tfrecords(example):</span><br><span class="line">    features = &#123;</span><br><span class="line">        <span class="string">"label"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64),</span><br><span class="line">        <span class="string">"feature"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.<span class="built_in">string</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    parsed_features = <span class="keyword">tf</span>.parse_single_example(example, features)</span><br><span class="line"></span><br><span class="line">    feature = <span class="keyword">tf</span>.decode_raw(parsed_features[<span class="string">'feature'</span>], <span class="keyword">tf</span>.float64)</span><br><span class="line">    feature = <span class="keyword">tf</span>.reshape(<span class="keyword">tf</span>.cast(feature, <span class="keyword">tf</span>.float32), [<span class="number">1</span>, <span class="number">121</span>])</span><br><span class="line">    # 特征顺序 <span class="number">1</span> channel_id,  <span class="number">100</span> article_vector, <span class="number">10</span> user_weights, <span class="number">10</span> article_weights</span><br><span class="line">    # <span class="number">1</span> channel_id类别型特征， <span class="number">100</span>维文章向量求平均值当连续特征，<span class="number">10</span>维用户权重求平均值当连续特征</span><br><span class="line">    channel_id = <span class="keyword">tf</span>.cast(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]), <span class="keyword">tf</span>.int32)</span><br><span class="line">    vector = <span class="keyword">tf</span>.reduce_sum(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">100</span>]), axis=<span class="number">1</span>, keep_dims=True)</span><br><span class="line">    user_weights = <span class="keyword">tf</span>.reduce_sum(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">101</span>], [<span class="number">1</span>, <span class="number">10</span>]), axis=<span class="number">1</span>, keep_dims=True)</span><br><span class="line">    article_weights = <span class="keyword">tf</span>.reduce_sum(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">111</span>], [<span class="number">1</span>, <span class="number">10</span>]), axis=<span class="number">1</span>, keep_dims=True)</span><br><span class="line"></span><br><span class="line">    label = <span class="keyword">tf</span>.reshape(<span class="keyword">tf</span>.cast(parsed_features[<span class="string">'label'</span>], <span class="keyword">tf</span>.float32), [<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    # 构造字典 名称-tensor</span><br><span class="line">    FEATURE_COLUMNS = [<span class="string">'channel_id'</span>, <span class="string">'vector'</span>, <span class="string">'user_weigths'</span>, <span class="string">'article_weights'</span>]</span><br><span class="line">    tensor_list = [channel_id, vector, user_weights, article_weights]</span><br><span class="line"></span><br><span class="line">    feature_dict = dict(zip(FEATURE_COLUMNS, tensor_list))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feature_dict, label</span><br></pre></td></tr></table></figure></p><p>指定输入特征的数据类型，并定义 Wide&amp;Deep 模型 model<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 离散类型</span></span><br><span class="line">channel_id = tf.feature_column.categorical_column_with_identity('channel_id', num_buckets=25)</span><br><span class="line"><span class="comment"># 连续类型</span></span><br><span class="line">vector = tf.feature_column.numeric_column('vector')</span><br><span class="line">user_weigths = tf.feature_column.numeric_column('user_weigths')</span><br><span class="line">article_weights = tf.feature_column.numeric_column('article_weights')</span><br><span class="line"></span><br><span class="line">wide_columns = [channel_id]</span><br><span class="line"></span><br><span class="line"><span class="comment"># embedding_column用来表示类别型的变量</span></span><br><span class="line">deep_columns = [tf.feature_column.embedding_column(channel_id, dimension=25),</span><br><span class="line">                vector, user_weigths, article_weights]</span><br><span class="line"></span><br><span class="line">estimator = tf.estimator.DNNLinearCombinedClassifier(model_dir=<span class="string">"./ckpt/wide_and_deep"</span>,</span><br><span class="line">                                                     linear_feature_columns=wide_columns,</span><br><span class="line">                                                     dnn_feature_columns=deep_columns,</span><br><span class="line">                                                     dnn_hidden_units=[1024, 512, 256])</span><br></pre></td></tr></table></figure></p><p>通过调用 read_ctr_records() 方法，来读取 TFRecod 文件中的训练数据，并设置训练步长，对定义好的 FTRL 模型进行训练及预估<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">model</span>.train(read_ctr_records, steps=<span class="number">1000</span>)</span><br><span class="line">result = <span class="keyword">model</span>.evaluate(read_ctr_records)</span><br></pre></td></tr></table></figure></p><p>可以用上一次模型的参数作为当前模型的初始化参数，训练完成后，通常会进行离线指标分析，若符合预期即可导出模型<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">columns</span> = wide_columns + deep_columns</span><br><span class="line">feature_spec = tf<span class="selector-class">.feature_column</span><span class="selector-class">.make_parse_example_spec</span>(<span class="attribute">columns</span>)</span><br><span class="line">serving_input_receiver_fn = tf<span class="selector-class">.estimator</span><span class="selector-class">.export</span><span class="selector-class">.build_parsing_serving_input_receiver_fn</span>(feature_spec)</span><br><span class="line">model.export_savedmodel(<span class="string">"./serving_model/wdl/"</span>, serving_input_receiver_fn)</span><br></pre></td></tr></table></figure></p><h1 id="TFServing-部署"><a href="#TFServing-部署" class="headerlink" title="TFServing 部署"></a>TFServing 部署</h1><p>安装<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> pull tensorflow/serving</span><br></pre></td></tr></table></figure></p><p>启动<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p <span class="number">8501</span>:<span class="number">8501</span> -p <span class="number">8500</span>:<span class="number">8500</span> --mount type=bind,<span class="keyword">source</span>=<span class="regexp">/root/</span>toutiao_project<span class="regexp">/reco_sys/</span>server<span class="regexp">/models/</span>serving_model<span class="regexp">/wdl,target=/m</span>odels<span class="regexp">/wdl -e MODEL_NAME=wdl -t tensorflow/</span>serving</span><br></pre></td></tr></table></figure></p><ul><li>-p 8501:8501 为端口映射（-p 主机端口 : docker 容器程序）</li><li>TFServing 使用 8501 端口对外提供 HTTP 服务，使用8500对外提供 gRPC 服务，这里同时开放了两个端口的使用</li><li>—mount type=bind,source=/home/ubuntu/detectedmodel/wdl,target=/models/wdl 为文件映射，将主机（source）的模型文件映射到 docker 容器程序（target）的位置，以便 TFServing 使用模型，target 参数为 /models/模型名称</li><li>-e MODEL_NAME= wdl 设置了一个环境变量，名为 MODEL_NAME，此变量被 TFServing 读取，用来按名字寻找模型，与上面 target 参数中的模型名称对应</li><li>-t 为 TFServing 创建一个伪终端，供程序运行</li><li>tensorflow/serving 为镜像名称</li></ul><h1 id="在线排序"><a href="#在线排序" class="headerlink" title="在线排序"></a>在线排序</h1><p>通常在线排序是根据用户实时的推荐请求，对召回结果进行 CTR 预估，进而计算出排序结果并返回。我们需要根据召回结果构造测试集，其中每个测试样本包括用户特征和文章特征。首先，根据用户 ID 和频道 ID 读取用户特征（用户在每个频道的特征不同，所以是分频道存储的）<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    user_feature = eval(hbu.get_table_row(<span class="string">'ctr_feature_user'</span>,</span><br><span class="line">                              <span class="string">'&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                              <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode()))</span><br><span class="line">except Exception <span class="keyword">as</span> e:</span><br><span class="line">    user_feature = []</span><br></pre></td></tr></table></figure></p><p>再根据用户 ID 读取召回结果<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recall_set = read_hbase_recall('cb_recall', </span><br><span class="line">                'recall<span class="symbol">:user</span>:&#123;&#125;'.format(<span class="name">temp</span>.user_id).encode(), </span><br><span class="line">                'als:&#123;&#125;'.format(<span class="name">temp</span>.channel_id).encode())</span><br></pre></td></tr></table></figure></p><p>接着，遍历召回结果，获取文章特征，并将用户特征合并，构建样本<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">examples = []</span><br><span class="line"><span class="keyword">for</span> article_id in recall_se<span class="variable">t:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        article_feature = <span class="built_in">eval</span>(hbu.get_table_row(<span class="string">'ctr_feature_article'</span>,</span><br><span class="line">                                  <span class="string">'&#123;&#125;'</span>.format(article_id).encode(),</span><br><span class="line">                                  <span class="string">'article:&#123;&#125;'</span>.format(article_id).encode()))</span><br><span class="line">    except Exception <span class="keyword">as</span> <span class="keyword">e</span>:</span><br><span class="line">        article_feature = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> not article_feature:</span><br><span class="line">        article_feature = [<span class="number">0.0</span>] * <span class="number">111</span></span><br><span class="line">    </span><br><span class="line">    channel_id = <span class="keyword">int</span>(article_feature[<span class="number">0</span>])</span><br><span class="line">    # 计算后面若干向量的平均值</span><br><span class="line">    vector = np.mean(article_feature[<span class="number">11</span>:])</span><br><span class="line">    # 用户权重特征</span><br><span class="line">    user_feature = np.mean(user_feature)</span><br><span class="line">    # 文章权重特征</span><br><span class="line">    article_feature = np.mean(article_feature[<span class="number">1</span>:<span class="number">11</span>])</span><br><span class="line"></span><br><span class="line">    # 构建example</span><br><span class="line">    example = <span class="keyword">tf</span>.train.Example(features=<span class="keyword">tf</span>.train.Features(feature=&#123;</span><br><span class="line">                <span class="string">"channel_id"</span>: <span class="keyword">tf</span>.train.Feature(int64_list=<span class="keyword">tf</span>.train.Int64List(value=[channel_id])),</span><br><span class="line">                <span class="string">"vector"</span>: <span class="keyword">tf</span>.train.Feature(float_list=<span class="keyword">tf</span>.train.FloatList(value=[vector])),</span><br><span class="line">                <span class="string">'user_weigths'</span>: <span class="keyword">tf</span>.train.Feature(float_list=<span class="keyword">tf</span>.train.FloatList(value=[user_feature])),</span><br><span class="line">                <span class="string">'article_weights'</span>: <span class="keyword">tf</span>.train.Feature(float_list=<span class="keyword">tf</span>.train.FloatList(value=[article_feature])),</span><br><span class="line">            &#125;))</span><br><span class="line"></span><br><span class="line">    examples.<span class="keyword">append</span>(example)</span><br></pre></td></tr></table></figure></p><p>调用 TFServing 的模型服务，获取排序结果<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> grpc.insecure_channel(<span class="string">"127.0.0.1:8500"</span>) as channel:</span><br><span class="line">    <span class="attr">stub</span> = prediction_service_pb2_grpc.PredictionServiceStub(channel)</span><br><span class="line">    <span class="attr">request</span> = classification_pb2.ClassificationRequest()</span><br><span class="line">    <span class="comment"># 构造请求，指定模型名称，指定输入样本</span></span><br><span class="line">    request.model_spec.<span class="attr">name</span> = 'wdl'</span><br><span class="line">    request.input.example_list.examples.extend(examples)</span><br><span class="line">    <span class="comment"># 发送请求，获取排序结果</span></span><br><span class="line">    <span class="attr">response</span> = stub.Classify(request, <span class="number">10.0</span>)</span><br></pre></td></tr></table></figure></p><p>这样，我们就实现了 Wide&amp;Deep 模型的离线训练和 TFServing 模型部署以及在线排序服务的调用。使用这种方式，线上服务需要将特征发送给TF Serving，这不可避免引入了网络 IO，给带宽和预估时延带来压力。可以通过并发请求，召回多个召回结果集合，然后并发请求 TF Serving 模型服务，这样可以有效降低整体预估时延。还可以通过特征 ID 化，将字符串类型的特征名哈希到 64 位整型空间，这样有效减少传输的数据量，降低使用的带宽。</p><h1 id="模型同步"><a href="#模型同步" class="headerlink" title="模型同步"></a>模型同步</h1><p>实际环境中，我们可能还要经常将离线训练好的模型同步到线上服务机器，大致同步过程如下：</p><ul><li>同步前，检查模型 md5 文件，只有该文件更新了，才需要同步</li><li>同步时，随机链接 HTTPFS 机器并限制下载速度</li><li>同步后，校验模型文件 md5 值并备份旧模型</li></ul><p>同步过程中，需要处理发生错误或者超时的情况，可以设定触发报警或重试机制。通常模型的同步时间都在分钟级别。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/12790782-66263ecac4ac1688.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之12、基于FTRL优化方法的模型在线排序.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B12%E3%80%81%E5%9F%BA%E4%BA%8EFTRL%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%9C%A8%E7%BA%BF%E6%8E%92%E5%BA%8F/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之12、基于FTRL优化方法的模型在线排序/</id>
    <published>2019-12-05T13:21:32.000Z</published>
    <updated>2020-01-10T02:33:55.399Z</updated>
    
    <content type="html"><![CDATA[<h1 id="构造-TFRecord-训练集"><a href="#构造-TFRecord-训练集" class="headerlink" title="构造 TFRecord 训练集"></a>构造 TFRecord 训练集</h1><p>和前面的 LR 离线模型一样，FTRL 模型首先也是要完成训练集的构建。在上篇文章中，我们已经知道，可以通过读取用户历史行为数据，及文章特征和用户特征，构建出训练集 <code>train</code>，其中包括 features 和 label 两列数据，features 是文章特征和用户特征的组合。在 TensorFlow 通常使用 TFRecord 文件进行数据的存取。接下来，我们就要将 <code>train</code> 保存到 TFRecord 文件中。首先开启会话，将 <code>train</code> 中的特征和标签分别传入 <code>write_to_tfrecords()</code> 方法，并利用多线程执行<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">    <span class="comment"># 创建线程协调器</span></span><br><span class="line">    <span class="attr">coord</span> = tf.train.Coordinator()</span><br><span class="line">    <span class="comment"># 开启子线程去读取数据</span></span><br><span class="line">    <span class="attr">threads</span> = tf.train.start_queue_runners(<span class="attr">sess=sess,</span> <span class="attr">coord=coord)</span></span><br><span class="line">    <span class="comment"># 存入数据</span></span><br><span class="line">    write_to_tfrecords(train.iloc[:, <span class="number">0</span>], train.iloc[:, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 关闭子线程，回收</span></span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure></p><p>接着，在 <code>write_to_tfrecords()</code> 方法中，遍历训练集数据，将每个样本构造为 <code>tf.train.Example</code>，其中 feature 为 BytesList 类型，label 为 Int64List 类型，并保存到 TFRecords 文件中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_tfrecords</span><span class="params">(feature_batch, click_batch)</span>:</span></span><br><span class="line">    <span class="string">"""将用户与文章的点击日志构造的样本写入TFRecords文件</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 1、构造tfrecords的存储实例</span></span><br><span class="line">    writer = tf.python_io.TFRecordWriter(<span class="string">"./train_ctr_20190605.tfrecords"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2、循环将所有样本一个个封装成example，写入文件</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(click_batch)):</span><br><span class="line">        <span class="comment"># 取出第i个样本的特征值和目标值，格式转换</span></span><br><span class="line">        click = click_batch[i]</span><br><span class="line">        feature = feature_batch[i].tostring()</span><br><span class="line">        <span class="comment"># 构造example</span></span><br><span class="line">        example = tf.train.Example(features=tf.train.Features(feature=&#123;</span><br><span class="line">            <span class="string">"feature"</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[feature])),</span><br><span class="line">            <span class="string">"label"</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[click]))</span><br><span class="line">        &#125;))</span><br><span class="line">        <span class="comment"># 序列化example,写入文件</span></span><br><span class="line">        writer.write(example.SerializeToString())</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure></p><h1 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h1><p>FTRL（Follow The Regularized Leader）模型是一种获得稀疏模型的优化方法，我们利用构建好的 TFRecord 样本数据对 FTRL 模型进行离线训练。首先，定义 <code>read_ctr_records()</code> 方法来读取 TFRecord 文件，并通过调用 <code>parse_tfrecords()</code> 方法遍历解析每个样本，并设置了批大小和迭代次数<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def read_ctr_records():</span><br><span class="line">    <span class="attr">train</span> = tf.data.TFRecordDataset([<span class="string">"./train_ctr_20190605.tfrecords"</span>])</span><br><span class="line">    <span class="attr">train</span> = train.<span class="built_in">map</span>(parse_tfrecords)</span><br><span class="line">    <span class="attr">train</span> = train.batch(<span class="number">64</span>)</span><br><span class="line">    <span class="attr">train</span> = train.repeat(<span class="number">10000</span>)</span><br></pre></td></tr></table></figure></p><p>解析每个样本，将 TFRecord 中序列化的 feature 列，解析成 channel_id (1), article_vector (100), user_weights (10), article_weights (10)<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">FEATURE_COLUMNS = [<span class="string">'channel_id'</span>, <span class="string">'article_vector'</span>, <span class="string">'user_weigths'</span>, <span class="string">'article_weights'</span>]</span><br><span class="line"></span><br><span class="line">def parse_tfrecords(example):</span><br><span class="line">    features = &#123;</span><br><span class="line">        <span class="string">"feature"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.<span class="built_in">string</span>),</span><br><span class="line">        <span class="string">"label"</span>: <span class="keyword">tf</span>.FixedLenFeature([], <span class="keyword">tf</span>.int64)</span><br><span class="line">    &#125;</span><br><span class="line">    parsed_features = <span class="keyword">tf</span>.parse_single_example(example, features)</span><br><span class="line">    feature = <span class="keyword">tf</span>.decode_raw(parsed_features[<span class="string">'feature'</span>], <span class="keyword">tf</span>.float64)</span><br><span class="line">    feature = <span class="keyword">tf</span>.reshape(<span class="keyword">tf</span>.cast(feature, <span class="keyword">tf</span>.float32), [<span class="number">1</span>, <span class="number">121</span>])</span><br><span class="line">    </span><br><span class="line">    channel_id = <span class="keyword">tf</span>.cast(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]), <span class="keyword">tf</span>.int32)</span><br><span class="line">    article_vector = <span class="keyword">tf</span>.reduce_sum(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">100</span>]), axis=<span class="number">1</span>)</span><br><span class="line">    user_weights = <span class="keyword">tf</span>.reduce_sum(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">101</span>], [<span class="number">1</span>, <span class="number">10</span>]), axis=<span class="number">1</span>)</span><br><span class="line">    article_weights = <span class="keyword">tf</span>.reduce_sum(<span class="keyword">tf</span>.slice(feature, [<span class="number">0</span>, <span class="number">111</span>], [<span class="number">1</span>, <span class="number">10</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    label = <span class="keyword">tf</span>.cast(parsed_features[<span class="string">'label'</span>], <span class="keyword">tf</span>.float32)</span><br><span class="line"></span><br><span class="line">    # 构造字典 名称-tensor</span><br><span class="line">    tensor_list = [channel_id, article_vector, user_weights, article_weights]</span><br><span class="line">    feature_dict = dict(zip(FEATURE_COLUMNS, tensor_list))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feature_dict, label</span><br></pre></td></tr></table></figure></p><p>指定输入特征的数据类型，并定义 FTRL 模型 <code>model</code><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义离散类型特征</span></span><br><span class="line">article_id = tf.feature_column.categorical_column_with_identity('channel_id', num_buckets=25)</span><br><span class="line"><span class="comment"># 定义连续类型特征</span></span><br><span class="line">article_vector = tf.feature_column.numeric_column('article_vector')</span><br><span class="line">user_weigths = tf.feature_column.numeric_column('user_weigths')</span><br><span class="line">article_weights = tf.feature_column.numeric_column('article_weights')</span><br><span class="line"></span><br><span class="line">feature_columns = [article_id, article_vector, user_weigths, article_weights]</span><br><span class="line"></span><br><span class="line">model = tf.estimator.LinearClassifier(feature_columns=feature_columns,</span><br><span class="line">                                           optimizer=tf.train.FtrlOptimizer(learning_rate=0.1,</span><br><span class="line">                                                                            l1_regularization_strength=10,</span><br><span class="line">                                                                            l2_regularization_strength=10))</span><br></pre></td></tr></table></figure></p><p>通过调用 <code>read_ctr_records()</code> 方法，来读取 TFRecod 文件中的训练数据，并设置训练步长，对定义好的 FTRL 模型进行训练及预估<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">model</span>.train(read_ctr_records, steps=<span class="number">1000</span>)</span><br><span class="line">result = <span class="keyword">model</span>.evaluate(read_ctr_records)</span><br></pre></td></tr></table></figure></p><p>通常需要编写离线任务，定时读取用户行为数据作为训练集和验证集，对训练集及验证集进行 CTR 预估，并根据离线指标对结果进行分析，决定是否更新模型。</p><h1 id="在线排序"><a href="#在线排序" class="headerlink" title="在线排序"></a>在线排序</h1><p>通常在线排序是根据用户实时的推荐请求，对召回结果进行 CTR 预估，进而计算出排序结果并返回。我们需要根据召回结果构造测试集，其中每个测试样本包括用户特征和文章特征。首先，根据用户 ID 和频道 ID 读取用户特征（用户在每个频道的特征不同，所以是分频道存储的）<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    user_feature = eval(hbu.get_table_row(<span class="string">'ctr_feature_user'</span>,</span><br><span class="line">                              <span class="string">'&#123;&#125;'</span>.<span class="built_in">format</span>(temp.user_id).encode(),</span><br><span class="line">                              <span class="string">'channel:&#123;&#125;'</span>.<span class="built_in">format</span>(temp.channel_id).encode()))</span><br><span class="line">except Exception <span class="keyword">as</span> e:</span><br><span class="line">    user_feature = []</span><br></pre></td></tr></table></figure></p><p>再根据用户 ID 读取召回结果<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recall_set = read_hbase_recall('cb_recall', </span><br><span class="line">                'recall<span class="symbol">:user</span>:&#123;&#125;'.format(<span class="name">temp</span>.user_id).encode(), </span><br><span class="line">                'als:&#123;&#125;'.format(<span class="name">temp</span>.channel_id).encode())</span><br></pre></td></tr></table></figure></p><p>接着，遍历召回结果，获取文章特征，并将用户特征合并，作为测试样本<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">test = []</span><br><span class="line"><span class="keyword">for</span> article_id in recall_se<span class="variable">t:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        article_feature = <span class="built_in">eval</span>(hbu.get_table_row(<span class="string">'ctr_feature_article'</span>,</span><br><span class="line">                                  <span class="string">'&#123;&#125;'</span>.format(article_id).encode(),</span><br><span class="line">                                  <span class="string">'article:&#123;&#125;'</span>.format(article_id).encode()))</span><br><span class="line">    except Exception <span class="keyword">as</span> <span class="keyword">e</span>:</span><br><span class="line">        article_feature = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> not article_feature:</span><br><span class="line">        article_feature = [<span class="number">0.0</span>] * <span class="number">111</span></span><br><span class="line">    feature = []</span><br><span class="line">    feature.<span class="built_in">extend</span>(user_feature)</span><br><span class="line">    feature.<span class="built_in">extend</span>(article_feature)</span><br><span class="line"></span><br><span class="line">    test.<span class="keyword">append</span>(<span class="keyword">f</span>)</span><br></pre></td></tr></table></figure></p><p>加载本地 FTRL 模型并对测试样本进行 CTR 预估<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">test_array = np.array(test)</span><br><span class="line">model.load_weights(<span class="string">'/root/toutiao_project/reco_sys/offline/models/ckpt/ctr_lr_ftrl.h5'</span>)</span><br><span class="line">init = <span class="keyword">tf</span>.global_variables_initializer()</span><br><span class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">    sess.run(init)</span><br><span class="line">    predictions = self.model.predict(sess.run(<span class="keyword">tf</span>.constant(test_array)))</span><br></pre></td></tr></table></figure></p><p>对结果进行排序并提取 CTR 最高的前 K 个文章，这样就得到了 FTRL 模型在线排序的结果。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">res</span> = pd.DataFrame(np.concatenate((np.array(recall_set).reshape(len(recall_set), <span class="number">1</span>), predictions),</span><br><span class="line">                                 <span class="attr">axis=1),</span> <span class="attr">columns=['article_id',</span> 'prob'])</span><br><span class="line"></span><br><span class="line"><span class="attr">res_sort</span> = res.sort_values(<span class="attr">by=['prob'],</span> <span class="attr">ascending=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序后，只将排名在前100个文章ID作为推荐结果返回给用户</span></span><br><span class="line"><span class="keyword">if</span> len(res_sort) &gt; <span class="number">100</span>:</span><br><span class="line">    <span class="attr">recall_set</span> = list(res_sort.iloc[:<span class="number">100</span>, <span class="number">0</span>])</span><br><span class="line"><span class="attr">recall_set</span> = list(res_sort.iloc[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;构造-TFRecord-训练集&quot;&gt;&lt;a href=&quot;#构造-TFRecord-训练集&quot; class=&quot;headerlink&quot; title=&quot;构造 TFRecord 训练集&quot;&gt;&lt;/a&gt;构造 TFRecord 训练集&lt;/h1&gt;&lt;p&gt;和前面的 LR 离线模型一样，FTR
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之11、基于FTRL优化方法的模型在线排序.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B11%E3%80%81%E5%9F%BA%E4%BA%8EFTRL%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%9C%A8%E7%BA%BF%E6%8E%92%E5%BA%8F/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之11、基于FTRL优化方法的模型在线排序/</id>
    <published>2019-12-05T13:21:31.000Z</published>
    <updated>2020-01-10T02:33:57.681Z</updated>
    
    <content type="html"><![CDATA[<p>前面，我们已经完成了召回阶段的全部工作，通过召回，我们可以从数百万甚至上亿的原始物品数据中，筛选出和用户相关的几百、几千个可能感兴趣的物品。接下来，我们将要进入到排序阶段，对召回的几百、几千个物品进行进一步的筛选和排序。</p><p>排序流程包括离线排序和在线排序：</p><ul><li><p>离线排序<br>读取前天（第 T - 2 天）之前的用户行为数据作为训练集，对离线模型进行训练；训练完成后，读取昨天（第 T - 1 天）的用户行为数据作为验证集进行预测，根据预测结果对离线模型进行评估；若评估通过，当天（第 T 天）即可将离线模型更新到定时任务中，定时执行预测任务；明天（第 T + 1 天）就能根据今天的用户行为数据来观察更新后离线模型的预测效果。（注意：数据生产有一天时间差，第 T 天生成第 T - 1 天的数据）</p></li><li><p>在线排序<br>读取前天（第 T - 2 天）之前的用户行为数据作为训练集，对在线模型进行训练；训练完成后，读取昨天（第 T - 1 天）的用户行为数据作为验证集进行预测，根据预测结果对在线模型进行评估；若评估通过，当天（第 T 天）即可将在线模型更新到线上，实时执行排序任务；明天（第 T + 1 天）就能根据今天的用户行为数据来观察更新后在线模型的预测效果。</p></li></ul><p>这里再补充一个数据集划分的小技巧：可以横向划分，随机或按用户或其他样本选择策略；也可以纵向划分，按照时间跨度，比如一周的数据中，周一到周四是训练集，周五周六是测试集，周日是验证集。</p><p>利用排序模型可以进行评分预测和用户行为预测，通常推荐系统利用排序模型进行用户行为预测，比如点击率（CTR）预估，进而根据点击率对物品进行排序，目前工业界常用的点击率预估模型有如下 3 种类型：</p><ul><li>宽模型 + 特征⼯程<br>LR / MLR + 非 ID 类特征（⼈⼯离散 / GBDT / FM），可以使用 Spark 进行训练</li><li>宽模型 + 深模型<br>Wide&amp;Deep，DeepFM，可以使用 TensorFlow 进行训练</li><li>深模型：<br>DNN + 特征 Embedding，可以使用 TensorFlow 进行训练</li></ul><p>这里的宽模型即指线性模型，线性模型的优点包括：</p><ul><li>相对简单，训练和预测的计算复杂度都相对较低</li><li>可以集中精力发掘新的有效特征，且可以并行化工作</li><li>解释性较好，可以根据特征权重做解释</li></ul><p>本文我们将采用逻辑回归作为离线模型，进行点击率预估。逻辑回归（Logistic Regression，LR）是基础的二分类模型，也是监督学习的一种，通过对有标签的训练集数据进行特征学习，进而可以对测试集（新数据）的标签进行预测。我们这里的标签就是指用户是否对文章发生了点击行为。</p><h1 id="构造训练集"><a href="#构造训练集" class="headerlink" title="构造训练集"></a>构造训练集</h1><p>读取用户历史行为数据，将 clicked 作为训练集标签<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use profile"</span>)</span><br><span class="line">user_article_basic = spark.sql(<span class="string">"select * from user_article_basic"</span>).<span class="keyword">select</span>([<span class="symbol">'user_id</span>', <span class="symbol">'article_id</span>', <span class="symbol">'clicked</span>'])</span><br></pre></td></tr></table></figure></p><p><code>user_article_basic</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-3e75d46d736f96ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>之前我们已经计算好了文章特征和用户特征，并存储到了 Hbase 中。这里我们遍历用户历史行为数据，根据其中文章 ID 和用户 ID 分别获取文章特征和用户特征，再将标签转为 int 类型，这样就将一条用户行为数据构造成为了一个样本，再将所有样本加入到训练集中<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">train = []</span><br><span class="line"><span class="keyword">for</span> user_id, article_id, clicked <span class="keyword">in</span> user_article_basic:</span><br><span class="line">    try:</span><br><span class="line">        article_feature = eval(hbu.get_table_row('ctr_feature_article', '&#123;&#125;'.<span class="keyword">format</span>(article_id).<span class="keyword">encode</span>(), 'article:&#123;&#125;'.<span class="keyword">format</span>(article_id).<span class="keyword">encode</span>()))</span><br><span class="line">    except Exception <span class="keyword">as</span> <span class="keyword">e</span>:</span><br><span class="line">        article_feature = []</span><br><span class="line">    try:</span><br><span class="line">        user_feature = eval(hbu.get_table_row('ctr_feature_user', '&#123;&#125;'.<span class="keyword">format</span>(temp.user_id).<span class="keyword">encode</span>(), 'channel:&#123;&#125;'.<span class="keyword">format</span>(temp.channel_id).<span class="keyword">encode</span>()))</span><br><span class="line">    except Exception <span class="keyword">as</span> <span class="keyword">e</span>:</span><br><span class="line">        user_feature = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> not article_feature:</span><br><span class="line">        article_feature = [0.0] * 111</span><br><span class="line">    <span class="keyword">if</span> not user_feature:</span><br><span class="line">        user_feature = [0.0] * 10</span><br><span class="line"></span><br><span class="line">    <span class="keyword">sample</span> = []</span><br><span class="line">    <span class="keyword">sample</span>.<span class="keyword">append</span>(user_feature)</span><br><span class="line">    <span class="keyword">sample</span>.<span class="keyword">append</span>(article_feature)</span><br><span class="line">    <span class="keyword">sample</span>.<span class="keyword">append</span>(int(clicked))</span><br><span class="line"></span><br><span class="line">    train.<span class="keyword">append</span>(<span class="keyword">sample</span>)</span><br></pre></td></tr></table></figure></p><p>接下来，还需要利用 Spark 的 Vectors 将 <code>array&lt;double&gt;</code> 类型的 article_feature 和 user_feature 转为 <code>vector</code> 类型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">columns = [<span class="string">'article_feature'</span>, <span class="string">'user_feature'</span>, <span class="string">'clicked'</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_to_vector</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Vectors.dense(row[<span class="number">0</span>]), Vectors.dense(row[<span class="number">1</span>]), row[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">train = train.rdd.map(list_to_vector).toDF(columns)</span><br></pre></td></tr></table></figure></p><p>再将 article_feature, user_feature 合并为统一输入到 LR 模型的特征列 features，这样就完成训练集的构建<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">train</span> = VectorAssembler().setInputCols(columns[<span class="number">0</span>:<span class="number">1</span>]).setOutputCol(<span class="string">'features'</span>).transform(train)</span><br></pre></td></tr></table></figure></p><h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>Spark 已经实现好了 LR 模型，通过指定训练集 train 的特征列 features 和标签列 clicked，即可对 LR 模型进行训练，再将训练好的模型保存到 HDFS<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from pyspark<span class="selector-class">.ml</span><span class="selector-class">.feature</span> import VectorAssembler</span><br><span class="line">from pyspark<span class="selector-class">.ml</span><span class="selector-class">.classification</span> import LogisticRegression</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">model = lr.setLabelCol(<span class="string">"clicked"</span>).setFeaturesCol(<span class="string">"features"</span>).fit(train)</span><br><span class="line">model.save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/lr.obj"</span>)</span><br></pre></td></tr></table></figure></p><p>加载训练好的 LR 模型，调用 <code>transform()</code> 对训练集做出预测（实际场景应该对验证集和训练集进行预测）<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.classification import LogisticRegressionModel</span><br><span class="line"></span><br><span class="line">online_model = LogisticRegressionModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/lr.obj"</span>)</span><br><span class="line">sort_res = online_model.transform(train)</span><br></pre></td></tr></table></figure></p><p>预测结果 <code>sort_res</code> 中包括 clicked 和 probability 列，其中 clicked 为样本标签的真实值，probability 是包含两个元素的列表，第一个元素是预测的不点击概率，第二个元素则是预测的点击概率，可以提取点击率（CTR）<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_ctr(<span class="built_in">row</span>):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">float</span>(<span class="built_in">row</span>.clicked), <span class="built_in">float</span>(<span class="built_in">row</span>.probability[<span class="number">1</span>]) </span><br><span class="line"></span><br><span class="line">score_label = sort_res.select([<span class="string">"clicked"</span>, <span class="string">"probability"</span>]).rdd.<span class="built_in">map</span>(get_ctr)</span><br></pre></td></tr></table></figure></p><h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><p>离线模型评估指标包括：</p><ul><li>评分准确度<br>通常是均方根误差（RMSE），用来评估预测评分的效果</li><li>排序能力<br>通常采用 AUC（Area Under the Curve），即 ROC 曲线下方的面积</li><li>分类准确率（Precision）<br>表示在 Top K 推荐列表中，用户真实点击的物品所占的比例</li><li>分类召回率（Recall）<br>表示在用户真实点击的物品中，出现在 Top K 推荐列表中所占的比例</li></ul><p>当模型更新后，还可以根据商业指标进行评估，比例类的包括： 点击率（CTR）、转化率（CVR），绝对类的包括：社交关系数量、用户停留时长、成交总额（GMV）等。</p><p>推荐系统的广度评估指标包括：</p><ul><li>覆盖率<br>表示被有效推荐（推荐列表长度大于 c）的用户占全站用户的比例，公式如下：<script type="math/tex; mode=display">Con_{UV}=\frac{N_{l >c}}{N_{UV}}</script></li><li>失效率<br>表示被无效推荐（推荐列表长度为 0）的用户占全站用户的比例，公式如下：<script type="math/tex; mode=display">Lost_{UV}=\frac{N_{l =0}}{N_{UV}}</script></li><li>新颖性</li><li>更新率<br>表示推荐列表的变化程度，当前周期与上个周期相比，推荐列表中不同物品的比例<script type="math/tex; mode=display">Update=\frac{N_{diff}}{N_{last}}</script></li></ul><p>推荐系统的健康评估指标包括：</p><ul><li>个性化<br>用于衡量推荐的个性化程度，是否大部分用户只消费小部分物品，可以计算所有用户推荐列表的平均相似度</li><li>基尼系数<br>用于衡量推荐系统的马太效应，反向衡量推荐的个性化程度。将物品按照累计推荐次数排序，排序位置为 i，推荐次数占总推荐次数的比例为 $P_i$，推荐次数越不平均，基尼系数越接近 1，公式为：<script type="math/tex; mode=display">Gini=\frac{1}{n}\sum_{i=1}^n P_i(2i-n-i)</script></li><li>多样性<br>通常是在类别维度上衡量推荐结果的多样性，可以衡量各个类别在推荐时的熵<script type="math/tex; mode=display">Div=\frac{\sum_{i=1}^n-P_i\log(P_i)}{n\log(n)}</script>其中，物品共包括 n 个类别，类别 i 被推荐次数占总推荐次数的比例为 $P_i$，分母是各个类别最均匀时对应的熵，分子是实际推荐结果的类别分布熵。这是整体推荐的多样性，还可以计算每次推荐和每个用户推荐的多样性。</li></ul><p>我们这里主要根据 AUC 进行评估，首先利用 <code>model.summary.roc</code> 绘制 ROC 曲线<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'r--'</span>)</span><br><span class="line">plt.plot(model<span class="selector-class">.summary</span><span class="selector-class">.roc</span><span class="selector-class">.select</span>(<span class="string">'FPR'</span>).collect(),</span><br><span class="line">         model<span class="selector-class">.summary</span><span class="selector-class">.roc</span><span class="selector-class">.select</span>(<span class="string">'TPR'</span>).collect())</span><br><span class="line">plt.xlabel(<span class="string">'FPR'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'TPR'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>ROC 曲线如下所示，曲线下面的面积即为 AUC（Area Under the Curve），AUC 值越大，排序效果越好</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ebdcbb926acd114a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>利用 Spark 的 <code>BinaryClassificationMetrics()</code> 计算 AUC<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">from</span> pyspark.mllib.evaluation <span class="meta">import</span> <span class="keyword">BinaryClassificationMetrics</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">metrics </span>= <span class="keyword">BinaryClassificationMetrics(score_label)</span></span><br><span class="line"><span class="keyword">metrics.areaUnderROC</span></span><br></pre></td></tr></table></figure></p><p>也可以利用 sklearn 的 <code>roc_auc_score()</code> 计算 AUC，<code>accuracy_score()</code> 计算准确率<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, roc_auc_score,</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.<span class="built_in">array</span>(score_label.collect())</span><br><span class="line"><span class="comment"># AUC</span></span><br><span class="line">roc_auc_score(arr[:, <span class="number">0</span>], arr[:, <span class="number">1</span>]) <span class="comment"># 0.719274521004087</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line">accuracy_score(arr[:, <span class="number">0</span>], arr[:, <span class="number">1</span>].round()) <span class="comment"># 0.9051438053097345</span></span><br></pre></td></tr></table></figure></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://book.douban.com/subject/34872145/" target="_blank" rel="external">https://book.douban.com/subject/34872145/</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面，我们已经完成了召回阶段的全部工作，通过召回，我们可以从数百万甚至上亿的原始物品数据中，筛选出和用户相关的几百、几千个可能感兴趣的物品。接下来，我们将要进入到排序阶段，对召回的几百、几千个物品进行进一步的筛选和排序。&lt;/p&gt;
&lt;p&gt;排序流程包括离线排序和在线排序：&lt;/p
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之10、基于热门文章和新文章的在线召回.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B10%E3%80%81%E5%9F%BA%E4%BA%8E%E7%83%AD%E9%97%A8%E6%96%87%E7%AB%A0%E5%92%8C%E6%96%B0%E6%96%87%E7%AB%A0%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8F%AC%E5%9B%9E/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之10、基于热门文章和新文章的在线召回/</id>
    <published>2019-12-05T13:21:30.000Z</published>
    <updated>2020-01-10T02:31:31.525Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中我们实现了基于内容的在线召回，接下来，我们将实现基于热门文章和新文章的在线召回。主要思路是根据点击次数，统计每个频道下的热门文章，根据发布时间统计每个频道下的新文章，当推荐文章不足时，可以根据这些文章进行补足。</p><p>由于数据量较小，这里采用 Redis 存储热门文章和新文章的召回结果，数据结构如下所示</p><div class="table-container"><table><thead><tr><th>热门文章召回</th><th>结构</th><th>示例</th></tr></thead><tbody><tr><td>popular_recall</td><td>ch:{}:hot</td><td>ch:18:hot</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>新文章召回</th><th>结构</th><th>示例</th></tr></thead><tbody><tr><td>new_article</td><td>ch:{}:new</td><td>ch:18:new</td></tr></tbody></table></div><p>热门文章存储，键为 <code>ch:频道ID:hot</code> 值为 <code>分数</code> 和 <code>文章ID</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ZINCRBY key increment member</span></span><br><span class="line"><span class="comment"># ZSCORE</span></span><br><span class="line"><span class="comment"># 为有序集 key 的成员 member 的 score 值加上增量 increment 。</span></span><br><span class="line">client.zincrby(<span class="string">"ch:&#123;&#125;:hot"</span>.format(row[<span class="string">'channelId'</span>]), <span class="number">1</span>, row[<span class="string">'param'</span>][<span class="string">'articleId'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># ZREVRANGE key start stop [WITHSCORES]</span></span><br><span class="line">client.zrevrange(ch:&#123;&#125;:new, <span class="number">0</span>, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure></p><p>新文章存储，键为 <code>ch:{频道ID}:new</code> 值为 <code>文章ID:时间戳</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ZADD ZRANGE</span></span><br><span class="line"><span class="comment"># ZADD key score member [[score member] [score member] ...]</span></span><br><span class="line"><span class="comment"># ZRANGE page_rank 0 -1</span></span><br><span class="line">client.zadd(<span class="string">"ch:&#123;&#125;:new"</span>.format(channel_id), &#123;article_id: time.time()&#125;)</span><br></pre></td></tr></table></figure></p><h1 id="热门文章在线召回"><a href="#热门文章在线召回" class="headerlink" title="热门文章在线召回"></a>热门文章在线召回</h1><p>首先，添加 Spark Streaming 和 Kafka 的配置，热门文章读取由业务系统发送到 Kafka 的 click-trace 主题中的用户实时行为数据<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">KAFKA_SERVER</span> = <span class="string">"192.168.19.137:9092"</span></span><br><span class="line"><span class="attr">click_kafkaParams</span> = &#123;<span class="string">"metadata.broker.list"</span>: KAFKA_SERVER&#125;</span><br><span class="line"><span class="attr">HOT_DS</span> = KafkaUtils.createDirectStream(stream_c, [<span class="string">'click-trace'</span>], click_kafkaParams)</span><br></pre></td></tr></table></figure></p><p>接下来，利用 Spark Streaming 读取 Kafka 中的用户行为数据，筛选出被点击过的文章，将 Redis 中的文章热度分数进行累加即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">client = redis.StrictRedis(host=DefaultConfig.REDIS_HOST, port=DefaultConfig.REDIS_PORT, db=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_hot_redis</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    收集用户行为，更新热门文章分数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_hot_article</span><span class="params">(rdd)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> rdd.collect():</span><br><span class="line">            <span class="comment"># 过滤用户行为</span></span><br><span class="line">            <span class="keyword">if</span> data[<span class="string">'param'</span>][<span class="string">'action'</span>] <span class="keyword">in</span> [<span class="string">'exposure'</span>, <span class="string">'read'</span>]:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                client.zincrby(<span class="string">"ch:&#123;&#125;:hot"</span>.format(data[<span class="string">'channelId'</span>]), <span class="number">1</span>, data[<span class="string">'param'</span>][<span class="string">'articleId'</span>])</span><br><span class="line"></span><br><span class="line">    HOT_DS.map(<span class="keyword">lambda</span> x: json.loads(x[<span class="number">1</span>])).foreachRDD(update_hot_article)</span><br></pre></td></tr></table></figure></p><p>测试，写入用户行为日志<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#123;\"actionTime\":\"2019-04-10 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":&#123;\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"&#125;&#125; &gt;&gt; userClick.log</span><br></pre></td></tr></table></figure></p><p>查询热门文章<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span><span class="selector-pseudo">:6379</span><span class="selector-attr">[10]</span>&gt; <span class="selector-tag">keys</span> *</span><br><span class="line">1) "<span class="selector-tag">ch</span><span class="selector-pseudo">:18</span><span class="selector-pseudo">:hot"</span></span><br><span class="line">127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span><span class="selector-pseudo">:6379</span><span class="selector-attr">[10]</span>&gt; <span class="selector-tag">ZRANGE</span> "<span class="selector-tag">ch</span><span class="selector-pseudo">:18</span><span class="selector-pseudo">:hot"</span> 0 <span class="selector-tag">-1</span></span><br><span class="line">1) "14299"</span><br></pre></td></tr></table></figure></p><h1 id="新文章在线召回"><a href="#新文章在线召回" class="headerlink" title="新文章在线召回"></a>新文章在线召回</h1><p>首先，添加 Spark Streaming 和 Kafka 的配置，新文章读取由业务系统发送到 Kafka 的 new-article 主题中的最新发布文章数据<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">NEW_ARTICLE_DS</span> = KafkaUtils.createDirectStream(stream_c, [<span class="string">'new-article'</span>], click_kafkaParams)</span><br></pre></td></tr></table></figure></p><p>接下来，利用 Spark Streaming 读取 Kafka 的新文章，将其按频道添加到 Redis 中，Redis 的值为当前时间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span>  <span class="title">update_new_redis</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""更新频道最新文章</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_article</span><span class="params">(rdd)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> rdd.collect():</span><br><span class="line">            channel_id, article_id = row.split(<span class="string">','</span>)</span><br><span class="line">            client.zadd(<span class="string">"ch:&#123;&#125;:new"</span>.format(channel_id), &#123;article_id: time.time()&#125;)</span><br><span class="line"></span><br><span class="line">    NEW_ARTICLE_DS.map(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).foreachRDD(add_new_article)</span><br></pre></td></tr></table></figure></p><p>还需要在 Kafka 的启动脚本中添加 new-article 主题监听配置，这样就可以收到业务系统发送过来的新文章了，重新启动 Flume 和 Kafka<br><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/bigdata/kafka/bin/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic<span class="built_in"> new-article </span>--partitions 1</span><br></pre></td></tr></table></figure></p><p>测试，向 Kafka 发送新文章数据<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> kafka import KafkaProducer </span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka消息生产者</span></span><br><span class="line">kafka_producer = KafkaProducer(bootstrap_servers=[<span class="string">'192.168.19.137:9092'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造消息并发送</span></span><br><span class="line">msg = <span class="string">'&#123;&#125;,&#123;&#125;'</span>.<span class="built_in">format</span>(<span class="number">18</span>, <span class="number">13891</span>)</span><br><span class="line">kafka_producer.<span class="built_in">send</span>(<span class="string">'new-article'</span>, msg.encode())</span><br></pre></td></tr></table></figure></p><p>查看新文章<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379[10]&gt; keys *</span><br><span class="line">1) "ch:18:hot"</span><br><span class="line">2) "ch:18:new"</span><br><span class="line">127.0.0.1:6379[10]&gt; ZRANGE "ch:18:new" 0 -1</span><br><span class="line">1) "13890"</span><br><span class="line">2) "13891"</span><br></pre></td></tr></table></figure></p><p>最后，修改 <code>online_update.py</code>，加入基于热门文章和新文章的在线召回逻辑，开启实时运行即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    ore = OnlineRecall()</span><br><span class="line">    ore.update_content_recall()</span><br><span class="line">    ore.update_hot_redis()</span><br><span class="line">    ore.update_new_redis()</span><br><span class="line">    stream_sc.start()</span><br><span class="line">    <span class="comment"># 使用 ctrl+c 可以退出服务</span></span><br><span class="line">    _ONE_DAY_IN_SECONDS = <span class="number">60</span> * <span class="number">60</span> * <span class="number">24</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            time.sleep(_ONE_DAY_IN_SECONDS)</span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><p>到这里，我们就完成了召回阶段的全部工作，包括基于模型和基于内容的离线召回，以及基于内容、热门文章和新文章的在线召回。通过召回，我们可以从数百万甚至上亿的原始物品数据中，筛选出和用户相关的几百、几千个可能感兴趣的物品，后面，我们将要进入到排序阶段，对召回的几百、几千个物品进行进一步的筛选和排序。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上篇文章中我们实现了基于内容的在线召回，接下来，我们将实现基于热门文章和新文章的在线召回。主要思路是根据点击次数，统计每个频道下的热门文章，根据发布时间统计每个频道下的新文章，当推荐文章不足时，可以根据这些文章进行补足。&lt;/p&gt;
&lt;p&gt;由于数据量较小，这里采用 Redis
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之9、基于内容的离线及在线召回.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B9%E3%80%81%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%8F%8A%E5%9C%A8%E7%BA%BF%E5%8F%AC%E5%9B%9E/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之9、基于内容的离线及在线召回/</id>
    <published>2019-12-05T13:21:29.000Z</published>
    <updated>2020-01-10T02:26:39.805Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中，我们实现了基于模型的离线召回，属于基于协同过滤的召回算法。接下来，本文就讲一下另一个经典的召回方式，那就是如何实现基于内容的离线召回。相比于协同过滤来说，基于内容的召回会简单很多，主要思路就是召回用户点击过的文章的相似文章，通常也被叫做 u2i2i。</p><h1 id="离线召回"><a href="#离线召回" class="headerlink" title="离线召回"></a>离线召回</h1><p>首先，读取用户历史行为数据，得到用户历史点击过的文章<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql('use profile')</span><br><span class="line">user_article_basic = spark.sql(<span class="string">"select * from user_article_basic"</span>)</span><br><span class="line">user_article_basic = user_article_basic.filter('clicked=True')</span><br></pre></td></tr></table></figure></p><p><code>user_article_basic</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-349e372abc54e502.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，遍历用户历史点击过的文章，获取与之相似度最高的 K 篇文章即可。可以根据之前计算好的文章相似度表 article_similar 进行相似文章查询，接着根据历史召回结果进行过滤，防止重复推荐。最后将召回结果按照频道分别存入召回结果表及历史召回结果表<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">user_article_basic.foreachPartition(get_clicked_similar_article)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_clicked_similar_article</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="string">"""召回用户点击文章的相似文章</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">import</span> happybase</span><br><span class="line">    pool = happybase.ConnectionPool(size=<span class="number">10</span>, host=<span class="string">'hadoop-master'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">        similar_table = conn.table(<span class="string">'article_similar'</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">            <span class="comment"># 读取文章相似度表,根据文章ID获取相似文章</span></span><br><span class="line">            similar_article = similar_table.row(str(row.article_id).encode(),</span><br><span class="line">                                                columns=[<span class="string">b'similar'</span>])</span><br><span class="line">            <span class="comment"># 按照相似度进行排序</span></span><br><span class="line">            similar_article_sorted = sorted(similar_article.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">if</span> similar_article_sorted:</span><br><span class="line">                <span class="comment"># 每次行为推荐10篇文章</span></span><br><span class="line">                similar_article_topk = [int(i[<span class="number">0</span>].split(<span class="string">b':'</span>)[<span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> similar_article_sorted][:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 根据历史召回结果进行过滤</span></span><br><span class="line">                history_table = conn.table(<span class="string">'history_recall'</span>)</span><br><span class="line">                history_article_data = history_table.cells(<span class="string">'reco:his:&#123;&#125;'</span>.format(row.user_id).encode(), <span class="string">'channel:&#123;&#125;'</span>.format(row.channel_id).encode())</span><br><span class="line">                <span class="comment"># 将多个版本都加入历史文章ID列表</span></span><br><span class="line">                history_article = []</span><br><span class="line">                <span class="keyword">if</span> len(history_article_data) &gt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">for</span> article <span class="keyword">in</span> history_article_data[:<span class="number">-1</span>]:</span><br><span class="line">                        history_article.extend(eval(article))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    history_article = []</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 过滤history_article</span></span><br><span class="line">                recall_article = list(set(similar_article_topk) - set(history_article))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 存储到召回结果表及历史召回结果表</span></span><br><span class="line">                <span class="keyword">if</span> recall_article:</span><br><span class="line">                    content_table = conn.table(<span class="string">'cb_recall'</span>)</span><br><span class="line">                    content_table.put(<span class="string">"recall:user:&#123;&#125;"</span>.format(row.user_id).encode(), &#123;<span class="string">'content:&#123;&#125;'</span>.format(row.channel_id).encode(): str(recall_article).encode()&#125;)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 放入历史召回结果表</span></span><br><span class="line">                    history_table.put(<span class="string">"reco:his:&#123;&#125;"</span>.format(row.user_id).encode(), &#123;<span class="string">'channel:&#123;&#125;'</span>.format(row.channel_id).encode(): str(recall_article).encode()&#125;)</span><br></pre></td></tr></table></figure></p><p>可以根据用户 ID 和频道 ID 来查询召回结果<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">028</span>:<span class="number">0</span>&gt; get <span class="string">'cb_recall'</span>, <span class="string">'recall:user:2'</span></span><br><span class="line">COLUMN                     CELL                                                                        </span><br><span class="line"><span class="attribute">content</span>:<span class="number">13</span>                    timestamp=<span class="number">1558041569201</span>, value=[<span class="number">141431</span>,<span class="number">14381</span>, <span class="number">17966</span>, <span class="number">17454</span>, <span class="number">14125</span>, <span class="number">16174</span>]</span><br></pre></td></tr></table></figure></p><p>最后，使用 Apscheduler 定时更新。在用户召回方法 <code>update_user_recall()</code> 中，增加基于内容的离线召回方法 <code>update_content_recall()</code>，首先读取用户行为日志，并筛选用户点击的文章，接着读取文章相似表，获取相似度最高的 K 篇文章，然后根据历史召回结果进行过滤，防止重复推荐，最后，按频道分别存入召回结果表及历史召回结果表<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_user_recall</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用户的频道推荐召回结果更新逻辑</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ur = UpdateRecall(<span class="number">500</span>)</span><br><span class="line">    ur.update_als_recall()</span><br><span class="line">    ur.update_content_recall()</span><br></pre></td></tr></table></figure></p><p>之前已经添加好了定时更新用户召回结果的任务，每隔 3 小时运行一次，这样就完成了基于内容的离线召回。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from apscheduler.schedulers.blocking <span class="built_in">import</span> BlockingScheduler</span><br><span class="line">from apscheduler.executors.pool <span class="built_in">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建scheduler，多进程执行</span></span><br><span class="line"><span class="attr">executors</span> = &#123;</span><br><span class="line">    'default': ProcessPoolExecutor(<span class="number">3</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">scheduler</span> = BlockingScheduler(<span class="attr">executors=executors)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_article_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=1)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_user_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=2)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行用户召回更新的任务，每隔3小时运行一次</span></span><br><span class="line">scheduler.add_job(update_user_recall, <span class="attr">trigger='interval',</span> <span class="attr">hours=3)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行特征中心平台的任务，每隔4小时更新一次</span></span><br><span class="line">scheduler.add_job(update_ctr_feature, <span class="attr">trigger='interval',</span> <span class="attr">hours=4)</span></span><br><span class="line"></span><br><span class="line">scheduler.start()</span><br></pre></td></tr></table></figure></p><h1 id="在线召回"><a href="#在线召回" class="headerlink" title="在线召回"></a>在线召回</h1><p>前面我们实现了基于内容的离线召回，接下来我们将实现基于内容的在线召回。在线召回的实时性更好，能够根据用户的线上行为实时反馈，快速跟踪用户的偏好，也能够解决用户冷启动问题。离线召回和在线召回唯一的不同就是，离线召回读取的是用户历史行为数据，而在线召回读取的是用户实时的行为数据，从而召回用户当前正在阅读的文章的相似文章。</p><p>首先，我们通过 Spark Streaming 读取 Kafka 中的用户实时行为数据，Spark Streaming 配置如下<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from pyspark <span class="built_in">import</span> SparkConf</span><br><span class="line">from pyspark.sql <span class="built_in">import</span> SparkSession</span><br><span class="line">from pyspark <span class="built_in">import</span> SparkContext</span><br><span class="line">from pyspark.streaming <span class="built_in">import</span> StreamingContext</span><br><span class="line">from pyspark.streaming.kafka <span class="built_in">import</span> KafkaUtils</span><br><span class="line">from setting.default <span class="built_in">import</span> DefaultConfig</span><br><span class="line"><span class="built_in">import</span> happybase</span><br><span class="line"></span><br><span class="line"><span class="attr">SPARK_ONLINE_CONFIG</span> = (</span><br><span class="line">        (<span class="string">"spark.app.name"</span>, <span class="string">"onlineUpdate"</span>), </span><br><span class="line">        (<span class="string">"spark.master"</span>, <span class="string">"yarn"</span>),</span><br><span class="line">        (<span class="string">"spark.executor.instances"</span>, <span class="number">4</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="attr">KAFKA_SERVER</span> = <span class="string">"192.168.19.137:9092"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于读取hbase缓存结果配置</span></span><br><span class="line"><span class="attr">pool</span> = happybase.ConnectionPool(<span class="attr">size=10,</span> <span class="attr">host='hadoop-master',</span> <span class="attr">port=9090)</span></span><br><span class="line"><span class="attr">conf</span> = SparkConf()</span><br><span class="line">conf.setAll(SPARK_ONLINE_CONFIG)</span><br><span class="line"><span class="attr">sc</span> = SparkContext(<span class="attr">conf=conf)</span></span><br><span class="line"><span class="attr">stream_c</span> = StreamingContext(sc, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于内容召回配置,用于收集用户行为</span></span><br><span class="line"><span class="attr">similar_kafkaParams</span> = &#123;<span class="string">"metadata.broker.list"</span>: DefaultConfig.KAFKA_SERVER, <span class="string">"group.id"</span>: 'similar'&#125;</span><br><span class="line"><span class="attr">SIMILAR_DS</span> = KafkaUtils.createDirectStream(stream_c, ['click-trace'], similar_kafkaParams)</span><br></pre></td></tr></table></figure></p><p>Kafka 中的用户行为数据，如下所示<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"actionTime"</span>:<span class="string">"2019-12-10 21:04:39"</span>,<span class="attr">"readTime"</span>:<span class="string">""</span>,<span class="attr">"channelId"</span>:<span class="number">18</span>,<span class="attr">"param"</span>:&#123;<span class="attr">"action"</span>: <span class="string">"click"</span>, <span class="attr">"userId"</span>: <span class="string">"2"</span>, <span class="attr">"articleId"</span>: <span class="string">"116644"</span>, <span class="attr">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>接下来，利用 Spark Streaming 将用户行为数据传入到 <code>get_similar_online_recall()</code> 方法中，这里利用 <code>json.loads()</code> 方法先将其转换为了 json 格式，注意用户行为数据在每条 Kafka 消息的第二个位置<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SIMILAR_DS.map(<span class="keyword">lambda</span> x: json.loads(x[<span class="number">1</span>])).foreachRDD(get_similar_online_recall)</span><br></pre></td></tr></table></figure></p><p>接着，遍历用户行为数据，这里可能每次读取到多条用户行为数据。筛选出被点击、收藏或分享过的文章，并获取与其相似度最高的 K 篇文章，再根据历史召回结果表进行过滤，防止重复推荐，最后，按频道分别存入召回结果表及历史召回结果表<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_online_similar_recall</span><span class="params">(rdd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取在线相似文章</span></span><br><span class="line"><span class="string">    :param rdd:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line">    topk = <span class="number">10</span></span><br><span class="line">    <span class="comment"># 初始化happybase连接</span></span><br><span class="line">    pool = happybase.ConnectionPool(size=<span class="number">10</span>, host=<span class="string">'hadoop-master'</span>, port=<span class="number">9090</span>)</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> rdd.collect():</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据用户行为筛选文章</span></span><br><span class="line">        <span class="keyword">if</span> data[<span class="string">'param'</span>][<span class="string">'action'</span>] <span class="keyword">in</span> [<span class="string">"click"</span>, <span class="string">"collect"</span>, <span class="string">"share"</span>]:</span><br><span class="line">            <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">                similar_table = conn.table(<span class="string">"article_similar"</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 根据用户行为数据涉及文章找出与之最相似文章(基于内容的相似)</span></span><br><span class="line">                similar_article = similar_table.row(str(data[<span class="string">"param"</span>][<span class="string">"articleId"</span>]).encode(), columns=[<span class="string">b"similar"</span>])</span><br><span class="line">                similar_article = sorted(similar_article.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)  <span class="comment"># 按相似度排序</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> similar_article:</span><br><span class="line">                    similar_article_topk = [int(i[<span class="number">0</span>].split(<span class="string">b":"</span>)[<span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> similar_article[:topk]] <span class="comment"># 选取K篇作为召回推荐结果</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 根据历史召回结果进行过滤</span></span><br><span class="line">                    history_table = conn.table(<span class="string">'history_recall'</span>)</span><br><span class="line">                    history_article_data = history_table.cells(<span class="string">b"reco:his:%s"</span> % data[<span class="string">"param"</span>][<span class="string">"userId"</span>].encode(), <span class="string">b"channel:%d"</span> % data[<span class="string">"channelId"</span>])</span><br><span class="line">                    <span class="comment"># 将多个版本都加入历史文章ID列表</span></span><br><span class="line">                    history_article = []</span><br><span class="line">                    <span class="keyword">if</span> len(history_article_data) &gt;<span class="number">1</span>:</span><br><span class="line">                        <span class="keyword">for</span> article <span class="keyword">in</span> history_article_data[:<span class="number">-1</span>]:</span><br><span class="line">                            history_article.extend(eval(article))</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        history_article = []</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 过滤history_article</span></span><br><span class="line">                    recall_article = list(set(similar_article_topk) - set(history_article))</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 如果有召回结果,按频道分别存入召回结果表及历史召回结果表</span></span><br><span class="line">                    <span class="keyword">if</span> recall_article:</span><br><span class="line">                        recall_table = conn.table(<span class="string">"cb_recall"</span>)</span><br><span class="line">                        recall_table.put(<span class="string">b"recall:user:%s"</span> % data[<span class="string">"param"</span>][<span class="string">"userId"</span>].encode(), &#123;<span class="string">b"online:%d"</span> % data[<span class="string">"channelId"</span>]: str(recall_article).encode()&#125;)</span><br><span class="line">                        history_table.put(<span class="string">b"reco:his:%s"</span> % data[<span class="string">"param"</span>][<span class="string">"userId"</span>].encode(), &#123;<span class="string">b"channel:%d"</span> % data[<span class="string">"channelId"</span>]: str(recall_article).encode()&#125;)</span><br><span class="line"></span><br><span class="line">                conn.close()</span><br></pre></td></tr></table></figure></p><p>可以根据用户 ID 和频道 ID 来查询召回结果<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">028</span>:<span class="number">0</span>&gt; get <span class="string">'cb_recall'</span>, <span class="string">'recall:user:2'</span></span><br><span class="line">COLUMN                     CELL                                                                        </span><br><span class="line">online:<span class="number">13</span>                    timestamp=<span class="number">1558041569201</span>, value=[<span class="number">141431</span>,<span class="number">14381</span>, <span class="number">17966</span>, <span class="number">17454</span>, <span class="number">14125</span>, <span class="number">16174</span>]</span><br></pre></td></tr></table></figure></p><p>创建 <code>online_update.py</code>，加入基于内容的在线召回逻辑<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="variable">__name__</span> == <span class="string">'__main__'</span>:</span><br><span class="line">    ore = OnlineRecall()</span><br><span class="line">    ore.update_content_recall()</span><br><span class="line">    stream_sc.start()</span><br><span class="line">    <span class="variable">_ONE_DAY_IN_SECONDS</span> = <span class="number">60</span> * <span class="number">60</span> * <span class="number">24</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="built_in">time</span>.<span class="built_in">sleep</span>(<span class="variable">_ONE_DAY_IN_SECONDS</span>)</span><br><span class="line">    except KeyboardInterrupt:</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure></p><p>利用 Supervisor 进行进程管理，并开启实时运行，配置如下，其中 environment 需要指定运行所需环境<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[program:online]</span></span><br><span class="line"><span class="attr">environment</span>=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python ,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_SUBMIT_ARGS=<span class="string">'--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.2 pyspark-shell'</span></span><br><span class="line"><span class="attr">command</span>=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/reco_sys/<span class="literal">on</span>line/<span class="literal">on</span>line_update.py</span><br><span class="line"><span class="attr">directory</span>=/root/toutiao_project/reco_sys/<span class="literal">on</span>line</span><br><span class="line"><span class="attr">user</span>=root</span><br><span class="line"><span class="attr">autorestart</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">redirect_stderr</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">stdout_logfile</span>=/root/logs/<span class="literal">on</span>linesuper.log</span><br><span class="line"><span class="attr">loglevel</span>=info</span><br><span class="line"><span class="attr">stopsignal</span>=KILL</span><br><span class="line"><span class="attr">stopasgroup</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">killasgroup</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上篇文章中，我们实现了基于模型的离线召回，属于基于协同过滤的召回算法。接下来，本文就讲一下另一个经典的召回方式，那就是如何实现基于内容的离线召回。相比于协同过滤来说，基于内容的召回会简单很多，主要思路就是召回用户点击过的文章的相似文章，通常也被叫做 u2i2i。&lt;/p&gt;

      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之8、基于模型的离线召回.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B8%E3%80%81%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A6%BB%E7%BA%BF%E5%8F%AC%E5%9B%9E/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之8、基于模型的离线召回/</id>
    <published>2019-12-05T13:21:28.000Z</published>
    <updated>2020-01-10T02:25:40.329Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们完成了所有的数据准备，接下来，就要开始召回阶段的工作了，可以做离线召回，也可以做在线召回，召回算法通常包括基于内容的召回和基于协同过滤的召回。ALS 模型是一种基于模型的协同过滤召回算法，本文将通过 ALS 模型实现离线召回。</p><p>首先，我们在 Hbase 中创建召回结果表 cb_recall，这里用不同列族来存储不同方式的召回结果，其中 als 表示模型召回，content 表示内容召回，online 表示在线召回。通过设置多个版本来存储多次召回结果，通过设置生存期来清除长时间未被使用的召回结果。<br><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'cb_recall'</span>, &#123;<span class="function"><span class="params">NAME</span>=&gt;</span><span class="string">'als'</span>, <span class="function"><span class="params">TTL</span>=&gt;</span><span class="number">7776000</span>, <span class="function"><span class="params">VERSIONS</span>=&gt;</span><span class="number">999999</span>&#125;</span><br><span class="line">alter <span class="string">'cb_recall'</span>, &#123;<span class="function"><span class="params">NAME</span>=&gt;</span><span class="string">'content'</span>, <span class="function"><span class="params">TTL</span>=&gt;</span><span class="number">7776000</span>, <span class="function"><span class="params">VERSIONS</span>=&gt;</span><span class="number">999999</span>&#125;</span><br><span class="line">alter <span class="string">'cb_recall'</span>, &#123;<span class="function"><span class="params">NAME</span>=&gt;</span><span class="string">'online'</span>, <span class="function"><span class="params">TTL</span>=&gt;</span><span class="number">7776000</span>, <span class="function"><span class="params">VERSIONS</span>=&gt;</span><span class="number">999999</span>&#125;</span><br><span class="line"></span><br><span class="line"># 插入样例</span><br><span class="line">put <span class="string">'cb_recall'</span>, <span class="string">'recall:user:5'</span>, <span class="string">'als:2'</span>,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">put <span class="string">'cb_recall'</span>, <span class="string">'recall:user:2'</span>, <span class="string">'content:1'</span>,[<span class="number">45</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">289</span>,<span class="number">11</span>,<span class="number">65</span>,<span class="number">52</span>,<span class="number">109</span>,<span class="number">8</span>]</span><br><span class="line">put <span class="string">'cb_recall'</span>, <span class="string">'recall:user:2'</span>, <span class="string">'online:2'</span>,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br></pre></td></tr></table></figure></p><p>在 Hive 中建立外部表，用于离线分析<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> cb_recall_hbase</span><br><span class="line">(</span><br><span class="line">    user_id <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"userID"</span>,</span><br><span class="line">    als     <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">ARRAY</span>&lt;<span class="built_in">BIGINT</span>&gt;&gt; <span class="keyword">comment</span> <span class="string">"als recall"</span>,</span><br><span class="line">    <span class="keyword">content</span> <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">ARRAY</span>&lt;<span class="built_in">BIGINT</span>&gt;&gt; <span class="keyword">comment</span> <span class="string">"content recall"</span>,</span><br><span class="line">    <span class="keyword">online</span>  <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">ARRAY</span>&lt;<span class="built_in">BIGINT</span>&gt;&gt; <span class="keyword">comment</span> <span class="string">"online recall"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"user recall table"</span></span><br><span class="line">    <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line">        <span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,als:,content:,online:"</span>)</span><br><span class="line">    TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"cb_recall"</span>);</span><br></pre></td></tr></table></figure></p><p>接着，在 Hbase 中创建历史召回结果表，用于过滤已历史召回结果，避免重复推荐。这里同样设置了多个版本来存储多次历史召回结果，设置了生存期来清除很长时间以前的历史召回结果<br><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'history_recall'</span>, &#123;<span class="function"><span class="params">NAME</span>=&gt;</span><span class="string">'channel'</span>, <span class="function"><span class="params">TTL</span>=&gt;</span><span class="number">7776000</span>, <span class="function"><span class="params">VERSIONS</span>=&gt;</span><span class="number">999999</span>&#125;</span><br><span class="line"></span><br><span class="line"># 插入示例</span><br><span class="line">put <span class="string">'history_recall'</span>, <span class="string">'recall:user:5'</span>, <span class="string">'als:1'</span>,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">put <span class="string">'history_recall'</span>, <span class="string">'recall:user:5'</span>, <span class="string">'als:1'</span>,[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">put <span class="string">'history_recall'</span>, <span class="string">'recall:user:5'</span>, <span class="string">'als:1'</span>,[<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br></pre></td></tr></table></figure></p><h1 id="ALS-原理"><a href="#ALS-原理" class="headerlink" title="ALS 原理"></a>ALS 原理</h1><p><img src="https://upload-images.jianshu.io/upload_images/12790782-e608ec3e758c49d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我们先简单了解一下 ALS 模型，上图为用户和物品的关系矩阵，其中，每一行代表一个用户，每一列代表一个物品。蓝色元素代表用户查看过该物品，灰色元素代表用户未查看过该物品，假设有 m 个用户，n 个物品，为了得到用户对物品的评分，我们可以利用矩阵分解将原本较大的稀疏矩阵拆分成两个较小的稠密矩阵，即 m x k 维的用户隐含矩阵和 k x n 维的物品隐含矩阵，如下所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-26825ddee138897c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>其中，用户矩阵的每一行就包括了影响用户偏好的 k 个隐含因子，物品矩阵的每一列就包括了影响物品内容的 k 个隐含因子。这里用户矩阵和物品矩阵中每个隐含因子的值就是利用交替最小二乘（Alternating Least Squares，ALS）优化算法计算而得的，所以叫做 ALS 模型。接下来，再将用户矩阵和物品矩阵相乘即可得到用户对物品的  m x n  维的评分矩阵，其中就包括每一个用户对每一个物品的评分了，进而可以根据评分进行推荐。</p><h1 id="ALS-模型训练和预测"><a href="#ALS-模型训练和预测" class="headerlink" title="ALS 模型训练和预测"></a>ALS 模型训练和预测</h1><p>Spark 已经实现了 ALS 模型，我们可以直接调用。首先，我们读取用户历史点击行为，构造训练集数据，其中只需要包括用户 ID、文章 ID 以及是否点击<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql('use profile')</span><br><span class="line">user_article_basic = spark.sql(<span class="string">"select user_id, article_id, clicked from user_article_basic"</span>)</span><br></pre></td></tr></table></figure></p><p><code>user_article_basic</code> 结果如下所示，其中 clicked 表示用户对文章是否发生过点击</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-abb6c53b98c88373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我们需要将 clicked 由 boolean 类型转成 int 类型，即 true 为 1，false 为 0<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_boolean_int</span><span class="params">(row)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">return</span> row.user_id, row.article_id, int(row.clicked)</span><br><span class="line"></span><br><span class="line">user_article_basic = user_article_basic.rdd.map(convert_boolean_int).toDF([<span class="string">'user_id'</span>, <span class="string">'article_id'</span>, <span class="string">'clicked'</span>])</span><br></pre></td></tr></table></figure></p><p><code>user_article_basic</code> 结果如下所示，clicked 已经是 int 类型</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-f0075684b769f328.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>另外，Spark 的 ALS 模型还要求输入的用户 ID 和文章 ID 必须是从 1 开始递增的连续数字，所以需要利用 Spark 的 <code>Pipeline</code> 和 <code>StringIndexer</code>，将用户 ID 和文章 ID 建立从 1 开始递增的索引<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml import Pipeline</span><br><span class="line"></span><br><span class="line">user_indexer = StringIndexer(<span class="attribute">inputCol</span>=<span class="string">'user_id'</span>, <span class="attribute">outputCol</span>=<span class="string">'als_user_id'</span>)</span><br><span class="line">article_indexer = StringIndexer(<span class="attribute">inputCol</span>=<span class="string">'article_id'</span>, <span class="attribute">outputCol</span>=<span class="string">'als_article_id'</span>)</span><br><span class="line">pip = Pipeline(stages=[user_indexer, article_indexer])</span><br><span class="line">pip_model = pip.fit(user_article_basic)</span><br><span class="line">als_user_article = pip_model.transform(user_article_basic)</span><br></pre></td></tr></table></figure></p><p><code>als_user_article</code> 结果如下所示，als_user_id 和 als_article_id 即是 ALS 模型所需的用户索引和文章索引</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-98ff5968a2cdd731.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，将用户行为数据中的 als_user_id, als_article_id, clicked 三列作为训练集，对 ALS 模型进行训练，并利用 ALS 模型计算用户对文章的偏好得分，这里可以指定为每个用户保留偏好得分最高的 K 篇文章<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.recommendation import ALS</span><br><span class="line"></span><br><span class="line">top_k = 100</span><br><span class="line">als = ALS(<span class="attribute">userCol</span>=<span class="string">'als_user_id'</span>, <span class="attribute">itemCol</span>=<span class="string">'als_article_id'</span>, <span class="attribute">ratingCol</span>=<span class="string">'clicked'</span>)</span><br><span class="line">als_model = als.fit(als_user_article)</span><br><span class="line"></span><br><span class="line">recall_res = als_model.recommendForAllUsers(top_k)</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，其中，als_user_id 为用户索引，recommendations 为每个用户的推荐列表，包括文章索引和偏好得分，如 [[255,0.1], [10,0.08], …]</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-a966583c33d30c06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="预测结果处理"><a href="#预测结果处理" class="headerlink" title="预测结果处理"></a>预测结果处理</h1><p>接着，我们要将推荐结果中的用户索引和文章索引还原为用户 ID 和文章 ID，这就需要建立用户 ID 与用户索引的映射及文章 ID 与文章索引的映射，可以将前面包含用户索引和文章索引的用户行为数据 <code>als_user_article</code> 分别按照 user_id 和 article_id 分组，即可得到用户 ID 与用户索引的映射以及文章 ID 与文章索引的映射<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">user_real_index</span> = als_user_article.groupBy([<span class="string">'user_id'</span>]).max(<span class="string">'als_user_id'</span>).withColumnRenamed(<span class="string">'max(als_user_id)'</span>, <span class="string">'als_user_id'</span>)</span><br><span class="line"><span class="attr">article_real_index</span> = als_user_article.groupBy([<span class="string">'article_id'</span>]).max(<span class="string">'als_article_id'</span>).withColumnRenamed(<span class="string">'max(als_article_id)'</span>, <span class="string">'als_article_id'</span>)</span><br></pre></td></tr></table></figure></p><p><code>user_real_index</code> 结果如下所示，即用户 ID 与用户索引的映射</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4517cce545326e63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>再利用 <code>als_user_id</code> 将 <code>recall_res</code> 和 <code>user_real_index</code> 进行连接，加入用户 ID<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recall_res = recall_res.<span class="keyword">join</span>(user_real_index, <span class="keyword">on</span>=[<span class="string">'als_user_id'</span>], how=<span class="string">'left'</span>).<span class="keyword">select</span>([<span class="string">'als_user_id'</span>, <span class="string">'recommendations'</span>, <span class="string">'user_id'</span>])</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，得到用户索引，推荐列表和用户 ID</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-6939d47b670c5716.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">存储</p><p>接下来，我们要构建出用户和文章的关系，利用 <code>explode()</code> 方法将 recommendations 中的每篇文章都转换为单独的一条记录，并只保留用户 ID 和文章索引这两列数据<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import pyspark<span class="selector-class">.sql</span><span class="selector-class">.functions</span> as F</span><br><span class="line"></span><br><span class="line">recall_res = recall_res.withColumn(<span class="string">'als_article_id'</span>, F.explode(<span class="string">'recommendations'</span>)).drop(<span class="string">'recommendations'</span>).select([<span class="string">'user_id'</span>, <span class="string">'als_article_id'</span>])</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，als_article_id 包括文章索引和偏好得分</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4f010be4233f8f6d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我们将 als_article_id 中的偏好得分去除，只保留文章索引<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_index</span><span class="params">(row)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">return</span> row.user_id, row.als_article_id[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">recall_res = recall_res.rdd.map(get_article_index).toDF([<span class="string">'user_id'</span>, <span class="string">'als_article_id'</span>])</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，得到用户 ID 和文章索引</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-251a47b794797f2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>之前我们将文章 ID 和文章索引保存到了 <code>article_real_index</code>，这里利用 als_article_id 将 recall_res 和 <code>article_real_index</code> 进行连接，得到文章 ID<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recall_res = recall_res.<span class="keyword">join</span>(article_real_index, <span class="keyword">on</span>=[<span class="string">'als_article_id'</span>], how=<span class="string">'left'</span>).<span class="keyword">select</span>([<span class="string">'user_id'</span>, <span class="string">'article_id'</span>])</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，得到用户 ID 和要向其推荐的文章 ID</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-d42ac7e8e540cdb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="推荐结果存储"><a href="#推荐结果存储" class="headerlink" title="推荐结果存储"></a>推荐结果存储</h1><p>为了方便查询，我们需要将推荐结果按频道分别进行存储。首先，读取文章完整信息，得到频道 ID<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql('use article')</span><br><span class="line">article_data = spark.sql(<span class="string">"select article_id, channel_id from article_data"</span>)</span><br></pre></td></tr></table></figure></p><p>利用 article_id 将 <code>recall_res</code> 和 <code>article_data</code> 进行连接，在推荐结果中加入频道 ID<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">recall_res</span> = recall_res.join(article_data, <span class="literal">on</span>=[<span class="string">'article_id'</span>], how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，推荐结果加入了频道 ID</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-d23360fcb5da1901.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>将推荐结果按照 user_id 和 channel_id 进行分组，利用 <code>collect_list()</code> 方法将文章 ID 合并为文章列表<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">recall_res</span> = recall_res.groupBy([<span class="string">'user_id'</span>, <span class="string">'channel_id'</span>]).agg(F.collect_list(<span class="string">'article_id'</span>)).withColumnRenamed(<span class="string">'collect_list(article_id)'</span>, <span class="string">'article_list'</span>)</span><br></pre></td></tr></table></figure></p><p><code>recall_res</code> 结果如下所示，article_list 为某用户在某频道下的推荐文章列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-29f5ad6c579f4ee4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>最后，将推荐结果按频道分别存入召回结果表 cb_recall 及历史召回结果表 history_recall。注意，在保存新的召回结果之前需要根据历史召回结果进行过滤，防止重复推荐<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">recall_res = recall_res.dropna()</span><br><span class="line">recall_res.foreachPartition(save_offline_recall_hbase)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_offline_recall_hbase</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="string">"""ALS模型离线召回结果存储</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">import</span> happybase</span><br><span class="line">    pool = happybase.ConnectionPool(size=<span class="number">10</span>, host=<span class="string">'hadoop-master'</span>, port=<span class="number">9090</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">            <span class="comment"># 读取历史召回结果表</span></span><br><span class="line">            history_table = conn.table(<span class="string">'history_recall'</span>)</span><br><span class="line">            <span class="comment"># 读取包含多个版本的历史召回结果</span></span><br><span class="line">            history_article_data = history_table.cells(<span class="string">'reco:his:&#123;&#125;'</span>.format(row.user_id).encode(),</span><br><span class="line">                                       <span class="string">'channel:&#123;&#125;'</span>.format(row.channel_id).encode())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 合并多个版本历史召回结果</span></span><br><span class="line">            history_article = []（比如有的用户会比较怀旧）</span><br><span class="line">            <span class="keyword">if</span> len(history_article_data) &gt;= <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">for</span> article <span class="keyword">in</span> history_article_data[:<span class="number">-1</span>]:</span><br><span class="line">                    history_article.extend(eval(article))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                history_article = []</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 过滤history_article</span></span><br><span class="line">            recall_article = list(set(row.article_list) - set(history_article))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> recall_article:</span><br><span class="line">                table = conn.table(<span class="string">'cb_recall'</span>)</span><br><span class="line">                table.put(<span class="string">'recall:user:&#123;&#125;'</span>.format(row.user_id).encode(), &#123;<span class="string">'als:&#123;&#125;'</span>.format(row.channel_id).encode(): str(recall_article).encode()&#125;)</span><br><span class="line">                history_table.put(<span class="string">"reco:his:&#123;&#125;"</span>.format(row.user_id).encode(), &#123;<span class="string">'channel:&#123;&#125;'</span>.format(row.channel_id): str(recall_article).encode()&#125;)</span><br><span class="line">            conn.close()</span><br></pre></td></tr></table></figure></p><p>可以根据用户 ID 和频道 ID 来查询召回结果<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">028</span>:<span class="number">0</span>&gt; get <span class="string">'cb_recall'</span>, <span class="string">'recall:user:2'</span></span><br><span class="line">COLUMN                     CELL                                                                        </span><br><span class="line">als:<span class="number">13</span>                    timestamp=<span class="number">1558041569201</span>, value=[<span class="number">141431</span>,<span class="number">14381</span>, <span class="number">17966</span>, <span class="number">17454</span>, <span class="number">14125</span>, <span class="number">16174</span>]</span><br></pre></td></tr></table></figure></p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>在用户召回方法 <code>update_user_recall()</code> 中，增加基于模型的离线召回方法 <code>update_content_recall()</code>，首先读取用户行为日志，进行数据预处理，构建训练集，接着对 ALS 模型进行训练和预测，最后对预测出的推荐结果进行解析并按频道分别存入召回结果表和历史召回结果表<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_user_recall</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用户的频道推荐召回结果更新逻辑</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ur = UpdateRecall(<span class="number">500</span>)</span><br><span class="line">    ur.update_als_recall()</span><br></pre></td></tr></table></figure></p><p>添加定时更新用户召回结果的任务，每隔 3 小时运行一次<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from apscheduler.schedulers.blocking <span class="built_in">import</span> BlockingScheduler</span><br><span class="line">from apscheduler.executors.pool <span class="built_in">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建scheduler，多进程执行</span></span><br><span class="line"><span class="attr">executors</span> = &#123;</span><br><span class="line">    'default': ProcessPoolExecutor(<span class="number">3</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">scheduler</span> = BlockingScheduler(<span class="attr">executors=executors)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_article_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=1)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_user_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=2)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行用户召回更新的任务，每隔3小时运行一次</span></span><br><span class="line">scheduler.add_job(update_user_recall, <span class="attr">trigger='interval',</span> <span class="attr">hours=3)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行特征中心平台的任务，每隔4小时更新一次</span></span><br><span class="line">scheduler.add_job(update_ctr_feature, <span class="attr">trigger='interval',</span> <span class="attr">hours=4)</span></span><br><span class="line"></span><br><span class="line">scheduler.start()</span><br></pre></td></tr></table></figure></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面我们完成了所有的数据准备，接下来，就要开始召回阶段的工作了，可以做离线召回，也可以做在线召回，召回算法通常包括基于内容的召回和基于协同过滤的召回。ALS 模型是一种基于模型的协同过滤召回算法，本文将通过 ALS 模型实现离线召回。&lt;/p&gt;
&lt;p&gt;首先，我们在 Hbase
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之7、构建离线文章特征和用户特征.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B7%E3%80%81%E6%9E%84%E5%BB%BA%E7%A6%BB%E7%BA%BF%E6%96%87%E7%AB%A0%E7%89%B9%E5%BE%81%E5%92%8C%E7%94%A8%E6%88%B7%E7%89%B9%E5%BE%81/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/文章推荐系统》系列之7、构建离线文章特征和用户特征/</id>
    <published>2019-12-05T13:21:27.000Z</published>
    <updated>2020-01-10T02:24:21.509Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们完成了文章画像和用户画像的构建，画像数据主要是提供给召回阶段的各种召回算法使用。接下来，我们还要为排序阶段的各种排序模型做数据准备，通过特征工程将画像数据进一步加工为特征数据，以供排序模型直接使用。</p><p>我们可以将特征数据存储到 Hbase 中，这里我们先在 Hbase 中创建好 ctr_feature_article 表 和 ctr_feature_user 表，分别存储文章特征数据和用户特征数据<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 文章特征表</span></span><br><span class="line"><span class="keyword">create</span> <span class="string">'ctr_feature_article'</span>, <span class="string">'article'</span></span><br><span class="line"><span class="comment">-- 如 article:13401 timestamp=1555635749357, value=[18.0,0.08196639249252607,0.11217275332895373,0.1353835167902181,0.16086650318453152,0.16356418791892943,0.16740082750337945,0.18091837445730974,0.1907214431716628,0.2........................-0.04634634410271921,-0.06451843378804649,-0.021564142420785692,0.10212902152136256]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 用户特征表</span></span><br><span class="line"><span class="keyword">create</span> <span class="string">'ctr_feature_user'</span>, <span class="string">'channel'</span></span><br><span class="line"><span class="comment">-- 如 4 column=channel:13, timestamp=1555647172980, value=[]</span></span><br></pre></td></tr></table></figure></p><h1 id="构建文章特征"><a href="#构建文章特征" class="headerlink" title="构建文章特征"></a>构建文章特征</h1><p>文章特征包括文章关键词权重、文章频道以及文章向量，我们首先读取文章画像<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_profile = spark.sql(<span class="string">"select * from article_profile"</span>)</span><br></pre></td></tr></table></figure></p><p>在文章画像中筛选出权重最高的 K 个关键词的权重，作为文章关键词的权重向量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">article_profile_to_feature</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        article_weights = sorted(row.keywords.values())[:<span class="number">10</span>]</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        article_weights = [<span class="number">0.0</span>] * <span class="number">10</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, row.channel_id, article_weights</span><br><span class="line">article_profile = article_profile.rdd.map(article_profile_to_feature).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'weights'</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_profile</code> 结果如下所示，weights 即为文章关键词的权重向量</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-bdc45d1e743a3c1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，读取文章向量信息，再将频道 ID 和文章向量加入进来，利用 article_id 将 article_profile 和 article_vector 进行内连接，并将 weights 和 articlevector 转为 <code>vector</code> 类型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">article_vector = spark.sql(<span class="string">"select * from article_vector"</span>)</span><br><span class="line">article_feature = article_profile.join(article_vector, on=[<span class="string">'article_id'</span>], how=<span class="string">'inner'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_to_vector</span><span class="params">(row)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> row.article_id, row.channel_id, Vectors.dense(row.weights), Vectors.dense(row.articlevector)</span><br><span class="line"></span><br><span class="line">article_feature = article_feature.rdd.map(feature_to_vector).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'weights'</span>, <span class="string">'articlevector'</span>])</span><br></pre></td></tr></table></figure></p><p>最后，我们将 channel_id, weights, articlevector 合并为一列 features 即可（通常 channel_id 可以进行 one-hot 编码，我们这里先省略了）<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line">columns = [<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'weights'</span>, <span class="string">'articlevector'</span>]</span><br><span class="line">article_feature = VectorAssembler().setInputCols(columns[<span class="number">1</span>:<span class="number">4</span>]).setOutputCol(<span class="string">"features"</span>).transform(article_feature)</span><br></pre></td></tr></table></figure></p><p><code>article_feature</code> 结果如下所示，features 就是我们准备好的文章特征</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-1d9a977180832173.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>最后，将文章特征结果保存到 Hbase 中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_article_feature_to_hbase</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> happybase</span><br><span class="line">    pool = happybase.ConnectionPool(size=<span class="number">10</span>, host=<span class="string">'hadoop-master'</span>)</span><br><span class="line">    <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">        table = conn.table(<span class="string">'ctr_feature_article'</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">            table.put(<span class="string">'&#123;&#125;'</span>.format(row.article_id).encode(),</span><br><span class="line">                     &#123;<span class="string">'article:&#123;&#125;'</span>.format(row.article_id).encode(): str(row.features).encode()&#125;)</span><br><span class="line"></span><br><span class="line">article_feature.foreachPartition(save_article_feature_to_hbase)</span><br></pre></td></tr></table></figure></p><h1 id="构建用户特征"><a href="#构建用户特征" class="headerlink" title="构建用户特征"></a>构建用户特征</h1><p>由于用户在不同频道的偏好差异较大，所以我们要计算用户在每个频道的特征。首先读取用户画像，将空值列删除<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use profile"</span>)</span><br><span class="line"></span><br><span class="line">user_profile_hbase = spark.sql(<span class="string">"select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase"</span>)</span><br></pre></td></tr></table></figure></p><p><code>user_profile_hbase</code> 结果如下所示，其中 article_partial 为用户标签及权重，如 ([‘18:vars’: 0.2, ‘18: python’:0.2, …], [‘19:java’: 0.2, ‘19: javascript’:0.2, …], …) 表示某个用户在 18 号频道的标签包括 var、python 等，在 19 号频道的标签包括 java、javascript等。</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-655ec8ea6979ab94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>由于 gender 和 birthday 两列空值较多，我们将这两列去除（实际场景中也可以根据数据情况选择填充）<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除空值列</span></span><br><span class="line"><span class="attr">user_profile_hbase</span> = user_profile_hbase.drop(<span class="string">'env'</span>, <span class="string">'birthday'</span>, <span class="string">'gender'</span>)</span><br></pre></td></tr></table></figure></p><p>提取用户 ID，获取 user_id 列的内容中 <code>:</code> 后面的数值即为用户 ID<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_user_id(<span class="built_in">row</span>):</span><br><span class="line">    <span class="built_in">return</span> int(<span class="built_in">row</span>.user_id.<span class="built_in">split</span>(<span class="string">":"</span>)[<span class="number">1</span>]), <span class="built_in">row</span>.article_partial</span><br><span class="line"></span><br><span class="line">user_profile_hbase = user_profile_hbase.rdd.<span class="built_in">map</span>(get_user_id)</span><br></pre></td></tr></table></figure></p><p>将 <code>user_profile_hbase</code> 转为 DataFrame 类型<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.types import *</span><br><span class="line"></span><br><span class="line">_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"user_id"</span>, LongType()),</span><br><span class="line">    StructField(<span class="string">"weights"</span>, MapType(StringType(), DoubleType()))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">user_profile_hbase = spark.createDataFrame(user_profile_hbase, schema=_schema)</span><br></pre></td></tr></table></figure></p><p>接着，将每个频道内权重最高的 K 个标签的权重作为用户标签权重向量<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def frature_preprocess(<span class="built_in">row</span>):</span><br><span class="line"></span><br><span class="line">    from pyspark.ml.linalg import Vectors</span><br><span class="line"></span><br><span class="line">    user_weights = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">26</span>):</span><br><span class="line">        try:</span><br><span class="line">            channel_weights = sorted([<span class="built_in">row</span>.weights[<span class="built_in">key</span>] <span class="keyword">for</span> <span class="built_in">key</span> <span class="keyword">in</span> <span class="built_in">row</span>.weights.keys() <span class="keyword">if</span> <span class="built_in">key</span>.<span class="built_in">split</span>(':')[<span class="number">0</span>] == str(i)])[:<span class="number">10</span>]</span><br><span class="line">            user_weights.<span class="built_in">append</span>(channel_weights)</span><br><span class="line">        except:</span><br><span class="line">            user_weights.<span class="built_in">append</span>([<span class="number">0.0</span>] * <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">row</span>.user_id, user_weights</span><br><span class="line"></span><br><span class="line">user_features = user_profile_hbase.rdd.<span class="built_in">map</span>(frature_preprocess).collect()</span><br></pre></td></tr></table></figure></p><p><code>user_features</code> 就是我们计算好的用户特征，数据结构类似 (10, [[0.2, 2.1, …], [0.2, 2.1, …]], …)，其中元组第一个元素 10 即为用户 ID，第二个元素是长度为 25 的用户频道标签权重列表，列表中每个元素是长度为 K 的用户标签权重列表，代表用户在某个频道下的标签权重向量。</p><p>最后，将用户特征结果保存到 Hbase，利用 Spark 的 <code>batch()</code> 方法，按频道批量存储用户特征<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import happybase</span><br><span class="line"></span><br><span class="line"># 批量插入Hbase数据库中</span><br><span class="line">pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)</span><br><span class="line">with pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">    ctr_feature = conn.<span class="keyword">table</span>('ctr_feature_user')</span><br><span class="line">    with ctr_feature.batch(transaction=True) <span class="keyword">as</span> b:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">range</span>(len(user_features)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="keyword">range</span>(25):</span><br><span class="line">                b.put(<span class="string">"&#123;&#125;"</span>.<span class="keyword">format</span>(res[i][0]).<span class="keyword">encode</span>(),&#123;<span class="string">"channel:&#123;&#125;"</span>.<span class="keyword">format</span>(j+1).<span class="keyword">encode</span>(): str(res[i][1][j]).<span class="keyword">encode</span>()&#125;)</span><br><span class="line">    conn.<span class="keyword">close</span>()</span><br></pre></td></tr></table></figure></p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>定义文章特征和用户特征的离线更新方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_ctr_feature</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    更新文章特征和用户特征</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fp = FeaturePlatform()</span><br><span class="line">    fp.update_user_ctr_feature_to_hbase()</span><br><span class="line">    fp.update_article_ctr_feature_to_hbase()</span><br></pre></td></tr></table></figure></p><p>在 Apscheduler 中添加定时更新文章特征和用户特征的任务，每隔 4 小时运行一次<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from apscheduler.schedulers.blocking <span class="built_in">import</span> BlockingScheduler</span><br><span class="line">from apscheduler.executors.pool <span class="built_in">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建scheduler，多进程执行</span></span><br><span class="line"><span class="attr">executors</span> = &#123;</span><br><span class="line">    'default': ProcessPoolExecutor(<span class="number">3</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">scheduler</span> = BlockingScheduler(<span class="attr">executors=executors)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_article_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=1)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_user_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=2)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行特征中心平台的任务，每隔4小时更新一次</span></span><br><span class="line">scheduler.add_job(update_ctr_feature, <span class="attr">trigger='interval',</span> <span class="attr">hours=4)</span></span><br><span class="line"></span><br><span class="line">scheduler.start()</span><br></pre></td></tr></table></figure></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘， 提取码：eakp）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面我们完成了文章画像和用户画像的构建，画像数据主要是提供给召回阶段的各种召回算法使用。接下来，我们还要为排序阶段的各种排序模型做数据准备，通过特征工程将画像数据进一步加工为特征数据，以供排序模型直接使用。&lt;/p&gt;
&lt;p&gt;我们可以将特征数据存储到 Hbase 中，这里我们先
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之6、构建离线用户画像.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B6%E3%80%81%E6%9E%84%E5%BB%BA%E7%A6%BB%E7%BA%BF%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之6、构建离线用户画像/</id>
    <published>2019-12-05T13:21:26.000Z</published>
    <updated>2020-01-10T02:21:53.945Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们完成了文章画像的构建以及文章相似度的计算，接下来，我们就要实现用户画像的构建了。用户画像往往是大型网站的重要模块，基于用户画像不仅可以实现个性化推荐，还可以实现用户分群、精准推送、精准营销以及用户行为预测、商业化转化分析等，为商业决策提供数据支持。通常用户画像包括用户属性信息（性别、年龄、出生日期等）、用户行为信息（浏览、收藏、点赞等）以及环境信息（时间、地理位置等）。</p><h1 id="处理用户行为数据"><a href="#处理用户行为数据" class="headerlink" title="处理用户行为数据"></a>处理用户行为数据</h1><p>在数据准备阶段，我们通过 Flume 已经可以将用户行为数据收集到 Hive 的 user_action 表的 HDFS 路径中，先来看一下这些数据长什么样子，我们读取当天的用户行为数据，注意读取之前要先关联分区<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">_day = time.strftime(<span class="string">"%Y-%m-%d"</span>, time.localtime())</span><br><span class="line">_localions = <span class="string">'/user/hive/warehouse/profile.db/user_action/'</span> + _day</span><br><span class="line"><span class="keyword">if</span> fs.exists(_localions):</span><br><span class="line">    <span class="comment"># 如果有该文件直接关联，捕获关联重复异常</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.spark.sql(<span class="string">"alter table user_action add partition (dt='%s') location '%s'"</span> % (_day, _localions))</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    self.spark.sql(<span class="string">"use profile"</span>)</span><br><span class="line">    user_action = self.spark.sql(<span class="string">"select actionTime, readTime, channelId, param.articleId, param.algorithmCombine, param.action, param.userId from user_action where dt&gt;="</span> + _day)</span><br></pre></td></tr></table></figure></p><p><code>user_action</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-04c3417b7c70dacd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>可以发现，上面的一条记录代表用户对文章的一次行为，但通常我们需要查询某个用户对某篇文章的所有行为，所以，我们要将这里用户对文章的多条行为数据合并为一条，其中包括用户对文章的所有行为。我们需要新建一个 Hive 表 user_article_basic，这张表包括了用户 ID、文章 ID、是否曝光、是否点击、阅读时间等等，随后我们将处理好的用户行为数据存储到此表中<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_article_basic</span><br><span class="line">(</span><br><span class="line">    user_id     <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"userID"</span>,</span><br><span class="line">    action_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"user actions time"</span>,</span><br><span class="line">    article_id  <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"articleid"</span>,</span><br><span class="line">    channel_id  <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    <span class="keyword">shared</span>      <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is shared"</span>,</span><br><span class="line">    clicked     <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is clicked"</span>,</span><br><span class="line">    collected   <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is collected"</span>,</span><br><span class="line">    exposure    <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is exposured"</span>,</span><br><span class="line">    read_time   <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"reading time"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"user_article_basic"</span></span><br><span class="line">    CLUSTERED <span class="keyword">by</span> (user_id) <span class="keyword">into</span> <span class="number">2</span> buckets</span><br><span class="line">    <span class="keyword">STORED</span> <span class="keyword">as</span> textfile</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/profile.db/user_article_basic'</span>;</span><br></pre></td></tr></table></figure></p><p>遍历每一条原始用户行为数据，判断用户对文章的行为，在 user_action_basic 中将该用户与该文章对应的行为设置为 True<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> user_action.<span class="keyword">collect</span>():</span><br><span class="line">    <span class="keyword">def</span> _generate(row):</span><br><span class="line">        _list = []</span><br><span class="line">        <span class="keyword">if</span> row.action == <span class="string">'exposure'</span>:</span><br><span class="line">            <span class="keyword">for</span> article_id in eval(row.articleId):</span><br><span class="line">                # [<span class="string">"user_id"</span>, <span class="string">"action_time"</span>,<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"shared"</span>, <span class="string">"clicked"</span>, <span class="string">"collected"</span>, <span class="string">"exposure"</span>, <span class="string">"read_time"</span>]</span><br><span class="line">                _list.<span class="keyword">append</span>(</span><br><span class="line">                    [row.userId, row.actionTime, article_id, row.channelId, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, row.readTime])</span><br><span class="line">            <span class="keyword">return</span> _list</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">class</span> Temp(object):</span><br><span class="line">                shared = <span class="keyword">False</span></span><br><span class="line">                clicked = <span class="keyword">False</span></span><br><span class="line">                collected = <span class="keyword">False</span></span><br><span class="line">                read_time = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">            _tp = Temp()</span><br><span class="line">            <span class="keyword">if</span> row.action == <span class="string">'click'</span>:</span><br><span class="line">                _tp.clicked = <span class="keyword">True</span></span><br><span class="line">            elif row.action == <span class="string">'share'</span>:</span><br><span class="line">                _tp.shared = <span class="keyword">True</span></span><br><span class="line">            elif row.action == <span class="string">'collect'</span>:</span><br><span class="line">                _tp.collected = <span class="keyword">True</span></span><br><span class="line">            elif row.action == <span class="string">'read'</span>:</span><br><span class="line">                _tp.clicked = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">            _list.<span class="keyword">append</span>(</span><br><span class="line">                [row.userId, row.actionTime, <span class="keyword">int</span>(row.articleId), row.channelId, _tp.shared, _tp.clicked, _tp.collected,</span><br><span class="line">                 <span class="keyword">True</span>, row.readTime])</span><br><span class="line">            <span class="keyword">return</span> _list</span><br><span class="line"></span><br><span class="line">    user_action_basic = user_action.rdd.flatMap(_generate)</span><br><span class="line">    user_action_basic = user_action_basic.toDF(</span><br><span class="line">        [<span class="string">"user_id"</span>, <span class="string">"action_time"</span>, <span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"shared"</span>, <span class="string">"clicked"</span>, <span class="string">"collected"</span>, <span class="string">"exposure"</span>,</span><br><span class="line">         <span class="string">"read_time"</span>])</span><br></pre></td></tr></table></figure></p><p><code>user_action_basic</code> 结果如下所示，这里的一条记录包括了某个用户对某篇文章的所有行为</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-651af8651abfca3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>由于 Hive 目前还不支持 pyspark 的原子性操作，所以 user_article_basic 表的用户行为数据只能全量更新（实际场景中可以选择其他语言或数据库实现）。这里，我们需要将当天的用户行为与 user_action_basic 的历史用户行为进行合并<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">old_data</span> = uup.spark.sql(<span class="string">"select * from user_article_basic"</span>)</span><br><span class="line"><span class="attr">new_data</span> = old_data.unionAll(user_action_basic)</span><br></pre></td></tr></table></figure></p><p>合并后又会产生一个新的问题，那就是用户 ID 和文章 ID 可能重复，因为今天某个用户对某篇文章的记录可能在历史数据中也存在，而 <code>unionAll()</code> 方法并没有去重，这里我们可以按照用户 ID 和文章 ID 进行分组，利用 <code>max()</code> 方法得到 action_time, channel_id, shared, clicked, collected, exposure, read_time 即可，去重后直接存储到 user_article_basic 表中<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">new_data.registerTempTable("temptable")</span><br><span class="line"></span><br><span class="line">self.spark.sql('''<span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_article_basic <span class="keyword">select</span> user_id, <span class="keyword">max</span>(action_time) <span class="keyword">as</span> action_time, </span><br><span class="line">        article_id, <span class="keyword">max</span>(channel_id) <span class="keyword">as</span> channel_id, <span class="keyword">max</span>(<span class="keyword">shared</span>) <span class="keyword">as</span> <span class="keyword">shared</span>, <span class="keyword">max</span>(clicked) <span class="keyword">as</span> clicked, </span><br><span class="line">        <span class="keyword">max</span>(collected) <span class="keyword">as</span> collected, <span class="keyword">max</span>(exposure) <span class="keyword">as</span> exposure, <span class="keyword">max</span>(read_time) <span class="keyword">as</span> read_time <span class="keyword">from</span> temptable </span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span> user_id, article_id<span class="string">''')</span></span><br></pre></td></tr></table></figure></p><p>表 user_article_basic 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-508d84da9f16c36f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="计算用户画像"><a href="#计算用户画像" class="headerlink" title="计算用户画像"></a>计算用户画像</h1><p>我们选择将用户画像存储在 Hbase 中，因为 Hbase 支持原子性操作和快速读取，并且 Hive 也可以通过创建外部表关联到 Hbase，进行离线分析，如果要删除 Hive 外部表的话，对 Hbase 也没有影响。首先，在 Hbase 中创建用户画像表<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">'user_profile'</span>, <span class="string">'basic'</span>,<span class="string">'partial'</span>,<span class="string">'env'</span></span><br></pre></td></tr></table></figure></p><p>在 Hive 中创建 Hbase 外部表，注意字段类型设置为 map<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> user_profile_hbase</span><br><span class="line">(</span><br><span class="line">    user_id         <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"userID"</span>,</span><br><span class="line">    information     <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="keyword">DOUBLE</span>&gt; <span class="keyword">comment</span> <span class="string">"user basic information"</span>,</span><br><span class="line">    article_partial <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="keyword">DOUBLE</span>&gt; <span class="keyword">comment</span> <span class="string">"article partial"</span>,</span><br><span class="line">    env             <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>, <span class="built_in">INT</span>&gt; <span class="keyword">comment</span> <span class="string">"user env"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"user profile table"</span></span><br><span class="line">    <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line">        <span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,basic:,partial:,env:"</span>)</span><br><span class="line">    TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"user_profile"</span>);</span><br></pre></td></tr></table></figure></p><p>创建外部表之后，还需要导入一些依赖包<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp -r /root/bigdata/hbase/<span class="class"><span class="keyword">lib</span>/<span class="title">hbase</span>-*.<span class="title">jar</span> /<span class="title">root</span>/<span class="title">bigdata</span>/<span class="title">spark</span>/<span class="title">jars</span>/</span></span><br><span class="line">cp -r /root/bigdata/hive/<span class="class"><span class="keyword">lib</span>/<span class="title">h</span>*.<span class="title">jar</span> /<span class="title">root</span>/<span class="title">bigdata</span>/<span class="title">spark</span>/<span class="title">jars</span>/</span></span><br></pre></td></tr></table></figure></p><p>接下来，读取处理好的用户行为数据，由于日志中的 channel_id 有可能是来自于推荐频道（0），而不是文章真实的频道，所以这里要将 channel_id 列删除<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use profile"</span>)</span><br><span class="line">user_article_basic = spark.sql(<span class="string">"select * from user_article_basic"</span>).drop('channel_id')</span><br></pre></td></tr></table></figure></p><p>通过文章 ID，将用户行为数据与文章画像数据进行连接，从而得到文章频道 ID 和文章主题词<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">'use article'</span>)</span><br><span class="line">article_topic = spark.sql(<span class="string">"select article_id, channel_id, topics from article_profile"</span>)</span><br><span class="line">user_article_topic = user_article_basic.<span class="keyword">join</span>(article_topic, how=<span class="string">'left'</span>, <span class="keyword">on</span>=[<span class="string">'article_id'</span>])</span><br></pre></td></tr></table></figure></p><p><code>user_article_topic</code> 结果如下图所示，其中 topics 列即为文章主题词列表，如 [‘补码’, ‘字符串’, ‘李白’, …]</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-8c0d357cbea1d7e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，我们需要计算每一个主题词对于用户的权重，所以需要将 topics 列中的每个主题词都拆分为单独的一条记录。可以利用 Spark 的 <code>explode()</code> 方法，达到类似“爆炸”的效果<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import pyspark<span class="selector-class">.sql</span><span class="selector-class">.functions</span> as F</span><br><span class="line"></span><br><span class="line">user_article_topic = user_topic.withColumn(<span class="string">'topic'</span>, F.explode(<span class="string">'topics'</span>)).drop(<span class="string">'topics'</span>)</span><br></pre></td></tr></table></figure></p><p><code>user_article_topic</code> 如下图所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4c1b56aad8ec412a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我们通过用户对哪些文章发生了行为以及该文章有哪些主题词，计算出了用户对哪些主题词发生了行为。这样，我们就可以根据用户对主题词的行为来计算主题词对用户的权重，并且将这些主题词作为用户的标签。那么，用户标签权重的计算公式为：用户标签权重 =（用户行为分值之和）x 时间衰减。其中，时间衰减公式为：时间衰减系数 = 1 / (log(t) + 1)，其中 t 为发生行为的时间距离当前时间的大小</p><p>不同的用户行为对应不同的权重，如下所示</p><div class="table-container"><table><thead><tr><th>用户行为</th><th>分值</th></tr></thead><tbody><tr><td>阅读时间(&lt;1000)</td><td>1</td></tr><tr><td>阅读时间(&gt;=1000)</td><td>2</td></tr><tr><td>收藏</td><td>2</td></tr><tr><td>分享</td><td>3</td></tr><tr><td>点击</td><td>5</td></tr></tbody></table></div><p>计算用户标签及权重，并存储到 Hbase 中 user_profile 表的 partial 列族中。注意，这里我们将频道 ID 和标签一起作为 partial 列族的键存储，这样我们就方便查询不同频道的标签及权重了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_user_label_weights</span><span class="params">(partitions)</span>:</span></span><br><span class="line">    <span class="string">""" 计算用户标签权重</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    action_weight = &#123;</span><br><span class="line">        <span class="string">"read_min"</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">"read_middle"</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">"collect"</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">"share"</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">"click"</span>: <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 循环处理每个用户对应的每个主题词</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partitions:</span><br><span class="line">        <span class="comment"># 计算时间衰减系数</span></span><br><span class="line">        t = datetime.now() - datetime.strptime(row.action_time, <span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">        alpha = <span class="number">1</span> / (np.log(t.days + <span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> row.read_time  == <span class="string">''</span>:</span><br><span class="line">            read_t = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            read_t = int(row.read_time)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算阅读时间的行为分数</span></span><br><span class="line">        read_score = action_weight[<span class="string">'read_middle'</span>] <span class="keyword">if</span> read_t &gt; <span class="number">1000</span> <span class="keyword">else</span> action_weight[<span class="string">'read_min'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算各种行为的权重和并乘以时间衰减系数</span></span><br><span class="line">        weights = alpha * (row.shared * action_weight[<span class="string">'share'</span>] + row.clicked * action_weight[<span class="string">'click'</span>] +</span><br><span class="line">                          row.collected * action_weight[<span class="string">'collect'</span>] + read_score)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新到user_profilehbase表</span></span><br><span class="line">        <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">            table = conn.table(<span class="string">'user_profile'</span>)</span><br><span class="line">            table.put(<span class="string">'user:&#123;&#125;'</span>.format(row.user_id).encode(),</span><br><span class="line">                      &#123;<span class="string">'partial:&#123;&#125;:&#123;&#125;'</span>.format(row.channel_id, row.topic).encode(): json.dumps(</span><br><span class="line">                          weights).encode()&#125;)</span><br><span class="line">            conn.close()</span><br><span class="line"></span><br><span class="line">user_topic.foreachPartition(compute_user_label_weights)</span><br></pre></td></tr></table></figure></p><p>在 Hive 中查询用户标签及权重<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> user_profile_hbase limit <span class="number">1</span>;</span><br><span class="line">OK</span><br><span class="line">user:<span class="number">1</span>  &#123;<span class="string">"birthday"</span>:<span class="number">0.0</span>,<span class="string">"gender"</span>:<span class="literal">null</span>&#125;  &#123;<span class="string">"18:##"</span>:<span class="number">0.25704484358604845</span>,<span class="string">"18:&amp;#"</span>:<span class="number">0.25704484358604845</span>,<span class="string">"18:+++"</span>:<span class="number">0.23934588700996243</span>,<span class="string">"18:+++++"</span>:<span class="number">0.23934588700996243</span>,<span class="string">"18:AAA"</span>:<span class="number">0.2747964402379244</span>,<span class="string">"18:Animal"</span>:<span class="number">0.2747964402379244</span>,<span class="string">"18:Author"</span>:<span class="number">0.2747964402379244</span>,<span class="string">"18:BASE"</span>:<span class="number">0.23934588700996243</span>,<span class="string">"18:BBQ"</span>:<span class="number">0.23934588700996243</span>,<span class="string">"18:Blueprint"</span>:<span class="number">1.6487786414275463</span>,<span class="string">"18:Code"</span>:<span class="number">0.23934588700996243</span>,<span class="string">"18:DIR......</span></span><br></pre></td></tr></table></figure></p><p>接下来，要将用户属性信息加入到用户画像中。读取用户基础信息，存储到用户画像表的 basic 列族即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_user_info</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    更新用户画像的属性信息</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    spark.sql(<span class="string">"use toutiao"</span>)</span><br><span class="line">    user_basic = spark.sql(<span class="string">"select user_id, gender, birthday from user_profile"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">udapte_user_basic</span><span class="params">(partition)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> happybase</span><br><span class="line">        <span class="comment">#  用于读取hbase缓存结果配置</span></span><br><span class="line">        pool = happybase.ConnectionPool(size=<span class="number">10</span>, host=<span class="string">'172.17.0.134'</span>, port=<span class="number">9090</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">            <span class="keyword">from</span> datetime <span class="keyword">import</span> date</span><br><span class="line">            age = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> row.birthday != <span class="string">'null'</span>:</span><br><span class="line">                born = datetime.strptime(row.birthday, <span class="string">'%Y-%m-%d'</span>)</span><br><span class="line">                today = date.today()</span><br><span class="line">                age = today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">                table = conn.table(<span class="string">'user_profile'</span>)</span><br><span class="line">                table.put(<span class="string">'user:&#123;&#125;'</span>.format(row.user_id).encode(),</span><br><span class="line">                          &#123;<span class="string">'basic:gender'</span>.encode(): json.dumps(row.gender).encode()&#125;)</span><br><span class="line">                table.put(<span class="string">'user:&#123;&#125;'</span>.format(row.user_id).encode(),</span><br><span class="line">                          &#123;<span class="string">'basic:birthday'</span>.encode(): json.dumps(age).encode()&#125;)</span><br><span class="line">                conn.close()</span><br><span class="line"></span><br><span class="line">    user_basic.foreachPartition(udapte_user_basic)</span><br></pre></td></tr></table></figure></p><p>到这里，我们的用户画像就计算完成了。</p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>定义更新用户画像方法，首先处理用户行为日志，拆分文章主题词，接着计算用户标签的权重，最后再将用户属性信息加入到用户画像中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_user_profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定时更新用户画像的逻辑</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    up = UpdateUserProfile()</span><br><span class="line">    <span class="keyword">if</span> up.update_user_action_basic():</span><br><span class="line">        up.update_user_label()</span><br><span class="line">        up.update_user_info()</span><br></pre></td></tr></table></figure></p><p>在 Apscheduler 中添加定时更新用户画像任务，设定每隔 2 个小时更新一次<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from apscheduler.schedulers.blocking <span class="built_in">import</span> BlockingScheduler</span><br><span class="line">from apscheduler.executors.pool <span class="built_in">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建scheduler，多进程执行</span></span><br><span class="line"><span class="attr">executors</span> = &#123;</span><br><span class="line">    'default': ProcessPoolExecutor(<span class="number">3</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attr">scheduler</span> = BlockingScheduler(<span class="attr">executors=executors)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个定时运行文章画像更新的任务， 每隔1个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_article_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=1)</span></span><br><span class="line"><span class="comment"># 添加一个定时运行用户画像更新的任务， 每隔2个小时运行一次</span></span><br><span class="line">scheduler.add_job(update_user_profile, <span class="attr">trigger='interval',</span> <span class="attr">hours=2)</span></span><br><span class="line"></span><br><span class="line">scheduler.start()</span><br></pre></td></tr></table></figure></p><p>另外说一下，在实际场景中，用户画像往往是非常复杂的，下面是电商场景的用户画像，可以了解一下。</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-c44e8dbbbe82fd26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a><br><a href="https://pan.baidu.com/s/1SPtttZZK5Nzh5wY7ryaBtg" target="_blank" rel="external">https://pan.baidu.com/s/1SPtttZZK5Nzh5wY7ryaBtg</a>（学习资源已保存至网盘）</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面我们完成了文章画像的构建以及文章相似度的计算，接下来，我们就要实现用户画像的构建了。用户画像往往是大型网站的重要模块，基于用户画像不仅可以实现个性化推荐，还可以实现用户分群、精准推送、精准营销以及用户行为预测、商业化转化分析等，为商业决策提供数据支持。通常用户画像包括用
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之5、计算文章相似度</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B5%E3%80%81%E8%AE%A1%E7%AE%97%E6%96%87%E7%AB%A0%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之5、计算文章相似度/</id>
    <published>2019-12-05T13:21:25.000Z</published>
    <updated>2020-01-10T02:22:34.471Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中，我们已经完成了离线文章画像的构建，接下来，我们要为相似文章推荐做准备，那就是计算文章之间的相似度。首先，我们要计算出文章的词向量，然后利用文章的词向量来计算文章的相似度。</p><h1 id="计算文章词向量"><a href="#计算文章词向量" class="headerlink" title="计算文章词向量"></a>计算文章词向量</h1><p>我们可以通过大量的历史文章数据，训练文章中每个词的词向量，由于文章数据过多，通常是分频道进行词向量训练，即每个频道训练一个词向量模型，我们包括的频道如下所示<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">channel_info = &#123;</span><br><span class="line">            1: <span class="string">"html"</span>,</span><br><span class="line">            2: <span class="string">"开发者资讯"</span>,</span><br><span class="line">            3: <span class="string">"ios"</span>,</span><br><span class="line">            4: <span class="string">"c++"</span>,</span><br><span class="line">            5: <span class="string">"android"</span>,</span><br><span class="line">            6: <span class="string">"css"</span>,</span><br><span class="line">            7: <span class="string">"数据库"</span>,</span><br><span class="line">            8: <span class="string">"区块链"</span>,</span><br><span class="line">            9: <span class="string">"go"</span>,</span><br><span class="line">            10: <span class="string">"产品"</span>,</span><br><span class="line">            11: <span class="string">"后端"</span>,</span><br><span class="line">            12: <span class="string">"linux"</span>,</span><br><span class="line">            13: <span class="string">"人工智能"</span>,</span><br><span class="line">            14: <span class="string">"php"</span>,</span><br><span class="line">            15: <span class="string">"javascript"</span>,</span><br><span class="line">            16: <span class="string">"架构"</span>,</span><br><span class="line">            17: <span class="string">"前端"</span>,</span><br><span class="line">            18: <span class="string">"python"</span>,</span><br><span class="line">            19: <span class="string">"java"</span>,</span><br><span class="line">            20: <span class="string">"算法"</span>,</span><br><span class="line">            21: <span class="string">"面试"</span>,</span><br><span class="line">            22: <span class="string">"科技动态"</span>,</span><br><span class="line">            23: <span class="string">"js"</span>,</span><br><span class="line">            24: <span class="string">"设计"</span>,</span><br><span class="line">            25: <span class="string">"数码产品"</span>,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></p><p>接下来，分别对各自频道内的文章进行分词处理，这里先选取 18 号频道内的所有文章，进行分词处理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_data = spark.sql(<span class="string">"select * from article_data where channel_id=18"</span>)</span><br><span class="line">words_df = article_data.rdd.mapPartitions(segmentation).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'words'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segmentation</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line">    <span class="keyword">import</span> jieba.analyse</span><br><span class="line">    <span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">    <span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">    abspath = <span class="string">"/root/words"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结巴加载用户词典</span></span><br><span class="line">    userDict_path = os.path.join(abspath, <span class="string">"ITKeywords.txt"</span>)</span><br><span class="line">    jieba.load_userdict(userDict_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 停用词文本</span></span><br><span class="line">    stopwords_path = os.path.join(abspath, <span class="string">"stopwords.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stopwords_list</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">"""返回stopwords列表"""</span></span><br><span class="line">        stopwords_list = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> codecs.open(stopwords_path).readlines()]</span><br><span class="line">        <span class="keyword">return</span> stopwords_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有的停用词列表</span></span><br><span class="line">    stopwords_list = get_stopwords_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut_sentence</span><span class="params">(sentence)</span>:</span></span><br><span class="line">        <span class="string">"""对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词"""</span></span><br><span class="line">        <span class="comment"># eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]</span></span><br><span class="line">        seg_list = pseg.lcut(sentence)</span><br><span class="line">        seg_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> seg_list <span class="keyword">if</span> i.flag <span class="keyword">not</span> <span class="keyword">in</span> stopwords_list]</span><br><span class="line">        filtered_words_list = []</span><br><span class="line">        <span class="keyword">for</span> seg <span class="keyword">in</span> seg_list:</span><br><span class="line">            <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> seg.flag == <span class="string">"eng"</span>:</span><br><span class="line">                <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag.startswith(<span class="string">"n"</span>):</span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag <span class="keyword">in</span> [<span class="string">"x"</span>, <span class="string">"eng"</span>]:  <span class="comment"># 是自定一个词语或者是英文单词</span></span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">        <span class="keyword">return</span> filtered_words_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        sentence = re.sub(<span class="string">"&lt;.*?&gt;"</span>, <span class="string">""</span>, row.sentence)    <span class="comment"># 替换掉标签数据</span></span><br><span class="line">        words = cut_sentence(sentence)</span><br><span class="line">        <span class="keyword">yield</span> row.article_id, row.channel_id, words</span><br></pre></td></tr></table></figure></p><p><code>words_df</code> 结果如下所示，words 为分词后的词语列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4cbedc535c6b965c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接着，使用分词后的所有词语，对 Word2Vec 模型进行训练并将模型保存到 HDFS，其中 vectorSize 为词向量的长度，minCount 为词语的最小出现次数，windowSize 为训练窗口的大小，inputCol 为输入的列名，outputCol 为输出的列名<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import Word2Vec</span><br><span class="line"></span><br><span class="line">w2v_model = Word2Vec(<span class="attribute">vectorSize</span>=100, <span class="attribute">inputCol</span>=<span class="string">'words'</span>, <span class="attribute">outputCol</span>=<span class="string">'vector'</span>, <span class="attribute">minCount</span>=3)</span><br><span class="line">model = w2v_model.fit(words_df)</span><br><span class="line">model.save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_18_python.word2vec"</span>)</span><br></pre></td></tr></table></figure></p><p>加载训练好的 Word2Vec 模型<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.feature import Word2VecModel</span><br><span class="line"></span><br><span class="line">w2v_model = Word2VecModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_18_python.word2vec"</span>)</span><br><span class="line">vectors = w2v_model.getVectors()</span><br></pre></td></tr></table></figure></p><p><code>vectors</code> 结果如下所示，其中 vector 是训练后的每个词的 100 维词向量，是 vector 类型格式的，如 [0.2 -0.05 -0.1 …]</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ee792e4abfe00e75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这里，我们计算出了所有词语的词向量，接下来，还要得到关键词的词向量，因为我们需要通过关键词的词向量来计算文章的词向量。那么，首先通过读取频道内的文章画像来得到关键词（实际场景应该只读取新增文章画像）<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">article_profile</span> = spark.sql(<span class="string">"select * from article_profile where channel_id=18"</span>)</span><br></pre></td></tr></table></figure></p><p>在文章画像表中，关键词和权重是存储在同一列的，我们可以利用 <code>LATERAL VIEW explode()</code> 方法，将 map 类型的 keywords 列中的关键词和权重转换成单独的两列数据<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">article_profile.registerTempTable('profile')</span><br><span class="line">keyword_weight = spark.sql("<span class="keyword">select</span> article_id, channel_id, keyword, weight <span class="keyword">from</span> profile LATERAL <span class="keyword">VIEW</span> explode(keywords) <span class="keyword">AS</span> keyword, weight<span class="string">")</span></span><br></pre></td></tr></table></figure></p><p><code>keyword_weight</code> 结果如下所示，keyword 为关键词，weight 为对应的权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-39f3605d616afc31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这时就可以利用关键词 keyword 列，将文章关键词 <code>keyword_weight</code> 与词向量结果 <code>vectors</code> 进行内连接，从而得到每个关键词的词向量<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">keywords_vector</span> = keyword_weight.join(vectors, vectors.word==keyword_weight.keyword, <span class="string">'inner'</span>)</span><br></pre></td></tr></table></figure></p><p><code>keywords_vector</code> 结果如下所示，vector 即对应关键词的 100 维词向量</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-8cc8e64b8f642fec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，将文章每个关键词的词向量加入权重信息，这里使每个关键词的词向量 = 关键词的权重 x 关键词的词向量，即 weight_vector = weight x vector，注意这里的 <code>vector</code> 为 vector 类型，所以 weight x vector 是权重和向量的每个元素相乘，向量的长度保持不变<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def compute_vector(<span class="built_in">row</span>):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">row</span>.article_id, <span class="built_in">row</span>.channel_id, <span class="built_in">row</span>.keyword, <span class="built_in">row</span>.weight * <span class="built_in">row</span>.<span class="built_in">vector</span></span><br><span class="line"></span><br><span class="line">article_keyword_vectors = keywords_vector.rdd.<span class="built_in">map</span>(compute_vector).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"weightingVector"</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_keyword_vectors</code> 结果如下所示，weightingVector 即为加入权重信息后的关键词的词向量</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-cb450dceb634a754.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>再将上面的结果按照 article_id 进行分组，利用 <code>collect_set()</code> 方法，将一篇文章内所有关键词的词向量合并为一个列表<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">article_keyword_vectors.registerTempTable('temptable')</span><br><span class="line">article_keyword_vectors = spark.sql("<span class="keyword">select</span> article_id, <span class="built_in">min</span>(channel_id) channel_id, collect_set(weightingVector) vectors <span class="keyword">from</span> temptable <span class="keyword">group</span> <span class="keyword">by</span> article_id<span class="string">")</span></span><br></pre></td></tr></table></figure></p><p><code>article_keyword_vectors</code> 结果如下所示，vectors 即为文章内所有关键词向量的列表，如 [[0.6 0.2 …], [0.1 -0.07 …], …]</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-9e68516da76d0189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，利用上面得出的二维列表，计算每篇文章内所有关键词的词向量的平均值，作为文章的词向量。注意，这里的 <code>vectors</code> 是包含多个词向量的列表，词向量列表的平均值等于其中每个词向量的对应元素相加再除以词向量的个数<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_avg_vectors</span><span class="params">(row)</span></span><span class="symbol">:</span></span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> row.<span class="symbol">vectors:</span></span><br><span class="line">        x += i</span><br><span class="line">    <span class="comment"># 求平均值</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, row.channel_id, x / len(row.vectors)</span><br><span class="line"></span><br><span class="line">article_vector = article_keyword_vectors.rdd.map(compute_avg_vectors).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'vector'</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_vector</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-a86e19255d39bf47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>此时，<code>article_vector</code> 中的 <code>vector</code> 列还是 vector 类型，而 Hive 不支持该数据类型，所以需要将 vector 类型转成 array 类型（list）<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def to_list(<span class="built_in">row</span>):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">row</span>.article_id, <span class="built_in">row</span>.channel_id, [<span class="built_in">float</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">row</span>.<span class="built_in">vector</span>.toArray()]</span><br><span class="line"></span><br><span class="line">article_vector = article_vector.rdd.<span class="built_in">map</span>(to_list).toDF(['article_id', 'channel_id', '<span class="built_in">vector</span>'])</span><br></pre></td></tr></table></figure></p><p>在 Hive 中创建文章词向量表 article_vector<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> article_vector</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    articlevector <span class="built_in">ARRAY</span> <span class="keyword">comment</span> <span class="string">"keyword"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p>最后，将 18 号频道内的所有文章的词向量存储到 Hive 的文章词向量表 article_vector 中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">article_vector.write.insertInto(<span class="string">"article_vector"</span>)</span><br></pre></td></tr></table></figure></p><p>这样，我们就计算出了 18 号频道下每篇文章的词向量，在实际场景中，我们还要分别计算出其他所有频道下每篇文章的词向量。</p><h1 id="计算文章相似度"><a href="#计算文章相似度" class="headerlink" title="计算文章相似度"></a>计算文章相似度</h1><p>前面我们计算出了文章的词向量，接下来就可以根据文章的词向量来计算文章的相似度了。通常我们会有几百万、几千万甚至上亿规模的文章数据，为了优化计算性能，我们可以只计算每个频道内文章之间的相似度，因为通常只有相同频道的文章关联性较高，而不同频道之间的文章通常关联性较低。在每个频道内，我们还可以用聚类或局部敏感哈希对文章进行分桶，将文章相似度的计算限制在更小的范围，只计算相同分类内或相同桶内的文章相似度。</p><ul><li>聚类（Clustering），对每个频道内的文章进行聚类，可以使用 KMeans 算法，需要提前设定好类别个数 K，聚类算法的时间复杂度并不小，也可以使用一些优化的聚类算法，比如二分聚类、层次聚类等。但通常聚类算法也比较耗时，所以通常被使用更多的是局部敏感哈希。</li></ul><p>Spark 的 BisectingKMeans 模型训练代码示例<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.clustering import BisectingKMeans</span><br><span class="line"></span><br><span class="line">bkmeans = BisectingKMeans(<span class="attribute">k</span>=100, <span class="attribute">minDivisibleClusterSize</span>=50, <span class="attribute">featuresCol</span>=<span class="string">"articlevector"</span>, <span class="attribute">predictionCol</span>=<span class="string">'group'</span>)</span><br><span class="line">bkmeans_model = bkmeans.fit(article_vector)</span><br><span class="line">bkmeans_model.save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/articleBisKmeans/channel_%d_%s.bkmeans"</span> % (channel_id, channel))</span><br></pre></td></tr></table></figure></p><ul><li>局部敏感哈希 LSH（Locality Sensitive Hashing），LSH 算法是基于一个假设，如果两个文本在原有的数据空间是相似的，那么经过哈希函数转换以后，它们仍然具有很高的相似度，即越相似的文本在哈希之后，落到相同的桶内的概率就越高。所以，我们只需要将目标文章进行哈希映射并得到其桶号，然后取出该桶内的所有文章，再进行线性匹配即可查找到与目标文章相邻的文章。其实 LSH 并不能保证一定能够查找到与目标文章最相邻的文章，而是在减少需要匹配的文章个数的同时，保证查找到最近邻的文章的概率很大。</li></ul><p>下面我们将使用 LSH 模型来计算文章相似度，首先，读取 18 号频道内所有文章的 ID 和词向量作为训练集<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">article_vector</span> = spark.sql(<span class="string">"select article_id, articlevector from article_vector where channel_id=18"</span>)</span><br><span class="line"><span class="attr">train</span> = articlevector.select([<span class="string">'article_id'</span>, <span class="string">'articlevector'</span>])</span><br></pre></td></tr></table></figure></p><p>文章词向量表中的词向量是被存储为 array 类型的，我们利用 Spark 的 <code>Vectors.dense()</code> 方法，将 array 类型（list）转为 vector 类型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_to_vector</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, Vectors.dense(row.articlevector)</span><br><span class="line"></span><br><span class="line">train = train.rdd.map(list_to_vector).toDF([<span class="string">'article_id'</span>, <span class="string">'articlevector'</span>])</span><br></pre></td></tr></table></figure></p><p>使用训练集 <code>train</code> 对 Spark 的 <code>BucketedRandomProjectionLSH</code> 模型进行训练，其中 inputCol 为输入特征列，outputCol 为输出特征列，numHashTables 为哈希表数量，bucketLength 为桶的数量，数量越多，相同数据进入到同一个桶的概率就越高<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import BucketedRandomProjectionLSH</span><br><span class="line"></span><br><span class="line">brp = BucketedRandomProjectionLSH(<span class="attribute">inputCol</span>=<span class="string">'articlevector'</span>, <span class="attribute">outputCol</span>=<span class="string">'hashes'</span>, <span class="attribute">numHashTables</span>=4.0, <span class="attribute">bucketLength</span>=10.0)</span><br><span class="line">model = brp.fit(train)</span><br></pre></td></tr></table></figure></p><p>训练好模型后，调用 <code>approxSimilarityJoin()</code> 方法即可计算数据之间的相似度，如 <code>model.approxSimilarityJoin(df1, df2, 2.0, distCol=&#39;EuclideanDistance&#39;)</code> 就是利用欧几里得距离作为相似度，计算在 df1 与 df2 每条数据的相似度，这里我们计算训练集中所有文章之间的相似度<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">similar</span> = model.approxSimilarityJoin(train, train, <span class="number">2.0</span>, distCol=<span class="string">'EuclideanDistance'</span>)</span><br></pre></td></tr></table></figure></p><p><code>similar</code> 结果如下所示，EuclideanDistance 就是两篇文章的欧几里得距离，即相似度</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-8e98744a01f7d121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>在后面的推荐流程中，会经常查询文章相似度，所以出于性能考虑，我们选择将文章相似度结果存储到  Hbase 中。首先创建文章相似度表<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">create</span> <span class="string">'article_similar'</span>, <span class="string">'similar'</span></span><br></pre></td></tr></table></figure></p><p>然后存储文章相似度结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_hbase</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> happybase</span><br><span class="line">    pool = happybase.ConnectionPool(size=<span class="number">3</span>, host=<span class="string">'hadoop-master'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">        <span class="comment"># 建立表连接</span></span><br><span class="line">        table = conn.table(<span class="string">'article_similar'</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">            <span class="keyword">if</span> row.datasetA.article_id != row.datasetB.article_id:</span><br><span class="line">                table.put(str(row.datasetA.article_id).encode(), &#123;<span class="string">"similar:&#123;&#125;"</span>.format(row.datasetB.article_id).encode(): <span class="string">b'%0.4f'</span> % (row.EuclideanDistance)&#125;)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># 手动关闭所有的连接</span></span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line">similar.foreachPartition(save_hbase)</span><br></pre></td></tr></table></figure></p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>将文章相似度计算加入到文章画像更新方法中，首先合并最近一个小时的文章完整信息，接着计算 TF-IDF 和 TextRank 权重，并根据 TF-IDF 和 TextRank 权重计算得出关键词和主题词，最后计算文章的词向量及文章的相似度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_article_profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定时更新文章画像及文章相似度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ua = UpdateArticle()</span><br><span class="line">    sentence_df = ua.merge_article_data()</span><br><span class="line">    <span class="keyword">if</span> sentence_df.rdd.collect():</span><br><span class="line">        textrank_keywords_df, keywordsIndex = ua.generate_article_label()</span><br><span class="line">        article_profile = ua.get_article_profile(textrank_keywords_df, keywordsIndex)</span><br><span class="line">        ua.compute_article_similar(article_profile)</span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上篇文章中，我们已经完成了离线文章画像的构建，接下来，我们要为相似文章推荐做准备，那就是计算文章之间的相似度。首先，我们要计算出文章的词向量，然后利用文章的词向量来计算文章的相似度。&lt;/p&gt;
&lt;h1 id=&quot;计算文章词向量&quot;&gt;&lt;a href=&quot;#计算文章词向量&quot; clas
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之4、构建离线文章画像</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B4%E3%80%81%E6%9E%84%E5%BB%BA%E7%A6%BB%E7%BA%BF%E6%96%87%E7%AB%A0%E7%94%BB%E5%83%8F/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之4、构建离线文章画像/</id>
    <published>2019-12-05T13:21:24.000Z</published>
    <updated>2020-01-10T02:21:17.381Z</updated>
    
    <content type="html"><![CDATA[<p>在上述步骤中，我们已经将业务数据和用户行为数据同步到了推荐系统数据库当中，接下来，我们就要对文章数据和用户数据进行分析，构建文章画像和用户画像，本文我们主要讲解如何构建文章画像。文章画像由关键词和主题词组成，我们将每个词的 TF-IDF 权重和 TextRank 权重的乘积作为关键词权重，筛选出权重最高的 K 个词作为关键词，将 TextRank 权重最高的 K 个词与 TF-IDF 权重最高的 K 个词的共现词作为主题词。</p><p>首先，在 Hive 中创建文章数据库 article 及相关表，其中表 article_data 用于存储完整的文章信息，表 idf_keywords_values 用于存储关键词和索引信息，表 tfidf_keywords_values 用于存储关键词和 TF-IDF 权重信息，表 textrank_keywords_values 用于存储关键词和 TextRank 权重信息，表 article_profile 用于存储文章画像信息。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建文章数据库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> article <span class="keyword">comment</span> <span class="string">"artcile information"</span> location <span class="string">'/user/hive/warehouse/article.db/'</span>;</span><br><span class="line"><span class="comment">-- 创建文章信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> article_data</span><br><span class="line">(</span><br><span class="line">    article_id   <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id   <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    channel_name <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"channel_name"</span>,</span><br><span class="line">    title        <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"title"</span>,</span><br><span class="line">    <span class="keyword">content</span>      <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"content"</span>,</span><br><span class="line">    sentence     <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"sentence"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_channel"</span></span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/article.db/article_data'</span>;</span><br><span class="line"><span class="comment">-- 创建关键词索引信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> idf_keywords_values</span><br><span class="line">(</span><br><span class="line">    keyword <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    idf     <span class="keyword">DOUBLE</span> <span class="keyword">comment</span> <span class="string">"idf"</span>,</span><br><span class="line">    <span class="keyword">index</span>   <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"index"</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 创建关键词TF-IDF权重信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tfidf_keywords_values</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    keyword    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"keyword"</span>,</span><br><span class="line">    tfidf      <span class="keyword">DOUBLE</span> <span class="keyword">comment</span> <span class="string">"tfidf"</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 创建关键词TextRank权重信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> textrank_keywords_values</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    keyword    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"keyword"</span>,</span><br><span class="line">    textrank   <span class="keyword">DOUBLE</span> <span class="keyword">comment</span> <span class="string">"textrank"</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 创建文章画像信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> article_profile</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    keyword    <span class="keyword">map</span> <span class="keyword">comment</span> <span class="string">"keyword"</span>,</span><br><span class="line">    topics     <span class="built_in">array</span> <span class="keyword">comment</span> <span class="string">"topics"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><h1 id="计算文章完整信息"><a href="#计算文章完整信息" class="headerlink" title="计算文章完整信息"></a>计算文章完整信息</h1><p>为了计算文章画像，需要将文章信息表（news_article_basic）、文章内容表（news_article_content）及频道表（news_channel）进行合并，从而得到完整的文章信息，通常使用 Spark SQL 进行处理。</p><p>通过关联表 news_article_basic, news_article_content 和 news_channel 获得文章完整信息，包括 article_id, channel_id, channel_name, title, content，这里获取一个小时内的文章信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use toutiao"</span>)</span><br><span class="line">_now = datetime.today().replace(minute=<span class="number">0</span>, second=<span class="number">0</span>, microsecond=<span class="number">0</span>)</span><br><span class="line">start = datetime.strftime(_now + timedelta(days=<span class="number">0</span>, hours=<span class="number">-1</span>, minutes=<span class="number">0</span>), <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">end = datetime.strftime(_now, <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">basic_content = spark.sql(</span><br><span class="line">            <span class="string">"select a.article_id, a.channel_id, a.title, b.content from news_article_basic a "</span></span><br><span class="line">            <span class="string">"inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '&#123;&#125;' "</span></span><br><span class="line">            <span class="string">"and a.review_time &lt; '&#123;&#125;' and a.status = 2"</span>.format(start, end))</span><br><span class="line">basic_content.registerTempTable(<span class="string">"temp_article"</span>)</span><br><span class="line">channel_basic_content = spark.sql(</span><br><span class="line">            <span class="string">"select t.*, n.channel_name from temp_article t left join news_channel n on t.channel_id=n.channel_id"</span>)</span><br></pre></td></tr></table></figure></p><p><code>channel_basic_content</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-96f4fc864f469275.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>利用 <code>concat_ws()</code> 方法，将 channel_name, title, content 这 3 列数据合并为一列 sentence，并将结果写入文章完整信息表 article_data 中<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import pyspark.sql.<span class="built_in">functions</span> as F</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">sentence_df = channel_basic_content.select(<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"channel_name"</span>, <span class="string">"title"</span>, <span class="string">"content"</span>, \</span><br><span class="line">                                           F.concat_ws(</span><br><span class="line">                                               <span class="string">","</span>,</span><br><span class="line">                                               channel_basic_content.channel_name,</span><br><span class="line">                                               channel_basic_content.<span class="built_in">title</span>,</span><br><span class="line">                                               channel_basic_content.<span class="built_in">content</span></span><br><span class="line">                                           ).<span class="built_in">alias</span>(<span class="string">"sentence"</span>)</span><br><span class="line">                                           )</span><br><span class="line"><span class="built_in">del</span> basic_content</span><br><span class="line"><span class="built_in">del</span> channel_basic_content</span><br><span class="line">gc.collect() # 垃圾回收</span><br><span class="line"></span><br><span class="line">sentence_df.write.insertInto(<span class="string">"article_data"</span>)</span><br></pre></td></tr></table></figure></p><p><code>sentence_df</code> 结果如下所示，文章完整信息包括 article_id, channel_id, channel_name, title, content, sentence，其中 sentence 为 channel_name, title, content 合并而成的长文本内容</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-b87fe4016480095b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="计算-TF-IDF"><a href="#计算-TF-IDF" class="headerlink" title="计算 TF-IDF"></a>计算 TF-IDF</h1><p>前面我们得到了文章的完整内容信息，接下来，我们要先对文章进行分词，然后计算每个词的 TF-IDF 权重，将 TF-IDF 权重最高的 K 个词作为文章的关键词。首先，读取文章信息<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_dataframe = spark.sql(<span class="string">"select * from article_data"</span>)</span><br></pre></td></tr></table></figure></p><p>利用 <code>mapPartitions()</code> 方法，对每篇文章进行分词，这里使用的是 <code>jieba</code> 分词器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"words"</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segmentation</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line">    <span class="keyword">import</span> jieba.analyse</span><br><span class="line">    <span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">    <span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">    abspath = <span class="string">"/root/words"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结巴加载用户词典</span></span><br><span class="line">    userdict_path = os.path.join(abspath, <span class="string">"ITKeywords.txt"</span>)</span><br><span class="line">    jieba.load_userdict(userdict_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 停用词文本</span></span><br><span class="line">    stopwords_path = os.path.join(abspath, <span class="string">"stopwords.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stopwords_list</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">"""返回stopwords列表"""</span></span><br><span class="line">        stopwords_list = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> codecs.open(stopwords_path).readlines()]</span><br><span class="line">        <span class="keyword">return</span> stopwords_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有的停用词列表</span></span><br><span class="line">    stopwords_list = get_stopwords_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut_sentence</span><span class="params">(sentence)</span>:</span></span><br><span class="line">        <span class="string">"""对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词"""</span></span><br><span class="line">        seg_list = pseg.lcut(sentence)</span><br><span class="line">        seg_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> seg_list <span class="keyword">if</span> i.flag <span class="keyword">not</span> <span class="keyword">in</span> stopwords_list]</span><br><span class="line">        filtered_words_list = []</span><br><span class="line">        <span class="keyword">for</span> seg <span class="keyword">in</span> seg_list:</span><br><span class="line">            <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> seg.flag == <span class="string">"eng"</span>:</span><br><span class="line">                <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag.startswith(<span class="string">"n"</span>):</span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag <span class="keyword">in</span> [<span class="string">"x"</span>, <span class="string">"eng"</span>]:  <span class="comment"># 是自定一个词语或者是英文单词</span></span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">        <span class="keyword">return</span> filtered_words_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        sentence = re.sub(<span class="string">"&lt;.*?&gt;"</span>, <span class="string">""</span>, row.sentence)    <span class="comment"># 替换掉标签数据</span></span><br><span class="line">        words = cut_sentence(sentence)</span><br><span class="line">        <span class="keyword">yield</span> row.article_id, row.channel_id, words</span><br></pre></td></tr></table></figure></p><p><code>words_df</code> 结果如下所示，words 为将 sentence 分词后的单词列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-55862171336abc52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>使用分词结果对词频统计模型（CountVectorizer）进行词频统计训练，并将 CountVectorizer 模型保存到 HDFS 中<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import CountVectorizer</span><br><span class="line"><span class="comment"># vocabSize是总词汇的大小，minDF是文本中出现的最少次数</span></span><br><span class="line">cv = CountVectorizer(<span class="attribute">inputCol</span>=<span class="string">"words"</span>, <span class="attribute">outputCol</span>=<span class="string">"countFeatures"</span>, <span class="attribute">vocabSize</span>=200*10000, <span class="attribute">minDF</span>=1.0)</span><br><span class="line"><span class="comment"># 训练词频统计模型</span></span><br><span class="line">cv_model = cv.fit(words_df)</span><br><span class="line">cv_model.write().overwrite().save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/CV.model"</span>)</span><br></pre></td></tr></table></figure></p><p>加载 CountVectorizer 模型，计算词频向量<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.feature import CountVectorizerModel</span><br><span class="line">cv_model = CountVectorizerModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/CV.model"</span>)</span><br><span class="line"><span class="comment"># 得出词频向量结果</span></span><br><span class="line">cv_result = cv_model.transform(words_df)</span><br></pre></td></tr></table></figure></p><p><code>cv_result</code> 结果如下所示，countFeatures 为词频向量，如 (986, [2, 4, …], [3.0, 5.0, …]) 表示总词汇的大小为 986 个，索引为 2 和 4 的词在某篇文章中分别出现 3 次和 5 次，</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-760b54a16a9e780b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>得到词频向量后，再利用逆文本频率模型（ IDF ），根据词频向量进行 IDF 统计训练，并将 IDF 模型保存到 HDFS<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import IDF</span><br><span class="line">idf = IDF(<span class="attribute">inputCol</span>=<span class="string">"countFeatures"</span>, <span class="attribute">outputCol</span>=<span class="string">"idfFeatures"</span>)</span><br><span class="line">idf_model = idf.fit(cv_result)</span><br><span class="line">idf_model.write().overwrite().save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/IDF.model"</span>)</span><br></pre></td></tr></table></figure></p><p>我们已经分别计算了文章信息中每个词的 TF 和 IDF，这时就可以加载 CountVectorizer 模型和 IDF 模型，计算每个词的 TF-IDF<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.feature import CountVectorizerModel</span><br><span class="line">cv_model = CountVectorizerModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model"</span>)</span><br><span class="line">from pyspark.ml.feature import IDFModel</span><br><span class="line">idf_model = IDFModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model"</span>)</span><br><span class="line"></span><br><span class="line">cv_result = cv_model.transform(words_df)</span><br><span class="line">tfidf_result = idf_model.transform(cv_result)</span><br></pre></td></tr></table></figure></p><p><code>tfidf_result</code> 结果如下所示，idfFeatures 为 TF-IDF 权重向量，如 (986, [2, 4, …], [0.3, 0.5, …]) 表示总词汇的大小为 986 个，索引为 2 和 4 的词在某篇文章中的 TF-IDF 值分别为 0.3 和 0.5</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-44cdf662e1e2adb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>对文章的每个词都根据 TF-IDF 权重排序，保留 TF-IDF 权重最高的前 K 个词作为关键词<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def sort_by_tfidf(<span class="built_in">partition</span>):</span><br><span class="line">    TOPK = <span class="number">20</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">row</span> <span class="keyword">in</span> <span class="built_in">partition</span>:</span><br><span class="line">        # 找到索引与IDF值并进行排序</span><br><span class="line">        _dict = list(zip(<span class="built_in">row</span>.idfFeatures.<span class="built_in">indices</span>, <span class="built_in">row</span>.idfFeatures.<span class="built_in">values</span>))</span><br><span class="line">        _dict = sorted(_dict, <span class="built_in">key</span>=<span class="built_in">lambda</span> x: x[<span class="number">1</span>], <span class="built_in">reverse</span>=True)</span><br><span class="line">        result = _dict[:TOPK]</span><br><span class="line">        <span class="keyword">for</span> word_index, tfidf <span class="keyword">in</span> result:</span><br><span class="line">            yield <span class="built_in">row</span>.article_id, <span class="built_in">row</span>.channel_id, int(word_index), <span class="built_in">round</span>(<span class="built_in">float</span>(tfidf), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">keywords_by_tfidf = tfidf_result.rdd.mapPartitions(sort_by_tfidf).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"index"</span>, <span class="string">"weights"</span>])</span><br></pre></td></tr></table></figure></p><p><code>keywords_by_tfidf</code> 结果如下所示，每篇文章保留了权重最高的 K 个单词，index 为单词索引，weights 为对应单词的 TF-IDF 权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-566af87857d1c5bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，我们需要知道每个词的对应的 TF-IDF 值，可以利用 <code>zip()</code> 方法，将所有文章中的每个词及其 TF-IDF 权重组成字典，再加入索引列，由此得到每个词对应的 TF-IDF 值，将该结果保存到 idf_keywords_values 表<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">keywords_list_with_idf = <span class="keyword">list</span>(zip(cv_model.vocabulary, idf_model.idf.toArray()))</span><br><span class="line">def append_index(data):</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">index</span> in <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        data[<span class="built_in">index</span>] = <span class="keyword">list</span>(data[<span class="built_in">index</span>]) # 将元组转为<span class="keyword">list</span></span><br><span class="line">        data[<span class="built_in">index</span>].<span class="keyword">append</span>(<span class="built_in">index</span>)       # 加入索引 </span><br><span class="line">        data[<span class="built_in">index</span>][<span class="number">1</span>] = float(data[<span class="built_in">index</span>][<span class="number">1</span>])</span><br><span class="line">append_index(keywords_list_with_idf)</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">rdd = sc.parallelize(keywords_list_with_idf) # 创建rdd</span><br><span class="line">idf_keywords = rdd.toDF([<span class="string">"keywords"</span>, <span class="string">"idf"</span>, <span class="string">"index"</span>])</span><br><span class="line"></span><br><span class="line">idf_keywords.<span class="keyword">write</span>.insertInto(<span class="string">'idf_keywords_values'</span>)</span><br></pre></td></tr></table></figure></p><p><code>idf_keywords</code> 结果如下所示，包含了所有单词的名称、TF-IDF 权重及索引</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-bbf3babed9a9ccfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>通过 index 列，将 <code>keywords_by_tfidf</code> 与表 idf_keywords_values 进行连接，选取文章 ID、频道 ID、关键词、TF-IDF 权重作为结果，并保存到 TF-IDF 关键词表 tfidf_keywords_values<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keywords_index = spark.sql(<span class="string">"select keyword, index idx from idf_keywords_values"</span>)</span><br><span class="line">keywords_result = keywords_by_tfidf.join(keywords_index, keywords_index<span class="selector-class">.idx</span> == keywords_by_tfidf.index).select([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"weights"</span>])</span><br><span class="line">keywords_result<span class="selector-class">.write</span><span class="selector-class">.insertInto</span>(<span class="string">"tfidf_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p><code>keywords_result</code> 结果如下所示，keyword 和 weights 即为所有词在每个文章中的 TF-IDF 权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-7c39214d62083c30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="计算-TextRank"><a href="#计算-TextRank" class="headerlink" title="计算 TextRank"></a>计算 TextRank</h1><p>前面我们已经计算好了每个词的 TF-IDF 权重，为了计算关键词，还需要得到每个词的 TextRank 权重，接下来，还是先读取文章完整信息<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_dataframe = spark.sql(<span class="string">"select * from article_data"</span>)</span><br></pre></td></tr></table></figure></p><p>对文章 sentence 列的内容进行分词，计算每个词的 TextRank 权重，并将每篇文章 TextRank 权重最高的 K 个词保存到 TextRank 结果表 textrank_keywords_values<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">textrank_keywords_df = article_dataframe<span class="selector-class">.rdd</span><span class="selector-class">.mapPartitions</span>(textrank).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"textrank"</span>])</span><br><span class="line"></span><br><span class="line">textrank_keywords_df<span class="selector-class">.write</span><span class="selector-class">.insertInto</span>(<span class="string">"textrank_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p>TextRank 计算细节：分词后只保留指定词性的词，滑动截取长度为 K 的窗口，计算窗口内的各个词的投票数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textrank</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line">    <span class="keyword">import</span> jieba.analyse</span><br><span class="line">    <span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">    <span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">    abspath = <span class="string">"/root/words"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结巴加载用户词典</span></span><br><span class="line">    userDict_path = os.path.join(abspath, <span class="string">"ITKeywords.txt"</span>)</span><br><span class="line">    jieba.load_userdict(userDict_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 停用词文本</span></span><br><span class="line">    stopwords_path = os.path.join(abspath, <span class="string">"stopwords.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stopwords_list</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">"""返回stopwords列表"""</span></span><br><span class="line">        stopwords_list = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> codecs.open(stopwords_path).readlines()]</span><br><span class="line">        <span class="keyword">return</span> stopwords_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有的停用词列表</span></span><br><span class="line">    stopwords_list = get_stopwords_list()</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">TextRank</span><span class="params">(jieba.analyse.TextRank)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, window=<span class="number">20</span>, word_min_len=<span class="number">2</span>)</span>:</span></span><br><span class="line">            super(TextRank, self).__init__()</span><br><span class="line">            self.span = window  <span class="comment"># 窗口大小</span></span><br><span class="line">            self.word_min_len = word_min_len  <span class="comment"># 单词的最小长度</span></span><br><span class="line">            <span class="comment"># 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac</span></span><br><span class="line">            self.pos_filt = frozenset(</span><br><span class="line">                (<span class="string">'n'</span>, <span class="string">'x'</span>, <span class="string">'eng'</span>, <span class="string">'f'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">"nw"</span>, <span class="string">"nz"</span>, <span class="string">"PER"</span>, <span class="string">"LOC"</span>, <span class="string">"ORG"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">pairfilter</span><span class="params">(self, wp)</span>:</span></span><br><span class="line">            <span class="string">"""过滤条件，返回True或者False"""</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> wp.flag == <span class="string">"eng"</span>:</span><br><span class="line">                <span class="keyword">if</span> len(wp.word) &lt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> wp.flag <span class="keyword">in</span> self.pos_filt <span class="keyword">and</span> len(wp.word.strip()) &gt;= self.word_min_len \</span><br><span class="line">                    <span class="keyword">and</span> wp.word.lower() <span class="keyword">not</span> <span class="keyword">in</span> stopwords_list:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="comment"># TextRank过滤窗口大小为5，单词最小为2</span></span><br><span class="line">    textrank_model = TextRank(window=<span class="number">5</span>, word_min_len=<span class="number">2</span>)</span><br><span class="line">    allowPOS = (<span class="string">'n'</span>, <span class="string">"x"</span>, <span class="string">'eng'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">"nw"</span>, <span class="string">"nz"</span>, <span class="string">"c"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        tags = textrank_model.textrank(row.sentence, topK=<span class="number">20</span>, withWeight=<span class="keyword">True</span>, allowPOS=allowPOS, withFlag=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> tags:</span><br><span class="line">            <span class="keyword">yield</span> row.article_id, row.channel_id, tag[<span class="number">0</span>], tag[<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><p><code>textrank_keywords_df</code> 结果如下所示，keyword 和 textrank 即为每个单词在文章中的 TextRank 权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4453fb49ff5e39e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="画像计算"><a href="#画像计算" class="headerlink" title="画像计算"></a>画像计算</h1><p>我们计算出 TF-IDF 和 TextRank 后，就可以计算关键词和主题词了，读取 TF-IDF 权重<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">idf_keywords_values = oa<span class="selector-class">.spark</span><span class="selector-class">.sql</span>(<span class="string">"select * from idf_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p>读取 TextRank 权重<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textrank_keywords_values = oa<span class="selector-class">.spark</span><span class="selector-class">.sql</span>(<span class="string">"select * from textrank_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p>通过 <code>keyword</code> 关联 TF-IDF 权重和 TextRank 权重<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">keywords_res</span> = textrank_keywords_values.join(idf_keywords_values, <span class="literal">on</span>=[<span class="string">'keyword'</span>], how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure></p><p>计算 TF-IDF 权重和 TextRank 权重的乘积作为关键词权重<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">keywords_weights</span> = keywords_res.withColumn(<span class="string">'weights'</span>, keywords_res.textrank * keywords_res.idf).select([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"weights"</span>])</span><br></pre></td></tr></table></figure></p><p><code>keywords_weights</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-bd60d34728a7aff8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这里，我们需要将相同文章的词都合并到一条记录中，将 <code>keywords_weights</code> 按照 article_id 分组，并利用 <code>collect_list()</code> 方法，分别将关键词和权重合并为列表<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keywords_weights.registerTempTable('temp')</span><br><span class="line"></span><br><span class="line">keywords_weights = spark.sql("<span class="keyword">select</span> article_id, <span class="built_in">min</span>(channel_id) channel_id, collect_list(keyword) keywords, collect_list(weights) weights <span class="keyword">from</span> temp <span class="keyword">group</span> <span class="keyword">by</span> article_id<span class="string">")`</span></span><br></pre></td></tr></table></figure></p><p><code>keywords_weights</code> 结果如下所示，keywords 为每篇文章的关键词列表，weights 为关键词对应的权重列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-562748d12a395121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>为了方便查询，我们需要将关键词和权重合并为一列，并存储为 map 类型，这里利用 <code>dict()</code> 和 <code>zip()</code> 方法，将每个关键词及其权重组合成字典<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_map</span><span class="params">(row)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, row.channel_id, dict(zip(row.keywords, row.weights))</span><br><span class="line"></span><br><span class="line">article_keywords = keywords_weights.rdd.map(to_map).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'keywords'</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_keywords</code> 结果如下所示，keywords 即为每篇文章的关键词和对应权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-5a02c70d3f93eb78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>前面我们计算完了关键词，接下来我们将 TF-IDF 和 TextRank 的共现词作为主题词，将 TF-IDF 权重表 tfidf_keywords_values 和 TextRank 权重表 textrank_keywords_values 进行关联，并利用 <code>collect_set()</code> 对结果进行去重，即可得到 TF-IDF 和 TextRank 的共现词，即主题词<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">topic_sql</span> = <span class="string">"""</span></span><br><span class="line"><span class="string">                select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t</span></span><br><span class="line"><span class="string">                inner join </span></span><br><span class="line"><span class="string">                textrank_keywords_values r</span></span><br><span class="line"><span class="string">                where t.keyword=r.keyword</span></span><br><span class="line"><span class="string">                group by article_id2</span></span><br><span class="line"><span class="string">                """</span></span><br><span class="line"><span class="attr">article_topics</span> = spark.sql(topic_sql)</span><br></pre></td></tr></table></figure></p><p><code>article_topics</code> 结果如下所示，topics 即为每篇文章的主题词列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ca3825c269a0581b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>最后，将主题词结果和关键词结果合并，即为文章画像，保存到表 <code>article_profile</code><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">article_profile = article_keywords.join(article_topics, article_keywords.article_id==article_topics.article_id2).select([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keywords"</span>, <span class="string">"topics"</span>])</span><br><span class="line"></span><br><span class="line">article_profile<span class="selector-class">.write</span><span class="selector-class">.insertInto</span>(<span class="string">"article_profile"</span>)</span><br></pre></td></tr></table></figure></p><p>文章画像数据查询测试<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="built_in">select</span> * <span class="keyword">from</span> article_profile limit <span class="number">1</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="number">26</span>      <span class="number">17</span>      &#123;<span class="string">"策略"</span>:<span class="number">0.3973770571351729</span>,<span class="string">"jpg"</span>:<span class="number">0.9806348975390871</span>,<span class="string">"用户"</span>:<span class="number">1.2794959063944176</span>,<span class="string">"strong"</span>:<span class="number">1.6488457985625076</span>,<span class="string">"文件"</span>:<span class="number">0.28144603583387057</span>,<span class="string">"逻辑"</span>:<span class="number">0.45256526469610714</span>,<span class="string">"形式"</span>:<span class="number">0.4123994242601279</span>,<span class="string">"全自"</span>:<span class="number">0.9594604850547191</span>,<span class="string">"h2"</span>:<span class="number">0.6244481634710125</span>,<span class="string">"版本"</span>:<span class="number">0.44280276959510817</span>,<span class="string">"Adobe"</span>:<span class="number">0.8553618185108718</span>,<span class="string">"安装"</span>:<span class="number">0.8305037437573172</span>,<span class="string">"检查更新"</span>:<span class="number">1.8088946300014435</span>,<span class="string">"产品"</span>:<span class="number">0.774842382276899</span>,<span class="string">"下载页"</span>:<span class="number">1.4256311032544344</span>,<span class="string">"过程"</span>:<span class="number">0.19827163395829256</span>,<span class="string">"json"</span>:<span class="number">0.6423301791599972</span>,<span class="string">"方式"</span>:<span class="number">0.582762869780791</span>,<span class="string">"退出应用"</span>:<span class="number">1.2338671268242603</span>,<span class="string">"Setup"</span>:<span class="number">1.004399549339134</span>&#125;   [<span class="string">"Electron"</span>,<span class="string">"全自动"</span>,<span class="string">"产品"</span>,<span class="string">"版本号"</span>,<span class="string">"安装包"</span>,<span class="string">"检查更新"</span>,<span class="string">"方案"</span>,<span class="string">"版本"</span>,<span class="string">"退出应用"</span>,<span class="string">"逻辑"</span>,<span class="string">"安装过程"</span>,<span class="string">"方式"</span>,<span class="string">"定性"</span>,<span class="string">"新版本"</span>,<span class="string">"Setup"</span>,<span class="string">"静默"</span>,<span class="string">"用户"</span>]</span><br><span class="line"><span class="built_in">Time</span> taken: <span class="number">0.322</span> seconds, Fetched: <span class="number">1</span> row(s)</span><br></pre></td></tr></table></figure></p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>定义离线更新文章画像的方法，首先合并最近一个小时的文章信息，接着计算每个词的 TF-IDF 和 TextRank 权重，并根据 TF-IDF 和 TextRank 权重计算得出文章关键词和主题词，最后将文章画像信息保存到 Hive<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_article_profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定时更新文章画像</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ua = UpdateArticle()</span><br><span class="line">    <span class="comment"># 合并文章信息</span></span><br><span class="line">    sentence_df = ua.merge_article_data()</span><br><span class="line">    <span class="keyword">if</span> sentence_df.rdd.collect():</span><br><span class="line">        textrank_keywords_df, keywordsIndex = ua.generate_article_label()</span><br><span class="line">        ua.get_article_profile(textrank_keywords_df, keywordsIndex)</span><br></pre></td></tr></table></figure></p><p>利用 Apscheduler 添加定时更新文章画像任务，设定每隔 1 个小时更新一次<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">from</span> apscheduler.schedulers.blocking <span class="keyword">import</span> BlockingScheduler</span><br><span class="line"><span class="title">from</span> apscheduler.executors.pool <span class="keyword">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建scheduler,多进程执行</span></span><br><span class="line"><span class="title">executors</span> = &#123;</span><br><span class="line">    '<span class="keyword">default</span>': <span class="type">ProcessPoolExecutor</span>(3)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="title">scheduler</span> = <span class="type">BlockingScheduler</span>(executors=executors)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 添加一个定时更新文章画像的任务,每隔1个小时运行一次</span></span><br><span class="line"><span class="title">scheduler</span>.add_job(update_article_profile, trigger='interval', hours=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="title">scheduler</span>.start()</span><br></pre></td></tr></table></figure></p><p>利用 Supervisor 进行进程管理，配置文件如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[program:offline]</span><br><span class="line">environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python</span><br><span class="line">command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py</span><br><span class="line">directory=/root/toutiao_project/scheduler</span><br><span class="line">user=root</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stdout_logfile=/root/logs/offlinesuper.log</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上述步骤中，我们已经将业务数据和用户行为数据同步到了推荐系统数据库当中，接下来，我们就要对文章数据和用户数据进行分析，构建文章画像和用户画像，本文我们主要讲解如何构建文章画像。文章画像由关键词和主题词组成，我们将每个词的 TF-IDF 权重和 TextRank 权重的乘积
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之3、收集用户行为数据</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B3%E3%80%81%E6%94%B6%E9%9B%86%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之3、收集用户行为数据/</id>
    <published>2019-12-05T13:21:23.000Z</published>
    <updated>2020-01-10T02:21:11.036Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中，我们完成了业务数据的同步，在推荐系统中另一个必不可少的数据就是用户行为数据，可以说用户行为数据是推荐系统的基石，巧妇难为无米之炊，所以接下来，我们就要将用户的行为数据同步到推荐系统数据库中。</p><p>在文章推荐系统中，用户行为包括曝光、点击、停留、收藏、分享等，所以这里我们定义的用户行为数据的字段包括：发生时间（actionTime）、停留时间（readTime）、频道 ID（channelId）、事件名称（action）、用户 ID（userId）、文章 ID（articleId）以及算法 ID（algorithmCombine），这里采用 json 格式，如下所示<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 曝光的参数</span></span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:35"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">0</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"exposure"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"[18577, 14299]"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># 对文章触发行为的参数</span></span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:36"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"18577"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:38"</span>,<span class="string">"readTime"</span>:<span class="string">"1621"</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"read"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"18577"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:39"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: <span class="string">"1"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:39"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:41"</span>,<span class="string">"readTime"</span>:<span class="string">"914"</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"read"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:47"</span>,<span class="string">"readTime"</span>:<span class="string">"7256"</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"read"</span>, <span class="string">"userId"</span>: <span class="string">"1"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p><h1 id="用户离线行为数据"><a href="#用户离线行为数据" class="headerlink" title="用户离线行为数据"></a>用户离线行为数据</h1><p>由于用户行为数据规模庞大，通常是每天更新一次，以供离线计算使用。首先，在 Hive 中创建用户行为数据库 profile 及用户行为表 user_action，设置按照日期进行分区，并匹配 json 格式<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建用户行为数据库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> profile <span class="keyword">comment</span> <span class="string">"use action"</span> location <span class="string">'/user/hive/warehouse/profile.db/'</span>;</span><br><span class="line"><span class="comment">-- 创建用户行为信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_action</span><br><span class="line">(</span><br><span class="line">    actionTime <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"user actions time"</span>,</span><br><span class="line">    readTime   <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"user reading time"</span>,</span><br><span class="line">    channelId  <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article channel id"</span>,</span><br><span class="line">    param <span class="keyword">map</span> <span class="keyword">comment</span> <span class="string">"action parameter"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"user primitive action"</span></span><br><span class="line">    PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">STRING</span>) # 按照日期分区</span><br><span class="line">    <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hive.hcatalog.data.JsonSerDe'</span> # 匹配<span class="keyword">json</span>格式</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/profile.db/user_action'</span>;</span><br></pre></td></tr></table></figure></p><p>通常用户行为数据被保存在应用服务器的日志文件中，我们可以利用 Flume 监听应用服务器上的日志文件，将用户行为数据收集到 Hive 的 user_action 表对应的 HDFS 目录中，Flume 配置如下<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.s1.channels= c1</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /root/logs/userClick.log</span><br><span class="line">a1.sources.s1.<span class="attribute">interceptors</span>=i1 i2</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">type</span>=regex_filter</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">regex</span>=\\&#123;.*\\&#125;</span><br><span class="line">a1.sources.s1.interceptors.i2.<span class="attribute">type</span>=timestamp</span><br><span class="line"></span><br><span class="line"><span class="comment"># c1</span></span><br><span class="line">a1.channels.c1.<span class="attribute">type</span>=memory</span><br><span class="line">a1.channels.c1.<span class="attribute">capacity</span>=30000</span><br><span class="line">a1.channels.c1.<span class="attribute">transactionCapacity</span>=1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># k1</span></span><br><span class="line">a1.sinks.k1.<span class="attribute">type</span>=hdfs</span><br><span class="line">a1.sinks.k1.<span class="attribute">channel</span>=c1</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">path</span>=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">fileType</span>=DataStream</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">writeFormat</span>=Text</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollInterval</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollSize</span>=10240</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollCount</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">idleTimeout</span>=60</span><br></pre></td></tr></table></figure></p><p>编写 Flume 启动脚本 collect_click.sh<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/root/bigdata/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/root/bigdata/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1</span><br></pre></td></tr></table></figure></p><p>Flume 自动生成目录后，需要手动关联 Hive 分区后才能加载到数据<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> user_action <span class="keyword">add</span> <span class="keyword">partition</span> (dt=<span class="string">'2019-11-11'</span>) location <span class="string">"/user/hive/warehouse/profile.db/user_action/2011-11-11/"</span></span><br></pre></td></tr></table></figure></p><h1 id="用户实时行为数据"><a href="#用户实时行为数据" class="headerlink" title="用户实时行为数据"></a>用户实时行为数据</h1><p>为了提高推荐的实时性，我们也需要收集用户的实时行为数据，以供在线计算使用。这里利用 Flume 将日志收集到 Kafka，在线计算任务可以从 Kafka 读取用户实时行为数据。首先，开启 zookeeper，以守护进程运行<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/zookeeper-server-start.sh -daemon /</span>root<span class="regexp">/bigdata/</span>kafka<span class="regexp">/config/</span>zookeeper.properties</span><br></pre></td></tr></table></figure></p><p>开启 Kafka<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启消息生产者</span></span><br><span class="line">/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.19092 --sync --topic click-trace</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启消费者</span></span><br><span class="line">/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace</span><br></pre></td></tr></table></figure></p><p>修改 Flume 的日志收集配置文件，添加 c2 和 k2 ，将日志数据收集到 Kafka<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">a1.sources.s1.channels= c1 c2</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /root/logs/userClick.log</span><br><span class="line">a1.sources.s1.<span class="attribute">interceptors</span>=i1 i2</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">type</span>=regex_filter</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">regex</span>=\\&#123;.*\\&#125;</span><br><span class="line">a1.sources.s1.interceptors.i2.<span class="attribute">type</span>=timestamp</span><br><span class="line"></span><br><span class="line"><span class="comment"># c1</span></span><br><span class="line">a1.channels.c1.<span class="attribute">type</span>=memory</span><br><span class="line">a1.channels.c1.<span class="attribute">capacity</span>=30000</span><br><span class="line">a1.channels.c1.<span class="attribute">transactionCapacity</span>=1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># c2</span></span><br><span class="line">a1.channels.c2.<span class="attribute">type</span>=memory</span><br><span class="line">a1.channels.c2.<span class="attribute">capacity</span>=30000</span><br><span class="line">a1.channels.c2.<span class="attribute">transactionCapacity</span>=1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># k1</span></span><br><span class="line">a1.sinks.k1.<span class="attribute">type</span>=hdfs</span><br><span class="line">a1.sinks.k1.<span class="attribute">channel</span>=c1</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">path</span>=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">fileType</span>=DataStream</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">writeFormat</span>=Text</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollInterval</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollSize</span>=10240</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollCount</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">idleTimeout</span>=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># k2</span></span><br><span class="line">a1.sinks.k2.<span class="attribute">channel</span>=c2</span><br><span class="line">a1.sinks.k2.<span class="attribute">type</span>=org.apache.flume.supervisorctl</span><br><span class="line">我们可以利用supervisorctl来管理supervisor。sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k2.kafka.bootstrap.<span class="attribute">servers</span>=192.168.19.137:9092</span><br><span class="line">a1.sinks.k2.kafka.<span class="attribute">topic</span>=click-trace</span><br><span class="line">a1.sinks.k2.kafka.<span class="attribute">batchSize</span>=20</span><br><span class="line">a1.sinks.k2.kafka.producer.<span class="attribute">requiredAcks</span>=1</span><br></pre></td></tr></table></figure></p><p>编写 Kafka 启动脚本 start_kafka.sh<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># 启动zookeeper</span></span><br><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/zookeeper-server-start.sh -daemon /</span>root<span class="regexp">/bigdata/</span>kafka<span class="regexp">/config/</span>zookeeper.properties</span><br><span class="line"><span class="comment"># 启动kafka</span></span><br><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/kafka-server-start.sh /</span>root<span class="regexp">/bigdata/</span>kafka<span class="regexp">/config/</span>server.properties</span><br><span class="line"><span class="comment"># 增加topic</span></span><br><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic click-trace --partitions 1</span></span><br></pre></td></tr></table></figure></p><h1 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h1><p>我们这里使用 Supervisor 进行进程管理，当进程异常时可以自动重启，Flume 进程配置如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[program:collect-click]</span><br><span class="line">command=/bin/bash /root/toutiao_project/scripts/collect_click.sh</span><br><span class="line">user=root</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stdout_logfile=/root/logs/collect.log</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br></pre></td></tr></table></figure></p><p>Kafka 进程配置如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[program:kafka]</span><br><span class="line">command=/bin/bash /root/toutiao_project/scripts/start_kafka.sh</span><br><span class="line">user=root</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stdout_logfile=/root/logs/kafka.log</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br></pre></td></tr></table></figure></p><p>启动 Supervisor<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">supervisord -c <span class="regexp">/etc/</span>supervisord.conf</span><br></pre></td></tr></table></figure></p><p>启动 Kafka 消费者，并在应用服务器日志文件中写入日志数据，Kafka 消费者即可收集到实时行为数据<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动Kafka消费者</span></span><br><span class="line">/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 写入日志数据</span></span><br><span class="line">echo &#123;\"actionTime\":\"2019-04-10 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":&#123;\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"&#125;&#125; &gt;&gt; userClick.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 消费者接收到日志数据</span></span><br><span class="line">&#123;"actionTime":"2019-04-10 21:04:39","readTime":"","channelId":18,"param":&#123;"action": "click", "userId": "2", "articleId": "14299", "algorithmCombine": "C2"&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>Supervisor 常用命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">supervisorctl</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> status              <span class="comment"># 查看程序状态</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> start apscheduler   <span class="comment"># 启动apscheduler单一程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> stop toutiao:*      <span class="comment"># 关闭toutiao组程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> start toutiao:*     <span class="comment"># 启动toutiao组程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> restart toutiao:*   <span class="comment"># 重启toutiao组程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> update              <span class="comment"># 重启配置文件修改过的程序</span></span></span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一篇文章中，我们完成了业务数据的同步，在推荐系统中另一个必不可少的数据就是用户行为数据，可以说用户行为数据是推荐系统的基石，巧妇难为无米之炊，所以接下来，我们就要将用户的行为数据同步到推荐系统数据库中。&lt;/p&gt;
&lt;p&gt;在文章推荐系统中，用户行为包括曝光、点击、停留、收藏
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之2、同步业务数据.md</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B2%E3%80%81%E5%90%8C%E6%AD%A5%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之2、同步业务数据/</id>
    <published>2019-12-05T13:21:22.000Z</published>
    <updated>2020-01-10T02:21:06.052Z</updated>
    
    <content type="html"><![CDATA[<p>在推荐系统架构中，推荐系统的数据库和业务系统的数据库是分离的，这样才能有效避免推荐系统的数据读写、计算等对业务系统的影响，所以同步业务数据往往也是推荐系统要做的第一件事情。通常业务数据是存储在关系型数据库中，比如 MySQL，而推荐系统通常需要使用大数据平台，比如 Hadoop，我们可以利用 Sqoop 将 MySQL 中的业务数据同步到 Hive 中，在我们的文章推荐系统中，需要同步的数据包括：</p><ul><li>用户信息表（user_profile）：包括 user_id | gender | birthday   | real_name | create_time | update_time   | register_media_time | id_number | id_card_front | id_card_back | id_card_handheld | area | company | career 等字段。</li><li>用户基础信息表（user_basic）：包括 user_id | mobile      | password | user_name       | profile_photo                | last_login          | is_media | article_count | following_count | fans_count | like_count | read_count | introduction    | certificate | is_verified | account | email       | status 等字段。</li><li>文章信息表（news_article_basic）：包括 article_id | user_id | channel_id | title                         | cover                   | is_advertising | create_time         | status | reviewer_id | review_time | delete_time | comment_count | allow_comment | update_time         | reject_reason 等字段。</li><li>文章内容表（news_article_content）：包括 article_id | content | update_time 等字段。</li><li>频道信息表（news_channel）：包括 channel_id | channel_name | create_time         | update_time         | sequence | is_visible | is_default 等字段。</li></ul><p>首先，在 Hive 中创建业务数据库 toutiao，并创建相关表<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> toutiao <span class="keyword">comment</span> <span class="string">"user,news information of mysql"</span> location <span class="string">'/user/hive/warehouse/toutiao.db/'</span>;</span><br><span class="line"><span class="comment">-- 用户信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_profile</span><br><span class="line">(</span><br><span class="line">    user_id             <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"userid"</span>,</span><br><span class="line">    gender              <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"gender"</span>,</span><br><span class="line">    birthday            <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"birthday"</span>,</span><br><span class="line">    real_name           <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"real_name"</span>,</span><br><span class="line">    create_time         <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"create_time"</span>,</span><br><span class="line">    update_time         <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span>,</span><br><span class="line">    register_media_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"register_media_time"</span>,</span><br><span class="line">    id_number           <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_number"</span>,</span><br><span class="line">    id_card_front       <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_card_front"</span>,</span><br><span class="line">    id_card_back        <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_card_back"</span>,</span><br><span class="line">    id_card_handheld    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_card_handheld"</span>,</span><br><span class="line">    area                <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"area"</span>,</span><br><span class="line">    company             <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"company"</span>,</span><br><span class="line">    career              <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"career"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao user profile"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/user_profile'</span>;</span><br><span class="line"><span class="comment">-- 用户基础信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_basic</span><br><span class="line">(</span><br><span class="line">    user_id         <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"user_id"</span>,</span><br><span class="line">    mobile          <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"mobile"</span>,</span><br><span class="line">    <span class="keyword">password</span>        <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"password"</span>,</span><br><span class="line">    profile_photo   <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"profile_photo"</span>,</span><br><span class="line">    last_login      <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"last_login"</span>,</span><br><span class="line">    is_media        <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_media"</span>,</span><br><span class="line">    article_count   <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_count"</span>,</span><br><span class="line">    following_count <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"following_count"</span>,</span><br><span class="line">    fans_count      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"fans_count"</span>,</span><br><span class="line">    like_count      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"like_count"</span>,</span><br><span class="line">    read_count      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"read_count"</span>,</span><br><span class="line">    introduction    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"introduction"</span>,</span><br><span class="line">    certificate     <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"certificate"</span>,</span><br><span class="line">    is_verified     <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_verified"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao user basic"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/user_basic'</span>;</span><br><span class="line"><span class="comment">-- 文章基础信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> news_article_basic</span><br><span class="line">(</span><br><span class="line">    article_id  <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    user_id     <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"user_id"</span>,</span><br><span class="line">    channel_id  <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    title       <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"title"</span>,</span><br><span class="line">    <span class="keyword">status</span>      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"status"</span>,</span><br><span class="line">    update_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_article_basic"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/news_article_basic'</span>;</span><br><span class="line"><span class="comment">-- 文章频道表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> news_channel</span><br><span class="line">(</span><br><span class="line">    channel_id   <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    channel_name <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"channel_name"</span>,</span><br><span class="line">    create_time  <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"create_time"</span>,</span><br><span class="line">    update_time  <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span>,</span><br><span class="line">    <span class="keyword">sequence</span>     <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"sequence"</span>,</span><br><span class="line">    is_visible   <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_visible"</span>,</span><br><span class="line">    is_default   <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_default"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_channel"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/news_channel'</span>;</span><br><span class="line"><span class="comment">-- 文章内容表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> news_article_content</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    <span class="keyword">content</span>    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"content"</span>,</span><br><span class="line">    update_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_article_content"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/news_article_content'</span>;</span><br></pre></td></tr></table></figure></p><p>查看 Sqoop 连接到 MySQL 的数据库列表<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P</span><br><span class="line"></span><br><span class="line">mysql</span><br><span class="line">sys</span><br><span class="line">toutiao</span><br></pre></td></tr></table></figure></p><p>Sqoop 可以指定全量导入和增量导入，通常我们可以先执行一次全量导入，将历史数据全部导入进来，后面再通过定时任务执行增量导入，来保持 MySQL 和 Hive 的数据同步，全量导入不需要提前创建 Hive 表，可以自动创建<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">array=(user_profile user_basic news_article_basic news_channel news_article_content)</span><br><span class="line"></span><br><span class="line">for table_name in $&#123;array[@]&#125;;</span><br><span class="line">do</span><br><span class="line">    sqoop import \</span><br><span class="line">        --connect jdbc:mysql://192.168.19.137/toutiao \</span><br><span class="line">        --username root \</span><br><span class="line">        --password password \</span><br><span class="line">        --table $table_name \</span><br><span class="line">        --m 5 \ # 线程数</span><br><span class="line">        --hive-home /root/bigdata/hive \ # hive路径</span><br><span class="line">        --hive-import \ # 导入形式</span><br><span class="line">        --create-hive-table  \ # 自动创建表</span><br><span class="line">        --hive-drop-import-delims \</span><br><span class="line">        --warehouse-dir /user/hive/warehouse/toutiao.db \ # 导入地址</span><br><span class="line">        --hive-table toutiao.$table_name </span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>增量导入，有 append 和 lastmodified 两种模式</p><ul><li><p>append：通过指定一个递增的列进行更新，只能追加，不能修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">num=0</span><br><span class="line">declare -A check</span><br><span class="line">check=([user_profile]=user_id [user_basic]=user_id [news_article_basic]=article_id [news_channel]=channel_id [news_article_content]=channel_id)</span><br><span class="line"></span><br><span class="line">for k in $&#123;!check[@]&#125;</span><br><span class="line">do</span><br><span class="line">    sqoop import \</span><br><span class="line">        --connect jdbc:mysql://192.168.19.137/toutiao \</span><br><span class="line">        --username root \</span><br><span class="line">        --password password \</span><br><span class="line">        --table $k \</span><br><span class="line">        --m 4 \</span><br><span class="line">        --hive-home /root/bigdata/hive \ # hive路径</span><br><span class="line">        --hive-import \ # 导入到hive</span><br><span class="line">        --create-hive-table  \ # 自动创建表</span><br><span class="line">        --incremental append \ # 按照id更新</span><br><span class="line">        --check-column $&#123;check[$k]&#125; \ # 指定id列</span><br><span class="line">        --last-value $&#123;num&#125; # 指定最后更新的id</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>lastmodified：按照最后修改时间更新，支持追加和修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">time=`date +"%Y-%m-%d" -d "-1day"`</span><br><span class="line">declare -A check</span><br><span class="line">check=([user_profile]=update_time [user_basic]=last_login [news_article_basic]=update_time [news_channel]=update_time)</span><br><span class="line">declare -A merge</span><br><span class="line">merge=([user_profile]=user_id [user_basic]=user_id [news_article_basic]=article_id [news_channel]=channel_id)</span><br><span class="line"></span><br><span class="line">for k in $&#123;!check[@]&#125;</span><br><span class="line">do</span><br><span class="line">    sqoop import \</span><br><span class="line">        --connect jdbc:mysql://192.168.19.137/toutiao \</span><br><span class="line">        --username root \</span><br><span class="line">        --password password \</span><br><span class="line">        --table $k \</span><br><span class="line">        --m 4 \</span><br><span class="line">        --target-dir /user/hive/warehouse/toutiao.db/$k \ # hive路径</span><br><span class="line">        --incremental lastmodified \ # 按照最后修改时间更新</span><br><span class="line">        --check-column $&#123;check[$k]&#125; \ # 指定时间列</span><br><span class="line">        --merge-key $&#123;merge[$k]&#125; \ # 根据指定列合并重复或修改数据</span><br><span class="line">        --last-value $&#123;time&#125; # 指定最后修改时间</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li></ul><p>Sqoop 可以直接导入到 Hive，自动创建 Hive 表，但是 lastmodified 模式不支持<br>Sqoop 也可以先导入到 HDFS，然后再建立 Hive 表关联，为了使用 lastmodified 模式，通常使用这种方法<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--target-dir <span class="meta-keyword">/user/</span>hive<span class="meta-keyword">/warehouse/</span>toutiao.db/user_profile <span class="meta"># 指定导入的HDFS目录</span></span><br></pre></td></tr></table></figure></p><p>若先导入到 HDFS，需要注意 HDFS 默认分隔符为 <code>,</code> 而 Hive 默认分隔符为 <code>/u0001</code>，所以需要在创建 Hive 表时指定分隔符为 HDFS 分隔符 <code>,</code>，若不指定分隔符，查询结果将全部为 NULL<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">row <span class="built_in">format</span> delimited fields terminated <span class="keyword">by</span> <span class="string">','</span> <span class="comment"># 指定分隔符</span></span><br></pre></td></tr></table></figure></p><p>如果 MySQL 数据中存在特殊字符，如 <code>, \t \n</code> 都会导致 Hive 读取失败（但不影响导入到 HDFS 中），可以利用 <code>--query</code> 参数，在读取时使用 <code>REPLACE() CHAR() CHR()</code> 进行字符替换。如果特殊字符过多，比如 news_article_content 表，可以选择全量导入<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--<span class="keyword">query</span> 'select article_id, user_id, channel_id, <span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(title, <span class="built_in">CHAR</span>(13),<span class="string">""</span>),<span class="built_in">CHAR</span>(10),<span class="string">""</span>), <span class="string">","</span>, <span class="string">" "</span>) title, status, update_time from news_article_basic WHERE <span class="variable">$CONDITIONS</span>' \</span><br></pre></td></tr></table></figure></p><p>如果 MySQL 数据中存在 tinyint 类型,必须在 <code>--connet</code> 中加入 <code>?tinyInt1isBit=false</code>，防止 Hive 将 tinyint 类型默认转化为 boolean 类型<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--connect <span class="string">jdbc:</span><span class="string">mysql:</span><span class="comment">//192.168.19.137/toutiao?tinyInt1isBit=false</span></span><br></pre></td></tr></table></figure></p><p>MySQL 与 Hive 对应类型如下<br>MySQL|Hive<br>-|-<br>bigint|bigint<br>tinyint|boolean<br>int|int<br>double|double<br>bit|boolean<br>varchar|string<br>decimal|double<br>date / timestamp|string<br>我们可以利用 crontab 来创建定时任务，将更新命令写入脚本 import_incremental.sh，输入 <code>crontab -e</code> 打开编辑界面，输入如下内容，即可定时执行数据同步任务<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每隔半小时增量导入一次</span></span><br><span class="line">*<span class="regexp">/30 * * * * /</span>root<span class="regexp">/toutiao_project/</span>scripts<span class="regexp">/import_incremental.sh</span></span><br></pre></td></tr></table></figure></p><p>crontab 命令格式为 <code>* * * * * shell</code>，其中前五个 <code>*</code> 分别代表分钟 (0-59)、小时(0-59)、月内的某天(1-31)、月(1-12)、周内的某天(0-7，周日为 0 或 7)，<code>shell</code> 表示要执行的命令或脚本，使用方法如下<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 每隔5分钟运行一次backupscript脚本</span></span><br><span class="line"><span class="emphasis">*/5 *</span> <span class="bullet">* *</span> * /root/backupscript.sh</span><br><span class="line"><span class="section"># 每天的凌晨1点运行backupscript脚本</span></span><br><span class="line">0 1 <span class="bullet">* *</span> * /root/backupscript.sh</span><br><span class="line"><span class="section"># 每月的第一个凌晨3:15运行backupscript脚本</span></span><br><span class="line">15 3 1 <span class="bullet">* *</span> /root/backupscript.sh</span><br></pre></td></tr></table></figure></p><p>crontab 常用命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">crontab -e      # 修改 crontab 文件，如果文件不存在会自动创建。 </span><br><span class="line">crontab -l      # 显示 crontab 文件。 </span><br><span class="line">crontab -r      # 删除 crontab 文件。</span><br><span class="line">crontab -ir     # 删除 crontab 文件前提醒用户。</span><br><span class="line"></span><br><span class="line">service crond status      # 查看crontab服务状态</span><br><span class="line">service crond start       # 启动服务 </span><br><span class="line">service crond stop        # 关闭服务 </span><br><span class="line">service crond restart     # 重启服务 </span><br><span class="line">service crond reload      # 重新载入配置</span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在推荐系统架构中，推荐系统的数据库和业务系统的数据库是分离的，这样才能有效避免推荐系统的数据读写、计算等对业务系统的影响，所以同步业务数据往往也是推荐系统要做的第一件事情。通常业务数据是存储在关系型数据库中，比如 MySQL，而推荐系统通常需要使用大数据平台，比如 Hado
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之1、推荐流程设计</title>
    <link href="http://thinkgamer.cn/2019/12/05/%E6%8E%A8%E8%8D%90%E4%B8%8E%E6%8E%92%E5%BA%8F/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B1%E3%80%81%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B%E8%AE%BE%E8%AE%A1/"/>
    <id>http://thinkgamer.cn/2019/12/05/推荐与排序/文章推荐系统/《文章推荐系统》系列之1、推荐流程设计/</id>
    <published>2019-12-05T13:21:21.000Z</published>
    <updated>2020-01-10T02:21:37.842Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统主要解决的是信息过载的问题，目标是从海量物品筛选出不同用户各自喜欢的物品，从而为每个用户提供个性化的推荐。推荐系统往往架设在大规模的业务系统之上，面临着用户的不断增长，物品的不断变化，并且有着全面的推荐评价指标和严格的性能要求（Netflix 的请求时间在 250 ms 以内，今日头条的请求时间在 200ms 以内），所以推荐系统很难一次性地快速计算出用户所喜好的物品，并且同时满足准确度、多样性等评价指标。为了解决如上这些问题，推荐系统通常被设计为三个阶段：召回、排序和调整，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ee044ed9dd9cc759.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="召回、排序和调整"></p><p>在召回阶段，首先筛选出和用户直接相关或间接相关的物品，将原始数据从万、百万、亿级别缩小到万、千级别；在排序阶段，通常使用二分类算法来预测用户对物品的喜好程度（或者是点击率），然后将物品按照喜好程序从大到小依次排列，筛选出用户最有可能喜欢的物品，这里又将召回数据从万、千级别缩小到千、百级别；最后在调整阶段，需要过滤掉重复推荐的、已经购买或阅读的、已经下线的物品，当召回和排序结果不足时，需要使用热门物品进行补充，最后合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。</p><p>这里以文章推荐系统为例，讲述一下推荐系统的完整流程，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-40327d59055eba56.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="推荐系统的完整流程"></p><ol><li><p>同步业务数据<br>为了避免推荐系统的数据读写、计算等对应用产生影响，我们首先要将业务数据从应用数据库 MySQL 同步到推荐系统数据库 Hive 中，这里利用 Sqoop 先将 MySQL 中的业务数据同步到推荐系统的 HDFS 中，再关联到指定的 Hive 表中，这样就可以在推荐系统数据库 Hive 中使用用户数据和文章数据了，并且不会对应用产生任何影响。</p></li><li><p>收集用户行为数据<br>除了用户数据和文章数据，我们还需要得到用户对文章的行为数据，比如曝光、点击、阅读、点赞、收藏、分享、评论等。我们的用户行为数据是记录在应用服务器的日志文件中的，所以可以利用 Flume 对应用服务器的日志文件进行监听，一方面将收集到的用户行为数据同步到 HDFS 中，并关联到 Hive 的用户行为表，每天更新一次，以供离线计算使用。另一方面将 Flume 收集到的用户行为数据同步到 Kafka，实时更新，以供在线计算使用。</p></li><li><p>构建离线画像和特征</p></li></ol><ul><li><p>文章画像由关键词和主题词组成，我们首先读取 Hive 中的文章数据，将文章内容进行分词，根据 TF-IDF 模型计算每个词的权重，将 TF-IDF 权重最高的 K 个词作为关键词，再根据 TextRank 模型计算每个词的权重，将 TextRank 权重最高的 K 个词与 TF-IDF 权重最高的 K 个词的共现词作为主题词，将关键词和主题词存储到 Hive 的文章画像表中。接下来，利用 Word2Vec 模型，计算得到所有关键词的平均向量，作为文章的词向量，存储到 Hive 的文章向量表中，并利用 BucketedRandomProjectionLSH 模型计算得到文章的相似度，将每篇文章相似度最高的 K 篇文章，存储到 Hbase 的文章相似表中。这样我们就得到了每篇文章的画像、词向量以及相似文章列表。</p></li><li><p>构建离线用户画像<br>我们可以将用户喜欢的文章的主题词作为用户标签，以便后面根据用户标签来推荐符合其偏好的文章。首先读取用户行为数据和文章画像数据，计算在用户产生过行为的所有文章中，每个主题词的权重，不同的行为，权重不同，计算公式为：用户标签权重 =（用户行为分值之和）x 时间衰减，这样就计算得到了用户的标签及标签权重，接着读取用户数据，得到用户基础信息，将用户标签、标签权重及用户基础信息一并存储到 Hbase 的用户画像表中。到这里我们已经通过机器学习算法，基于用户和文章的业务数据得到了用户和文章的画像，但为了后面可以更方便地将数据提供给深度学习模型进行训练，我们还需要将画像数据进一步抽象为特征数据。</p></li><li><p>构建离线文章特征<br>由于已经有了画像信息，特征构造就变得简单了。读取文章画像数据，将文章权重最高的 K 个关键词的权重作为文章关键词权重向量，将频道 ID、关键词权重向量、词向量作为文章特征存储到 Hbase 的文章特征表。</p></li><li><p>构建离线用户特征<br>读取用户画像数据，将权重最高的 K 个标签的权重作为用户标签权重向量，将用户标签权重向量作为用户特征存储到 Hbase 的用户特征表。</p></li></ul><ol><li>多路召回</li></ol><ul><li><p>基于模型的离线召回<br>我们可以根据用户的历史点击行为来预测相似用户，并利用相似用户的点击行为来预测对文章的偏好得分，这种召回方式称为 u2u2i。获取用户历史点击行为数据，利用 ALS 模型计算得到用户对文章的偏好得分及文章列表，读取并过滤历史召回结果，防止重复推荐，将过滤后的偏好得分最高的 K 篇文章存入 Hbase 的召回结果表中，key 为 als，表明召回类型为 ALS 模型召回，并记录到 Hbase 的历史召回结果表。</p></li><li><p>基于内容的离线召回<br>我们可以根据用户的历史点击行为，向用户推荐其以前喜欢的文章的相似文章，这种方式称为 u2i2i。读取用户历史行为数据，获取用户历史发生过点击、阅读、收藏、分享等行为的文章，接着读取文章相似表，获取与发生行为的每篇文章相似度最高的 K 篇文章，然后读取并过滤历史召回结果，防止重复推荐，最后将过滤后的文章存入 Hbase 的召回结果表中，key 为 content，表明召回类型为内容召回，并记录到 Hbase 的历史召回结果表。</p></li><li><p>基于内容的在线召回<br>和上面一样，还是根据用户的点击行为，向用户推荐其喜欢的文章的相似文章，不过这里是用户实时发生的行为，所以叫做在线召回。读取 Kafka 中的用户实时行为数据，获取用户实时发生点击、阅读、收藏、分享等行为的文章，接着读取文章相似表，获取与发生行为的每篇文章相似度最高的 K 篇文章，然后读取并过滤历史召回结果，防止重复推荐，最后将过滤后的文章存入 Hbase 的召回结果表中，key 为 online，表明召回类型为在线召回，并记录到 Hbase 的历史召回结果表。</p></li><li><p>基于热门文章的在线召回<br>读取 Kafka 中的用户实时行为数据，获取用户当前发生点击、阅读、收藏、分享等行为的文章，增加这些文章在 Redis 中的热度分数。</p></li><li><p>基于新文章的在线召回<br>读取 Kafka 中的实时用户行为数据，获取新发布的文章，将其加入到 Redis 中，并设置过期时间。</p></li></ul><ol><li>排序</li></ol><p>不同模型的做法大致相同，这里以 LR 模型为例。</p><ul><li><p>基于 LR 模型的离线训练<br>读取 Hive 的用户历史行为数据，并切分为训练集和测试集，根据其中的用户 ID 和文章 ID，读取 Hbase 的用户特征数据和文章特征数据，将二者合并作为训练集的输入特征，将用户对文章是否点击作为训练集的标签，将上一次的模型参数作为 LR 模型的初始化参数，进行点击率预估训练，计算得出 AUC 等评分指标并进行推荐效果分析。</p></li><li><p>基于 LR 模型的在线排序<br>当推荐中心读取 Hbase 的推荐结果表无数据时，推荐中心将调用在线排序服务来重新获取推荐结果。排序服务首先读取 Hbase 的召回结果作为测试集，读取 Hbase 的用户特征数据和文章特征数据，将二者合并作为测试集的输入特征，使用 LR 模型进行点击率预估，计算得到点击率最高的前 K 个文章，然后读取并过滤历史推荐结果，防止重复推荐，最后将过滤后的文章列表存入 Hbase 的推荐结果表中，key 为 lr，表明排序类型为 LR 排序。</p></li></ul><ol><li>推荐中心</li></ol><ul><li><p>流量切分（ABTest）<br>我们可以根据用户 ID 进行哈希分桶，将流量切分到多个桶，每个桶对应一种排序策略，从而对比不同排序策略在线上环境的效果。</p></li><li><p>推荐数据读取逻辑<br>优先读取 Redis 和 Hbase 中缓存的推荐结果，若 Redis 和 Hbase 都为空，则调用在线排序服务获得推荐结果。</p></li><li><p>兜底补足（超时截断）<br>当调用排序服务无结果，或者读取超时的时候，推荐中心会截断当前请求，直接读取 Redis 中的热门文章和新文章作为推荐结果。</p></li><li><p>合并信息<br>合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。</p></li></ul><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://space.bilibili.com/61036655/channel/detail?cid=91348" target="_blank" rel="external">https://space.bilibili.com/61036655/channel/detail?cid=91348</a>（强烈推荐，蚂蚁大神的视频讲得很棒）</li><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;推荐系统主要解决的是信息过载的问题，目标是从海量物品筛选出不同用户各自喜欢的物品，从而为每个用户提供个性化的推荐。推荐系统往往架设在大规模的业务系统之上，面临着用户的不断增长，物品的不断变化，并且有着全面的推荐评价指标和严格的性能要求（Netflix 的请求时间在 250 
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Django3.0和Python3.7连接Mysql报：Error loading MySQLdb module. Did you install mysqlclient</title>
    <link href="http://thinkgamer.cn/2019/12/04/Python/Django3.0%E5%92%8CPython3.7%E8%BF%9E%E6%8E%A5Mysql%E6%8A%A5%EF%BC%9AError%20loading%20MySQLdb%20module.%20Did%20you%20install%20mysqlclient/"/>
    <id>http://thinkgamer.cn/2019/12/04/Python/Django3.0和Python3.7连接Mysql报：Error loading MySQLdb module. Did you install mysqlclient/</id>
    <published>2019-12-04T07:45:02.000Z</published>
    <updated>2020-01-10T02:56:42.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h1><ul><li>Python 3.7.3</li><li>Django 3.0<ul><li>安装：pip3 install -U Django</li><li>文档：<a href="https://docs.djangoproject.com/zh-hans/3.0/contents/" target="_blank" rel="external">https://docs.djangoproject.com/zh-hans/3.0/contents/</a></li></ul></li></ul><h1 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h1><p>创建项目</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">django-admin startproject mysite</span></span><br></pre></td></tr></table></figure><p>配置Mysql<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATABASES = &#123;</span><br><span class="line">    'default': &#123;</span><br><span class="line">        'ENGINE': 'django.db.backends.mysql',</span><br><span class="line">        'NAME': 'mysite',</span><br><span class="line">        'USER': 'root',</span><br><span class="line">        'PASSWORD': '<span class="number">123456</span>',</span><br><span class="line">        'HOST': '127.0.0.1',</span><br><span class="line">        'PORT': '<span class="number">3306</span>',</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>创建视图<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">python3</span> manage.<span class="keyword">py</span> startapp <span class="built_in">index</span></span><br></pre></td></tr></table></figure></p><p>报错<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[MeetUpRec] python3 manage.py startapp index                                                                                                           14:45:34  ☁  master ☂ ✭</span><br><span class="line">Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/backends/mysql/base.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">import</span> MySQLdb <span class="keyword">as</span> <span class="keyword">Database</span></span><br><span class="line">ModuleNotFoundError: <span class="keyword">No</span> <span class="keyword">module</span> named <span class="string">'MySQLdb'</span></span><br><span class="line"></span><br><span class="line">The above <span class="keyword">exception</span> was the direct cause <span class="keyword">of</span> the <span class="keyword">following</span> <span class="keyword">exception</span>:</span><br><span class="line"></span><br><span class="line">Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"manage.py"</span>, line <span class="number">21</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">main</span>()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"manage.py"</span>, line <span class="number">17</span>, <span class="keyword">in</span> <span class="keyword">main</span></span><br><span class="line">    execute_from_command_line(sys.argv)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py"</span>, line <span class="number">401</span>, <span class="keyword">in</span> execute_from_command_line</span><br><span class="line">    utility.execute()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py"</span>, line <span class="number">377</span>, <span class="keyword">in</span> <span class="keyword">execute</span></span><br><span class="line">    django.setup()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/__init__.py"</span>, line <span class="number">24</span>, <span class="keyword">in</span> setup</span><br><span class="line">    apps.populate(settings.INSTALLED_APPS)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/apps/registry.py"</span>, line <span class="number">114</span>, <span class="keyword">in</span> populate</span><br><span class="line">    app_config.import_models()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/apps/config.py"</span>, line <span class="number">211</span>, <span class="keyword">in</span> import_models</span><br><span class="line">    self.models_module = import_module(models_module_name)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"</span>, line <span class="number">127</span>, <span class="keyword">in</span> import_module</span><br><span class="line">    <span class="keyword">return</span> _bootstrap._gcd_import(<span class="keyword">name</span>[<span class="keyword">level</span>:], <span class="keyword">package</span>, <span class="keyword">level</span>)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">1006</span>, <span class="keyword">in</span> _gcd_import</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">983</span>, <span class="keyword">in</span> _find_and_load</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">967</span>, <span class="keyword">in</span> _find_and_load_unlocked</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">677</span>, <span class="keyword">in</span> _load_unlocked</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap_external&gt;"</span>, line <span class="number">728</span>, <span class="keyword">in</span> exec_module</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">219</span>, <span class="keyword">in</span> _call_with_frames_removed</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/contrib/auth/models.py"</span>, line <span class="number">2</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">from</span> django.contrib.auth.base_user <span class="keyword">import</span> AbstractBaseUser, BaseUserManager</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/contrib/auth/base_user.py"</span>, line <span class="number">47</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">class</span> AbstractBaseUser(models.Model):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/models/base.py"</span>, line <span class="number">121</span>, <span class="keyword">in</span> __new__</span><br><span class="line">    new_class.add_to_class(<span class="string">'_meta'</span>, Options(meta, app_label))</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/models/base.py"</span>, line <span class="number">325</span>, <span class="keyword">in</span> add_to_class</span><br><span class="line">    value.contribute_to_class(cls, <span class="keyword">name</span>)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/models/options.py"</span>, line <span class="number">208</span>, <span class="keyword">in</span> contribute_to_class</span><br><span class="line">    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/__init__.py"</span>, line <span class="number">28</span>, <span class="keyword">in</span> __getattr__</span><br><span class="line">    <span class="keyword">return</span> getattr(connections[DEFAULT_DB_ALIAS], item)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/utils.py"</span>, line <span class="number">207</span>, <span class="keyword">in</span> __getitem__</span><br><span class="line">    backend = load_backend(db[<span class="string">'ENGINE'</span>])</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/utils.py"</span>, line <span class="number">111</span>, <span class="keyword">in</span> load_backend</span><br><span class="line">    <span class="keyword">return</span> import_module(<span class="string">'%s.base'</span> % backend_name)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"</span>, line <span class="number">127</span>, <span class="keyword">in</span> import_module</span><br><span class="line">    <span class="keyword">return</span> _bootstrap._gcd_import(<span class="keyword">name</span>[<span class="keyword">level</span>:], <span class="keyword">package</span>, <span class="keyword">level</span>)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/backends/mysql/base.py"</span>, line <span class="number">21</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    ) <span class="keyword">from</span> err</span><br><span class="line">django.core.exceptions.ImproperlyConfigured: <span class="keyword">Error</span> loading MySQLdb module.</span><br><span class="line">Did you <span class="keyword">install</span> mysqlclient?</span><br></pre></td></tr></table></figure></p><p>原因：在Python2中用的是mysqldb，但在Python3中用的是 mysqlclient</p><p>尝试解决：执行mysqlclient安装命令<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 <span class="keyword">install</span> mysqlclient</span><br></pre></td></tr></table></figure></p><p>报错：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Collecting mysqlclient</span><br><span class="line">  Using cached <span class="keyword">https</span>://<span class="built_in">files</span>.pythonhosted.org/packages/d0/<span class="number">97</span>/<span class="number">7326248</span>ac8d5049968bf4ec708a5d3d4806e412a42e74160d7f266a3e03a/mysqlclient<span class="number">-1.4</span><span class="number">.6</span>.tar.gz</span><br><span class="line">    ERROR: Complete output <span class="built_in">from</span> <span class="keyword">command</span> <span class="title">python</span> <span class="title">setup</span>.<span class="title">py</span> <span class="title">egg_info</span>:</span><br><span class="line">    ERROR: /bin/sh: mysql_config: <span class="keyword">command</span> <span class="title">not</span> <span class="title">found</span></span><br><span class="line">    /bin/sh: mariadb_config: <span class="keyword">command</span> <span class="title">not</span> <span class="title">found</span></span><br><span class="line">    /bin/sh: mysql_config: <span class="keyword">command</span> <span class="title">not</span> <span class="title">found</span></span><br><span class="line">    Traceback (most recent call <span class="keyword">last</span>):</span><br><span class="line">      File <span class="string">"&lt;string&gt;"</span>, <span class="built_in">line</span> <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">      File <span class="string">"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup.py"</span>, <span class="built_in">line</span> <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">        metadata, options = get_config()</span><br><span class="line">      File <span class="string">"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup_posix.py"</span>, <span class="built_in">line</span> <span class="number">61</span>, <span class="keyword">in</span> get_config</span><br><span class="line">        libs = mysql_config(<span class="string">"libs"</span>)</span><br><span class="line">      File <span class="string">"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup_posix.py"</span>, <span class="built_in">line</span> <span class="number">29</span>, <span class="keyword">in</span> mysql_config</span><br><span class="line">        raise EnvironmentError(<span class="string">"%s not found"</span> % (<span class="title">_mysql</span>_config_path,))</span><br><span class="line">    OSError: mysql_config <span class="keyword">not</span> found</span><br><span class="line">    <span class="comment">----------------------------------------</span></span><br><span class="line">ERROR: Command <span class="string">"python setup.py egg_info"</span> failed <span class="keyword">with</span> error code <span class="number">1</span> <span class="keyword">in</span> /<span class="keyword">private</span>/var/<span class="built_in">folders</span>/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install<span class="number">-9</span>cqr2rno/mysqlclient/</span><br></pre></td></tr></table></figure></p><p>没找到mysql配置，解决如下：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=<span class="variable">$PATH</span><span class="symbol">:/usr/local/mysql/bin</span></span><br></pre></td></tr></table></figure><p>再次安装，成功</p><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><center>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！---<center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center></center></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;环境说明&quot;&gt;&lt;a href=&quot;#环境说明&quot; class=&quot;headerlink&quot; title=&quot;环境说明&quot;&gt;&lt;/a&gt;环境说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Python 3.7.3&lt;/li&gt;
&lt;li&gt;Django 3.0&lt;ul&gt;
&lt;li&gt;安装：pip3 install
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Python" scheme="http://thinkgamer.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Spark使用Libsvm格式数据构造LabeledPoint格错误</title>
    <link href="http://thinkgamer.cn/2019/11/29/Spark/Spark%E4%BD%BF%E7%94%A8Libsvm%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0LabeledPoint%E6%A0%BC%E9%94%99%E8%AF%AF/"/>
    <id>http://thinkgamer.cn/2019/11/29/Spark/Spark使用Libsvm格式数据构造LabeledPoint格错误/</id>
    <published>2019-11-29T02:29:22.000Z</published>
    <updated>2020-01-10T03:11:05.685Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>使用libsvm格式的数据构造LabeledPoint格式，例如我的libsvm格式数据如下(索引下标最大值为，3000)：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">790718 1<span class="selector-pseudo">:1</span> 2<span class="selector-pseudo">:1</span> 4<span class="selector-pseudo">:1</span> 5<span class="selector-pseudo">:1</span> 6<span class="selector-pseudo">:1</span> 7<span class="selector-pseudo">:1</span> 9<span class="selector-pseudo">:1</span> 11<span class="selector-pseudo">:1</span> 13<span class="selector-pseudo">:1</span> 16<span class="selector-pseudo">:1</span> 19<span class="selector-pseudo">:1</span> 21<span class="selector-pseudo">:1</span> 28<span class="selector-pseudo">:1</span> 31<span class="selector-pseudo">:1</span> 43<span class="selector-pseudo">:1</span> 64<span class="selector-pseudo">:1</span> 65<span class="selector-pseudo">:1</span> 140<span class="selector-pseudo">:1</span> 164<span class="selector-pseudo">:1</span> 184<span class="selector-pseudo">:1</span> 296<span class="selector-pseudo">:1</span> 463<span class="selector-pseudo">:1</span> 481<span class="selector-pseudo">:1</span> 642<span class="selector-pseudo">:1</span> 813<span class="selector-pseudo">:1</span> 1093<span class="selector-pseudo">:1</span> 2288<span class="selector-pseudo">:1</span></span><br><span class="line">692384 9<span class="selector-pseudo">:1</span> 10<span class="selector-pseudo">:1</span> 16<span class="selector-pseudo">:1</span> 19<span class="selector-pseudo">:1</span> 30<span class="selector-pseudo">:1</span> 31<span class="selector-pseudo">:1</span> 54<span class="selector-pseudo">:1</span> 56<span class="selector-pseudo">:1</span> 69<span class="selector-pseudo">:1</span> 140<span class="selector-pseudo">:1</span> 142<span class="selector-pseudo">:1</span> 224<span class="selector-pseudo">:1</span> 232<span class="selector-pseudo">:1</span> 307<span class="selector-pseudo">:1</span> 601<span class="selector-pseudo">:1</span> 649<span class="selector-pseudo">:1</span> 692<span class="selector-pseudo">:1</span> 2851<span class="selector-pseudo">:1</span></span><br></pre></td></tr></table></figure></p><p>但是在构造LabeledPoint格式数据的时候忽略的应该创建的数组长度，使用如下代码：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val dataset = rdd</span><br><span class="line">    .<span class="built_in">map</span>( l =&gt; ( l._1, l._2.<span class="built_in">split</span>(<span class="string">" "</span>).<span class="built_in">map</span>(<span class="symbol">_</span>.<span class="built_in">split</span>(<span class="string">":"</span>)).<span class="built_in">map</span>(e =&gt; (e(<span class="number">0</span>).toInt-<span class="number">1</span>, e(<span class="number">1</span>).toDouble)) ) )</span><br><span class="line">    .<span class="built_in">map</span>(l =&gt; LabeledPoint(l._1.toDouble, Vectors.<span class="built_in">sparse</span>(l._2.<span class="built_in">length</span>, l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._1), l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._2))))</span><br><span class="line">    .toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</span><br></pre></td></tr></table></figure></p><p>所以报了 java.lang.IllegalArgumentException: requirement failed: Index 2287 out of bounds for vector of size 27 错误</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>将创建LabeledPoint数据的长度改为3000即可，如下：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val dataset = rdd</span><br><span class="line">    .<span class="built_in">map</span>( l =&gt; ( l._1, l._2.<span class="built_in">split</span>(<span class="string">" "</span>).<span class="built_in">map</span>(<span class="symbol">_</span>.<span class="built_in">split</span>(<span class="string">":"</span>)).<span class="built_in">map</span>(e =&gt; (e(<span class="number">0</span>).toInt-<span class="number">1</span>, e(<span class="number">1</span>).toDouble)) ) )</span><br><span class="line">    .<span class="built_in">map</span>(l =&gt; LabeledPoint(l._1.toDouble, Vectors.<span class="built_in">sparse</span>(<span class="number">3000</span>, l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._1), l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._2))))</span><br><span class="line">    .toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</span><br></pre></td></tr></table></figure><p>打印信息显示如下：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |<span class="string">-- label: double (nullable = false)</span></span><br><span class="line"><span class="string"> </span>|<span class="string">-- features: vector (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="string">+---------+--------------------+</span></span><br><span class="line">|<span class="string">    label</span>|<span class="string">            features</span>|</span><br><span class="line">+---------+--------------------+</span><br><span class="line">|<span class="string"> 790718.0</span>|<span class="string">(3275,[0,1,3,4,5,...</span>|</span><br><span class="line">|<span class="string"> 692384.0</span>|<span class="string">(3275,[8,9,15,18,...</span>|</span><br><span class="line">|<span class="string"> 672331.0</span>|<span class="string">(3275,[0,1,2,7,8,...</span>|</span><br><span class="line">|<span class="string">1646601.0</span>|<span class="string">   (3275,[30],[1.0])</span>|</span><br><span class="line">|<span class="string">1740585.0</span>|<span class="string">(3275,[0,3,6,9,11...</span>|</span><br><span class="line">|<span class="string"> 615659.0</span>|<span class="string">(3275,[2,4,5,30,4...</span>|</span><br><span class="line">|<span class="string"> 169763.0</span>|<span class="string">(3275,[1,2,3,4,7,...</span>|</span><br><span class="line">|<span class="string"> 639653.0</span>|<span class="string">(3275,[1,2,4,10,1...</span>|</span><br><span class="line">|<span class="string">1774993.0</span>|<span class="string">(3275,[6,11,13,14...</span>|</span><br><span class="line">|<span class="string">1680621.0</span>|<span class="string">(3275,[11,16,31],...</span>|</span><br><span class="line">+---------+--------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></p><p>完美解决问题！！！希望本文能够帮助到你！</p><p>不得不说对于Spark中一些格式的数据使用还是不太熟悉！</p><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><center>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！---<center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center></center></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;使用libsvm格式的数据构造LabeledPoint格式，例如我的libsvm格式数据如下(索引下标最大值为，3000)：&lt;br&gt;&lt;fig
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark与大数据" scheme="http://thinkgamer.cn/tags/Spark%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>NLP实战之基于TFIDF的文本相似度计算</title>
    <link href="http://thinkgamer.cn/2019/11/26/NLP/NLP%E5%AE%9E%E6%88%98%E4%B9%8B%E5%9F%BA%E4%BA%8ETFIDF%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    <id>http://thinkgamer.cn/2019/11/26/NLP/NLP实战之基于TFIDF的文本相似度计算/</id>
    <published>2019-11-26T09:03:03.000Z</published>
    <updated>2019-12-06T03:14:43.914Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TFIDF算法介绍"><a href="#TFIDF算法介绍" class="headerlink" title="TFIDF算法介绍"></a>TFIDF算法介绍</h3><p>TF-IDF（Term Frequency–InverseDocument Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><p>TF-IDF实际是TF*IDF，其中TF（Term Frequency）表示词条$t$在文档$D_i$中的出现的频率，TF的计算公式如下所示：</p><script type="math/tex; mode=display">TF_{t,D_i} = \frac{count(t)}{D_i}</script><p>其中IDF（InverseDocument Frequency）表示总文档与包含词条t的文档的比值求对数，IDF的计算公式如下所示： </p><script type="math/tex; mode=display">IDF_t = log \frac{N}{ \sum_{i=1}^{N} I(t,D_i) }</script><p>其中$N$为所有的文档总数，$I(t,D_i)$表示文档$D_i$是否包含词条$t$，若包含为1，不包含为0。但此处存在一个问题，即当词条$t$在所有文档中都没有出现的话IDF计算公式的分母为0，此时就需要对IDF做平滑处理，改善后的IDF计算公式如下所示：</p><script type="math/tex; mode=display">IDF_t = log \frac{N}{ 1 + \sum_{i=1}^{N} I(t,D_i) }</script><p>那么最终词条$t$在文档$D_i$中的TF-IDF值为：$TF-IDF_{t,D_i} = TF_{t,D_i} * IDF_t$ 。</p><p>从上述的计算词条$t$在文档$D_i$中的TF-IDF值计算可以看出：当一个词条在文档中出现的频率越高，且新鲜度低（即普遍度低），则其对应的TF-IDF值越高。<br>比如现在有一个预料库，包含了100篇（$N$）论文，其中涉及包含推荐系统（$t$）这个词条的有20篇，在第一篇论文（$D1$）中总共有200个技术词汇，其中推荐系统出现了15次，则词条推荐系统的在第一篇论文（$D1$）中的TF-IDF值为：</p><script type="math/tex; mode=display">TF-IDF_{推荐系统} = \frac {15} {200} * log \frac{200}{20+1} = 0.051</script><p>更多详细的关于TFIDF的介绍可以参考</p><ul><li><a href="https://thinkgamer.blog.csdn.net/article/details/48811033" target="_blank" rel="external">搜索引擎：文本分类——TF/IDF算法</a></li></ul><p>关于TF-IDF的其他实战：</p><ul><li><a href="https://thinkgamer.blog.csdn.net/article/details/85690389" target="_blank" rel="external">基于TF-IDF算法的短标题关键词提取</a></li></ul><h3 id="基于TFIDF计算文本相似度"><a href="#基于TFIDF计算文本相似度" class="headerlink" title="基于TFIDF计算文本相似度"></a>基于TFIDF计算文本相似度</h3><p>这里需要注意的是在spark2.x中默认不支持dataframe的笛卡尔积操作，需要在创建Spark对象时开启。</p><p>创建spark对象，并设置日志等级<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// spark.sql.crossJoin.enabled=true spark <span class="number">2.0</span> x不支持笛卡尔积操作，需要开启支持</span><br><span class="line">val spark = SparkSession</span><br><span class="line"><span class="meta">    .builder</span>()</span><br><span class="line"><span class="meta">    .appName</span>(<span class="string">"docSimCalWithTFIDF"</span>)</span><br><span class="line"><span class="meta">    .config</span>(<span class="string">"spark.sql.crossJoin.enabled"</span>,<span class="string">"true"</span>)</span><br><span class="line"><span class="meta">    .master</span>(<span class="string">"local[10]"</span>)</span><br><span class="line"><span class="meta">    .enableHiveSupport</span>()</span><br><span class="line"><span class="meta">    .getOrCreate</span>()</span><br><span class="line">Logger.getRootLogger.setLevel(Level.WARN)</span><br></pre></td></tr></table></figure></p><p>这里以官方样例代码中的三行英文句子为例，创建数据集，并进行分词（spark中的中文分词包有很多，比如jieba，han，ansj，fudannlp等，这里不展开介绍）<br><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">val</span> sentenceData = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Hi I heard about Spark"</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"I wish Java could use case classes"</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Logistic regression models are neat"</span>)</span><br><span class="line">)).toDF(<span class="string">"label"</span>, <span class="string">"sentence"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">val</span> tokenizer = <span class="function"><span class="keyword">new</span> <span class="title">Tokenizer</span>()</span></span><br><span class="line"><span class="function">    .<span class="title">setInputCol</span>("sentence")</span></span><br><span class="line"><span class="function">    .<span class="title">setOutputCol</span>("words")</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">wordsData</span> = <span class="title">tokenizer</span>.<span class="title">transform</span>(sentenceData)</span></span><br><span class="line"><span class="function"><span class="title">wordsData</span>.<span class="title">show</span>(<span class="number">10</span>)</span></span><br></pre></td></tr></table></figure></p><p>展示的结果为：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|<span class="string">label</span>|<span class="string">            sentence</span>|<span class="string">               words</span>|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|<span class="string">    0</span>|<span class="string">Hi I heard about ...</span>|<span class="string">[hi, i, heard, ab...</span>|</span><br><span class="line">|<span class="string">    1</span>|<span class="string">I wish Java could...</span>|<span class="string">[i, wish, java, c...</span>|</span><br><span class="line">|<span class="string">    2</span>|<span class="string">Logistic regressi...</span>|<span class="string">[logistic, regres...</span>|</span><br><span class="line">+-----+--------------------+--------------------+</span><br></pre></td></tr></table></figure></p><p>调用官方的tfidf包计算向量：<br><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// setNumFeatures(5)表示将Hash分桶的数量设置为5个,可以根据你的词语数量来调整，一般来说，这个值越大不同的词被计算为一个Hash值的概率就越小，数据也更准确，但需要消耗更大的内存</span></span><br><span class="line"></span><br><span class="line"><span class="meta">val</span> hashingTF = <span class="function"><span class="keyword">new</span> <span class="title">HashingTF</span>()</span></span><br><span class="line"><span class="function">    .<span class="title">setInputCol</span>("words")</span></span><br><span class="line"><span class="function">    .<span class="title">setOutputCol</span>("rawFeatures")</span></span><br><span class="line"><span class="function">    .<span class="title">setNumFeatures</span>(<span class="number">5</span>)</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">featurizedData</span> = <span class="title">hashingTF</span></span></span><br><span class="line"><span class="function">    .<span class="title">transform</span>(wordsData)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">featurizedData</span>.<span class="title">show</span>(<span class="number">10</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">idf</span> = <span class="title">new</span> <span class="title">IDF</span>()</span></span><br><span class="line"><span class="function">    .<span class="title">setInputCol</span>("rawFeatures")</span></span><br><span class="line"><span class="function">    .<span class="title">setOutputCol</span>("features")</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">idfModel</span> = <span class="title">idf</span>.<span class="title">fit</span>(featurizedData)</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">rescaledData</span> = <span class="title">idfModel</span>.<span class="title">transform</span>(featurizedData)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">rescaledData</span>.<span class="title">show</span>(<span class="number">10</span>)</span></span><br><span class="line"><span class="function"><span class="title">rescaledData</span>.<span class="title">select</span>("label", "features").<span class="title">show</span>()</span></span><br></pre></td></tr></table></figure></p><p>展示的结果为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------------------+--------------------+--------------------+</span><br><span class="line">|label|            sentence|               words|         rawFeatures|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+</span><br><span class="line">|    <span class="number">0</span>|Hi I heard about ...|[hi, i, heard, ab...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">2.0</span>,<span class="number">2.</span>..|</span><br><span class="line">|    <span class="number">1</span>|I wish Java could...|[i, wish, java, c...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|</span><br><span class="line">|    <span class="number">2</span>|Logistic regressi...|[logistic, regres...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line">+-----+--------------------+--------------------+--------------------+--------------------+</span><br><span class="line">|label|            sentence|               words|         rawFeatures|            features|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+--------------------+</span><br><span class="line">|    <span class="number">0</span>|Hi I heard about ...|[hi, i, heard, ab...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">2.0</span>,<span class="number">2.</span>..|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">0.0</span>,<span class="number">0.</span>..|</span><br><span class="line">|    <span class="number">1</span>|I wish Java could...|[i, wish, java, c...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">|    <span class="number">2</span>|Logistic regressi...|[logistic, regres...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line">+-----+--------------------+</span><br><span class="line">|label|            features|</span><br><span class="line">+-----+--------------------+</span><br><span class="line">|    <span class="number">0</span>|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">0.0</span>,<span class="number">0.</span>..|</span><br><span class="line">|    <span class="number">1</span>|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">|    <span class="number">2</span>|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">+-----+--------------------+</span><br></pre></td></tr></table></figure></p><p>其中$(5,[0,2,4],[0.0,0…$ 是向量的一种压缩表示格式，例如$(3,[0,1],[0.1,0.2])$表示的是 向量的长度为3，其中第 1位和第2位的值为0.1 和0.3，第3位的值为0。</p><hr><p>这里需要将其转化为向量的形式，方便后续进行计算，可以直接通过dataframe进行转化，也可以先将dataframe转化为rdd，再进行转化。<br>datafram通过自定义UDF进行转化如下：<br><figure class="highlight puppet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">// 解析数据 转化为denseVector格式 datafra</span><br><span class="line">val sparseVectorToDenseVector = <span class="keyword">udf</span> &#123; </span><br><span class="line">    features: <span class="attr">SV</span> =&gt; features.toDense </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> <span class="keyword">df</span> = rescaledData</span><br><span class="line">    .select($<span class="string">"label"</span>, sparseVectorToDenseVector($<span class="string">"features"</span>))</span><br><span class="line">    .withColumn(<span class="string">"tag"</span>,lit(1))</span><br><span class="line">df.show(10)</span><br></pre></td></tr></table></figure></p><p>展示结果为：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+------+--------------------+---+</span><br><span class="line">|<span class="string">label1</span>|<span class="string">           features1</span>|<span class="string">tag</span>|</span><br><span class="line">+------+--------------------+---+</span><br><span class="line">|<span class="string">     0</span>|<span class="string">[0.0,0.0,0.575364...</span>|<span class="string">  1</span>|</span><br><span class="line">|<span class="string">     1</span>|<span class="string">[0.0,0.0,0.575364...</span>|<span class="string">  1</span>|</span><br><span class="line">|<span class="string">     2</span>|<span class="string">[0.0,0.6931471805...</span>|<span class="string">  1</span>|</span><br><span class="line">+------+--------------------+---+</span><br></pre></td></tr></table></figure></p><p>先转化为RDD，再进行转化如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val selectedRDD = rescaledData.select(<span class="string">"label"</span>, <span class="string">"features"</span>).rdd</span><br><span class="line">    .map( l=&gt;( l.get(<span class="number">0</span>)<span class="selector-class">.toString</span>, l<span class="selector-class">.getAs</span>[SV](<span class="number">1</span>).toDense))</span><br><span class="line">selectedRDD.take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure></p><p>展示结果为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0</span>,[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.5753641449035617</span>,<span class="number">0.0</span>,<span class="number">0.0</span>])</span><br><span class="line">(<span class="number">1</span>,[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.5753641449035617</span>,<span class="number">0.28768207245178085</span>,<span class="number">0.0</span>])</span><br><span class="line">(<span class="number">2</span>,[<span class="number">0.0</span>,<span class="number">0.6931471805599453</span>,<span class="number">0.0</span>,<span class="number">0.5753641449035617</span>,<span class="number">0.0</span>])</span><br></pre></td></tr></table></figure></p><hr><p>当然也可以在进行相似度计算时进行转化，实现代码如下：<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义相似度计算udf</span></span><br><span class="line">import spark.implicits._</span><br><span class="line">val df1 = rescaledData</span><br><span class="line">    .<span class="keyword">select</span>(<span class="string">$"label"</span>.<span class="keyword">alias</span>(<span class="string">"id1"</span>), <span class="string">$"features"</span>.<span class="keyword">alias</span>(<span class="string">"f1"</span>))</span><br><span class="line">    .withColumn(<span class="string">"tag"</span>,lit(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">val df2 = rescaledData</span><br><span class="line">    .<span class="keyword">select</span>(<span class="string">$"label"</span>.<span class="keyword">alias</span>(<span class="string">"id2"</span>), <span class="string">$"features"</span>.<span class="keyword">alias</span>(<span class="string">"f2"</span>))</span><br><span class="line">    .withColumn(<span class="string">"tag"</span>,lit(<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">val simTwoDoc = udf&#123; </span><br><span class="line">    (f1: SV, f2: SV) =&gt; calTwoDocSim(f1,f2) </span><br><span class="line">&#125;</span><br><span class="line">val df =  df1.<span class="keyword">join</span>(df2, Seq(<span class="string">"tag"</span>), <span class="string">"inner"</span>)</span><br><span class="line">    .<span class="keyword">where</span>(<span class="string">"id1 != id2"</span>)</span><br><span class="line">    .withColumn(<span class="string">"simscore"</span>,simTwoDoc(col(<span class="string">"f1"</span>), col(<span class="string">"f2"</span>)))</span><br><span class="line">    .<span class="keyword">where</span>(<span class="string">"simscore &gt; 0.0"</span>)</span><br><span class="line">    .<span class="keyword">select</span>(<span class="string">"id1"</span>,<span class="string">"id2"</span>,<span class="string">"simscore"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show(<span class="number">20</span>)</span><br></pre></td></tr></table></figure></p><p>其中calTwoDocSim 函数实现如下：<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * @Author: GaoYangtuan</span></span><br><span class="line"><span class="comment">  * @Description: 自定义计算两个文本的距离</span></span><br><span class="line"><span class="comment">  * @Thinkgamer: 《推荐系统开发实战》作者，「搜索与推荐Wiki」公号负责人，算法工程师</span></span><br><span class="line"><span class="comment">  * @Param: [f1, f2]</span></span><br><span class="line"><span class="comment">  * @return: double</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"><span class="symbol">def</span> calTwoDocSim(<span class="built_in">f1</span>: SV, <span class="built_in">f2</span>: SV): Double = &#123;</span><br><span class="line">    val <span class="keyword">breeze1 </span><span class="symbol">=new</span> SparseVector(<span class="built_in">f1</span>.indices,<span class="built_in">f1</span>.values, <span class="built_in">f1</span>.size)</span><br><span class="line">    val <span class="keyword">breeze2 </span><span class="symbol">=new</span> SparseVector(<span class="built_in">f2</span>.indices,<span class="built_in">f2</span>.values, <span class="built_in">f2</span>.size)</span><br><span class="line">    val cosineSim = <span class="keyword">breeze1.dot(breeze2) </span>/ (norm(<span class="keyword">breeze1) </span>* norm(<span class="keyword">breeze2))</span></span><br><span class="line"><span class="keyword"> </span>  cosineSim</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>打印结果如下：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |<span class="string">-- id1: integer (nullable = false)</span></span><br><span class="line"><span class="string"> </span>|<span class="string">-- id2: integer (nullable = false)</span></span><br><span class="line"><span class="string"> </span>|<span class="string">-- simscore: double (nullable = false)</span></span><br><span class="line"></span><br><span class="line"><span class="string">+---+---+------------------+</span></span><br><span class="line">|<span class="string">id1</span>|<span class="string">id2</span>|<span class="string">          simscore</span>|</span><br><span class="line">+---+---+------------------+</span><br><span class="line">|<span class="string">  0</span>|<span class="string">  1</span>|<span class="string">0.8944271909999159</span>|</span><br><span class="line">|<span class="string">  1</span>|<span class="string">  0</span>|<span class="string">0.8944271909999159</span>|</span><br><span class="line">|<span class="string">  1</span>|<span class="string">  2</span>|<span class="string">0.2856369296406274</span>|</span><br><span class="line">|<span class="string">  2</span>|<span class="string">  1</span>|<span class="string">0.2856369296406274</span>|</span><br><span class="line">+---+---+------------------+</span><br></pre></td></tr></table></figure></p><p>最后进行排序和保存，代码如下：<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">val sortAndSlice = udf &#123; simids: Seq[Row] =&gt;</span><br><span class="line">    simids.map&#123;</span><br><span class="line">        case Row(id2: <span class="keyword">Int</span>,  simscore: Double) =&gt; (id2,simscore)</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">    .sortBy</span>(_<span class="meta">._2</span>)</span><br><span class="line"><span class="meta">    .reverse</span></span><br><span class="line"><span class="meta">    .slice</span>(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line"><span class="meta">    .map</span>(e =&gt; e<span class="meta">._1</span> + <span class="string">":"</span> + e<span class="meta">._2</span>.formatted(<span class="string">"%.3f"</span>))</span><br><span class="line"><span class="meta">    .mkString</span>(<span class="string">","</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val result = df</span><br><span class="line"><span class="meta">    .groupBy</span>($<span class="string">"id1"</span>)</span><br><span class="line"><span class="meta">    .agg</span>(collect_list(struct($<span class="string">"id2"</span>, $<span class="string">"simscore"</span>)).as(<span class="string">"simids"</span>))</span><br><span class="line"><span class="meta">    .withColumn</span>(<span class="string">"simids"</span>, sortAndSlice(sort_array($<span class="string">"simids"</span>, asc = false)))</span><br><span class="line"></span><br><span class="line">result.show(<span class="number">10</span>)</span><br><span class="line">result.coalesce(<span class="number">1</span>).write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"data/tfidf"</span>)</span><br></pre></td></tr></table></figure></p><p>打印结果如下：<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+---+</span>---------------+</span><br><span class="line">|id1|         simids|</span><br><span class="line"><span class="code">+---+</span>---------------+</span><br><span class="line">|  1|0:0.894,2:0.286|</span><br><span class="line">|  2|        1:0.286|</span><br><span class="line">|  0|        1:0.894|</span><br><span class="line"><span class="code">+---+</span>---------------+</span><br></pre></td></tr></table></figure></p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;TFIDF算法介绍&quot;&gt;&lt;a href=&quot;#TFIDF算法介绍&quot; class=&quot;headerlink&quot; title=&quot;TFIDF算法介绍&quot;&gt;&lt;/a&gt;TFIDF算法介绍&lt;/h3&gt;&lt;p&gt;TF-IDF（Term Frequency–InverseDocument Freq
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="NLP" scheme="http://thinkgamer.cn/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>模型的独立学习方式</title>
    <link href="http://thinkgamer.cn/2019/11/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%8B%AC%E7%AB%8B%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"/>
    <id>http://thinkgamer.cn/2019/11/12/深度学习/神经网络笔记/模型的独立学习方式/</id>
    <published>2019-11-12T12:53:23.000Z</published>
    <updated>2020-01-10T03:15:57.276Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>针对一个给定的任务，通常采取的步骤是：准确一定非规模的数据集，这些数据要和真实数据集的分布一致；然后设定一个优化目标和方法；然后在训练集上训练模型。</p><p>不同的模型往往都是从零开始训练的，一切知识都需要从训练集中得到，这就意味着每个任务都需要大量的训练数据。在实际应用中，我们面对的任务很难满足上述需求，比如训练任务和目标任务的数据分布不一致，训练数据集过少等。这时机器学习的任务就会受到限制，因此人们开始关注一些新的任务学习方式。<br>本篇文章主要介绍一些“模型独立的学习方式”，比如：集成学习、协同学习、自学习、多任务学习、迁移学习、终身学习、小样本学习、元学习等。</p><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>集成学习（Ensemble Learning）就是通过某种策略将多个模型集成起来，通过群体决策来提高决策准确率。集成学习首要的问题是如何集成多个模型，比较常用的集成策略有直接平均、加权平均等。</p><p><strong>集成学习可以分为：Boosting、Bagging、Stacking</strong>，这三种的详细区分和流程可以参考《<a href="https://item.jd.com/12671716.html" target="_blank" rel="external">推荐系统开发实战</a>》一书中第八章 点击率预估部分。本文中主要介绍集成学习中的Boosting学习和AdaBoost算法。</p><p>集成学习的思想可以采用一句古老的谚语来描述：“三个臭皮匠，顶个诸葛亮”。但是一个有效的集成需要各个基模型的差异尽可能的大。为了增加模型之间的差异性，可以采取Bagging类和Boosting类两类方法。</p><h4 id="Bagging类方法"><a href="#Bagging类方法" class="headerlink" title="Bagging类方法"></a>Bagging类方法</h4><p>Bagging类方法是通过随机构造训练样本、随机选择特征等方法来提高每个基模型的独立性，代表性方法有Bagging和随机森林。</p><ul><li>Bagging（Bootstrap Aggregating）是一个通过不同模型的训练数据集的独立性来提高不同模型之间的独立性。我们在原始训练集上进行有放回的随机采样，得到M比较小的训练集并训练M个模型，然后通过投票的方法进行模型集成。</li><li>随机森林（Random Forest）是在Bagging的基础上再引入了随机特征，进一步提升每个基模型之间的独立性。在随机森林中，每个基模型都是一棵树。</li></ul><p>随机森林的算法步骤如下：</p><ul><li>从样本集中通过重采样的方式产生n个样本</li><li>假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点</li><li>重复m次，产生m棵决策树<br>-多数投票机制来进行预测<blockquote><p>需要注意的一点是，这里m是指循环的次数，n是指样本的数目，n个样本构成训练的样本集，而m次循环中又会产生m个这样的样本集</p></blockquote></li></ul><h4 id="Boosting类方法"><a href="#Boosting类方法" class="headerlink" title="Boosting类方法"></a>Boosting类方法</h4><p>Boosting类方法是按照一定的顺序来先后训练不同的基模型，每个模型都针对前续模型的错误进行专门训练。根据前序模型的结果，来提高训练样本的权重，从而增加不同基模型之间的差异性。Boosting类方法的代表性方法有AbaBoost，GBDT，XGB，LightGBM等。</p><p>关于GBDT的介绍同样可以参考《<a href="https://item.jd.com/12671716.html" target="_blank" rel="external">推荐系统开发实战</a>》一书。</p><p>Boosting类集成模型的目标是学习一个加性模型（additive model） ，其表达式如下：</p><script type="math/tex; mode=display">F(x) = \sum_{m=1}^{M} a_m f_m(x)</script><p>其中$f_m(x)$为弱分类器，或基分类器，$a_m$为弱分类器的集成权重，$F(x)$称为强分类器。</p><p>Boosting类方法的关键是如何训练每个弱分类器$f_m(x)$以及对应的权重$a_m$。为了提高集成的效果，应尽可能使得每个弱分类器的差异尽可能大。一种有效的方法是采用迭代的策略来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。</p><p>在学习了第m个弱分类器之后，增加分错样本的权重，使得第$m+1$个弱分类器“更关注”于前边弱分类器分错的样本。这样增加每个弱分类器的差异，最终提升的集成分类器的准确率。这种方法称为AdaBoost（其实其他的Boost模型采用的也是类似的策略，根据前m-1颗树的误差迭代第m颗树）。</p><h4 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h4><p>AdaBoost算法是一种迭代式的训练算法，通过改变数据分布来提高弱分类器的差异。在每一轮训练中，增加分错样本的权重，减少对分对样本的权重，从而得到一个新的数据分布。</p><p>以两类分类为例，弱分类器$f_m(x) \in \{ +1, -1\}$，AdaBoost算法的训练过程如下所示，最初赋予每个样本同样的权重。在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器。然后根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。</p><p><img src="https://img-blog.csdnimg.cn/20191106144746194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="AdaBoost算法流程"></p><p><strong>AdaBoost算法的统计学解释</strong></p><p>AdaBoost算法可以看做是一种分步优化的加性模型，其损失函数定义为：</p><script type="math/tex; mode=display">L(F) = exp(-y F(x))\\= exp(-y \sum_{m=1}^{M} a_m f_m(x))</script><p>其中$y,f_m(x)\in \{ +1,-1\}$</p><p>假设经过$m-1$次迭代，得到：</p><script type="math/tex; mode=display">F_{m-1}(x) = \sum_{t=1}^{T} a_t f_t(x)</script><p>则第$m$次迭代的目标是找一个$a_m$和$f_m(x)$使得下面的损失函数最小。</p><script type="math/tex; mode=display">L(a_m,f_m(x)) = \sum_{n=1}^{N}exp(-y^{(n)} (F_{m-1} (x^{(n)}) + a_m f_m(x^{(n)})))</script><p>令 $w_m^{(n)}=exp(-y^{(n)} F_{m-1}(x^{(n)}))$，则损失函数可以表示为：</p><script type="math/tex; mode=display">L(a_m,f_m(x)) = \sum_{n=1}^{N} w_m^{(n)} exp(-y^{(n)} a_m f_m(x^{(n)}))</script><p>因为$y,f_m(x) \in {+1, -1}$，有：</p><script type="math/tex; mode=display">yf_m(x) = 1-2I(y\neq f_m(x))</script><p>其中$I(x)$为指示函数。</p><p>将损失函数在$f_m(x)=0$处进行二阶泰勒展开，有：</p><script type="math/tex; mode=display">L(a_m, f_m(x)) = \sum_{n=1}^{N} w_m^{(n)} (  1 - a_m y^{(n)}f_m(x^{(n)}) + \frac{1}{2}a_m^2  ) \\\propto a_m \sum_{n=1}^{N} w_n^{(n)} I(y^{(n} \neq f_m(x^{(n)})</script><p>从上式可以看出，当$a_m&gt;0$时，最优的分类器$f_m(x)$为使得在样本权重为$w_m^{(n)}, 1 \leq n \leq N$时的加权错误率最小的分类器。</p><p>在求解出$f_m(x)$之后，上述的损失函数可以写为：</p><script type="math/tex; mode=display">L(a_m, f_m(x)) = \sum_{y^{(n)}=f_m(x^{(n)})} w_m^{(n)} exp(-a_m) + \sum_{y^{(n)} \neq f_m(x^{(n)})}  w_m^{(n)} exp(a_m) \\\propto (1-\epsilon _m) exp(-a_m) + \epsilon_m exp(a_m)</script><p>其中$\epsilon_m$为分类器$f_m(x)$的加权错误率</p><script type="math/tex; mode=display">\epsilon_m = \frac { \sum_{ y^{(n)} \neq f_m(x^{(n)}) } w_m^{(n)} } {\sum_{n} w_m^{(n)}}</script><p>求上式关于$a_m$的导数并令其为0，得到</p><script type="math/tex; mode=display">a_m = \frac {1}{2} log \frac {1-\epsilon_m}{\epsilon_m}</script><p><strong>AdaBoost算法的优缺点</strong><br>优点：</p><ul><li>作为分类器精度很高</li><li>可以使用各种算法构建子分类器，AdaBoost提供的是一个框架</li><li>使用简单分类器时，计算出的结果可理解，且构造简单</li><li>不需要做特征筛选</li><li>不同担心过拟合</li></ul><p>缺点：</p><ul><li>容易收到噪声干扰</li><li>训练时间长，因为需要遍历所有特征</li><li>执行效果依赖于弱分类器的选择</li></ul><h3 id="自训练和协同训练"><a href="#自训练和协同训练" class="headerlink" title="自训练和协同训练"></a>自训练和协同训练</h3><p>监督学习虽然准确度上有一定的保证，但往往需要大量的训练数据，但在一些场景中标注数据的成本是非常高的，因此如何利用大量的无标注数据提高监督学习的效率，有着十分重要的意义。这种利用少量样本标注数据和大量样本标注数据进行学习的方式称之为半监督学习（Semi-Supervised Learning，SSL）。</p><p>本节介绍两种无监督的学习算法：自训练和协同训练。</p><h4 id="自训练"><a href="#自训练" class="headerlink" title="自训练"></a>自训练</h4><p>自训练（Slef-Training）也叫自训练（Self-teaching）或者自举法（boostStrapping）。</p><p>自训练的思路是：利用已知的标注数据训练一个模型，利用该模型去预测无标注的样本数据，然后将置信度较高的样本以及其伪标签加入训练集，然后重新训练模型，进行迭代。下图给出了自训练的算法过程。</p><p><img src="https://img-blog.csdnimg.cn/20191109150434700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="自训练算法过程"></p><p>自训练和密度估计中EM算法有一定的相似之处，通过不断地迭代来提高模型能力。但自训练的缺点是无法保证每次加入训练集的样本的伪标签是正确的。如果选择样本的伪标签是错误的，反而会损害模型的预测能力。因此自训练最关键的步骤是如何设置挑选样本的标准。</p><h4 id="协同训练"><a href="#协同训练" class="headerlink" title="协同训练"></a>协同训练</h4><p>协同训练（Co-Training）是自训练的一种改进方法，通过两个基于不同视角的分类器来相互促进。很多数据都有相对独立的不同视角。比如互联网上的每个网页都由两种视角组成：文字内容和指向其他网页的链接。如果要确定一个网页的类别，可以根据文字内容来判断，也可以根据网页之间的链条关系来判断。</p><p>假设一个样本$x=[x_1,x_2]$，其中$x_1,x_2$分别表示两种不同视角$V_1,V_2$的特征，并满足下面两个假设：</p><ul><li>（1）：条件独立性，即给定样本标签y时，两种特征条件独立$p(x_1,x_2|y)=p(x_1|y)p(x_2|y)$</li><li>（2）：充足和冗余性，即当数据充分时，每种视角的特征都可以足以单独训练出一个正确的分类器。</li></ul><p>令$y=g(x)$为需要学习的真实映射函数，$f_1$和$f_2$分别为两个视角的分类器，有：</p><script type="math/tex; mode=display">\exists f_1, f_2,    \forall x \in X,\,\,\,\,\, f_1(x_1) = f_2(x_2) = g(x)</script><p>其中$X$为样本$x$的取值空间。</p><p>由于不同视角的条件独立性，在不同视角上训练出来的模型就相当于从不同的视角来理解问题，具有一定的互补性。协同训练就是利用这种互补行来进行自训练的一种方法。首先在训练集上根据不同视角分别训练两个模型$f_1$和$f_2$然后用$f_1$和$f_2$在无标记数据集上进行预测，各选取预测置信度比较高的样本加入到训练集，重新训练两个不同视角的模型，并不断重复这个过程（需要注意的是协同算法要求两种视图时条件独立的，如果两种视图完全一样，则协同训练退化成自训练算法）。</p><p>协同训练的算法过程如下：<br><img src="https://img-blog.csdnimg.cn/20191109153310892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="协同训练的算法过程"></p><h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>一般的机器学习模型是针对单个任务进行的，不同任务的模型需要在各自的训练集上单独学习得到。而多任务学习（Multi-task learning）是指同时学习多个相关任务，让 这些任务在学习的过程中共享知识，利用多个任务之间的相关性来改进模型的性能和泛化能力。</p><p>多任务学习可以看做时一种归纳迁移学习（Inductive Transfer Learning），即通过利用包含在相关任务中的信息作为归纳偏置（Inductive Bias）来提高泛化能力。</p><p><strong>多任务学习的主要挑战在于如何设计多任务之间的共享机制</strong>，在传统的机器学习任务中很难引入共享信息，但是在神经网络中就变得简单了许多，常见的以下四种：</p><ul><li><strong>硬共享模式</strong>：让不同任务的神经网络模型共同使用一些共享模块来提取一些通用的特征，然后再针对每个不同的任务设置一些私有模块来提取一些任务特定的特征。</li><li><strong>软共享模式</strong>：不显式设置共享模块，但每个任务都可以从其他任务中“窃取”一些信息来提高自己的能力。窃取的方式包括直接复制使用其他任务的隐状态，或使用注意力机制来主动选择有用的信息。</li><li><strong>层次共享模式</strong>：一般神经网络中不同层抽取的特征类型不同，底层一般抽取一些低级的局部特征，高层抽取一些高级的抽象语义特征。因此如果多任务学习中不同任务也有级别高低之分，那么一个合理的共享模式是让低级任务在底层输出，高级任务在高层输出。</li><li><strong>共享-私有模式</strong>：一个更加分工明确的方式是将共享模块和任务特定（私有）模块的责任分开。共享模块捕捉一些跨任务的共享特征，而私有模块只捕捉和特点任务相关的特征。最终的表示由共享特征和私有特征共同构成。</li></ul><p>在多任务学习中，每个任务都可以有自己单独的训练集。为了让所有任务同时学习，我们通常会使用交替训练的方式来“近似”的实现同时学习，下图给出了四种常见的共享模式图</p><p><img src="https://img-blog.csdnimg.cn/20191112101743882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="四种常见的共享模式图"></p><p>多任务学习的流程可以分为两个阶段：</p><ul><li>（1）联合训练阶段：每次迭代时，随机挑选一个任务，然后从这个任务中随机选择一些训练样本，计算梯度并更新参数</li><li>（2）单任务精调阶段：基于多任务学习到的参数，分别在每个单独任务进行精调，其中单任务精调阶段为可选阶段。当多个任务的差异性比较大时，在每个单任务上继续优化参数可以进一步提升模型能力。</li></ul><p>假设有M个相关任务，其模型分别为$f_m(x,\theta), 1\leq m \leq M$，多任务学习的联合目标函数为所有任务损失函数的线性加权：</p><script type="math/tex; mode=display">L(\theta) = \sum_{m=1}^{M}\sum_{n=1}^{N_m} \eta_m l_m(f_m(x^{(m,n)}, \theta ), y_{(m,n)})</script><p>其中$l_m$为第m个任务的损失函数，$\eta_m$是第m个任务的权重，$\theta$表示包含了共享模块和私有模块在内的所有参数。</p><p>多任务学习中联合训练阶段的具体过程如下所示：<br><img src="https://img-blog.csdnimg.cn/20191112102456841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="多任务学习中联合训练阶段的具体过程"></p><p>多任务学习通常比单任务学习获得更好的泛化能力，主要由于以下几个原因：</p><ul><li>1.多任务学习在多个数据集上进行训练，训练集范围更大，且多任务之间具有一定的相关性，相当于是一种隐式的数据增强，可以提高模型的泛化能力。</li><li>2.多任务学习中的共享模块需要兼顾所有任务，在一定程度上避免了模型过拟合到单个任务的训练集，可以看做是一种正则化。</li><li>3.多任务学习比单任务学习可以获得更好的表示</li><li>4.在多任务学习中，每个任务都可以“选择性”利用其他任务中学习到的隐藏特征，从而提高自身的能力。</li></ul><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>标准机器学习的前提假设只训练数据和测试数据的分布是相同的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差。如何将相关任务的训练数据中学习到的可泛化知识迁移到目标任务中，就是迁移学习（Transfer Learning）要解决的问题。</p><p>迁移学习根据不同的迁移方式又分为两个类型：归纳迁移学习（Inductive Transfer Learning）和推导迁移学习（Transductive Transfer Learning）。这两个类型分别对应两个机器学习的范式：归纳学习（Inductive Learning）和转导学习（Transductive Learning）。一般的机器学习任务都是指归纳学习，即希望再训练集上学习到使得期望风险最小的模型。而转导学习的目标是学习一种在给定测试集上错误率最小的模型，在训练阶段可以利用测试集的信息。</p><h4 id="归纳迁移学习"><a href="#归纳迁移学习" class="headerlink" title="归纳迁移学习"></a>归纳迁移学习</h4><p>一般而言，归纳迁移学习要求源领域和目标领域时相关的，并且源领域$D_S$有大量的训练样本，这些样本可以是有标注的样本也可以时无标注的样本。</p><ul><li>当源领域只有大量无标注数据时，源任务可以转换为无监督学习任务，比如自编码和密度估计，通过无监督任务学习一种可迁移的表示，然后将这些表示迁移到目标任务上。</li><li><p>当源领域有大量的标注数据时，可以直接将源领域上训练的模型迁移到目标领域上。<br>归纳迁移学习一般有下面两种迁移方式：</p></li><li><p>基于特征的方式：将预训练模型的输出或者中间隐藏层的输出作为特征直接加入到目标任务学习模型中。目标任务的学习模型可以时一般的浅层分类器（比如支持向量机等）或一个新的神经网络模型。</p></li><li>精调的方式：在目标任务上复用预训练模型的部分参数，并对其参数进行精调。</li></ul><p>假设预训练的模型是一个深层神经网络，不同层的可迁移性也不尽相同。通常来说网络的低层学习一些通用的低层特征，中层或者高层学习抽象的高级语义特征，而最后几层一般学习和特定任务相关的特征。因此根据目标任务的自身特点以及和源任务的相关性，可以针对性的选择预训练模型的不同层来迁移到目标任务中。</p><p>将预训练模型迁移到目标任务中通常会比从零开始学习的方式好，主要体现在以下三点：</p><ul><li>（1）初始模型的性能一般比随机初始化的模型要好</li><li>（2）训练时模型的学习速度比从零开始学习要快，收敛性更好</li><li>（3）模型的最终性能更好，具有更好的泛化性</li></ul><p>归纳迁移学习和多任务学习也比较类似，但是有下面两点区别：</p><ul><li>（1）多任务学习是同时学习多个不同任务，而归纳迁移学习通常分为两个阶段，即源任务上的学习阶段，和目标任务上的迁移学习阶段</li><li>（2）归纳迁移学习是单向的知识迁移，希望提高模型在目标任务上的性能，而多任务学习时希望提高所有任务的性能。</li></ul><h4 id="转导迁移学习"><a href="#转导迁移学习" class="headerlink" title="转导迁移学习"></a>转导迁移学习</h4><p>转导迁移学习是一种从样本到样本的迁移，直接利用源领域和目标领域的样本进行迁移学习。转导迁移学习可以看作是一种特殊的转导学习。转导迁移学习通常假设源领域有大量的标注数据，而目标领域没有（或少量）的标注数据，但是有大量的无标注数据。目标领域的数据在训练阶段是可见的。</p><p>转导迁移学习的一个常见子问题时领域适应（Domain Adaptation），在领域适应问题中，一般假设源领域和目标领域有相同的样本空间，但是数据分布不同$p_S(x,y) \neq p_T(x,y)$。</p><p>根据贝叶斯公式，$p(x,y)=p(x|y)p(y) = p(y|x)p(x)$，因此数据分布的不一致通常由三种情况造成。</p><ul><li>（1）协变量偏移（Covariate Shift）：源领域和目标领域的输入边际分布不同$p_S(x) \neq p_T(x)$，但后验分布相同$p_S(y|x) = p_T(y|x)$，即学习任务相同$T_s = T_T$</li><li>（2）概念偏移（Concept Shift）：输入边际分布相同$p_S(x) = p_T(x)$，但后验分布不同$p_S(y|x) \ neq p_T(y|x)$，即学习任务不同$T_S \neq T_T$</li><li>（3）先验偏移（Prior Shift）：源领域和目标领域中的输出$y$先验分布不同$p_S(y) \neq p_T(y)$，条件分布相同$p_S(x|y) = p_T(x|y)$。在这样的情况下，目标领域必须提供一定数量的标注样本。</li></ul><h3 id="终生学习"><a href="#终生学习" class="headerlink" title="终生学习"></a>终生学习</h3><p>终生学习（Lifelong Learning）也叫持续学习（Continuous Learning）是指像人类一样具有持续不断的学习能力，根据历史任务中学到的经验和知识来帮助学习不断出现的新任务，并且这些经验和知识是持续累积的，不会因为新的任务而忘记旧的知识。</p><p>在终生学习中，假设一个终生学习算法已经在历史人任务$T_1, T_2, …$上学习到一个模型，当出现一个新任务$T_{m+1}$时，这个算法可以根据过去在$m$个任务上学习到的知识来帮助第$m+1$个任务，同时积累所有的$m+1$个任务上的知识。</p><p>在终生学习中，一个关键的问题是如何避免<strong>灾难性遗忘（Catastrophic Forgetting）</strong>，即按照一定顺序学习多个任务时，在学习新任务的同时不忘记先前学习到的历史知识。比如在神经网络模型中，一些参数对任务$T_A$非常重要，如果在学习任务$T_B$时被改变了，就可能给任务$T_A$造成不好的影响。</p><p>解决灾难性遗忘的方法有很多，比如弹性权重巩固方法（Elastic Weight Coonsolidation）。</p><h3 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h3><p>根据没有免费午餐定理，没有一种通用的学习算法在所有任务上都有效。因此当使用机器学习算法实现某个任务时，我们通常需要“就事论事”，根据任务的特定来选择合适的模型、损失函数、优化算法以及超参数。</p><p>而这种动态调整学习方式的能力，称为元学习（Meta-Learning），也称为学习的学习（Learning to Learn）。</p><p>元学习的目的时从已有的任务中学习一种学习方法或元知识，可以加速新任务的学习。从这个角度来说，元学习十分类似于归纳迁移学习，但元学习更侧重从多种不同的任务中归纳出一种学习方法。</p><p>这里主要介绍两种典型的元学习方法：基于优化器的元学习和模型无关的元学习。</p><h4 id="基于优化器的元学习"><a href="#基于优化器的元学习" class="headerlink" title="基于优化器的元学习"></a>基于优化器的元学习</h4><p>目前神经网络的的学习方法主要是定义一个目标损失函数$L(\theta)$，并通过梯度下降算法来最小化$L(\theta)$</p><script type="math/tex; mode=display">\theta_t \leftarrow \theta_{t-1} - \alpha \bigtriangledown L(\theta_{t-1})</script><p>其中$\theta_t$为第$t$步时的模型参数，$\bigtriangledown L(\theta_{t-1})$为梯度，$\alpha$为学习率。在不同的任务上，通常选择不同的学习绿以及不同的优化方法，比如动量法，Adam等。这些优化算法的本质区别在于更新参数的规则不同，因此一种很自然的元学习就是自动学习一种更新参数的规则，即通过另一个神经网络（比如循环神经网络）来建模梯度下降的过程。下图给出了基于优化器的元学习示例。</p><p><img src="https://img-blog.csdnimg.cn/20191112151007633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="基于优化器的元学习示例"></p><p>我们用函数$g_t(.)$来预测第$t$步时参数更新的差值$\Delta \theta_t = \theta_t - \theta_{t-1}$，函数$g_t(.)$称为优化器，输入是当前时刻的梯度值，输出时参数的更新差值$\Delta \theta_t$，这样第$t$步的更新规则可以写为：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t + g_t(\bigtriangledown L(\theta_{t}), \phi )</script><p>其中$\phi$为优化器$g_t(.)$的参数。</p><p>学习优化器$g_t(.)$的过程可以看做是一种元学习过程，其目标是找到一个适用于多个不同任务的优化器。在标准的梯度下降中，每步迭代的目标是使得$L(\theta)$下降。而在优化器的元学习中，我们希望在每步迭代的目标是$L(\theta)$最小，具体的目标函数为：</p><script type="math/tex; mode=display">L(\phi) = E_f [ \sum_{t=1}^{T} w_t L(\theta_t) ]\\\theta_t = \theta_{t-1} + g_t\\[g_t: h_t] = LSTM( \bigtriangledown L(\theta_{t-1}), h_{t-1}, \phi)</script><p>其中$T$为最大迭代次数，$w_t&gt;0$为每一步的权重，一般可以设置$w_t=1,\forall t$。由于LSTM网络可以记忆梯度的历史信息，学习到的优化器可以看做是一个高阶的优化方法。</p><h4 id="模型无关的元学习"><a href="#模型无关的元学习" class="headerlink" title="模型无关的元学习"></a>模型无关的元学习</h4><p>模型无关的元学习（Model-Agnostic Meta-Learning， MAML）是一个简单的模型无关、任务无关的元学习算法。假设所有的任务都来自一个任务空间，其分布为$p(T)$，我们可以在这个任务空间的所有任务上学习一种通用的表示，这种表示可以经过梯度下降方法在一个特定的单任务上进行精调。假设一个模型为$f(\theta)$，如果我们让这个模型适应到一个新任务$T_m$上，通过一步或多步的梯度下降更新，学习到的任务适配参数为：</p><script type="math/tex; mode=display">\theta_m ' = \theta- \alpha \bigtriangledown _\theta L_{T_m}(f_\theta)</script><p>其中$\alpha$为学习率，这里的$\theta_m’$可以理解为关于$\theta$的函数，而不是真正的参数更新。</p><p>MAML的目标是学习一个参数$\theta$使得其经过一个梯度迭代就可以在新任务上达到最好的性能。</p><script type="math/tex; mode=display">\underset{ \theta }{ min } \sum_{T_m \sim  p(T)} L_{T_m}(f(\theta'_m)) = \sum_{T_m \sim  p(T)} L_{T_m} ( f(\theta - \alpha \bigtriangledown _\theta L_{T_m} (f_\theta )) )</script><p>即在所有任务上的元优化（Meta-Optimization）也采用梯度下降来进行优化，即：</p><script type="math/tex; mode=display">\theta \leftarrow \theta - \beta \bigtriangledown _\theta \sum_{m=1}^{M} L_{T_m}(f_{\theta_m'})</script><p>其中$\beta$为元学习率，这里为一个真正的参数更新步骤。需要计算关于$\theta$的二阶梯度，但用一级近似通常也可以达到比较好的性能。</p><p>MAML的具体过程算法如下：<br><img src="https://img-blog.csdnimg.cn/20191112155840241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="MAML的具体过程算法"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>目前神经网路的学习机制主要是以监督学习为主，这种学习方式得到的模型往往是定向的，也是孤立的，每个任务的模型都是从零开始训练的，一切知识都需要从训练数据中得到，导致每个任务都需要大量的训练数据。本章主要介绍了一些和模型无关的学习方式，包括集成学习、自训练和协同训练、多任务学习、迁移学习、元学习，这些都是深度学习中研究的重点。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;针对一个给定的任务，通常采取的步骤是：准确一定非规模的数据集，这些数据要和真实数据集的分布一致；然后设定一个优化目标和方法；然后在训练集上训
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习中的无监督特征学习、聚类和密度估计</title>
    <link href="http://thinkgamer.cn/2019/11/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0%E3%80%81%E8%81%9A%E7%B1%BB%E5%92%8C%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    <id>http://thinkgamer.cn/2019/11/05/深度学习/神经网络笔记/无监督学习中的无监督特征学习、聚类和密度估计/</id>
    <published>2019-11-05T02:58:36.000Z</published>
    <updated>2020-01-10T03:15:53.937Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习概述"><a href="#无监督学习概述" class="headerlink" title="无监督学习概述"></a>无监督学习概述</h2><p>无监督学习（Unsupervised Learning）是指从无标签的数据中学习出一些有用的模式，无监督学习一般直接从原始数据进行学习，不借助人工标签和反馈等信息。典型的无监督学习问题可以分为以下几类：</p><ul><li><p>无监督特征学习（Unsupervised Feature Learning）</p><blockquote><p>从无标签的训练数据中挖掘有效的特征表示，无监督特征学习一般用来进行降维，数据可视化或监督学习前期的特征预处理。</p></blockquote></li><li><p>密度估计（Density Estimation）</p><blockquote><p>是根据一组训练样本来估计样本空间的概率密度。密度估计可以分为：参数密度估计和非参数密度估计。参数密度估计是假设数据服从某个已知概率密度函数形式的分布，然后根据训练样本去估计该分布的参数。非参数密度估计是不假设服从某个概率分布，只利用训练样本对密度进行估计，可以进行任意形状的密度估计，非参数密度估计的方法包括：直方图、核密度估计等。</p></blockquote></li><li><p>聚类（Clustering）</p><blockquote><p>是将一组样本根据一定的准则划分到不同的组。一个通用的准则是组内的样本相似性要高于组间的样本相似性。常见的聚类方法包括：KMeans、谱聚类、层次聚类等。</p></blockquote></li></ul><p>聚类大家已经非常熟悉了，下文主要介绍无监督特征学习和概率密度估计。</p><h2 id="无监督特征学习"><a href="#无监督特征学习" class="headerlink" title="无监督特征学习"></a>无监督特征学习</h2><p>无监督特征学习是指从无标注的数据中自动学习有效的数据表示，从而能够帮助后续的机器学习模型达到更好的性能。无监督特征学习主要方法有：</p><ul><li>主成分分析</li><li>稀疏编码</li><li>自编码器</li></ul><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><p>主成分分析（Principal Component Analysis，PCA）是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。以下部分摘自于 <a href="https://zhuanlan.zhihu.com/p/32412043" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/32412043</a></p><h4 id="PCA中的最大可分性思想"><a href="#PCA中的最大可分性思想" class="headerlink" title="PCA中的最大可分性思想"></a>PCA中的最大可分性思想</h4><p>PCA降维，用原始样本数据中最主要的方面代替原始数据，最简单的情况是从2维降到1维，如下图所示，我们希望找到某一个维度方向，可以代表两个维度的数据，图中列了两个方向 $u_1, u_2$，那么哪个方向可以更好的代表原始数据呢？</p><p><img src="https://img-blog.csdnimg.cn/2019110418180564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="最大可分性示例"><br>从直观上看，$u_1$比$u_2$好，这就是所说的最大可分性。</p><h4 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h4><p><img src="https://img-blog.csdnimg.cn/20191104182446889.jpg" alt="基变换"></p><p>其中$p_i \in {p_1, p_2, …, p_R}$，$p_i \in R^{1<em>N}$是一个行向量，表示第i个基，$a_j \in {a_1, a_2, …, a_M}$，$a_i \in R^{N</em>1}$是一个列向量，表示第$j$个原始数据记录，特别要注意的是，这里R可以小于N，而R决定了变维后数据的维数。</p><p>从上图和文字解释我们可以得到一种矩阵相乘的物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学习矩阵相乘时，只是简单的记住了相乘的规则，但并不清楚其背后的物理意义。</p><h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><p>如何考虑一个方向或者基是最优的，看下图：</p><p><img src="https://img-blog.csdnimg.cn/20191104184311912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="周志华机器学习插图"></p><p>我们将所有的点向两条直线做投影，基于前面PCA最大可分性思想，我们要找的是降维后损失最小，可以理解为投影后数据尽可能的分开，那么在数学中去表示数据的分散使用的是方差，我们都知道方差越大，数据越分散，方差的表达式如下：</p><script type="math/tex; mode=display">Var(a) = \frac{1}{m} \sum_{i=1}^{m} (a_i - \mu)^2</script><p>其中$\mu$为样本均值，如果提前对样本做去中心化，则方差表达式为：</p><script type="math/tex; mode=display">Var(a) = \frac{1}{m} \sum_{i=1}^{m} (a_i)^2</script><p>到现在，我们知道了以下几点：</p><ul><li>对原始数据进行（线性变换）基变换可以对原始样本给出不同的表示</li><li>基的维度小于样本的维度可以起到降维的作用，</li><li>对基变换后新的样本求其方差，选取使其方差最大的基</li></ul><p>那么再考虑另外一个问题？</p><blockquote><p>上面只是说明了优化目标，但并没有给出一个可行性的操作方案或者算法，因为只说明了要什么，但没说怎么做，所以继续进行探讨。</p></blockquote><h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><p>从二维降到一维可以采用方差最大来选出能使基变换后数据分散最大的方向（基），但遇到高纬的基变换，当完成第一个方向（基）选择后，第二个投影方向应该和第一个“几乎重合在一起”，这样显然是没有用的，要有其他的约束，我们希望两个字段尽量表示更多的信息，使其不存在相关性。</p><p>数学上使用协方差表示其相关性。</p><script type="math/tex; mode=display">Cov(a,b)= \frac{1}{m} \sum_{i=1}^{m}a_i b_i</script><p>当Cov(a,b)=0时表示两个字段完全独立，也是我们优化的目标。</p><blockquote><p>注意这里的 $a_i,b_i$是经过去中心化处理的。</p></blockquote><h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>我们想要达到的目标与字段内方差及协方差有密切的关系，假如只有a、b两个字段，将他们按行组成矩阵X，表示如下：</p><p><img src="https://img-blog.csdnimg.cn/20191104190715730.png" alt="矩阵X"></p><p>然后用X乘以X的转置矩阵，并乘以系数 $\frac{1}{m}$得：</p><p><img src="https://img-blog.csdnimg.cn/20191104190820539.png" alt="在这里插入图片描述"></p><p>可见，协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差，而其他元素是a 和 b的协方差，然后会发现两者被合并到了一个矩阵内。</p><h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><p>我们的目标是使$\frac{1}{m}\sum_{i=1}^{m}a_ib_i=0$，根据上述的推导，可以看出优化目标是$C=\frac{1}{m}XX^T$等价于协方差矩阵对角化。即除对角线外的其他元素（如$\frac{1}{m} \sum_{i=1}^{m}a_i b_i$）化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达成了优化目的。</p><p>这样说可能不是很明晰，我们进一步看下原矩阵和基变换后矩阵协方差矩阵的关系：</p><p>设原始数据矩阵为X，对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p><script type="math/tex; mode=display">D=\frac{1}{m}YY^T\\= \frac{1}{m}(PX)(PX)^T\\= \frac{1}{m} PXX^TP^T\\=P(\frac{1}{m} XX^T)P^T\\= PCP^T\\=P \begin{pmatrix}\frac{1}{m} \sum_{i=1}^{m} a_i^2 & \frac{1}{m} \sum_{i=1}^{m} a_i b_i \\  \frac{1}{m} \sum_{i=1}^{m} a_ib_i  & \frac{1}{m} \sum_{i=1}^{m} b_i^2 \end{pmatrix} P^T</script><p>可见我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p>我们希望投影后的方差最大化，于是优化目标可以改写为：</p><script type="math/tex; mode=display">\underset{P}{max} \, tr(PCP^T)\\s.t. \,PP^T=I</script><p>利用拉格朗日函数可以得到：</p><script type="math/tex; mode=display">J(P) = tr(PCP^T) + \lambda(PP^T - I)</script><p>对P求导有$CP^T + \lambda P^T = 0$，整理得：</p><script type="math/tex; mode=display">CP^T = (- \lambda) P^T</script><p>于是，只需对协方差矩阵C进行特征分解，对求得的特征值进行排序，再对 $P^T = (P_1, P_2, …, P_R)$取前K列组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><h4 id="PCA算法流程"><a href="#PCA算法流程" class="headerlink" title="PCA算法流程"></a>PCA算法流程</h4><p>从上边可以看出，求样本$x_i$的$n’$维的主成分，其实就是求样本集的协方差矩阵$\frac{1}{m}XX^T$的前$n’$维个特征值对应特征向量矩阵P，然后对于每个样本$x_i$，做如下变换$y_i = P x_i$，即达到PCA降维的目的。</p><p>具体的算法流程如下：</p><ul><li>输入：n维的样本集 $X=(x_i, x_2,…,x_m)$，要降维到的维数$n’$</li><li>输出：降维后的维度Y</li></ul><ol><li>对所有的样本集去中心化 $x_i = x_i - \frac{1}{m} \sum_{j=1}^{m}x_j$</li><li>计算样本的协方差矩阵$C = \frac{1}{m}XX^T$</li><li>求出协方差矩阵对应的特征值和对应的特征向量</li><li>将特征向量按照特征值从大到小，从上到下按行排列成矩阵，取前k行组成矩阵P</li><li>$Y=PX$即为降维到K维之后的数据</li></ol><p>注意：有时候降维并不会指定维数，而是指定一个比例$t$，比如降维到原先的t比例。</p><h4 id="PCA算法总结"><a href="#PCA算法总结" class="headerlink" title="PCA算法总结"></a>PCA算法总结</h4><p>PCA算法的主要优点：</p><ul><li>仅仅需要以方差衡量信息量，不受数据集意外因素的影响</li><li>各主成分之间正交，可消除原始数据各成分间的相互影响的因素</li><li>方法设计简单，主要运算是特征值分解，易于实现</li></ul><p>PCA算法的主要缺点：</p><ul><li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的可解释性强</li><li>方差小的非主成分也可能包含对样本差异的重要信息，因降维丢弃可能会对后续数据处理有影响</li><li>当样本特征维度较大时，需要巨大的计算量（比如，10000*10000，这时候就需要SVD[奇异值分解]，SVD不仅可以得到PCA降维的结果，而且可以大大的减小计算量）</li></ul><h3 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h3><h4 id="稀疏编码（Sparse-Coding）介绍"><a href="#稀疏编码（Sparse-Coding）介绍" class="headerlink" title="稀疏编码（Sparse Coding）介绍"></a>稀疏编码（Sparse Coding）介绍</h4><p>在数学上，线性编码是指给定一组基向量$A=[a_1,a_2,…,a_p]$，将输入样本$x\in R$表示为这些基向量的线性组合</p><script type="math/tex; mode=display">x = \sum _{i=1}^{p} z_i a_i = Az</script><p>其中基向量的系数$z=[z_1,…,z_p]$称为输入样本x的编码，基向量A也称为字典（dictionary）。</p><p>编码是对d维空间中的样本x找到其在p维空间中的表示（或投影），其目标通常是编码的各个维度都是统计独立的，并且可以重构出输入样本。编码的关键是找到一组“完备”的基向量A，比如主成分分析等。但是是主成分分析得到的编码通常是稠密向量，没有稀疏性。</p><blockquote><p>如果p个基向量刚好可以支撑p维的欧式空间，则这p个基向量是完备的，如果p个基向量可以支撑d维的欧式空间，并且p&gt;d，则这p个基向量是过完备，冗余的。<br><br><br>“过完备”基向量一般指的是基向量个数远大于其支撑空间维度，因此这些基向量一般是不具备独立，正交等性质。</p></blockquote><p>给定一组N个输入向量$x^1, …, x^N$，其稀疏编码的目标函数定义为：</p><script type="math/tex; mode=display">L(A,Z)= \sum _{n=1}^{N}( || x^n - Az^n || ^2 + \eta \rho (z^n))</script><p>其中$\rho(.)$是一个稀疏性衡量函数，$\eta$是一个超参数，用来控制稀疏性的强度。</p><p>对于一个向量$z \in R$，其稀疏性定义为非零元素的比例。如果一个向量只有很少的几个非零元素，就说这个向量是稀疏的。稀疏性衡量函数$\rho(z)$是给向量z一个标量分数。z越稀疏，$\rho(z)$越小。</p><p>稀疏性衡量函数有多种选择，最直接的衡量向量z稀疏性的函数是$l_0$范式</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} I(|z_i| > 0)</script><p>但$l_0$范数不满足连续可导，因此很难进行优化，在实际中，稀疏性衡量函数通常选用$l_1$范数</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} |z_i|</script><p>或对数函数</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} log(1+z_i^2)</script><p>或指数函数</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} -exp(-z_i^2)</script><h4 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h4><p>给定一组N个输入向量$x^1, … , x^N$，需要同时学习基向量A以及每个输入样本对应的稀疏编码$z^1, …,z^N$。</p><p>稀疏编码的训练过程一般用交替优化的方法进行（这一点和ALS很相似）。</p><p>（1）固定基向量A，对每个输入$x^n$ ，计算其对应的最优编码（原内容为减去稀疏性衡量函数，觉得不对）</p><script type="math/tex; mode=display">\underset{x^n}{min} || x^n - Az^n ||^2 + \eta \rho (z^n), \forall n \in [1,N]</script><p>（2）固定上一步得到的编码$z^1, …,z^N$，计算其最优的基向量</p><script type="math/tex; mode=display">\underset{A}{min} \sum _{i=1}^{N} ( || x^n - Az^n ||^2 ) + \lambda \frac{1}{2} ||A||^2</script><p>其中第二项为正则化项，$\lambda$为正则化项系数。</p><h4 id="稀疏编码优缺点"><a href="#稀疏编码优缺点" class="headerlink" title="稀疏编码优缺点"></a>稀疏编码优缺点</h4><p>稀疏编码的每一维都可以看作是一种特征，和基于稠密向量的分布式表示相比，稀疏编码具有更小的计算量和更好的可解释性等优点。</p><p><strong>计算量</strong> 稀疏性带来的最大好处就是可以极大的降低计算量</p><p><strong>可解释性</strong> 因为稀疏编码只有少数的非零元素，相当于将一个输入样本表示为少数几个相关的特征，这样我们可以更好的描述其特征，并易于理解</p><p><strong>特征选择</strong> 稀疏性带来的另一个好处是可以实现特征的自动选择，只选择和输入样本相关的最少特征，从而可以更好的表示输入样本，降低噪声并减轻过拟合</p><h3 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h3><p>自编码器（Auto-Encoder，AE）是通过无监督的方式来学习一组数据的有效编码。</p><p>假设有一组d维的样本$x^n \in R^d, 1 \leq n \leq N$，自编码器将这组数据映射到特征空间得到每个样本的编码$z^n \in R^p, 1 \leq n \leq N$，并且希望这组编码可以重构出原来的样本。</p><p>自编码器的结构可分为两部分：编码器（encoder）：$f: R^d -&gt; R^p$和解码器（decoder）：$R^p -&gt; R^d$</p><p>自编码器的学习目标是最小化重构误差（reconstruction errors）</p><script type="math/tex; mode=display">L = \sum_{n=1}^{N} || x^n -g(f(x^n)) ||^2 = \sum || x^n -f \cdot  g(x^n) ||^2</script><p>如果特征空间的维度p小雨原始空间的维度d，自编码器相当于是一种降维或特征抽取方法。如果$p \geq d$，一定可以找到一组或多组解使得$f \cdot g$为单位函数（Identity Function），并使得重构错误为0。但是这样的解并没有太多的意义，但是如果再加上一些附加的约束，就可以得到一些有意义的解，比如编码的稀疏性、取值范围，f和g的具体形式等。如果我们让编码只能取k个不同的值（k&lt;N），那么自编码器就可以转换为一个k类的聚类问题。</p><p>最简单的自编码器如下图所示的两层神经网络，输入层到隐藏层用来编码，隐藏层到输出层用来解码，层与层之间互相全连接。</p><p><img src="https://img-blog.csdnimg.cn/20191104162315242.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="最简单的自编码器"></p><p>对于样本x，中间隐藏层为编码：</p><script type="math/tex; mode=display">z = s(W^1 x + b^l)</script><p>输出为重构的数据</p><script type="math/tex; mode=display">x' = s(W^2 z + b^l)</script><p>其中$W,b$为网格参数，$s(.)$为激活函数。如果令$W^2$等于$W^1$的转置，即$W^2=W^{(1)T}$，称为捆绑权重（tied weights）。</p><p>给定一组样本 $x^n \in [0,1]^d, 1 \leq n \leq N$，其重构错误为：</p><script type="math/tex; mode=display">L = \sum_{n=1}^{N} || x^n -x^{'n} ||^2 + \lambda ||W||_F^2</script><p>其中$\lambda$为正则化系数，通过最小化重构误差，可以有效的学习网格的参数。</p><p>我们使用自编码器是为了得到有效的数据表示，因此在训练数据后，我们一般去掉解码器，只保留编码器，编码器的输出可以直接作为后续机器学习模型的输入。</p><h3 id="稀疏自编码器"><a href="#稀疏自编码器" class="headerlink" title="稀疏自编码器"></a>稀疏自编码器</h3><p>自编码器除了可以学习低维编码之外，也学习高维的稀疏编码。假设中间隐藏层z的维度为p，大于输入样本的维度，并让z尽量稀疏，这就是稀疏自编码器（Sparse Auto-Encoder）。和稀疏编码一样，稀疏自编码器的优点是有很高的模型可解释性，并同时进行了隐式的特征选择。</p><p>通过给自编码器中隐藏单元z加上稀疏性限制，自编码器可以学习到数据中一些有用的结构。</p><h3 id="堆叠自编码器"><a href="#堆叠自编码器" class="headerlink" title="堆叠自编码器"></a>堆叠自编码器</h3><p>对于很多数据来说，仅使用两层神经网络的自编码器还不足以获取一种好的数据表示，为了获取更好的数据表示，我们可以使用更深层的神经网络。深层神经网络作为自编码器提取的数据表示一般会更加抽象，能够很好的捕捉到数据的语义信息。在实践中经常使用逐层堆叠的方式来训练一个深层的自编码器，称为堆叠自编码器（Stacked Auto-Encoder，SAE）。堆叠自编码一般可以采用逐层训练（layer-wise training）来学习网络参数。</p><h3 id="降噪自编码器"><a href="#降噪自编码器" class="headerlink" title="降噪自编码器"></a>降噪自编码器</h3><p>降噪自编码器（Denoising Autoencoder）就是一种通过引入噪声来增加编码鲁棒性的自编码器。对于一个向量x，我们首先根据一个比例$\mu$随机将x的一些维度的值设置为0，得到一个被损坏的向量$\tilde x$。然后将被损坏的向量$\tilde x$输入给自编码器得到编码z，并重构原始的无损输入x。</p><p>下图给出了自编码器和降噪自编码器的对比，其中$f_{\theta}$为编码器，$g_{\theta^’}$为解码器，$L(x,x’)$为重构错误。</p><p><img src="https://img-blog.csdnimg.cn/20191104175727219.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="自编码器和降噪自编码器的对比"></p><p>降噪自编码器的思想十分简单，通过引入噪声来学习更鲁棒性的数据编码，并提高模型的泛化能力。</p><h2 id="概率密度估计"><a href="#概率密度估计" class="headerlink" title="概率密度估计"></a>概率密度估计</h2><p>概率密度估计（Probabilistic Density Estimation）简称密度估计（Density Estimation），是基于一些观测样本来估计一个随机变量的概率密度函数。密度估计在机器学习和数学建模中应用十分广泛。</p><p>概率密度估计分为：</p><ul><li>参数密度估计</li><li>非参数密度估计</li></ul><h3 id="参数密度估计"><a href="#参数密度估计" class="headerlink" title="参数密度估计"></a>参数密度估计</h3><p>参数密度估计（Parametric Density Estimation）是根据先验知识假设随机变量服从某种分布，然后通过训练样本来估计分布的参数。</p><p>令 $D = {\{x^n\}}_{i=1}^{N}$为某个未知分布中独立抽取的N个训练样本，假设这些样本服从一个概率分布函数$p(x|\theta)$，其对数似然函数为：</p><script type="math/tex; mode=display">log\,p(D|\theta) = \sum_{n=1}^{N}log\,p(x^n|\theta)</script><p>要估计一个参数$\theta ^{ML}$来使得：</p><script type="math/tex; mode=display">\theta ^{ML} = \underset{\theta}{arg\,max } \sum_{n=1}^{N}log\,p(x^n|\theta)</script><p>这样参数估计问题就转化为最优化问题。</p><h4 id="正态分布中的参数密度估计"><a href="#正态分布中的参数密度估计" class="headerlink" title="正态分布中的参数密度估计"></a>正态分布中的参数密度估计</h4><p>假设样本$x \in X$服从正态分布 $X \sim N(\mu,\sigma^2)$，正态分布的表达式如下：</p><script type="math/tex; mode=display">X \sim N(\mu,\sigma^2) = \frac{1}{ \sqrt{2\pi} \sigma^2} e^{- \frac{(x-\mu)^2}{2\sigma^2}}</script><p>求 $\mu,\sigma^2$的最大似然估计量。</p><p>$X$的概率密度为：</p><script type="math/tex; mode=display">f(x;\mu,\sigma^2) = \frac{1}{ \sqrt{2\pi} \sigma^2} e^{- \frac{(x-\mu)^2}{2\sigma^2}}</script><p>似然函数为：</p><script type="math/tex; mode=display">L(\mu,\sigma^2) = \prod_{i=1}^{N} \frac{1}{ \sqrt{2\pi} \sigma^2} e^{- \frac{(x-\mu)^2}{2\sigma^2}}\\= (2\pi)^{-\frac{N}{2}} (\sigma^2)^{-\frac{N}{2}} e^{(-\frac{1}{2\sigma^2} \sum_{i=1}^{N} (x_i - \mu)^2)}</script><p>对其求导可得对数似然函数为：</p><script type="math/tex; mode=display">Ln\, L =-\frac{N}{2} ln(2\pi)-\frac{N}{2} ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{N}(x_i - \mu)^2</script><p>令：</p><script type="math/tex; mode=display">\left\{\begin{matrix}\frac{\partial }{\partial \mu }ln\, L = \frac{1}{\sigma^2} (\sum_{i=1}^{N} x_i -N\mu ) =0 & \\ \\\frac{\partial }{\partial \sigma^2 }ln\, L = - \frac{N}{2\sigma^2} + \frac{1}{ (2\sigma^2)^2} \sum_{i=1}^{N}(x_i-\mu)^2 =0& \end{matrix}\right.</script><p>由前一式解得$\tilde{\mu}=\frac{1}{N}\sum_{i=1}^{N}x_i = \bar{\mu}$，代入后一式得$\tilde{\sigma^2}=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})^2$，因此得$\mu,\sigma^2$的最大似然估计为：</p><script type="math/tex; mode=display">\tilde{\mu} = \bar{X},\tilde{\sigma^2}=\frac{1}{N}(x_i - \bar{x})^2</script><h4 id="多项分布中的参数密度估计"><a href="#多项分布中的参数密度估计" class="headerlink" title="多项分布中的参数密度估计"></a>多项分布中的参数密度估计</h4><p>假设样本服从K个状态的多态分布，令onehot向量$x\in[0,1]^K$来表示第K个状态，即$x_k=1$，其余$x_{i,k \neq k}=0$，则样本x的概率密度函数为：</p><script type="math/tex; mode=display">p(x|\mu) = \prod_{k=1}^{K}\mu_k ^{x_K}</script><p>其中$\mu_k$为第k个状态的概率，并且满足$\sum_{k=1}^{K} \mu_k =1$。</p><p>数据集$D={\{x^n\}}_{n=1}^{N}$的对数似然函数为：</p><script type="math/tex; mode=display">log(D|\mu) = \sum_{n=1}^{N} \sum_{k=1}^{K} x_n ^k log (\mu _k)</script><p>多项分布的参数估计为约束优化问题，引入拉格朗日乘子$\lambda$，将原问题转化为无约束优化问题。</p><script type="math/tex; mode=display">\underset{\mu, \lambda}{ max} \sum_{n=1}^{N} \sum_{k=1}^{K} x_k ^n log(\mu_k) + \lambda (\sum_{k=1}^{K} \mu_k -1)</script><p>上式分别对$\mu_k,\lambda$求偏，并令其等于0，得到：</p><script type="math/tex; mode=display">\mu_k ^{ML} = \frac{m_k}{N}, 1 \leq N \leq K</script><p>其中$m_k = \sum_{n=1}^{N} x_k ^n$为数据集中取值为第k个状态的样本数量。</p><p>在实际应用中，参数密度估计一般存在两个问题：</p><ul><li>（1）模型选择问题，即如何选择数据分布的密度函数，实际的数据分布往往是非常复杂的，而不是简单的正态分布或者多项分布。</li><li>（2）不可观测变量问题，即我们用来训练数据的样本只包含部分的可观测变量，还有一些非常关键的变量是无法观测的，这导致我们很难估计数据的真实分布。</li><li>（3）维度灾难问题，即高维的参数估计十分困难。随着维度的增加，估计参数所需要的样本量呈指数增加。在样本不足时会出现过拟合。</li></ul><h4 id="非参数密度估计"><a href="#非参数密度估计" class="headerlink" title="非参数密度估计"></a>非参数密度估计</h4><p>非参数密度估计（Nonparametric Density Estimation）是不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数。</p><p>对于高纬空间中的一个随机向量x，假设其服从一个未知分布p(x)，则x落入空间中的小区域R的概率为：  $P=\int_{R} p(x)dx$。</p><p>给定N个训练样本$D=\{x^n\}_{n=1}^{N}$，落入区域R的样本数量K服从二项分布：</p><script type="math/tex; mode=display">P_K = \binom{N}{K}P^K(1-P)^{1-K}</script><p>其中$K/N$的期望为$E[K/N]=P$，方差为$var(K/N)=P(1-P)/N$。当N非常大时，我们可以近似认为：$P\approx \frac{K}{N}$，假设区域R足够小，其内部的概率密度是相同的，则有$P\approx p(x)V$，其中V为区域R的提及，结合前边的两个公式，可得：$p(x)\approx \frac{K}{NV}$。</p><p>根据上式，要准确的估计p(x)需要尽量使得样本数量N足够大，区域体积V尽可能的小。但在具体的应用中吗，样本数量一般有限，过小的区域导致落入该区域的样本比较少，这样估计的概率密度就不太准确。</p><p>因此在实践中估计非参数密度通常使用两种方法：</p><ul><li>（1）固定区域大小V，统计落入不同区域的数量，这种方式包括直方图和核方法两种</li><li>（2）改变区域大小，以使得落入每个区域的样本数量为K，这种方法成为K近邻方法</li></ul><h5 id="直方图方法"><a href="#直方图方法" class="headerlink" title="直方图方法"></a>直方图方法</h5><p>直方图（Histogram Method）是一种非常直观的估计连续变量密度函数的方法，可以表示为一种柱状图。</p><p>以一维随机变量为例，首先将其取值范围划分为M个连续的、不重叠的区间，每个区间的宽度为$\Delta m$，给定$N$个训练样本，我们统计这些样本落入每个区间的数量$K_m$，然后将他们归一化为密度函数。</p><script type="math/tex; mode=display">p_m = \frac {K_m}{N\Delta m},1 \leq m \leq  M</script><p>直方图的关键问题是如何选择一个合适的$\Delta m$，如果该值太小，那么落入每个区间的样本会特别少，其估计的区间密度也会有很大的随机性，如果该值过大，其估计的密度函数会变得十分平滑。下图给出了两个直方图的例子，其中蓝色表示真实的密度函数，红色表示直方图估计的密度函数。</p><p><img src="https://img-blog.csdnimg.cn/20191104090336556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="直方图估计密度函数"></p><p>直方图通常用来处理低维随机变量，可以非常快速的对数据的分布进行可视化，但其<strong>缺点</strong>是很难扩展到高维变量，假设一个d维的随机变量，如果每一维都划分为M个空间，那么整个空间的区域数量为$M^d$，直方图估计的方法会随着空间的增大而指数增长，从而形成<strong>维度灾难（Curse Of Dimensionality）</strong></p><h5 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h5><p>核密度估计（Kernel Density Estimation），也叫Parzen窗方法，是一种直方图方法的改进。</p><p>假设$R$为$d$维空间中的一个以点x为中心的“超立方体”，并定义核函数</p><script type="math/tex; mode=display">\phi (\frac{z-x}{h}) = \left\{\begin{matrix}1 \,\,\,\,\,\, if \, |z_i - x_i|< \frac{h}{2},  1 \leq i \leq d & \\ 0 \,\,\,\,\,\, else & \end{matrix}\right.</script><p>来表示一个样本是否落入该超立方体中，其中$h$为超立方体的边长，也称为核函数的密宽度。</p><p>给定$N$个训练样本$D$，落入区域$R$的样本数量$K$为：</p><script type="math/tex; mode=display">K = \sum_{n=1}^{K} \phi (\frac {x^n - x}{h})</script><p>则点$x$的密度估计为：</p><script type="math/tex; mode=display">p(x) = \frac{K}{Nh^d} =\frac{1}{Nh^d} \sum_{n=1}^{K} \phi (\frac {x^n - x}{h})</script><p>其中$h^d$表示区域$R$的体积。</p><p>除了超立方体的核函数意外之外，我们还可以选择更加平滑的核函数，比如高斯核函数：</p><script type="math/tex; mode=display">\phi (\frac {z-x}{h}) = \frac {1}{ (2\pi)^{\frac{1}{2}} h} exp(- \frac{||z-x||^2}{2h^2})</script><p>其中$h^2$可以看做是高斯核函数的方差，这样点$x$的密度估计为：</p><script type="math/tex; mode=display">p (x) = \frac{1}{N} \sum_{n=1}^{N}   \frac {1}{ (2\pi)^{\frac{1}{2}} h} exp(- \frac{||z-x||^2}{2h^2})</script><h5 id="K近邻方法"><a href="#K近邻方法" class="headerlink" title="K近邻方法"></a>K近邻方法</h5><p>核密度估计方法中的核宽度是固定的，因此同一个宽度可能对高密度的区域过大，而对低密度的区域过小。一种更加灵活的方式是设置一种可变宽度的区域，并使得落入每个区域中的样本数量固定为K。</p><p>要估计点x的密度，首先找到一个以x为中心的球体，使得落入球体的样本数量为K，然后根据公式$p(x)\approx \frac{K}{NV}$就可以计算出点x的密度。因为落入球体的样本也是离x最近的K个样本，所以这种方法也称为K近邻（K-Nearest Neughbor）方法。</p><p>在K近邻方法中，K值的选择十分重要，如果K太小，无法有效的估计密度函数，而K太大也会使局部的密度不准确，并且会增加计算开销。</p><p>K近邻方法也经常用于分类问题，称为K近邻分类器。 当K=1时为最近邻分类器。</p><p>最近邻分类器的一个性质是，当 $N \rightarrow \infty$，其分类错误率不超过最优分类器错误率的两倍。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无监督学习是一种十分重要的机器学习方法，无监督学习问题主要可以分为聚类，特征学习，密度估计等几种类型。但是无监督学习并没有像有监督学习那样取得广泛的成功，主要原因在于其缺少有效客观评价的方法，导致很难衡量一个无监督学习方法的好坏。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无监督学习概述&quot;&gt;&lt;a href=&quot;#无监督学习概述&quot; class=&quot;headerlink&quot; title=&quot;无监督学习概述&quot;&gt;&lt;/a&gt;无监督学习概述&lt;/h2&gt;&lt;p&gt;无监督学习（Unsupervised Learning）是指从无标签的数据中学习出一些有用的模式，无
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
