<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>All In CTR、DL、ML、RL、NLP、KG</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2019-12-06T03:14:43.954Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【技术服务】和【商务合作】</title>
    <link href="http://thinkgamer.cn/2066/06/06/%E9%9A%8F%E7%BC%98/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2066/06/06/随缘/商务合作介绍/</id>
    <published>2066-06-05T22:06:06.000Z</published>
    <updated>2019-12-06T03:14:43.954Z</updated>
    
    <content type="html"><![CDATA[<hr><center><br><br><font size="8" style="font-weight:bold; color: green;">WelCome To “Thinkgamer 小站”</font><br><br></center><hr><p>全网唯一ID：Thinkgamer,个人微信公众号”搜索与推荐Wiki“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，数据挖掘，数据分析，全站开发，大数据。</p><h3 id="About"><a href="#About" class="headerlink" title="About"></a>About</h3><p>CyanScikit科技团队成立于2019年，由一群热爱技术，追求极致的Coders和Managers组成！</p><h3 id="Leader"><a href="#Leader" class="headerlink" title="Leader"></a>Leader</h3><p>全球唯一ID：Thinkgamer。《推荐系统开发实战》作者，CSDN博客技术专家，原Top电商算法工程师。擅长领域：</p><ul><li>推荐系统</li><li>数据开发/分析/挖掘</li></ul><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><center>    <img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center><ul><li>技术咨询、学习路线定制</li><li>推荐系统、技术培训</li><li>数据存储（方案/规范等）、数据开发、数据分析、数据挖掘</li><li>数据可视化、小程序/H5、全栈开发</li><li>爬虫、广告接入</li></ul><h3 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h3><p>使用技术去更好的服务于客户！</p><h3 id="Team"><a href="#Team" class="headerlink" title="Team"></a>Team</h3><center>    <img src="https://img-blog.csdnimg.cn/20191105121227125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center><h3 id="Contact-Me"><a href="#Contact-Me" class="headerlink" title="Contact Me"></a>Contact Me</h3><center>    <img src="https://img-blog.csdnimg.cn/20191105121227125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center><center>    <img src="https://img-blog.csdnimg.cn/20191105121446131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="99%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;center&gt;
&lt;br&gt;&lt;br&gt;
&lt;font size=&quot;8&quot; style=&quot;font-weight:bold; color: green;&quot;&gt;WelCome To “Thinkgamer 小站”&lt;/font&gt;
&lt;br&gt;&lt;br&gt;
&lt;/center&gt;

&lt;hr&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="商务合作" scheme="http://thinkgamer.cn/tags/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之4、构建离线文章画像</title>
    <link href="http://thinkgamer.cn/2019/12/10/RecSys/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B4%E3%80%81%E6%9E%84%E5%BB%BA%E7%A6%BB%E7%BA%BF%E6%96%87%E7%AB%A0%E7%94%BB%E5%83%8F/"/>
    <id>http://thinkgamer.cn/2019/12/10/RecSys/文章推荐系统/《文章推荐系统》系列之4、构建离线文章画像/</id>
    <published>2019-12-10T04:12:12.000Z</published>
    <updated>2019-12-06T04:59:41.647Z</updated>
    
    <content type="html"><![CDATA[<p>在上述步骤中，我们已经将业务数据和用户行为数据同步到了推荐系统数据库当中，接下来，我们就要对文章数据和用户数据进行分析，构建文章画像和用户画像，本文我们主要讲解如何构建文章画像。文章画像由关键词和主题词组成，我们将每个词的 TF-IDF 权重和 TextRank 权重的乘积作为关键词权重，筛选出权重最高的 K 个词作为关键词，将 TextRank 权重最高的 K 个词与 TF-IDF 权重最高的 K 个词的共现词作为主题词。</p><p>首先，在 Hive 中创建文章数据库 article 及相关表，其中表 article_data 用于存储完整的文章信息，表 idf_keywords_values 用于存储关键词和索引信息，表 tfidf_keywords_values 用于存储关键词和 TF-IDF 权重信息，表 textrank_keywords_values 用于存储关键词和 TextRank 权重信息，表 article_profile 用于存储文章画像信息。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建文章数据库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> article <span class="keyword">comment</span> <span class="string">"artcile information"</span> location <span class="string">'/user/hive/warehouse/article.db/'</span>;</span><br><span class="line"><span class="comment">-- 创建文章信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> article_data</span><br><span class="line">(</span><br><span class="line">    article_id   <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id   <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    channel_name <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"channel_name"</span>,</span><br><span class="line">    title        <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"title"</span>,</span><br><span class="line">    <span class="keyword">content</span>      <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"content"</span>,</span><br><span class="line">    sentence     <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"sentence"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_channel"</span></span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/article.db/article_data'</span>;</span><br><span class="line"><span class="comment">-- 创建关键词索引信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> idf_keywords_values</span><br><span class="line">(</span><br><span class="line">    keyword <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    idf     <span class="keyword">DOUBLE</span> <span class="keyword">comment</span> <span class="string">"idf"</span>,</span><br><span class="line">    <span class="keyword">index</span>   <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"index"</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 创建关键词TF-IDF权重信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tfidf_keywords_values</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    keyword    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"keyword"</span>,</span><br><span class="line">    tfidf      <span class="keyword">DOUBLE</span> <span class="keyword">comment</span> <span class="string">"tfidf"</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 创建关键词TextRank权重信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> textrank_keywords_values</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    keyword    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"keyword"</span>,</span><br><span class="line">    textrank   <span class="keyword">DOUBLE</span> <span class="keyword">comment</span> <span class="string">"textrank"</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 创建文章画像信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> article_profile</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    keyword    <span class="keyword">map</span> <span class="keyword">comment</span> <span class="string">"keyword"</span>,</span><br><span class="line">    topics     <span class="built_in">array</span> <span class="keyword">comment</span> <span class="string">"topics"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><h1 id="计算文章完整信息"><a href="#计算文章完整信息" class="headerlink" title="计算文章完整信息"></a>计算文章完整信息</h1><p>为了计算文章画像，需要将文章信息表（news_article_basic）、文章内容表（news_article_content）及频道表（news_channel）进行合并，从而得到完整的文章信息，通常使用 Spark SQL 进行处理。</p><p>通过关联表 news_article_basic, news_article_content 和 news_channel 获得文章完整信息，包括 article_id, channel_id, channel_name, title, content，这里获取一个小时内的文章信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use toutiao"</span>)</span><br><span class="line">_now = datetime.today().replace(minute=<span class="number">0</span>, second=<span class="number">0</span>, microsecond=<span class="number">0</span>)</span><br><span class="line">start = datetime.strftime(_now + timedelta(days=<span class="number">0</span>, hours=<span class="number">-1</span>, minutes=<span class="number">0</span>), <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">end = datetime.strftime(_now, <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">basic_content = spark.sql(</span><br><span class="line">            <span class="string">"select a.article_id, a.channel_id, a.title, b.content from news_article_basic a "</span></span><br><span class="line">            <span class="string">"inner join news_article_content b on a.article_id=b.article_id where a.review_time &gt;= '&#123;&#125;' "</span></span><br><span class="line">            <span class="string">"and a.review_time &lt; '&#123;&#125;' and a.status = 2"</span>.format(start, end))</span><br><span class="line">basic_content.registerTempTable(<span class="string">"temp_article"</span>)</span><br><span class="line">channel_basic_content = spark.sql(</span><br><span class="line">            <span class="string">"select t.*, n.channel_name from temp_article t left join news_channel n on t.channel_id=n.channel_id"</span>)</span><br></pre></td></tr></table></figure></p><p><code>channel_basic_content</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-96f4fc864f469275.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>利用 <code>concat_ws()</code> 方法，将 channel_name, title, content 这 3 列数据合并为一列 sentence，并将结果写入文章完整信息表 article_data 中<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import pyspark.sql.<span class="built_in">functions</span> as F</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">sentence_df = channel_basic_content.select(<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"channel_name"</span>, <span class="string">"title"</span>, <span class="string">"content"</span>, \</span><br><span class="line">                                           F.concat_ws(</span><br><span class="line">                                               <span class="string">","</span>,</span><br><span class="line">                                               channel_basic_content.channel_name,</span><br><span class="line">                                               channel_basic_content.<span class="built_in">title</span>,</span><br><span class="line">                                               channel_basic_content.<span class="built_in">content</span></span><br><span class="line">                                           ).<span class="built_in">alias</span>(<span class="string">"sentence"</span>)</span><br><span class="line">                                           )</span><br><span class="line"><span class="built_in">del</span> basic_content</span><br><span class="line"><span class="built_in">del</span> channel_basic_content</span><br><span class="line">gc.collect() # 垃圾回收</span><br><span class="line"></span><br><span class="line">sentence_df.write.insertInto(<span class="string">"article_data"</span>)</span><br></pre></td></tr></table></figure></p><p><code>sentence_df</code> 结果如下所示，文章完整信息包括 article_id, channel_id, channel_name, title, content, sentence，其中 sentence 为 channel_name, title, content 合并而成的长文本内容</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-b87fe4016480095b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="计算-TF-IDF"><a href="#计算-TF-IDF" class="headerlink" title="计算 TF-IDF"></a>计算 TF-IDF</h1><p>前面我们得到了文章的完整内容信息，接下来，我们要先对文章进行分词，然后计算每个词的 TF-IDF 权重，将 TF-IDF 权重最高的 K 个词作为文章的关键词。首先，读取文章信息<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_dataframe = spark.sql(<span class="string">"select * from article_data"</span>)</span><br></pre></td></tr></table></figure></p><p>利用 <code>mapPartitions()</code> 方法，对每篇文章进行分词，这里使用的是 <code>jieba</code> 分词器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">words_df = article_dataframe.rdd.mapPartitions(segmentation).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"words"</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segmentation</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line">    <span class="keyword">import</span> jieba.analyse</span><br><span class="line">    <span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">    <span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">    abspath = <span class="string">"/root/words"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结巴加载用户词典</span></span><br><span class="line">    userdict_path = os.path.join(abspath, <span class="string">"ITKeywords.txt"</span>)</span><br><span class="line">    jieba.load_userdict(userdict_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 停用词文本</span></span><br><span class="line">    stopwords_path = os.path.join(abspath, <span class="string">"stopwords.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stopwords_list</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">"""返回stopwords列表"""</span></span><br><span class="line">        stopwords_list = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> codecs.open(stopwords_path).readlines()]</span><br><span class="line">        <span class="keyword">return</span> stopwords_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有的停用词列表</span></span><br><span class="line">    stopwords_list = get_stopwords_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut_sentence</span><span class="params">(sentence)</span>:</span></span><br><span class="line">        <span class="string">"""对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词"""</span></span><br><span class="line">        seg_list = pseg.lcut(sentence)</span><br><span class="line">        seg_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> seg_list <span class="keyword">if</span> i.flag <span class="keyword">not</span> <span class="keyword">in</span> stopwords_list]</span><br><span class="line">        filtered_words_list = []</span><br><span class="line">        <span class="keyword">for</span> seg <span class="keyword">in</span> seg_list:</span><br><span class="line">            <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> seg.flag == <span class="string">"eng"</span>:</span><br><span class="line">                <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag.startswith(<span class="string">"n"</span>):</span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag <span class="keyword">in</span> [<span class="string">"x"</span>, <span class="string">"eng"</span>]:  <span class="comment"># 是自定一个词语或者是英文单词</span></span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">        <span class="keyword">return</span> filtered_words_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        sentence = re.sub(<span class="string">"&lt;.*?&gt;"</span>, <span class="string">""</span>, row.sentence)    <span class="comment"># 替换掉标签数据</span></span><br><span class="line">        words = cut_sentence(sentence)</span><br><span class="line">        <span class="keyword">yield</span> row.article_id, row.channel_id, words</span><br></pre></td></tr></table></figure></p><p><code>words_df</code> 结果如下所示，words 为将 sentence 分词后的单词列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-55862171336abc52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>使用分词结果对词频统计模型（CountVectorizer）进行词频统计训练，并将 CountVectorizer 模型保存到 HDFS 中<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import CountVectorizer</span><br><span class="line"><span class="comment"># vocabSize是总词汇的大小，minDF是文本中出现的最少次数</span></span><br><span class="line">cv = CountVectorizer(<span class="attribute">inputCol</span>=<span class="string">"words"</span>, <span class="attribute">outputCol</span>=<span class="string">"countFeatures"</span>, <span class="attribute">vocabSize</span>=200*10000, <span class="attribute">minDF</span>=1.0)</span><br><span class="line"><span class="comment"># 训练词频统计模型</span></span><br><span class="line">cv_model = cv.fit(words_df)</span><br><span class="line">cv_model.write().overwrite().save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/CV.model"</span>)</span><br></pre></td></tr></table></figure></p><p>加载 CountVectorizer 模型，计算词频向量<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.feature import CountVectorizerModel</span><br><span class="line">cv_model = CountVectorizerModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/CV.model"</span>)</span><br><span class="line"><span class="comment"># 得出词频向量结果</span></span><br><span class="line">cv_result = cv_model.transform(words_df)</span><br></pre></td></tr></table></figure></p><p><code>cv_result</code> 结果如下所示，countFeatures 为词频向量，如 (986, [2, 4, …], [3.0, 5.0, …]) 表示总词汇的大小为 986 个，索引为 2 和 4 的词在某篇文章中分别出现 3 次和 5 次，</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-760b54a16a9e780b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>得到词频向量后，再利用逆文本频率模型（ IDF ），根据词频向量进行 IDF 统计训练，并将 IDF 模型保存到 HDFS<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import IDF</span><br><span class="line">idf = IDF(<span class="attribute">inputCol</span>=<span class="string">"countFeatures"</span>, <span class="attribute">outputCol</span>=<span class="string">"idfFeatures"</span>)</span><br><span class="line">idf_model = idf.fit(cv_result)</span><br><span class="line">idf_model.write().overwrite().save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/IDF.model"</span>)</span><br></pre></td></tr></table></figure></p><p>我们已经分别计算了文章信息中每个词的 TF 和 IDF，这时就可以加载 CountVectorizer 模型和 IDF 模型，计算每个词的 TF-IDF<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.feature import CountVectorizerModel</span><br><span class="line">cv_model = CountVectorizerModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/countVectorizerOfArticleWords.model"</span>)</span><br><span class="line">from pyspark.ml.feature import IDFModel</span><br><span class="line">idf_model = IDFModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/IDFOfArticleWords.model"</span>)</span><br><span class="line"></span><br><span class="line">cv_result = cv_model.transform(words_df)</span><br><span class="line">tfidf_result = idf_model.transform(cv_result)</span><br></pre></td></tr></table></figure></p><p><code>tfidf_result</code> 结果如下所示，idfFeatures 为 TF-IDF 权重向量，如 (986, [2, 4, …], [0.3, 0.5, …]) 表示总词汇的大小为 986 个，索引为 2 和 4 的词在某篇文章中的 TF-IDF 值分别为 0.3 和 0.5</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-44cdf662e1e2adb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>对文章的每个词都根据 TF-IDF 权重排序，保留 TF-IDF 权重最高的前 K 个词作为关键词<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def sort_by_tfidf(<span class="built_in">partition</span>):</span><br><span class="line">    TOPK = <span class="number">20</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">row</span> <span class="keyword">in</span> <span class="built_in">partition</span>:</span><br><span class="line">        # 找到索引与IDF值并进行排序</span><br><span class="line">        _dict = list(zip(<span class="built_in">row</span>.idfFeatures.<span class="built_in">indices</span>, <span class="built_in">row</span>.idfFeatures.<span class="built_in">values</span>))</span><br><span class="line">        _dict = sorted(_dict, <span class="built_in">key</span>=<span class="built_in">lambda</span> x: x[<span class="number">1</span>], <span class="built_in">reverse</span>=True)</span><br><span class="line">        result = _dict[:TOPK]</span><br><span class="line">        <span class="keyword">for</span> word_index, tfidf <span class="keyword">in</span> result:</span><br><span class="line">            yield <span class="built_in">row</span>.article_id, <span class="built_in">row</span>.channel_id, int(word_index), <span class="built_in">round</span>(<span class="built_in">float</span>(tfidf), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">keywords_by_tfidf = tfidf_result.rdd.mapPartitions(sort_by_tfidf).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"index"</span>, <span class="string">"weights"</span>])</span><br></pre></td></tr></table></figure></p><p><code>keywords_by_tfidf</code> 结果如下所示，每篇文章保留了权重最高的 K 个单词，index 为单词索引，weights 为对应单词的 TF-IDF 权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-566af87857d1c5bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，我们需要知道每个词的对应的 TF-IDF 值，可以利用 <code>zip()</code> 方法，将所有文章中的每个词及其 TF-IDF 权重组成字典，再加入索引列，由此得到每个词对应的 TF-IDF 值，将该结果保存到 idf_keywords_values 表<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">keywords_list_with_idf = <span class="keyword">list</span>(zip(cv_model.vocabulary, idf_model.idf.toArray()))</span><br><span class="line">def append_index(data):</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">index</span> in <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        data[<span class="built_in">index</span>] = <span class="keyword">list</span>(data[<span class="built_in">index</span>]) # 将元组转为<span class="keyword">list</span></span><br><span class="line">        data[<span class="built_in">index</span>].<span class="keyword">append</span>(<span class="built_in">index</span>)       # 加入索引 </span><br><span class="line">        data[<span class="built_in">index</span>][<span class="number">1</span>] = float(data[<span class="built_in">index</span>][<span class="number">1</span>])</span><br><span class="line">append_index(keywords_list_with_idf)</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">rdd = sc.parallelize(keywords_list_with_idf) # 创建rdd</span><br><span class="line">idf_keywords = rdd.toDF([<span class="string">"keywords"</span>, <span class="string">"idf"</span>, <span class="string">"index"</span>])</span><br><span class="line"></span><br><span class="line">idf_keywords.<span class="keyword">write</span>.insertInto(<span class="string">'idf_keywords_values'</span>)</span><br></pre></td></tr></table></figure></p><p><code>idf_keywords</code> 结果如下所示，包含了所有单词的名称、TF-IDF 权重及索引</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-bbf3babed9a9ccfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>通过 index 列，将 <code>keywords_by_tfidf</code> 与表 idf_keywords_values 进行连接，选取文章 ID、频道 ID、关键词、TF-IDF 权重作为结果，并保存到 TF-IDF 关键词表 tfidf_keywords_values<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keywords_index = spark.sql(<span class="string">"select keyword, index idx from idf_keywords_values"</span>)</span><br><span class="line">keywords_result = keywords_by_tfidf.join(keywords_index, keywords_index<span class="selector-class">.idx</span> == keywords_by_tfidf.index).select([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"weights"</span>])</span><br><span class="line">keywords_result<span class="selector-class">.write</span><span class="selector-class">.insertInto</span>(<span class="string">"tfidf_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p><code>keywords_result</code> 结果如下所示，keyword 和 weights 即为所有词在每个文章中的 TF-IDF 权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-7c39214d62083c30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="计算-TextRank"><a href="#计算-TextRank" class="headerlink" title="计算 TextRank"></a>计算 TextRank</h1><p>前面我们已经计算好了每个词的 TF-IDF 权重，为了计算关键词，还需要得到每个词的 TextRank 权重，接下来，还是先读取文章完整信息<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_dataframe = spark.sql(<span class="string">"select * from article_data"</span>)</span><br></pre></td></tr></table></figure></p><p>对文章 sentence 列的内容进行分词，计算每个词的 TextRank 权重，并将每篇文章 TextRank 权重最高的 K 个词保存到 TextRank 结果表 textrank_keywords_values<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">textrank_keywords_df = article_dataframe<span class="selector-class">.rdd</span><span class="selector-class">.mapPartitions</span>(textrank).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"textrank"</span>])</span><br><span class="line"></span><br><span class="line">textrank_keywords_df<span class="selector-class">.write</span><span class="selector-class">.insertInto</span>(<span class="string">"textrank_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p>TextRank 计算细节：分词后只保留指定词性的词，滑动截取长度为 K 的窗口，计算窗口内的各个词的投票数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textrank</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line">    <span class="keyword">import</span> jieba.analyse</span><br><span class="line">    <span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">    <span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">    abspath = <span class="string">"/root/words"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结巴加载用户词典</span></span><br><span class="line">    userDict_path = os.path.join(abspath, <span class="string">"ITKeywords.txt"</span>)</span><br><span class="line">    jieba.load_userdict(userDict_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 停用词文本</span></span><br><span class="line">    stopwords_path = os.path.join(abspath, <span class="string">"stopwords.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stopwords_list</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">"""返回stopwords列表"""</span></span><br><span class="line">        stopwords_list = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> codecs.open(stopwords_path).readlines()]</span><br><span class="line">        <span class="keyword">return</span> stopwords_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有的停用词列表</span></span><br><span class="line">    stopwords_list = get_stopwords_list()</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">TextRank</span><span class="params">(jieba.analyse.TextRank)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, window=<span class="number">20</span>, word_min_len=<span class="number">2</span>)</span>:</span></span><br><span class="line">            super(TextRank, self).__init__()</span><br><span class="line">            self.span = window  <span class="comment"># 窗口大小</span></span><br><span class="line">            self.word_min_len = word_min_len  <span class="comment"># 单词的最小长度</span></span><br><span class="line">            <span class="comment"># 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac</span></span><br><span class="line">            self.pos_filt = frozenset(</span><br><span class="line">                (<span class="string">'n'</span>, <span class="string">'x'</span>, <span class="string">'eng'</span>, <span class="string">'f'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">"nw"</span>, <span class="string">"nz"</span>, <span class="string">"PER"</span>, <span class="string">"LOC"</span>, <span class="string">"ORG"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">pairfilter</span><span class="params">(self, wp)</span>:</span></span><br><span class="line">            <span class="string">"""过滤条件，返回True或者False"""</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> wp.flag == <span class="string">"eng"</span>:</span><br><span class="line">                <span class="keyword">if</span> len(wp.word) &lt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> wp.flag <span class="keyword">in</span> self.pos_filt <span class="keyword">and</span> len(wp.word.strip()) &gt;= self.word_min_len \</span><br><span class="line">                    <span class="keyword">and</span> wp.word.lower() <span class="keyword">not</span> <span class="keyword">in</span> stopwords_list:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="comment"># TextRank过滤窗口大小为5，单词最小为2</span></span><br><span class="line">    textrank_model = TextRank(window=<span class="number">5</span>, word_min_len=<span class="number">2</span>)</span><br><span class="line">    allowPOS = (<span class="string">'n'</span>, <span class="string">"x"</span>, <span class="string">'eng'</span>, <span class="string">'nr'</span>, <span class="string">'ns'</span>, <span class="string">'nt'</span>, <span class="string">"nw"</span>, <span class="string">"nz"</span>, <span class="string">"c"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        tags = textrank_model.textrank(row.sentence, topK=<span class="number">20</span>, withWeight=<span class="keyword">True</span>, allowPOS=allowPOS, withFlag=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> tags:</span><br><span class="line">            <span class="keyword">yield</span> row.article_id, row.channel_id, tag[<span class="number">0</span>], tag[<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><p><code>textrank_keywords_df</code> 结果如下所示，keyword 和 textrank 即为每个单词在文章中的 TextRank 权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4453fb49ff5e39e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="画像计算"><a href="#画像计算" class="headerlink" title="画像计算"></a>画像计算</h1><p>我们计算出 TF-IDF 和 TextRank 后，就可以计算关键词和主题词了，读取 TF-IDF 权重<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">idf_keywords_values = oa<span class="selector-class">.spark</span><span class="selector-class">.sql</span>(<span class="string">"select * from idf_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p>读取 TextRank 权重<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textrank_keywords_values = oa<span class="selector-class">.spark</span><span class="selector-class">.sql</span>(<span class="string">"select * from textrank_keywords_values"</span>)</span><br></pre></td></tr></table></figure></p><p>通过 <code>keyword</code> 关联 TF-IDF 权重和 TextRank 权重<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">keywords_res</span> = textrank_keywords_values.join(idf_keywords_values, <span class="literal">on</span>=[<span class="string">'keyword'</span>], how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure></p><p>计算 TF-IDF 权重和 TextRank 权重的乘积作为关键词权重<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">keywords_weights</span> = keywords_res.withColumn(<span class="string">'weights'</span>, keywords_res.textrank * keywords_res.idf).select([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"weights"</span>])</span><br></pre></td></tr></table></figure></p><p><code>keywords_weights</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-bd60d34728a7aff8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这里，我们需要将相同文章的词都合并到一条记录中，将 <code>keywords_weights</code> 按照 article_id 分组，并利用 <code>collect_list()</code> 方法，分别将关键词和权重合并为列表<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keywords_weights.registerTempTable('temp')</span><br><span class="line"></span><br><span class="line">keywords_weights = spark.sql("<span class="keyword">select</span> article_id, <span class="built_in">min</span>(channel_id) channel_id, collect_list(keyword) keywords, collect_list(weights) weights <span class="keyword">from</span> temp <span class="keyword">group</span> <span class="keyword">by</span> article_id<span class="string">")`</span></span><br></pre></td></tr></table></figure></p><p><code>keywords_weights</code> 结果如下所示，keywords 为每篇文章的关键词列表，weights 为关键词对应的权重列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-562748d12a395121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>为了方便查询，我们需要将关键词和权重合并为一列，并存储为 map 类型，这里利用 <code>dict()</code> 和 <code>zip()</code> 方法，将每个关键词及其权重组合成字典<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_map</span><span class="params">(row)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, row.channel_id, dict(zip(row.keywords, row.weights))</span><br><span class="line"></span><br><span class="line">article_keywords = keywords_weights.rdd.map(to_map).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'keywords'</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_keywords</code> 结果如下所示，keywords 即为每篇文章的关键词和对应权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-5a02c70d3f93eb78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>前面我们计算完了关键词，接下来我们将 TF-IDF 和 TextRank 的共现词作为主题词，将 TF-IDF 权重表 tfidf_keywords_values 和 TextRank 权重表 textrank_keywords_values 进行关联，并利用 <code>collect_set()</code> 对结果进行去重，即可得到 TF-IDF 和 TextRank 的共现词，即主题词<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">topic_sql</span> = <span class="string">"""</span></span><br><span class="line"><span class="string">                select t.article_id article_id2, collect_set(t.keyword) topics from tfidf_keywords_values t</span></span><br><span class="line"><span class="string">                inner join </span></span><br><span class="line"><span class="string">                textrank_keywords_values r</span></span><br><span class="line"><span class="string">                where t.keyword=r.keyword</span></span><br><span class="line"><span class="string">                group by article_id2</span></span><br><span class="line"><span class="string">                """</span></span><br><span class="line"><span class="attr">article_topics</span> = spark.sql(topic_sql)</span><br></pre></td></tr></table></figure></p><p><code>article_topics</code> 结果如下所示，topics 即为每篇文章的主题词列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ca3825c269a0581b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>最后，将主题词结果和关键词结果合并，即为文章画像，保存到表 <code>article_profile</code><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">article_profile = article_keywords.join(article_topics, article_keywords.article_id==article_topics.article_id2).select([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keywords"</span>, <span class="string">"topics"</span>])</span><br><span class="line"></span><br><span class="line">article_profile<span class="selector-class">.write</span><span class="selector-class">.insertInto</span>(<span class="string">"article_profile"</span>)</span><br></pre></td></tr></table></figure></p><p>文章画像数据查询测试<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="built_in">select</span> * <span class="keyword">from</span> article_profile limit <span class="number">1</span>;</span><br><span class="line">OK</span><br><span class="line"><span class="number">26</span>      <span class="number">17</span>      &#123;<span class="string">"策略"</span>:<span class="number">0.3973770571351729</span>,<span class="string">"jpg"</span>:<span class="number">0.9806348975390871</span>,<span class="string">"用户"</span>:<span class="number">1.2794959063944176</span>,<span class="string">"strong"</span>:<span class="number">1.6488457985625076</span>,<span class="string">"文件"</span>:<span class="number">0.28144603583387057</span>,<span class="string">"逻辑"</span>:<span class="number">0.45256526469610714</span>,<span class="string">"形式"</span>:<span class="number">0.4123994242601279</span>,<span class="string">"全自"</span>:<span class="number">0.9594604850547191</span>,<span class="string">"h2"</span>:<span class="number">0.6244481634710125</span>,<span class="string">"版本"</span>:<span class="number">0.44280276959510817</span>,<span class="string">"Adobe"</span>:<span class="number">0.8553618185108718</span>,<span class="string">"安装"</span>:<span class="number">0.8305037437573172</span>,<span class="string">"检查更新"</span>:<span class="number">1.8088946300014435</span>,<span class="string">"产品"</span>:<span class="number">0.774842382276899</span>,<span class="string">"下载页"</span>:<span class="number">1.4256311032544344</span>,<span class="string">"过程"</span>:<span class="number">0.19827163395829256</span>,<span class="string">"json"</span>:<span class="number">0.6423301791599972</span>,<span class="string">"方式"</span>:<span class="number">0.582762869780791</span>,<span class="string">"退出应用"</span>:<span class="number">1.2338671268242603</span>,<span class="string">"Setup"</span>:<span class="number">1.004399549339134</span>&#125;   [<span class="string">"Electron"</span>,<span class="string">"全自动"</span>,<span class="string">"产品"</span>,<span class="string">"版本号"</span>,<span class="string">"安装包"</span>,<span class="string">"检查更新"</span>,<span class="string">"方案"</span>,<span class="string">"版本"</span>,<span class="string">"退出应用"</span>,<span class="string">"逻辑"</span>,<span class="string">"安装过程"</span>,<span class="string">"方式"</span>,<span class="string">"定性"</span>,<span class="string">"新版本"</span>,<span class="string">"Setup"</span>,<span class="string">"静默"</span>,<span class="string">"用户"</span>]</span><br><span class="line"><span class="built_in">Time</span> taken: <span class="number">0.322</span> seconds, Fetched: <span class="number">1</span> row(s)</span><br></pre></td></tr></table></figure></p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>定义离线更新文章画像的方法，首先合并最近一个小时的文章信息，接着计算每个词的 TF-IDF 和 TextRank 权重，并根据 TF-IDF 和 TextRank 权重计算得出文章关键词和主题词，最后将文章画像信息保存到 Hive<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_article_profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定时更新文章画像</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ua = UpdateArticle()</span><br><span class="line">    <span class="comment"># 合并文章信息</span></span><br><span class="line">    sentence_df = ua.merge_article_data()</span><br><span class="line">    <span class="keyword">if</span> sentence_df.rdd.collect():</span><br><span class="line">        textrank_keywords_df, keywordsIndex = ua.generate_article_label()</span><br><span class="line">        ua.get_article_profile(textrank_keywords_df, keywordsIndex)</span><br></pre></td></tr></table></figure></p><p>利用 Apscheduler 添加定时更新文章画像任务，设定每隔 1 个小时更新一次<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">from</span> apscheduler.schedulers.blocking <span class="keyword">import</span> BlockingScheduler</span><br><span class="line"><span class="title">from</span> apscheduler.executors.pool <span class="keyword">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建scheduler,多进程执行</span></span><br><span class="line"><span class="title">executors</span> = &#123;</span><br><span class="line">    '<span class="keyword">default</span>': <span class="type">ProcessPoolExecutor</span>(3)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="title">scheduler</span> = <span class="type">BlockingScheduler</span>(executors=executors)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 添加一个定时更新文章画像的任务,每隔1个小时运行一次</span></span><br><span class="line"><span class="title">scheduler</span>.add_job(update_article_profile, trigger='interval', hours=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="title">scheduler</span>.start()</span><br></pre></td></tr></table></figure></p><p>利用 Supervisor 进行进程管理，配置文件如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[program:offline]</span><br><span class="line">environment=JAVA_HOME=/root/bigdata/jdk,SPARK_HOME=/root/bigdata/spark,HADOOP_HOME=/root/bigdata/hadoop,PYSPARK_PYTHON=/miniconda2/envs/reco_sys/bin/python,PYSPARK_DRIVER_PYTHON=/miniconda2/envs/reco_sys/bin/python</span><br><span class="line">command=/miniconda2/envs/reco_sys/bin/python /root/toutiao_project/scheduler/main.py</span><br><span class="line">directory=/root/toutiao_project/scheduler</span><br><span class="line">user=root</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stdout_logfile=/root/logs/offlinesuper.log</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上述步骤中，我们已经将业务数据和用户行为数据同步到了推荐系统数据库当中，接下来，我们就要对文章数据和用户数据进行分析，构建文章画像和用户画像，本文我们主要讲解如何构建文章画像。文章画像由关键词和主题词组成，我们将每个词的 TF-IDF 权重和 TextRank 权重的乘积
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之5、计算文章相似度</title>
    <link href="http://thinkgamer.cn/2019/12/10/RecSys/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B5%E3%80%81%E8%AE%A1%E7%AE%97%E6%96%87%E7%AB%A0%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <id>http://thinkgamer.cn/2019/12/10/RecSys/文章推荐系统/《文章推荐系统》系列之5、计算文章相似度/</id>
    <published>2019-12-10T04:12:12.000Z</published>
    <updated>2019-12-06T04:59:41.647Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中，我们已经完成了离线文章画像的构建，接下来，我们要为相似文章推荐做准备，那就是计算文章之间的相似度。首先，我们要计算出文章的词向量，然后利用文章的词向量来计算文章的相似度。</p><h1 id="计算文章词向量"><a href="#计算文章词向量" class="headerlink" title="计算文章词向量"></a>计算文章词向量</h1><p>我们可以通过大量的历史文章数据，训练文章中每个词的词向量，由于文章数据过多，通常是分频道进行词向量训练，即每个频道训练一个词向量模型，我们包括的频道如下所示<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">channel_info = &#123;</span><br><span class="line">            1: <span class="string">"html"</span>,</span><br><span class="line">            2: <span class="string">"开发者资讯"</span>,</span><br><span class="line">            3: <span class="string">"ios"</span>,</span><br><span class="line">            4: <span class="string">"c++"</span>,</span><br><span class="line">            5: <span class="string">"android"</span>,</span><br><span class="line">            6: <span class="string">"css"</span>,</span><br><span class="line">            7: <span class="string">"数据库"</span>,</span><br><span class="line">            8: <span class="string">"区块链"</span>,</span><br><span class="line">            9: <span class="string">"go"</span>,</span><br><span class="line">            10: <span class="string">"产品"</span>,</span><br><span class="line">            11: <span class="string">"后端"</span>,</span><br><span class="line">            12: <span class="string">"linux"</span>,</span><br><span class="line">            13: <span class="string">"人工智能"</span>,</span><br><span class="line">            14: <span class="string">"php"</span>,</span><br><span class="line">            15: <span class="string">"javascript"</span>,</span><br><span class="line">            16: <span class="string">"架构"</span>,</span><br><span class="line">            17: <span class="string">"前端"</span>,</span><br><span class="line">            18: <span class="string">"python"</span>,</span><br><span class="line">            19: <span class="string">"java"</span>,</span><br><span class="line">            20: <span class="string">"算法"</span>,</span><br><span class="line">            21: <span class="string">"面试"</span>,</span><br><span class="line">            22: <span class="string">"科技动态"</span>,</span><br><span class="line">            23: <span class="string">"js"</span>,</span><br><span class="line">            24: <span class="string">"设计"</span>,</span><br><span class="line">            25: <span class="string">"数码产品"</span>,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></p><p>接下来，分别对各自频道内的文章进行分词处理，这里先选取 18 号频道内的所有文章，进行分词处理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"use article"</span>)</span><br><span class="line">article_data = spark.sql(<span class="string">"select * from article_data where channel_id=18"</span>)</span><br><span class="line">words_df = article_data.rdd.mapPartitions(segmentation).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'words'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segmentation</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">import</span> jieba</span><br><span class="line">    <span class="keyword">import</span> jieba.analyse</span><br><span class="line">    <span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line">    <span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">    abspath = <span class="string">"/root/words"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结巴加载用户词典</span></span><br><span class="line">    userDict_path = os.path.join(abspath, <span class="string">"ITKeywords.txt"</span>)</span><br><span class="line">    jieba.load_userdict(userDict_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 停用词文本</span></span><br><span class="line">    stopwords_path = os.path.join(abspath, <span class="string">"stopwords.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stopwords_list</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">"""返回stopwords列表"""</span></span><br><span class="line">        stopwords_list = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> codecs.open(stopwords_path).readlines()]</span><br><span class="line">        <span class="keyword">return</span> stopwords_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有的停用词列表</span></span><br><span class="line">    stopwords_list = get_stopwords_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut_sentence</span><span class="params">(sentence)</span>:</span></span><br><span class="line">        <span class="string">"""对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词"""</span></span><br><span class="line">        <span class="comment"># eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]</span></span><br><span class="line">        seg_list = pseg.lcut(sentence)</span><br><span class="line">        seg_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> seg_list <span class="keyword">if</span> i.flag <span class="keyword">not</span> <span class="keyword">in</span> stopwords_list]</span><br><span class="line">        filtered_words_list = []</span><br><span class="line">        <span class="keyword">for</span> seg <span class="keyword">in</span> seg_list:</span><br><span class="line">            <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> seg.flag == <span class="string">"eng"</span>:</span><br><span class="line">                <span class="keyword">if</span> len(seg.word) &lt;= <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag.startswith(<span class="string">"n"</span>):</span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">            <span class="keyword">elif</span> seg.flag <span class="keyword">in</span> [<span class="string">"x"</span>, <span class="string">"eng"</span>]:  <span class="comment"># 是自定一个词语或者是英文单词</span></span><br><span class="line">                filtered_words_list.append(seg.word)</span><br><span class="line">        <span class="keyword">return</span> filtered_words_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">        sentence = re.sub(<span class="string">"&lt;.*?&gt;"</span>, <span class="string">""</span>, row.sentence)    <span class="comment"># 替换掉标签数据</span></span><br><span class="line">        words = cut_sentence(sentence)</span><br><span class="line">        <span class="keyword">yield</span> row.article_id, row.channel_id, words</span><br></pre></td></tr></table></figure></p><p><code>words_df</code> 结果如下所示，words 为分词后的词语列表</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-4cbedc535c6b965c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接着，使用分词后的所有词语，对 Word2Vec 模型进行训练并将模型保存到 HDFS，其中 vectorSize 为词向量的长度，minCount 为词语的最小出现次数，windowSize 为训练窗口的大小，inputCol 为输入的列名，outputCol 为输出的列名<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import Word2Vec</span><br><span class="line"></span><br><span class="line">w2v_model = Word2Vec(<span class="attribute">vectorSize</span>=100, <span class="attribute">inputCol</span>=<span class="string">'words'</span>, <span class="attribute">outputCol</span>=<span class="string">'vector'</span>, <span class="attribute">minCount</span>=3)</span><br><span class="line">model = w2v_model.fit(words_df)</span><br><span class="line">model.save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_18_python.word2vec"</span>)</span><br></pre></td></tr></table></figure></p><p>加载训练好的 Word2Vec 模型<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.ml.feature import Word2VecModel</span><br><span class="line"></span><br><span class="line">w2v_model = Word2VecModel.load(<span class="string">"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_18_python.word2vec"</span>)</span><br><span class="line">vectors = w2v_model.getVectors()</span><br></pre></td></tr></table></figure></p><p><code>vectors</code> 结果如下所示，其中 vector 是训练后的每个词的 100 维词向量，是 vector 类型格式的，如 [0.2 -0.05 -0.1 …]</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ee792e4abfe00e75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这里，我们计算出了所有词语的词向量，接下来，还要得到关键词的词向量，因为我们需要通过关键词的词向量来计算文章的词向量。那么，首先通过读取频道内的文章画像来得到关键词（实际场景应该只读取新增文章画像）<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">article_profile</span> = spark.sql(<span class="string">"select * from article_profile where channel_id=18"</span>)</span><br></pre></td></tr></table></figure></p><p>在文章画像表中，关键词和权重是存储在同一列的，我们可以利用 <code>LATERAL VIEW explode()</code> 方法，将 map 类型的 keywords 列中的关键词和权重转换成单独的两列数据<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">article_profile.registerTempTable('profile')</span><br><span class="line">keyword_weight = spark.sql("<span class="keyword">select</span> article_id, channel_id, keyword, weight <span class="keyword">from</span> profile LATERAL <span class="keyword">VIEW</span> explode(keywords) <span class="keyword">AS</span> keyword, weight<span class="string">")</span></span><br></pre></td></tr></table></figure></p><p><code>keyword_weight</code> 结果如下所示，keyword 为关键词，weight 为对应的权重</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-39f3605d616afc31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这时就可以利用关键词 keyword 列，将文章关键词 <code>keyword_weight</code> 与词向量结果 <code>vectors</code> 进行内连接，从而得到每个关键词的词向量<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">keywords_vector</span> = keyword_weight.join(vectors, vectors.word==keyword_weight.keyword, <span class="string">'inner'</span>)</span><br></pre></td></tr></table></figure></p><p><code>keywords_vector</code> 结果如下所示，vector 即对应关键词的 100 维词向量</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-8cc8e64b8f642fec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，将文章每个关键词的词向量加入权重信息，这里使每个关键词的词向量 = 关键词的权重 x 关键词的词向量，即 weight_vector = weight x vector，注意这里的 <code>vector</code> 为 vector 类型，所以 weight x vector 是权重和向量的每个元素相乘，向量的长度保持不变<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def compute_vector(<span class="built_in">row</span>):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">row</span>.article_id, <span class="built_in">row</span>.channel_id, <span class="built_in">row</span>.keyword, <span class="built_in">row</span>.weight * <span class="built_in">row</span>.<span class="built_in">vector</span></span><br><span class="line"></span><br><span class="line">article_keyword_vectors = keywords_vector.rdd.<span class="built_in">map</span>(compute_vector).toDF([<span class="string">"article_id"</span>, <span class="string">"channel_id"</span>, <span class="string">"keyword"</span>, <span class="string">"weightingVector"</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_keyword_vectors</code> 结果如下所示，weightingVector 即为加入权重信息后的关键词的词向量</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-cb450dceb634a754.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>再将上面的结果按照 article_id 进行分组，利用 <code>collect_set()</code> 方法，将一篇文章内所有关键词的词向量合并为一个列表<br><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">article_keyword_vectors.registerTempTable('temptable')</span><br><span class="line">article_keyword_vectors = spark.sql("<span class="keyword">select</span> article_id, <span class="built_in">min</span>(channel_id) channel_id, collect_set(weightingVector) vectors <span class="keyword">from</span> temptable <span class="keyword">group</span> <span class="keyword">by</span> article_id<span class="string">")</span></span><br></pre></td></tr></table></figure></p><p><code>article_keyword_vectors</code> 结果如下所示，vectors 即为文章内所有关键词向量的列表，如 [[0.6 0.2 …], [0.1 -0.07 …], …]</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-9e68516da76d0189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>接下来，利用上面得出的二维列表，计算每篇文章内所有关键词的词向量的平均值，作为文章的词向量。注意，这里的 <code>vectors</code> 是包含多个词向量的列表，词向量列表的平均值等于其中每个词向量的对应元素相加再除以词向量的个数<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_avg_vectors</span><span class="params">(row)</span></span><span class="symbol">:</span></span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> row.<span class="symbol">vectors:</span></span><br><span class="line">        x += i</span><br><span class="line">    <span class="comment"># 求平均值</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, row.channel_id, x / len(row.vectors)</span><br><span class="line"></span><br><span class="line">article_vector = article_keyword_vectors.rdd.map(compute_avg_vectors).toDF([<span class="string">'article_id'</span>, <span class="string">'channel_id'</span>, <span class="string">'vector'</span>])</span><br></pre></td></tr></table></figure></p><p><code>article_vector</code> 结果如下所示</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-a86e19255d39bf47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>此时，<code>article_vector</code> 中的 <code>vector</code> 列还是 vector 类型，而 Hive 不支持该数据类型，所以需要将 vector 类型转成 array 类型（list）<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def to_list(<span class="built_in">row</span>):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">row</span>.article_id, <span class="built_in">row</span>.channel_id, [<span class="built_in">float</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">row</span>.<span class="built_in">vector</span>.toArray()]</span><br><span class="line"></span><br><span class="line">article_vector = article_vector.rdd.<span class="built_in">map</span>(to_list).toDF(['article_id', 'channel_id', '<span class="built_in">vector</span>'])</span><br></pre></td></tr></table></figure></p><p>在 Hive 中创建文章词向量表 article_vector<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> article_vector</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    channel_id <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    articlevector <span class="built_in">ARRAY</span> <span class="keyword">comment</span> <span class="string">"keyword"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p>最后，将 18 号频道内的所有文章的词向量存储到 Hive 的文章词向量表 article_vector 中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">article_vector.write.insertInto(<span class="string">"article_vector"</span>)</span><br></pre></td></tr></table></figure></p><p>这样，我们就计算出了 18 号频道下每篇文章的词向量，在实际场景中，我们还要分别计算出其他所有频道下每篇文章的词向量。</p><h1 id="计算文章相似度"><a href="#计算文章相似度" class="headerlink" title="计算文章相似度"></a>计算文章相似度</h1><p>前面我们计算出了文章的词向量，接下来就可以根据文章的词向量来计算文章的相似度了。通常我们会有几百万、几千万甚至上亿规模的文章数据，为了优化计算性能，我们可以只计算每个频道内文章之间的相似度，因为通常只有相同频道的文章关联性较高，而不同频道之间的文章通常关联性较低。在每个频道内，我们还可以用聚类或局部敏感哈希对文章进行分桶，将文章相似度的计算限制在更小的范围，只计算相同分类内或相同桶内的文章相似度。</p><ul><li>聚类（Clustering），对每个频道内的文章进行聚类，可以使用 KMeans 算法，需要提前设定好类别个数 K，聚类算法的时间复杂度并不小，也可以使用一些优化的聚类算法，比如二分聚类、层次聚类等。但通常聚类算法也比较耗时，所以通常被使用更多的是局部敏感哈希。</li></ul><p>Spark 的 BisectingKMeans 模型训练代码示例<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.clustering import BisectingKMeans</span><br><span class="line"></span><br><span class="line">bkmeans = BisectingKMeans(<span class="attribute">k</span>=100, <span class="attribute">minDivisibleClusterSize</span>=50, <span class="attribute">featuresCol</span>=<span class="string">"articlevector"</span>, <span class="attribute">predictionCol</span>=<span class="string">'group'</span>)</span><br><span class="line">bkmeans_model = bkmeans.fit(article_vector)</span><br><span class="line">bkmeans_model.save(<span class="string">"hdfs://hadoop-master:9000/headlines/models/articleBisKmeans/channel_%d_%s.bkmeans"</span> % (channel_id, channel))</span><br></pre></td></tr></table></figure></p><ul><li>局部敏感哈希 LSH（Locality Sensitive Hashing），LSH 算法是基于一个假设，如果两个文本在原有的数据空间是相似的，那么经过哈希函数转换以后，它们仍然具有很高的相似度，即越相似的文本在哈希之后，落到相同的桶内的概率就越高。所以，我们只需要将目标文章进行哈希映射并得到其桶号，然后取出该桶内的所有文章，再进行线性匹配即可查找到与目标文章相邻的文章。其实 LSH 并不能保证一定能够查找到与目标文章最相邻的文章，而是在减少需要匹配的文章个数的同时，保证查找到最近邻的文章的概率很大。</li></ul><p>下面我们将使用 LSH 模型来计算文章相似度，首先，读取 18 号频道内所有文章的 ID 和词向量作为训练集<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">article_vector</span> = spark.sql(<span class="string">"select article_id, articlevector from article_vector where channel_id=18"</span>)</span><br><span class="line"><span class="attr">train</span> = articlevector.select([<span class="string">'article_id'</span>, <span class="string">'articlevector'</span>])</span><br></pre></td></tr></table></figure></p><p>文章词向量表中的词向量是被存储为 array 类型的，我们利用 Spark 的 <code>Vectors.dense()</code> 方法，将 array 类型（list）转为 vector 类型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_to_vector</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> row.article_id, Vectors.dense(row.articlevector)</span><br><span class="line"></span><br><span class="line">train = train.rdd.map(list_to_vector).toDF([<span class="string">'article_id'</span>, <span class="string">'articlevector'</span>])</span><br></pre></td></tr></table></figure></p><p>使用训练集 <code>train</code> 对 Spark 的 <code>BucketedRandomProjectionLSH</code> 模型进行训练，其中 inputCol 为输入特征列，outputCol 为输出特征列，numHashTables 为哈希表数量，bucketLength 为桶的数量，数量越多，相同数据进入到同一个桶的概率就越高<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature import BucketedRandomProjectionLSH</span><br><span class="line"></span><br><span class="line">brp = BucketedRandomProjectionLSH(<span class="attribute">inputCol</span>=<span class="string">'articlevector'</span>, <span class="attribute">outputCol</span>=<span class="string">'hashes'</span>, <span class="attribute">numHashTables</span>=4.0, <span class="attribute">bucketLength</span>=10.0)</span><br><span class="line">model = brp.fit(train)</span><br></pre></td></tr></table></figure></p><p>训练好模型后，调用 <code>approxSimilarityJoin()</code> 方法即可计算数据之间的相似度，如 <code>model.approxSimilarityJoin(df1, df2, 2.0, distCol=&#39;EuclideanDistance&#39;)</code> 就是利用欧几里得距离作为相似度，计算在 df1 与 df2 每条数据的相似度，这里我们计算训练集中所有文章之间的相似度<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">similar</span> = model.approxSimilarityJoin(train, train, <span class="number">2.0</span>, distCol=<span class="string">'EuclideanDistance'</span>)</span><br></pre></td></tr></table></figure></p><p><code>similar</code> 结果如下所示，EuclideanDistance 就是两篇文章的欧几里得距离，即相似度</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-8e98744a01f7d121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>在后面的推荐流程中，会经常查询文章相似度，所以出于性能考虑，我们选择将文章相似度结果存储到  Hbase 中。首先创建文章相似度表<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">create</span> <span class="string">'article_similar'</span>, <span class="string">'similar'</span></span><br></pre></td></tr></table></figure></p><p>然后存储文章相似度结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_hbase</span><span class="params">(partition)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> happybase</span><br><span class="line">    pool = happybase.ConnectionPool(size=<span class="number">3</span>, host=<span class="string">'hadoop-master'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> pool.connection() <span class="keyword">as</span> conn:</span><br><span class="line">        <span class="comment"># 建立表连接</span></span><br><span class="line">        table = conn.table(<span class="string">'article_similar'</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> partition:</span><br><span class="line">            <span class="keyword">if</span> row.datasetA.article_id != row.datasetB.article_id:</span><br><span class="line">                table.put(str(row.datasetA.article_id).encode(), &#123;<span class="string">"similar:&#123;&#125;"</span>.format(row.datasetB.article_id).encode(): <span class="string">b'%0.4f'</span> % (row.EuclideanDistance)&#125;)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># 手动关闭所有的连接</span></span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line">similar.foreachPartition(save_hbase)</span><br></pre></td></tr></table></figure></p><h1 id="Apscheduler-定时更新"><a href="#Apscheduler-定时更新" class="headerlink" title="Apscheduler 定时更新"></a>Apscheduler 定时更新</h1><p>将文章相似度计算加入到文章画像更新方法中，首先合并最近一个小时的文章完整信息，接着计算 TF-IDF 和 TextRank 权重，并根据 TF-IDF 和 TextRank 权重计算得出关键词和主题词，最后计算文章的词向量及文章的相似度<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_article_profile</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定时更新文章画像及文章相似度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ua = UpdateArticle()</span><br><span class="line">    sentence_df = ua.merge_article_data()</span><br><span class="line">    <span class="keyword">if</span> sentence_df.rdd.collect():</span><br><span class="line">        textrank_keywords_df, keywordsIndex = ua.generate_article_label()</span><br><span class="line">        article_profile = ua.get_article_profile(textrank_keywords_df, keywordsIndex)</span><br><span class="line">        ua.compute_article_similar(article_profile)</span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上篇文章中，我们已经完成了离线文章画像的构建，接下来，我们要为相似文章推荐做准备，那就是计算文章之间的相似度。首先，我们要计算出文章的词向量，然后利用文章的词向量来计算文章的相似度。&lt;/p&gt;
&lt;h1 id=&quot;计算文章词向量&quot;&gt;&lt;a href=&quot;#计算文章词向量&quot; clas
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之3、收集用户行为数据</title>
    <link href="http://thinkgamer.cn/2019/12/06/RecSys/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B3%E3%80%81%E6%94%B6%E9%9B%86%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE/"/>
    <id>http://thinkgamer.cn/2019/12/06/RecSys/文章推荐系统/《文章推荐系统》系列之3、收集用户行为数据/</id>
    <published>2019-12-06T04:12:12.000Z</published>
    <updated>2019-12-06T04:59:41.646Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中，我们完成了业务数据的同步，在推荐系统中另一个必不可少的数据就是用户行为数据，可以说用户行为数据是推荐系统的基石，巧妇难为无米之炊，所以接下来，我们就要将用户的行为数据同步到推荐系统数据库中。</p><p>在文章推荐系统中，用户行为包括曝光、点击、停留、收藏、分享等，所以这里我们定义的用户行为数据的字段包括：发生时间（actionTime）、停留时间（readTime）、频道 ID（channelId）、事件名称（action）、用户 ID（userId）、文章 ID（articleId）以及算法 ID（algorithmCombine），这里采用 json 格式，如下所示<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 曝光的参数</span></span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:35"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">0</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"exposure"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"[18577, 14299]"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># 对文章触发行为的参数</span></span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:36"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"18577"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:38"</span>,<span class="string">"readTime"</span>:<span class="string">"1621"</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"read"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"18577"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:39"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: <span class="string">"1"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:39"</span>,<span class="string">"readTime"</span>:<span class="string">""</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"click"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:41"</span>,<span class="string">"readTime"</span>:<span class="string">"914"</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"read"</span>, <span class="string">"userId"</span>: <span class="string">"2"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"actionTime"</span>:<span class="string">"2019-04-10 18:15:47"</span>,<span class="string">"readTime"</span>:<span class="string">"7256"</span>,<span class="string">"channelId"</span>:<span class="number">18</span>,<span class="string">"param"</span>:&#123;<span class="string">"action"</span>: <span class="string">"read"</span>, <span class="string">"userId"</span>: <span class="string">"1"</span>, <span class="string">"articleId"</span>: <span class="string">"14299"</span>, <span class="string">"algorithmCombine"</span>: <span class="string">"C2"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p><h1 id="用户离线行为数据"><a href="#用户离线行为数据" class="headerlink" title="用户离线行为数据"></a>用户离线行为数据</h1><p>由于用户行为数据规模庞大，通常是每天更新一次，以供离线计算使用。首先，在 Hive 中创建用户行为数据库 profile 及用户行为表 user_action，设置按照日期进行分区，并匹配 json 格式<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建用户行为数据库</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> profile <span class="keyword">comment</span> <span class="string">"use action"</span> location <span class="string">'/user/hive/warehouse/profile.db/'</span>;</span><br><span class="line"><span class="comment">-- 创建用户行为信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_action</span><br><span class="line">(</span><br><span class="line">    actionTime <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"user actions time"</span>,</span><br><span class="line">    readTime   <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"user reading time"</span>,</span><br><span class="line">    channelId  <span class="built_in">INT</span> <span class="keyword">comment</span> <span class="string">"article channel id"</span>,</span><br><span class="line">    param <span class="keyword">map</span> <span class="keyword">comment</span> <span class="string">"action parameter"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"user primitive action"</span></span><br><span class="line">    PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">STRING</span>) # 按照日期分区</span><br><span class="line">    <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hive.hcatalog.data.JsonSerDe'</span> # 匹配<span class="keyword">json</span>格式</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/profile.db/user_action'</span>;</span><br></pre></td></tr></table></figure></p><p>通常用户行为数据被保存在应用服务器的日志文件中，我们可以利用 Flume 监听应用服务器上的日志文件，将用户行为数据收集到 Hive 的 user_action 表对应的 HDFS 目录中，Flume 配置如下<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.s1.channels= c1</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /root/logs/userClick.log</span><br><span class="line">a1.sources.s1.<span class="attribute">interceptors</span>=i1 i2</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">type</span>=regex_filter</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">regex</span>=\\&#123;.*\\&#125;</span><br><span class="line">a1.sources.s1.interceptors.i2.<span class="attribute">type</span>=timestamp</span><br><span class="line"></span><br><span class="line"><span class="comment"># c1</span></span><br><span class="line">a1.channels.c1.<span class="attribute">type</span>=memory</span><br><span class="line">a1.channels.c1.<span class="attribute">capacity</span>=30000</span><br><span class="line">a1.channels.c1.<span class="attribute">transactionCapacity</span>=1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># k1</span></span><br><span class="line">a1.sinks.k1.<span class="attribute">type</span>=hdfs</span><br><span class="line">a1.sinks.k1.<span class="attribute">channel</span>=c1</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">path</span>=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">fileType</span>=DataStream</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">writeFormat</span>=Text</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollInterval</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollSize</span>=10240</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollCount</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">idleTimeout</span>=60</span><br></pre></td></tr></table></figure></p><p>编写 Flume 启动脚本 collect_click.sh<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/root/bigdata/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/root/bigdata/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1</span><br></pre></td></tr></table></figure></p><p>Flume 自动生成目录后，需要手动关联 Hive 分区后才能加载到数据<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> user_action <span class="keyword">add</span> <span class="keyword">partition</span> (dt=<span class="string">'2019-11-11'</span>) location <span class="string">"/user/hive/warehouse/profile.db/user_action/2011-11-11/"</span></span><br></pre></td></tr></table></figure></p><h1 id="用户实时行为数据"><a href="#用户实时行为数据" class="headerlink" title="用户实时行为数据"></a>用户实时行为数据</h1><p>为了提高推荐的实时性，我们也需要收集用户的实时行为数据，以供在线计算使用。这里利用 Flume 将日志收集到 Kafka，在线计算任务可以从 Kafka 读取用户实时行为数据。首先，开启 zookeeper，以守护进程运行<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/zookeeper-server-start.sh -daemon /</span>root<span class="regexp">/bigdata/</span>kafka<span class="regexp">/config/</span>zookeeper.properties</span><br></pre></td></tr></table></figure></p><p>开启 Kafka<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启消息生产者</span></span><br><span class="line">/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.19092 --sync --topic click-trace</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启消费者</span></span><br><span class="line">/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace</span><br></pre></td></tr></table></figure></p><p>修改 Flume 的日志收集配置文件，添加 c2 和 k2 ，将日志数据收集到 Kafka<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">a1.sources.s1.channels= c1 c2</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /root/logs/userClick.log</span><br><span class="line">a1.sources.s1.<span class="attribute">interceptors</span>=i1 i2</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">type</span>=regex_filter</span><br><span class="line">a1.sources.s1.interceptors.i1.<span class="attribute">regex</span>=\\&#123;.*\\&#125;</span><br><span class="line">a1.sources.s1.interceptors.i2.<span class="attribute">type</span>=timestamp</span><br><span class="line"></span><br><span class="line"><span class="comment"># c1</span></span><br><span class="line">a1.channels.c1.<span class="attribute">type</span>=memory</span><br><span class="line">a1.channels.c1.<span class="attribute">capacity</span>=30000</span><br><span class="line">a1.channels.c1.<span class="attribute">transactionCapacity</span>=1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># c2</span></span><br><span class="line">a1.channels.c2.<span class="attribute">type</span>=memory</span><br><span class="line">a1.channels.c2.<span class="attribute">capacity</span>=30000</span><br><span class="line">a1.channels.c2.<span class="attribute">transactionCapacity</span>=1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># k1</span></span><br><span class="line">a1.sinks.k1.<span class="attribute">type</span>=hdfs</span><br><span class="line">a1.sinks.k1.<span class="attribute">channel</span>=c1</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">path</span>=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">fileType</span>=DataStream</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">writeFormat</span>=Text</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollInterval</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollSize</span>=10240</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">rollCount</span>=0</span><br><span class="line">a1.sinks.k1.hdfs.<span class="attribute">idleTimeout</span>=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># k2</span></span><br><span class="line">a1.sinks.k2.<span class="attribute">channel</span>=c2</span><br><span class="line">a1.sinks.k2.<span class="attribute">type</span>=org.apache.flume.supervisorctl</span><br><span class="line">我们可以利用supervisorctl来管理supervisor。sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k2.kafka.bootstrap.<span class="attribute">servers</span>=192.168.19.137:9092</span><br><span class="line">a1.sinks.k2.kafka.<span class="attribute">topic</span>=click-trace</span><br><span class="line">a1.sinks.k2.kafka.<span class="attribute">batchSize</span>=20</span><br><span class="line">a1.sinks.k2.kafka.producer.<span class="attribute">requiredAcks</span>=1</span><br></pre></td></tr></table></figure></p><p>编写 Kafka 启动脚本 start_kafka.sh<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># 启动zookeeper</span></span><br><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/zookeeper-server-start.sh -daemon /</span>root<span class="regexp">/bigdata/</span>kafka<span class="regexp">/config/</span>zookeeper.properties</span><br><span class="line"><span class="comment"># 启动kafka</span></span><br><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/kafka-server-start.sh /</span>root<span class="regexp">/bigdata/</span>kafka<span class="regexp">/config/</span>server.properties</span><br><span class="line"><span class="comment"># 增加topic</span></span><br><span class="line"><span class="regexp">/root/</span>bigdata<span class="regexp">/kafka/</span>bin<span class="regexp">/kafka-topics.sh --zookeeper 192.168.19.137:2181 --create --replication-factor 1 --topic click-trace --partitions 1</span></span><br></pre></td></tr></table></figure></p><h1 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h1><p>我们这里使用 Supervisor 进行进程管理，当进程异常时可以自动重启，Flume 进程配置如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[program:collect-click]</span><br><span class="line">command=/bin/bash /root/toutiao_project/scripts/collect_click.sh</span><br><span class="line">user=root</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stdout_logfile=/root/logs/collect.log</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br></pre></td></tr></table></figure></p><p>Kafka 进程配置如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[program:kafka]</span><br><span class="line">command=/bin/bash /root/toutiao_project/scripts/start_kafka.sh</span><br><span class="line">user=root</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stdout_logfile=/root/logs/kafka.log</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br></pre></td></tr></table></figure></p><p>启动 Supervisor<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">supervisord -c <span class="regexp">/etc/</span>supervisord.conf</span><br></pre></td></tr></table></figure></p><p>启动 Kafka 消费者，并在应用服务器日志文件中写入日志数据，Kafka 消费者即可收集到实时行为数据<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动Kafka消费者</span></span><br><span class="line">/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 写入日志数据</span></span><br><span class="line">echo &#123;\"actionTime\":\"2019-04-10 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":&#123;\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"&#125;&#125; &gt;&gt; userClick.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 消费者接收到日志数据</span></span><br><span class="line">&#123;"actionTime":"2019-04-10 21:04:39","readTime":"","channelId":18,"param":&#123;"action": "click", "userId": "2", "articleId": "14299", "algorithmCombine": "C2"&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>Supervisor 常用命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">supervisorctl</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> status              <span class="comment"># 查看程序状态</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> start apscheduler   <span class="comment"># 启动apscheduler单一程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> stop toutiao:*      <span class="comment"># 关闭toutiao组程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> start toutiao:*     <span class="comment"># 启动toutiao组程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> restart toutiao:*   <span class="comment"># 重启toutiao组程序</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> update              <span class="comment"># 重启配置文件修改过的程序</span></span></span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一篇文章中，我们完成了业务数据的同步，在推荐系统中另一个必不可少的数据就是用户行为数据，可以说用户行为数据是推荐系统的基石，巧妇难为无米之炊，所以接下来，我们就要将用户的行为数据同步到推荐系统数据库中。&lt;/p&gt;
&lt;p&gt;在文章推荐系统中，用户行为包括曝光、点击、停留、收藏
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之2、同步业务数据.md</title>
    <link href="http://thinkgamer.cn/2019/12/04/RecSys/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B2%E3%80%81%E5%90%8C%E6%AD%A5%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE/"/>
    <id>http://thinkgamer.cn/2019/12/04/RecSys/文章推荐系统/《文章推荐系统》系列之2、同步业务数据/</id>
    <published>2019-12-04T13:21:21.000Z</published>
    <updated>2019-12-06T04:59:41.645Z</updated>
    
    <content type="html"><![CDATA[<p>在推荐系统架构中，推荐系统的数据库和业务系统的数据库是分离的，这样才能有效避免推荐系统的数据读写、计算等对业务系统的影响，所以同步业务数据往往也是推荐系统要做的第一件事情。通常业务数据是存储在关系型数据库中，比如 MySQL，而推荐系统通常需要使用大数据平台，比如 Hadoop，我们可以利用 Sqoop 将 MySQL 中的业务数据同步到 Hive 中，在我们的文章推荐系统中，需要同步的数据包括：</p><ul><li>用户信息表（user_profile）：包括 user_id | gender | birthday   | real_name | create_time | update_time   | register_media_time | id_number | id_card_front | id_card_back | id_card_handheld | area | company | career 等字段。</li><li>用户基础信息表（user_basic）：包括 user_id | mobile      | password | user_name       | profile_photo                | last_login          | is_media | article_count | following_count | fans_count | like_count | read_count | introduction    | certificate | is_verified | account | email       | status 等字段。</li><li>文章信息表（news_article_basic）：包括 article_id | user_id | channel_id | title                         | cover                   | is_advertising | create_time         | status | reviewer_id | review_time | delete_time | comment_count | allow_comment | update_time         | reject_reason 等字段。</li><li>文章内容表（news_article_content）：包括 article_id | content | update_time 等字段。</li><li>频道信息表（news_channel）：包括 channel_id | channel_name | create_time         | update_time         | sequence | is_visible | is_default 等字段。</li></ul><p>首先，在 Hive 中创建业务数据库 toutiao，并创建相关表<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> toutiao <span class="keyword">comment</span> <span class="string">"user,news information of mysql"</span> location <span class="string">'/user/hive/warehouse/toutiao.db/'</span>;</span><br><span class="line"><span class="comment">-- 用户信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_profile</span><br><span class="line">(</span><br><span class="line">    user_id             <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"userid"</span>,</span><br><span class="line">    gender              <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"gender"</span>,</span><br><span class="line">    birthday            <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"birthday"</span>,</span><br><span class="line">    real_name           <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"real_name"</span>,</span><br><span class="line">    create_time         <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"create_time"</span>,</span><br><span class="line">    update_time         <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span>,</span><br><span class="line">    register_media_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"register_media_time"</span>,</span><br><span class="line">    id_number           <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_number"</span>,</span><br><span class="line">    id_card_front       <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_card_front"</span>,</span><br><span class="line">    id_card_back        <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_card_back"</span>,</span><br><span class="line">    id_card_handheld    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"id_card_handheld"</span>,</span><br><span class="line">    area                <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"area"</span>,</span><br><span class="line">    company             <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"company"</span>,</span><br><span class="line">    career              <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"career"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao user profile"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/user_profile'</span>;</span><br><span class="line"><span class="comment">-- 用户基础信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_basic</span><br><span class="line">(</span><br><span class="line">    user_id         <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"user_id"</span>,</span><br><span class="line">    mobile          <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"mobile"</span>,</span><br><span class="line">    <span class="keyword">password</span>        <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"password"</span>,</span><br><span class="line">    profile_photo   <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"profile_photo"</span>,</span><br><span class="line">    last_login      <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"last_login"</span>,</span><br><span class="line">    is_media        <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_media"</span>,</span><br><span class="line">    article_count   <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_count"</span>,</span><br><span class="line">    following_count <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"following_count"</span>,</span><br><span class="line">    fans_count      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"fans_count"</span>,</span><br><span class="line">    like_count      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"like_count"</span>,</span><br><span class="line">    read_count      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"read_count"</span>,</span><br><span class="line">    introduction    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"introduction"</span>,</span><br><span class="line">    certificate     <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"certificate"</span>,</span><br><span class="line">    is_verified     <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_verified"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao user basic"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/user_basic'</span>;</span><br><span class="line"><span class="comment">-- 文章基础信息表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> news_article_basic</span><br><span class="line">(</span><br><span class="line">    article_id  <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    user_id     <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"user_id"</span>,</span><br><span class="line">    channel_id  <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    title       <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"title"</span>,</span><br><span class="line">    <span class="keyword">status</span>      <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"status"</span>,</span><br><span class="line">    update_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_article_basic"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/news_article_basic'</span>;</span><br><span class="line"><span class="comment">-- 文章频道表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> news_channel</span><br><span class="line">(</span><br><span class="line">    channel_id   <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"channel_id"</span>,</span><br><span class="line">    channel_name <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"channel_name"</span>,</span><br><span class="line">    create_time  <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"create_time"</span>,</span><br><span class="line">    update_time  <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span>,</span><br><span class="line">    <span class="keyword">sequence</span>     <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"sequence"</span>,</span><br><span class="line">    is_visible   <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_visible"</span>,</span><br><span class="line">    is_default   <span class="built_in">BOOLEAN</span> <span class="keyword">comment</span> <span class="string">"is_default"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_channel"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/news_channel'</span>;</span><br><span class="line"><span class="comment">-- 文章内容表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> news_article_content</span><br><span class="line">(</span><br><span class="line">    article_id <span class="built_in">BIGINT</span> <span class="keyword">comment</span> <span class="string">"article_id"</span>,</span><br><span class="line">    <span class="keyword">content</span>    <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"content"</span>,</span><br><span class="line">    update_time <span class="keyword">STRING</span> <span class="keyword">comment</span> <span class="string">"update_time"</span></span><br><span class="line">)</span><br><span class="line">    <span class="keyword">COMMENT</span> <span class="string">"toutiao news_article_content"</span></span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> # 指定分隔符</span><br><span class="line">    LOCATION <span class="string">'/user/hive/warehouse/toutiao.db/news_article_content'</span>;</span><br></pre></td></tr></table></figure></p><p>查看 Sqoop 连接到 MySQL 的数据库列表<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P</span><br><span class="line"></span><br><span class="line">mysql</span><br><span class="line">sys</span><br><span class="line">toutiao</span><br></pre></td></tr></table></figure></p><p>Sqoop 可以指定全量导入和增量导入，通常我们可以先执行一次全量导入，将历史数据全部导入进来，后面再通过定时任务执行增量导入，来保持 MySQL 和 Hive 的数据同步，全量导入不需要提前创建 Hive 表，可以自动创建<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">array=(user_profile user_basic news_article_basic news_channel news_article_content)</span><br><span class="line"></span><br><span class="line">for table_name in $&#123;array[@]&#125;;</span><br><span class="line">do</span><br><span class="line">    sqoop import \</span><br><span class="line">        --connect jdbc:mysql://192.168.19.137/toutiao \</span><br><span class="line">        --username root \</span><br><span class="line">        --password password \</span><br><span class="line">        --table $table_name \</span><br><span class="line">        --m 5 \ # 线程数</span><br><span class="line">        --hive-home /root/bigdata/hive \ # hive路径</span><br><span class="line">        --hive-import \ # 导入形式</span><br><span class="line">        --create-hive-table  \ # 自动创建表</span><br><span class="line">        --hive-drop-import-delims \</span><br><span class="line">        --warehouse-dir /user/hive/warehouse/toutiao.db \ # 导入地址</span><br><span class="line">        --hive-table toutiao.$table_name </span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>增量导入，有 append 和 lastmodified 两种模式</p><ul><li><p>append：通过指定一个递增的列进行更新，只能追加，不能修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">num=0</span><br><span class="line">declare -A check</span><br><span class="line">check=([user_profile]=user_id [user_basic]=user_id [news_article_basic]=article_id [news_channel]=channel_id [news_article_content]=channel_id)</span><br><span class="line"></span><br><span class="line">for k in $&#123;!check[@]&#125;</span><br><span class="line">do</span><br><span class="line">    sqoop import \</span><br><span class="line">        --connect jdbc:mysql://192.168.19.137/toutiao \</span><br><span class="line">        --username root \</span><br><span class="line">        --password password \</span><br><span class="line">        --table $k \</span><br><span class="line">        --m 4 \</span><br><span class="line">        --hive-home /root/bigdata/hive \ # hive路径</span><br><span class="line">        --hive-import \ # 导入到hive</span><br><span class="line">        --create-hive-table  \ # 自动创建表</span><br><span class="line">        --incremental append \ # 按照id更新</span><br><span class="line">        --check-column $&#123;check[$k]&#125; \ # 指定id列</span><br><span class="line">        --last-value $&#123;num&#125; # 指定最后更新的id</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>lastmodified：按照最后修改时间更新，支持追加和修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">time=`date +"%Y-%m-%d" -d "-1day"`</span><br><span class="line">declare -A check</span><br><span class="line">check=([user_profile]=update_time [user_basic]=last_login [news_article_basic]=update_time [news_channel]=update_time)</span><br><span class="line">declare -A merge</span><br><span class="line">merge=([user_profile]=user_id [user_basic]=user_id [news_article_basic]=article_id [news_channel]=channel_id)</span><br><span class="line"></span><br><span class="line">for k in $&#123;!check[@]&#125;</span><br><span class="line">do</span><br><span class="line">    sqoop import \</span><br><span class="line">        --connect jdbc:mysql://192.168.19.137/toutiao \</span><br><span class="line">        --username root \</span><br><span class="line">        --password password \</span><br><span class="line">        --table $k \</span><br><span class="line">        --m 4 \</span><br><span class="line">        --target-dir /user/hive/warehouse/toutiao.db/$k \ # hive路径</span><br><span class="line">        --incremental lastmodified \ # 按照最后修改时间更新</span><br><span class="line">        --check-column $&#123;check[$k]&#125; \ # 指定时间列</span><br><span class="line">        --merge-key $&#123;merge[$k]&#125; \ # 根据指定列合并重复或修改数据</span><br><span class="line">        --last-value $&#123;time&#125; # 指定最后修改时间</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li></ul><p>Sqoop 可以直接导入到 Hive，自动创建 Hive 表，但是 lastmodified 模式不支持<br>Sqoop 也可以先导入到 HDFS，然后再建立 Hive 表关联，为了使用 lastmodified 模式，通常使用这种方法<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--target-dir <span class="meta-keyword">/user/</span>hive<span class="meta-keyword">/warehouse/</span>toutiao.db/user_profile <span class="meta"># 指定导入的HDFS目录</span></span><br></pre></td></tr></table></figure></p><p>若先导入到 HDFS，需要注意 HDFS 默认分隔符为 <code>,</code> 而 Hive 默认分隔符为 <code>/u0001</code>，所以需要在创建 Hive 表时指定分隔符为 HDFS 分隔符 <code>,</code>，若不指定分隔符，查询结果将全部为 NULL<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">row <span class="built_in">format</span> delimited fields terminated <span class="keyword">by</span> <span class="string">','</span> <span class="comment"># 指定分隔符</span></span><br></pre></td></tr></table></figure></p><p>如果 MySQL 数据中存在特殊字符，如 <code>, \t \n</code> 都会导致 Hive 读取失败（但不影响导入到 HDFS 中），可以利用 <code>--query</code> 参数，在读取时使用 <code>REPLACE() CHAR() CHR()</code> 进行字符替换。如果特殊字符过多，比如 news_article_content 表，可以选择全量导入<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--<span class="keyword">query</span> 'select article_id, user_id, channel_id, <span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(title, <span class="built_in">CHAR</span>(13),<span class="string">""</span>),<span class="built_in">CHAR</span>(10),<span class="string">""</span>), <span class="string">","</span>, <span class="string">" "</span>) title, status, update_time from news_article_basic WHERE <span class="variable">$CONDITIONS</span>' \</span><br></pre></td></tr></table></figure></p><p>如果 MySQL 数据中存在 tinyint 类型,必须在 <code>--connet</code> 中加入 <code>?tinyInt1isBit=false</code>，防止 Hive 将 tinyint 类型默认转化为 boolean 类型<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--connect <span class="string">jdbc:</span><span class="string">mysql:</span><span class="comment">//192.168.19.137/toutiao?tinyInt1isBit=false</span></span><br></pre></td></tr></table></figure></p><p>MySQL 与 Hive 对应类型如下<br>MySQL|Hive<br>-|-<br>bigint|bigint<br>tinyint|boolean<br>int|int<br>double|double<br>bit|boolean<br>varchar|string<br>decimal|double<br>date / timestamp|string<br>我们可以利用 crontab 来创建定时任务，将更新命令写入脚本 import_incremental.sh，输入 <code>crontab -e</code> 打开编辑界面，输入如下内容，即可定时执行数据同步任务<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每隔半小时增量导入一次</span></span><br><span class="line">*<span class="regexp">/30 * * * * /</span>root<span class="regexp">/toutiao_project/</span>scripts<span class="regexp">/import_incremental.sh</span></span><br></pre></td></tr></table></figure></p><p>crontab 命令格式为 <code>* * * * * shell</code>，其中前五个 <code>*</code> 分别代表分钟 (0-59)、小时(0-59)、月内的某天(1-31)、月(1-12)、周内的某天(0-7，周日为 0 或 7)，<code>shell</code> 表示要执行的命令或脚本，使用方法如下<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 每隔5分钟运行一次backupscript脚本</span></span><br><span class="line"><span class="emphasis">*/5 *</span> <span class="bullet">* *</span> * /root/backupscript.sh</span><br><span class="line"><span class="section"># 每天的凌晨1点运行backupscript脚本</span></span><br><span class="line">0 1 <span class="bullet">* *</span> * /root/backupscript.sh</span><br><span class="line"><span class="section"># 每月的第一个凌晨3:15运行backupscript脚本</span></span><br><span class="line">15 3 1 <span class="bullet">* *</span> /root/backupscript.sh</span><br></pre></td></tr></table></figure></p><p>crontab 常用命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">crontab -e      # 修改 crontab 文件，如果文件不存在会自动创建。 </span><br><span class="line">crontab -l      # 显示 crontab 文件。 </span><br><span class="line">crontab -r      # 删除 crontab 文件。</span><br><span class="line">crontab -ir     # 删除 crontab 文件前提醒用户。</span><br><span class="line"></span><br><span class="line">service crond status      # 查看crontab服务状态</span><br><span class="line">service crond start       # 启动服务 </span><br><span class="line">service crond stop        # 关闭服务 </span><br><span class="line">service crond restart     # 重启服务 </span><br><span class="line">service crond reload      # 重新载入配置</span><br></pre></td></tr></table></figure></p><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在推荐系统架构中，推荐系统的数据库和业务系统的数据库是分离的，这样才能有效避免推荐系统的数据读写、计算等对业务系统的影响，所以同步业务数据往往也是推荐系统要做的第一件事情。通常业务数据是存储在关系型数据库中，比如 MySQL，而推荐系统通常需要使用大数据平台，比如 Hado
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Django3.0和Python3.7连接Mysql报：Error loading MySQLdb module. Did you install mysqlclient</title>
    <link href="http://thinkgamer.cn/2019/12/04/Python/Django3.0%E5%92%8CPython3.7%E8%BF%9E%E6%8E%A5Mysql%E6%8A%A5%EF%BC%9AError%20loading%20MySQLdb%20module.%20Did%20you%20install%20mysqlclient/"/>
    <id>http://thinkgamer.cn/2019/12/04/Python/Django3.0和Python3.7连接Mysql报：Error loading MySQLdb module. Did you install mysqlclient/</id>
    <published>2019-12-04T07:45:02.000Z</published>
    <updated>2019-12-06T04:59:41.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h1><ul><li>Python 3.7.3</li><li>Django 3.0<ul><li>安装：pip3 install -U Django</li><li>文档：<a href="https://docs.djangoproject.com/zh-hans/3.0/contents/" target="_blank" rel="external">https://docs.djangoproject.com/zh-hans/3.0/contents/</a></li></ul></li></ul><h1 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h1><p>创建项目</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">django-admin startproject mysite</span></span><br></pre></td></tr></table></figure><p>配置Mysql<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATABASES = &#123;</span><br><span class="line">    'default': &#123;</span><br><span class="line">        'ENGINE': 'django.db.backends.mysql',</span><br><span class="line">        'NAME': 'mysite',</span><br><span class="line">        'USER': 'root',</span><br><span class="line">        'PASSWORD': '<span class="number">123456</span>',</span><br><span class="line">        'HOST': '127.0.0.1',</span><br><span class="line">        'PORT': '<span class="number">3306</span>',</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>创建视图<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">python3</span> manage.<span class="keyword">py</span> startapp <span class="built_in">index</span></span><br></pre></td></tr></table></figure></p><p>报错<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[MeetUpRec] python3 manage.py startapp index                                                                                                           14:45:34  ☁  master ☂ ✭</span><br><span class="line">Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/backends/mysql/base.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">import</span> MySQLdb <span class="keyword">as</span> <span class="keyword">Database</span></span><br><span class="line">ModuleNotFoundError: <span class="keyword">No</span> <span class="keyword">module</span> named <span class="string">'MySQLdb'</span></span><br><span class="line"></span><br><span class="line">The above <span class="keyword">exception</span> was the direct cause <span class="keyword">of</span> the <span class="keyword">following</span> <span class="keyword">exception</span>:</span><br><span class="line"></span><br><span class="line">Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"manage.py"</span>, line <span class="number">21</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">main</span>()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"manage.py"</span>, line <span class="number">17</span>, <span class="keyword">in</span> <span class="keyword">main</span></span><br><span class="line">    execute_from_command_line(sys.argv)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py"</span>, line <span class="number">401</span>, <span class="keyword">in</span> execute_from_command_line</span><br><span class="line">    utility.execute()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/core/management/__init__.py"</span>, line <span class="number">377</span>, <span class="keyword">in</span> <span class="keyword">execute</span></span><br><span class="line">    django.setup()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/__init__.py"</span>, line <span class="number">24</span>, <span class="keyword">in</span> setup</span><br><span class="line">    apps.populate(settings.INSTALLED_APPS)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/apps/registry.py"</span>, line <span class="number">114</span>, <span class="keyword">in</span> populate</span><br><span class="line">    app_config.import_models()</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/apps/config.py"</span>, line <span class="number">211</span>, <span class="keyword">in</span> import_models</span><br><span class="line">    self.models_module = import_module(models_module_name)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"</span>, line <span class="number">127</span>, <span class="keyword">in</span> import_module</span><br><span class="line">    <span class="keyword">return</span> _bootstrap._gcd_import(<span class="keyword">name</span>[<span class="keyword">level</span>:], <span class="keyword">package</span>, <span class="keyword">level</span>)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">1006</span>, <span class="keyword">in</span> _gcd_import</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">983</span>, <span class="keyword">in</span> _find_and_load</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">967</span>, <span class="keyword">in</span> _find_and_load_unlocked</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">677</span>, <span class="keyword">in</span> _load_unlocked</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap_external&gt;"</span>, line <span class="number">728</span>, <span class="keyword">in</span> exec_module</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"&lt;frozen importlib._bootstrap&gt;"</span>, line <span class="number">219</span>, <span class="keyword">in</span> _call_with_frames_removed</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/contrib/auth/models.py"</span>, line <span class="number">2</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">from</span> django.contrib.auth.base_user <span class="keyword">import</span> AbstractBaseUser, BaseUserManager</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/contrib/auth/base_user.py"</span>, line <span class="number">47</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    <span class="keyword">class</span> AbstractBaseUser(models.Model):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/models/base.py"</span>, line <span class="number">121</span>, <span class="keyword">in</span> __new__</span><br><span class="line">    new_class.add_to_class(<span class="string">'_meta'</span>, Options(meta, app_label))</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/models/base.py"</span>, line <span class="number">325</span>, <span class="keyword">in</span> add_to_class</span><br><span class="line">    value.contribute_to_class(cls, <span class="keyword">name</span>)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/models/options.py"</span>, line <span class="number">208</span>, <span class="keyword">in</span> contribute_to_class</span><br><span class="line">    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/__init__.py"</span>, line <span class="number">28</span>, <span class="keyword">in</span> __getattr__</span><br><span class="line">    <span class="keyword">return</span> getattr(connections[DEFAULT_DB_ALIAS], item)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/utils.py"</span>, line <span class="number">207</span>, <span class="keyword">in</span> __getitem__</span><br><span class="line">    backend = load_backend(db[<span class="string">'ENGINE'</span>])</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/utils.py"</span>, line <span class="number">111</span>, <span class="keyword">in</span> load_backend</span><br><span class="line">    <span class="keyword">return</span> import_module(<span class="string">'%s.base'</span> % backend_name)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"</span>, line <span class="number">127</span>, <span class="keyword">in</span> import_module</span><br><span class="line">    <span class="keyword">return</span> _bootstrap._gcd_import(<span class="keyword">name</span>[<span class="keyword">level</span>:], <span class="keyword">package</span>, <span class="keyword">level</span>)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/usr/local/lib/python3.7/site-packages/django/db/backends/mysql/base.py"</span>, line <span class="number">21</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    ) <span class="keyword">from</span> err</span><br><span class="line">django.core.exceptions.ImproperlyConfigured: <span class="keyword">Error</span> loading MySQLdb module.</span><br><span class="line">Did you <span class="keyword">install</span> mysqlclient?</span><br></pre></td></tr></table></figure></p><p>原因：在Python2中用的是mysqldb，但在Python3中用的是 mysqlclient</p><p>尝试解决：执行mysqlclient安装命令<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 <span class="keyword">install</span> mysqlclient</span><br></pre></td></tr></table></figure></p><p>报错：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Collecting mysqlclient</span><br><span class="line">  Using cached <span class="keyword">https</span>://<span class="built_in">files</span>.pythonhosted.org/packages/d0/<span class="number">97</span>/<span class="number">7326248</span>ac8d5049968bf4ec708a5d3d4806e412a42e74160d7f266a3e03a/mysqlclient<span class="number">-1.4</span><span class="number">.6</span>.tar.gz</span><br><span class="line">    ERROR: Complete output <span class="built_in">from</span> <span class="keyword">command</span> <span class="title">python</span> <span class="title">setup</span>.<span class="title">py</span> <span class="title">egg_info</span>:</span><br><span class="line">    ERROR: /bin/sh: mysql_config: <span class="keyword">command</span> <span class="title">not</span> <span class="title">found</span></span><br><span class="line">    /bin/sh: mariadb_config: <span class="keyword">command</span> <span class="title">not</span> <span class="title">found</span></span><br><span class="line">    /bin/sh: mysql_config: <span class="keyword">command</span> <span class="title">not</span> <span class="title">found</span></span><br><span class="line">    Traceback (most recent call <span class="keyword">last</span>):</span><br><span class="line">      File <span class="string">"&lt;string&gt;"</span>, <span class="built_in">line</span> <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">      File <span class="string">"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup.py"</span>, <span class="built_in">line</span> <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">        metadata, options = get_config()</span><br><span class="line">      File <span class="string">"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup_posix.py"</span>, <span class="built_in">line</span> <span class="number">61</span>, <span class="keyword">in</span> get_config</span><br><span class="line">        libs = mysql_config(<span class="string">"libs"</span>)</span><br><span class="line">      File <span class="string">"/private/var/folders/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install-9cqr2rno/mysqlclient/setup_posix.py"</span>, <span class="built_in">line</span> <span class="number">29</span>, <span class="keyword">in</span> mysql_config</span><br><span class="line">        raise EnvironmentError(<span class="string">"%s not found"</span> % (<span class="title">_mysql</span>_config_path,))</span><br><span class="line">    OSError: mysql_config <span class="keyword">not</span> found</span><br><span class="line">    <span class="comment">----------------------------------------</span></span><br><span class="line">ERROR: Command <span class="string">"python setup.py egg_info"</span> failed <span class="keyword">with</span> error code <span class="number">1</span> <span class="keyword">in</span> /<span class="keyword">private</span>/var/<span class="built_in">folders</span>/sc/tw1zhq750h510tgxd4g32_dw0000gn/T/pip-install<span class="number">-9</span>cqr2rno/mysqlclient/</span><br></pre></td></tr></table></figure></p><p>没找到mysql配置，解决如下：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=<span class="variable">$PATH</span><span class="symbol">:/usr/local/mysql/bin</span></span><br></pre></td></tr></table></figure><p>再次安装，成功</p><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><center>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！---<center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center></center></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;环境说明&quot;&gt;&lt;a href=&quot;#环境说明&quot; class=&quot;headerlink&quot; title=&quot;环境说明&quot;&gt;&lt;/a&gt;环境说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Python 3.7.3&lt;/li&gt;
&lt;li&gt;Django 3.0&lt;ul&gt;
&lt;li&gt;安装：pip3 install
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Python" scheme="http://thinkgamer.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>《文章推荐系统》系列之1、推荐流程设计</title>
    <link href="http://thinkgamer.cn/2019/12/03/RecSys/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E3%80%8A%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E7%B3%BB%E5%88%97%E4%B9%8B1%E3%80%81%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B%E8%AE%BE%E8%AE%A1/"/>
    <id>http://thinkgamer.cn/2019/12/03/RecSys/文章推荐系统/《文章推荐系统》系列之1、推荐流程设计/</id>
    <published>2019-12-03T13:21:21.000Z</published>
    <updated>2019-12-06T04:59:41.645Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统主要解决的是信息过载的问题，目标是从海量物品筛选出不同用户各自喜欢的物品，从而为每个用户提供个性化的推荐。推荐系统往往架设在大规模的业务系统之上，面临着用户的不断增长，物品的不断变化，并且有着全面的推荐评价指标和严格的性能要求（Netflix 的请求时间在 250 ms 以内，今日头条的请求时间在 200ms 以内），所以推荐系统很难一次性地快速计算出用户所喜好的物品，并且同时满足准确度、多样性等评价指标。为了解决如上这些问题，推荐系统通常被设计为三个阶段：召回、排序和调整，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-ee044ed9dd9cc759.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="召回、排序和调整"></p><p>在召回阶段，首先筛选出和用户直接相关或间接相关的物品，将原始数据从万、百万、亿级别缩小到万、千级别；在排序阶段，通常使用二分类算法来预测用户对物品的喜好程度（或者是点击率），然后将物品按照喜好程序从大到小依次排列，筛选出用户最有可能喜欢的物品，这里又将召回数据从万、千级别缩小到千、百级别；最后在调整阶段，需要过滤掉重复推荐的、已经购买或阅读的、已经下线的物品，当召回和排序结果不足时，需要使用热门物品进行补充，最后合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。</p><p>这里以文章推荐系统为例，讲述一下推荐系统的完整流程，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12790782-40327d59055eba56.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="推荐系统的完整流程"></p><ol><li><p>同步业务数据<br>为了避免推荐系统的数据读写、计算等对应用产生影响，我们首先要将业务数据从应用数据库 MySQL 同步到推荐系统数据库 Hive 中，这里利用 Sqoop 先将 MySQL 中的业务数据同步到推荐系统的 HDFS 中，再关联到指定的 Hive 表中，这样就可以在推荐系统数据库 Hive 中使用用户数据和文章数据了，并且不会对应用产生任何影响。</p></li><li><p>收集用户行为数据<br>除了用户数据和文章数据，我们还需要得到用户对文章的行为数据，比如曝光、点击、阅读、点赞、收藏、分享、评论等。我们的用户行为数据是记录在应用服务器的日志文件中的，所以可以利用 Flume 对应用服务器的日志文件进行监听，一方面将收集到的用户行为数据同步到 HDFS 中，并关联到 Hive 的用户行为表，每天更新一次，以供离线计算使用。另一方面将 Flume 收集到的用户行为数据同步到 Kafka，实时更新，以供在线计算使用。</p></li><li><p>构建离线画像和特征</p></li></ol><ul><li><p>文章画像由关键词和主题词组成，我们首先读取 Hive 中的文章数据，将文章内容进行分词，根据 TF-IDF 模型计算每个词的权重，将 TF-IDF 权重最高的 K 个词作为关键词，再根据 TextRank 模型计算每个词的权重，将 TextRank 权重最高的 K 个词与 TF-IDF 权重最高的 K 个词的共现词作为主题词，将关键词和主题词存储到 Hive 的文章画像表中。接下来，利用 Word2Vec 模型，计算得到所有关键词的平均向量，作为文章的词向量，存储到 Hive 的文章向量表中，并利用 BucketedRandomProjectionLSH 模型计算得到文章的相似度，将每篇文章相似度最高的 K 篇文章，存储到 Hbase 的文章相似表中。这样我们就得到了每篇文章的画像、词向量以及相似文章列表。</p></li><li><p>构建离线用户画像<br>我们可以将用户喜欢的文章的主题词作为用户标签，以便后面根据用户标签来推荐符合其偏好的文章。首先读取用户行为数据和文章画像数据，计算在用户产生过行为的所有文章中，每个主题词的权重，不同的行为，权重不同，计算公式为：用户标签权重 =（用户行为分值之和）x 时间衰减，这样就计算得到了用户的标签及标签权重，接着读取用户数据，得到用户基础信息，将用户标签、标签权重及用户基础信息一并存储到 Hbase 的用户画像表中。到这里我们已经通过机器学习算法，基于用户和文章的业务数据得到了用户和文章的画像，但为了后面可以更方便地将数据提供给深度学习模型进行训练，我们还需要将画像数据进一步抽象为特征数据。</p></li><li><p>构建离线文章特征<br>由于已经有了画像信息，特征构造就变得简单了。读取文章画像数据，将文章权重最高的 K 个关键词的权重作为文章关键词权重向量，将频道 ID、关键词权重向量、词向量作为文章特征存储到 Hbase 的文章特征表。</p></li><li><p>构建离线用户特征<br>读取用户画像数据，将权重最高的 K 个标签的权重作为用户标签权重向量，将用户标签权重向量作为用户特征存储到 Hbase 的用户特征表。</p></li></ul><ol><li>多路召回</li></ol><ul><li><p>基于模型的离线召回<br>我们可以根据用户的历史点击行为来预测相似用户，并利用相似用户的点击行为来预测对文章的偏好得分，这种召回方式称为 u2u2i。获取用户历史点击行为数据，利用 ALS 模型计算得到用户对文章的偏好得分及文章列表，读取并过滤历史召回结果，防止重复推荐，将过滤后的偏好得分最高的 K 篇文章存入 Hbase 的召回结果表中，key 为 als，表明召回类型为 ALS 模型召回，并记录到 Hbase 的历史召回结果表。</p></li><li><p>基于内容的离线召回<br>我们可以根据用户的历史点击行为，向用户推荐其以前喜欢的文章的相似文章，这种方式称为 u2i2i。读取用户历史行为数据，获取用户历史发生过点击、阅读、收藏、分享等行为的文章，接着读取文章相似表，获取与发生行为的每篇文章相似度最高的 K 篇文章，然后读取并过滤历史召回结果，防止重复推荐，最后将过滤后的文章存入 Hbase 的召回结果表中，key 为 content，表明召回类型为内容召回，并记录到 Hbase 的历史召回结果表。</p></li><li><p>基于内容的在线召回<br>和上面一样，还是根据用户的点击行为，向用户推荐其喜欢的文章的相似文章，不过这里是用户实时发生的行为，所以叫做在线召回。读取 Kafka 中的用户实时行为数据，获取用户实时发生点击、阅读、收藏、分享等行为的文章，接着读取文章相似表，获取与发生行为的每篇文章相似度最高的 K 篇文章，然后读取并过滤历史召回结果，防止重复推荐，最后将过滤后的文章存入 Hbase 的召回结果表中，key 为 online，表明召回类型为在线召回，并记录到 Hbase 的历史召回结果表。</p></li><li><p>基于热门文章的在线召回<br>读取 Kafka 中的用户实时行为数据，获取用户当前发生点击、阅读、收藏、分享等行为的文章，增加这些文章在 Redis 中的热度分数。</p></li><li><p>基于新文章的在线召回<br>读取 Kafka 中的实时用户行为数据，获取新发布的文章，将其加入到 Redis 中，并设置过期时间。</p></li></ul><ol><li>排序</li></ol><p>不同模型的做法大致相同，这里以 LR 模型为例。</p><ul><li><p>基于 LR 模型的离线训练<br>读取 Hive 的用户历史行为数据，并切分为训练集和测试集，根据其中的用户 ID 和文章 ID，读取 Hbase 的用户特征数据和文章特征数据，将二者合并作为训练集的输入特征，将用户对文章是否点击作为训练集的标签，将上一次的模型参数作为 LR 模型的初始化参数，进行点击率预估训练，计算得出 AUC 等评分指标并进行推荐效果分析。</p></li><li><p>基于 LR 模型的在线排序<br>当推荐中心读取 Hbase 的推荐结果表无数据时，推荐中心将调用在线排序服务来重新获取推荐结果。排序服务首先读取 Hbase 的召回结果作为测试集，读取 Hbase 的用户特征数据和文章特征数据，将二者合并作为测试集的输入特征，使用 LR 模型进行点击率预估，计算得到点击率最高的前 K 个文章，然后读取并过滤历史推荐结果，防止重复推荐，最后将过滤后的文章列表存入 Hbase 的推荐结果表中，key 为 lr，表明排序类型为 LR 排序。</p></li></ul><ol><li>推荐中心</li></ol><ul><li><p>流量切分（ABTest）<br>我们可以根据用户 ID 进行哈希分桶，将流量切分到多个桶，每个桶对应一种排序策略，从而对比不同排序策略在线上环境的效果。</p></li><li><p>推荐数据读取逻辑<br>优先读取 Redis 和 Hbase 中缓存的推荐结果，若 Redis 和 Hbase 都为空，则调用在线排序服务获得推荐结果。</p></li><li><p>兜底补足（超时截断）<br>当调用排序服务无结果，或者读取超时的时候，推荐中心会截断当前请求，直接读取 Redis 中的热门文章和新文章作为推荐结果。</p></li><li><p>合并信息<br>合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。</p></li></ul><blockquote><p>原文出自（已授权）：<a href="https://www.jianshu.com/u/ac833cc5146e" target="_blank" rel="external">https://www.jianshu.com/u/ac833cc5146e</a></p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://space.bilibili.com/61036655/channel/detail?cid=91348" target="_blank" rel="external">https://space.bilibili.com/61036655/channel/detail?cid=91348</a>（强烈推荐，蚂蚁大神的视频讲得很棒）</li><li><a href="https://www.bilibili.com/video/av68356229" target="_blank" rel="external">https://www.bilibili.com/video/av68356229</a></li><li><a href="https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw" target="_blank" rel="external">https://pan.baidu.com/s/1-uvGJ-mEskjhtaial0Xmgw</a>（学习资源已保存至网盘，提取码 EakP）</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;推荐系统主要解决的是信息过载的问题，目标是从海量物品筛选出不同用户各自喜欢的物品，从而为每个用户提供个性化的推荐。推荐系统往往架设在大规模的业务系统之上，面临着用户的不断增长，物品的不断变化，并且有着全面的推荐评价指标和严格的性能要求（Netflix 的请求时间在 250 
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="文章推荐系统" scheme="http://thinkgamer.cn/tags/%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Spark使用Libsvm格式数据构造LabeledPoint格错误</title>
    <link href="http://thinkgamer.cn/2019/11/29/Spark/Spark%E4%BD%BF%E7%94%A8Libsvm%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0LabeledPoint%E6%A0%BC%E9%94%99%E8%AF%AF/"/>
    <id>http://thinkgamer.cn/2019/11/29/Spark/Spark使用Libsvm格式数据构造LabeledPoint格错误/</id>
    <published>2019-11-29T02:29:22.000Z</published>
    <updated>2019-12-06T04:59:41.648Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>使用libsvm格式的数据构造LabeledPoint格式，例如我的libsvm格式数据如下(索引下标最大值为，3000)：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">790718 1<span class="selector-pseudo">:1</span> 2<span class="selector-pseudo">:1</span> 4<span class="selector-pseudo">:1</span> 5<span class="selector-pseudo">:1</span> 6<span class="selector-pseudo">:1</span> 7<span class="selector-pseudo">:1</span> 9<span class="selector-pseudo">:1</span> 11<span class="selector-pseudo">:1</span> 13<span class="selector-pseudo">:1</span> 16<span class="selector-pseudo">:1</span> 19<span class="selector-pseudo">:1</span> 21<span class="selector-pseudo">:1</span> 28<span class="selector-pseudo">:1</span> 31<span class="selector-pseudo">:1</span> 43<span class="selector-pseudo">:1</span> 64<span class="selector-pseudo">:1</span> 65<span class="selector-pseudo">:1</span> 140<span class="selector-pseudo">:1</span> 164<span class="selector-pseudo">:1</span> 184<span class="selector-pseudo">:1</span> 296<span class="selector-pseudo">:1</span> 463<span class="selector-pseudo">:1</span> 481<span class="selector-pseudo">:1</span> 642<span class="selector-pseudo">:1</span> 813<span class="selector-pseudo">:1</span> 1093<span class="selector-pseudo">:1</span> 2288<span class="selector-pseudo">:1</span></span><br><span class="line">692384 9<span class="selector-pseudo">:1</span> 10<span class="selector-pseudo">:1</span> 16<span class="selector-pseudo">:1</span> 19<span class="selector-pseudo">:1</span> 30<span class="selector-pseudo">:1</span> 31<span class="selector-pseudo">:1</span> 54<span class="selector-pseudo">:1</span> 56<span class="selector-pseudo">:1</span> 69<span class="selector-pseudo">:1</span> 140<span class="selector-pseudo">:1</span> 142<span class="selector-pseudo">:1</span> 224<span class="selector-pseudo">:1</span> 232<span class="selector-pseudo">:1</span> 307<span class="selector-pseudo">:1</span> 601<span class="selector-pseudo">:1</span> 649<span class="selector-pseudo">:1</span> 692<span class="selector-pseudo">:1</span> 2851<span class="selector-pseudo">:1</span></span><br></pre></td></tr></table></figure></p><p>但是在构造LabeledPoint格式数据的时候忽略的应该创建的数组长度，使用如下代码：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val dataset = rdd</span><br><span class="line">    .<span class="built_in">map</span>( l =&gt; ( l._1, l._2.<span class="built_in">split</span>(<span class="string">" "</span>).<span class="built_in">map</span>(<span class="symbol">_</span>.<span class="built_in">split</span>(<span class="string">":"</span>)).<span class="built_in">map</span>(e =&gt; (e(<span class="number">0</span>).toInt-<span class="number">1</span>, e(<span class="number">1</span>).toDouble)) ) )</span><br><span class="line">    .<span class="built_in">map</span>(l =&gt; LabeledPoint(l._1.toDouble, Vectors.<span class="built_in">sparse</span>(l._2.<span class="built_in">length</span>, l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._1), l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._2))))</span><br><span class="line">    .toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</span><br></pre></td></tr></table></figure></p><p>所以报了 java.lang.IllegalArgumentException: requirement failed: Index 2287 out of bounds for vector of size 27 错误</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>将创建LabeledPoint数据的长度改为3000即可，如下：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val dataset = rdd</span><br><span class="line">    .<span class="built_in">map</span>( l =&gt; ( l._1, l._2.<span class="built_in">split</span>(<span class="string">" "</span>).<span class="built_in">map</span>(<span class="symbol">_</span>.<span class="built_in">split</span>(<span class="string">":"</span>)).<span class="built_in">map</span>(e =&gt; (e(<span class="number">0</span>).toInt-<span class="number">1</span>, e(<span class="number">1</span>).toDouble)) ) )</span><br><span class="line">    .<span class="built_in">map</span>(l =&gt; LabeledPoint(l._1.toDouble, Vectors.<span class="built_in">sparse</span>(<span class="number">3000</span>, l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._1), l._2.filter(<span class="symbol">_</span>._2!=<span class="number">0</span>).<span class="built_in">map</span>(<span class="symbol">_</span>._2))))</span><br><span class="line">    .toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</span><br></pre></td></tr></table></figure><p>打印信息显示如下：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |<span class="string">-- label: double (nullable = false)</span></span><br><span class="line"><span class="string"> </span>|<span class="string">-- features: vector (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="string">+---------+--------------------+</span></span><br><span class="line">|<span class="string">    label</span>|<span class="string">            features</span>|</span><br><span class="line">+---------+--------------------+</span><br><span class="line">|<span class="string"> 790718.0</span>|<span class="string">(3275,[0,1,3,4,5,...</span>|</span><br><span class="line">|<span class="string"> 692384.0</span>|<span class="string">(3275,[8,9,15,18,...</span>|</span><br><span class="line">|<span class="string"> 672331.0</span>|<span class="string">(3275,[0,1,2,7,8,...</span>|</span><br><span class="line">|<span class="string">1646601.0</span>|<span class="string">   (3275,[30],[1.0])</span>|</span><br><span class="line">|<span class="string">1740585.0</span>|<span class="string">(3275,[0,3,6,9,11...</span>|</span><br><span class="line">|<span class="string"> 615659.0</span>|<span class="string">(3275,[2,4,5,30,4...</span>|</span><br><span class="line">|<span class="string"> 169763.0</span>|<span class="string">(3275,[1,2,3,4,7,...</span>|</span><br><span class="line">|<span class="string"> 639653.0</span>|<span class="string">(3275,[1,2,4,10,1...</span>|</span><br><span class="line">|<span class="string">1774993.0</span>|<span class="string">(3275,[6,11,13,14...</span>|</span><br><span class="line">|<span class="string">1680621.0</span>|<span class="string">(3275,[11,16,31],...</span>|</span><br><span class="line">+---------+--------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></p><p>完美解决问题！！！希望本文能够帮助到你！</p><p>不得不说对于Spark中一些格式的数据使用还是不太熟悉！</p><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><center>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！---<center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center></center></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;使用libsvm格式的数据构造LabeledPoint格式，例如我的libsvm格式数据如下(索引下标最大值为，3000)：&lt;br&gt;&lt;fig
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>NLP实战之基于TFIDF的文本相似度计算</title>
    <link href="http://thinkgamer.cn/2019/11/26/NLP/NLP%E5%AE%9E%E6%88%98%E4%B9%8B%E5%9F%BA%E4%BA%8ETFIDF%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    <id>http://thinkgamer.cn/2019/11/26/NLP/NLP实战之基于TFIDF的文本相似度计算/</id>
    <published>2019-11-26T09:03:03.000Z</published>
    <updated>2019-12-06T03:14:43.914Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TFIDF算法介绍"><a href="#TFIDF算法介绍" class="headerlink" title="TFIDF算法介绍"></a>TFIDF算法介绍</h3><p>TF-IDF（Term Frequency–InverseDocument Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><p>TF-IDF实际是TF*IDF，其中TF（Term Frequency）表示词条$t$在文档$D_i$中的出现的频率，TF的计算公式如下所示：</p><script type="math/tex; mode=display">TF_{t,D_i} = \frac{count(t)}{D_i}</script><p>其中IDF（InverseDocument Frequency）表示总文档与包含词条t的文档的比值求对数，IDF的计算公式如下所示： </p><script type="math/tex; mode=display">IDF_t = log \frac{N}{ \sum_{i=1}^{N} I(t,D_i) }</script><p>其中$N$为所有的文档总数，$I(t,D_i)$表示文档$D_i$是否包含词条$t$，若包含为1，不包含为0。但此处存在一个问题，即当词条$t$在所有文档中都没有出现的话IDF计算公式的分母为0，此时就需要对IDF做平滑处理，改善后的IDF计算公式如下所示：</p><script type="math/tex; mode=display">IDF_t = log \frac{N}{ 1 + \sum_{i=1}^{N} I(t,D_i) }</script><p>那么最终词条$t$在文档$D_i$中的TF-IDF值为：$TF-IDF_{t,D_i} = TF_{t,D_i} * IDF_t$ 。</p><p>从上述的计算词条$t$在文档$D_i$中的TF-IDF值计算可以看出：当一个词条在文档中出现的频率越高，且新鲜度低（即普遍度低），则其对应的TF-IDF值越高。<br>比如现在有一个预料库，包含了100篇（$N$）论文，其中涉及包含推荐系统（$t$）这个词条的有20篇，在第一篇论文（$D1$）中总共有200个技术词汇，其中推荐系统出现了15次，则词条推荐系统的在第一篇论文（$D1$）中的TF-IDF值为：</p><script type="math/tex; mode=display">TF-IDF_{推荐系统} = \frac {15} {200} * log \frac{200}{20+1} = 0.051</script><p>更多详细的关于TFIDF的介绍可以参考</p><ul><li><a href="https://thinkgamer.blog.csdn.net/article/details/48811033" target="_blank" rel="external">搜索引擎：文本分类——TF/IDF算法</a></li></ul><p>关于TF-IDF的其他实战：</p><ul><li><a href="https://thinkgamer.blog.csdn.net/article/details/85690389" target="_blank" rel="external">基于TF-IDF算法的短标题关键词提取</a></li></ul><h3 id="基于TFIDF计算文本相似度"><a href="#基于TFIDF计算文本相似度" class="headerlink" title="基于TFIDF计算文本相似度"></a>基于TFIDF计算文本相似度</h3><p>这里需要注意的是在spark2.x中默认不支持dataframe的笛卡尔积操作，需要在创建Spark对象时开启。</p><p>创建spark对象，并设置日志等级<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// spark.sql.crossJoin.enabled=true spark <span class="number">2.0</span> x不支持笛卡尔积操作，需要开启支持</span><br><span class="line">val spark = SparkSession</span><br><span class="line"><span class="meta">    .builder</span>()</span><br><span class="line"><span class="meta">    .appName</span>(<span class="string">"docSimCalWithTFIDF"</span>)</span><br><span class="line"><span class="meta">    .config</span>(<span class="string">"spark.sql.crossJoin.enabled"</span>,<span class="string">"true"</span>)</span><br><span class="line"><span class="meta">    .master</span>(<span class="string">"local[10]"</span>)</span><br><span class="line"><span class="meta">    .enableHiveSupport</span>()</span><br><span class="line"><span class="meta">    .getOrCreate</span>()</span><br><span class="line">Logger.getRootLogger.setLevel(Level.WARN)</span><br></pre></td></tr></table></figure></p><p>这里以官方样例代码中的三行英文句子为例，创建数据集，并进行分词（spark中的中文分词包有很多，比如jieba，han，ansj，fudannlp等，这里不展开介绍）<br><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">val</span> sentenceData = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Hi I heard about Spark"</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"I wish Java could use case classes"</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Logistic regression models are neat"</span>)</span><br><span class="line">)).toDF(<span class="string">"label"</span>, <span class="string">"sentence"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">val</span> tokenizer = <span class="function"><span class="keyword">new</span> <span class="title">Tokenizer</span>()</span></span><br><span class="line"><span class="function">    .<span class="title">setInputCol</span>("sentence")</span></span><br><span class="line"><span class="function">    .<span class="title">setOutputCol</span>("words")</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">wordsData</span> = <span class="title">tokenizer</span>.<span class="title">transform</span>(sentenceData)</span></span><br><span class="line"><span class="function"><span class="title">wordsData</span>.<span class="title">show</span>(<span class="number">10</span>)</span></span><br></pre></td></tr></table></figure></p><p>展示的结果为：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|<span class="string">label</span>|<span class="string">            sentence</span>|<span class="string">               words</span>|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|<span class="string">    0</span>|<span class="string">Hi I heard about ...</span>|<span class="string">[hi, i, heard, ab...</span>|</span><br><span class="line">|<span class="string">    1</span>|<span class="string">I wish Java could...</span>|<span class="string">[i, wish, java, c...</span>|</span><br><span class="line">|<span class="string">    2</span>|<span class="string">Logistic regressi...</span>|<span class="string">[logistic, regres...</span>|</span><br><span class="line">+-----+--------------------+--------------------+</span><br></pre></td></tr></table></figure></p><p>调用官方的tfidf包计算向量：<br><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// setNumFeatures(5)表示将Hash分桶的数量设置为5个,可以根据你的词语数量来调整，一般来说，这个值越大不同的词被计算为一个Hash值的概率就越小，数据也更准确，但需要消耗更大的内存</span></span><br><span class="line"></span><br><span class="line"><span class="meta">val</span> hashingTF = <span class="function"><span class="keyword">new</span> <span class="title">HashingTF</span>()</span></span><br><span class="line"><span class="function">    .<span class="title">setInputCol</span>("words")</span></span><br><span class="line"><span class="function">    .<span class="title">setOutputCol</span>("rawFeatures")</span></span><br><span class="line"><span class="function">    .<span class="title">setNumFeatures</span>(<span class="number">5</span>)</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">featurizedData</span> = <span class="title">hashingTF</span></span></span><br><span class="line"><span class="function">    .<span class="title">transform</span>(wordsData)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">featurizedData</span>.<span class="title">show</span>(<span class="number">10</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">idf</span> = <span class="title">new</span> <span class="title">IDF</span>()</span></span><br><span class="line"><span class="function">    .<span class="title">setInputCol</span>("rawFeatures")</span></span><br><span class="line"><span class="function">    .<span class="title">setOutputCol</span>("features")</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">idfModel</span> = <span class="title">idf</span>.<span class="title">fit</span>(featurizedData)</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">rescaledData</span> = <span class="title">idfModel</span>.<span class="title">transform</span>(featurizedData)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">rescaledData</span>.<span class="title">show</span>(<span class="number">10</span>)</span></span><br><span class="line"><span class="function"><span class="title">rescaledData</span>.<span class="title">select</span>("label", "features").<span class="title">show</span>()</span></span><br></pre></td></tr></table></figure></p><p>展示的结果为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------------------+--------------------+--------------------+</span><br><span class="line">|label|            sentence|               words|         rawFeatures|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+</span><br><span class="line">|    <span class="number">0</span>|Hi I heard about ...|[hi, i, heard, ab...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">2.0</span>,<span class="number">2.</span>..|</span><br><span class="line">|    <span class="number">1</span>|I wish Java could...|[i, wish, java, c...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|</span><br><span class="line">|    <span class="number">2</span>|Logistic regressi...|[logistic, regres...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line">+-----+--------------------+--------------------+--------------------+--------------------+</span><br><span class="line">|label|            sentence|               words|         rawFeatures|            features|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+--------------------+</span><br><span class="line">|    <span class="number">0</span>|Hi I heard about ...|[hi, i, heard, ab...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">2.0</span>,<span class="number">2.</span>..|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">0.0</span>,<span class="number">0.</span>..|</span><br><span class="line">|    <span class="number">1</span>|I wish Java could...|[i, wish, java, c...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">|    <span class="number">2</span>|Logistic regressi...|[logistic, regres...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>...|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">+-----+--------------------+--------------------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line">+-----+--------------------+</span><br><span class="line">|label|            features|</span><br><span class="line">+-----+--------------------+</span><br><span class="line">|    <span class="number">0</span>|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">0.0</span>,<span class="number">0.</span>..|</span><br><span class="line">|    <span class="number">1</span>|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">|    <span class="number">2</span>|(<span class="number">5</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0.0</span>...|</span><br><span class="line">+-----+--------------------+</span><br></pre></td></tr></table></figure></p><p>其中$(5,[0,2,4],[0.0,0…$ 是向量的一种压缩表示格式，例如$(3,[0,1],[0.1,0.2])$表示的是 向量的长度为3，其中第 1位和第2位的值为0.1 和0.3，第3位的值为0。</p><hr><p>这里需要将其转化为向量的形式，方便后续进行计算，可以直接通过dataframe进行转化，也可以先将dataframe转化为rdd，再进行转化。<br>datafram通过自定义UDF进行转化如下：<br><figure class="highlight puppet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">// 解析数据 转化为denseVector格式 datafra</span><br><span class="line">val sparseVectorToDenseVector = <span class="keyword">udf</span> &#123; </span><br><span class="line">    features: <span class="attr">SV</span> =&gt; features.toDense </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> <span class="keyword">df</span> = rescaledData</span><br><span class="line">    .select($<span class="string">"label"</span>, sparseVectorToDenseVector($<span class="string">"features"</span>))</span><br><span class="line">    .withColumn(<span class="string">"tag"</span>,lit(1))</span><br><span class="line">df.show(10)</span><br></pre></td></tr></table></figure></p><p>展示结果为：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+------+--------------------+---+</span><br><span class="line">|<span class="string">label1</span>|<span class="string">           features1</span>|<span class="string">tag</span>|</span><br><span class="line">+------+--------------------+---+</span><br><span class="line">|<span class="string">     0</span>|<span class="string">[0.0,0.0,0.575364...</span>|<span class="string">  1</span>|</span><br><span class="line">|<span class="string">     1</span>|<span class="string">[0.0,0.0,0.575364...</span>|<span class="string">  1</span>|</span><br><span class="line">|<span class="string">     2</span>|<span class="string">[0.0,0.6931471805...</span>|<span class="string">  1</span>|</span><br><span class="line">+------+--------------------+---+</span><br></pre></td></tr></table></figure></p><p>先转化为RDD，再进行转化如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val selectedRDD = rescaledData.select(<span class="string">"label"</span>, <span class="string">"features"</span>).rdd</span><br><span class="line">    .map( l=&gt;( l.get(<span class="number">0</span>)<span class="selector-class">.toString</span>, l<span class="selector-class">.getAs</span>[SV](<span class="number">1</span>).toDense))</span><br><span class="line">selectedRDD.take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure></p><p>展示结果为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0</span>,[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.5753641449035617</span>,<span class="number">0.0</span>,<span class="number">0.0</span>])</span><br><span class="line">(<span class="number">1</span>,[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.5753641449035617</span>,<span class="number">0.28768207245178085</span>,<span class="number">0.0</span>])</span><br><span class="line">(<span class="number">2</span>,[<span class="number">0.0</span>,<span class="number">0.6931471805599453</span>,<span class="number">0.0</span>,<span class="number">0.5753641449035617</span>,<span class="number">0.0</span>])</span><br></pre></td></tr></table></figure></p><hr><p>当然也可以在进行相似度计算时进行转化，实现代码如下：<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义相似度计算udf</span></span><br><span class="line">import spark.implicits._</span><br><span class="line">val df1 = rescaledData</span><br><span class="line">    .<span class="keyword">select</span>(<span class="string">$"label"</span>.<span class="keyword">alias</span>(<span class="string">"id1"</span>), <span class="string">$"features"</span>.<span class="keyword">alias</span>(<span class="string">"f1"</span>))</span><br><span class="line">    .withColumn(<span class="string">"tag"</span>,lit(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">val df2 = rescaledData</span><br><span class="line">    .<span class="keyword">select</span>(<span class="string">$"label"</span>.<span class="keyword">alias</span>(<span class="string">"id2"</span>), <span class="string">$"features"</span>.<span class="keyword">alias</span>(<span class="string">"f2"</span>))</span><br><span class="line">    .withColumn(<span class="string">"tag"</span>,lit(<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">val simTwoDoc = udf&#123; </span><br><span class="line">    (f1: SV, f2: SV) =&gt; calTwoDocSim(f1,f2) </span><br><span class="line">&#125;</span><br><span class="line">val df =  df1.<span class="keyword">join</span>(df2, Seq(<span class="string">"tag"</span>), <span class="string">"inner"</span>)</span><br><span class="line">    .<span class="keyword">where</span>(<span class="string">"id1 != id2"</span>)</span><br><span class="line">    .withColumn(<span class="string">"simscore"</span>,simTwoDoc(col(<span class="string">"f1"</span>), col(<span class="string">"f2"</span>)))</span><br><span class="line">    .<span class="keyword">where</span>(<span class="string">"simscore &gt; 0.0"</span>)</span><br><span class="line">    .<span class="keyword">select</span>(<span class="string">"id1"</span>,<span class="string">"id2"</span>,<span class="string">"simscore"</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show(<span class="number">20</span>)</span><br></pre></td></tr></table></figure></p><p>其中calTwoDocSim 函数实现如下：<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * @Author: GaoYangtuan</span></span><br><span class="line"><span class="comment">  * @Description: 自定义计算两个文本的距离</span></span><br><span class="line"><span class="comment">  * @Thinkgamer: 《推荐系统开发实战》作者，「搜索与推荐Wiki」公号负责人，算法工程师</span></span><br><span class="line"><span class="comment">  * @Param: [f1, f2]</span></span><br><span class="line"><span class="comment">  * @return: double</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"><span class="symbol">def</span> calTwoDocSim(<span class="built_in">f1</span>: SV, <span class="built_in">f2</span>: SV): Double = &#123;</span><br><span class="line">    val <span class="keyword">breeze1 </span><span class="symbol">=new</span> SparseVector(<span class="built_in">f1</span>.indices,<span class="built_in">f1</span>.values, <span class="built_in">f1</span>.size)</span><br><span class="line">    val <span class="keyword">breeze2 </span><span class="symbol">=new</span> SparseVector(<span class="built_in">f2</span>.indices,<span class="built_in">f2</span>.values, <span class="built_in">f2</span>.size)</span><br><span class="line">    val cosineSim = <span class="keyword">breeze1.dot(breeze2) </span>/ (norm(<span class="keyword">breeze1) </span>* norm(<span class="keyword">breeze2))</span></span><br><span class="line"><span class="keyword"> </span>  cosineSim</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>打印结果如下：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |<span class="string">-- id1: integer (nullable = false)</span></span><br><span class="line"><span class="string"> </span>|<span class="string">-- id2: integer (nullable = false)</span></span><br><span class="line"><span class="string"> </span>|<span class="string">-- simscore: double (nullable = false)</span></span><br><span class="line"></span><br><span class="line"><span class="string">+---+---+------------------+</span></span><br><span class="line">|<span class="string">id1</span>|<span class="string">id2</span>|<span class="string">          simscore</span>|</span><br><span class="line">+---+---+------------------+</span><br><span class="line">|<span class="string">  0</span>|<span class="string">  1</span>|<span class="string">0.8944271909999159</span>|</span><br><span class="line">|<span class="string">  1</span>|<span class="string">  0</span>|<span class="string">0.8944271909999159</span>|</span><br><span class="line">|<span class="string">  1</span>|<span class="string">  2</span>|<span class="string">0.2856369296406274</span>|</span><br><span class="line">|<span class="string">  2</span>|<span class="string">  1</span>|<span class="string">0.2856369296406274</span>|</span><br><span class="line">+---+---+------------------+</span><br></pre></td></tr></table></figure></p><p>最后进行排序和保存，代码如下：<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">val sortAndSlice = udf &#123; simids: Seq[Row] =&gt;</span><br><span class="line">    simids.map&#123;</span><br><span class="line">        case Row(id2: <span class="keyword">Int</span>,  simscore: Double) =&gt; (id2,simscore)</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">    .sortBy</span>(_<span class="meta">._2</span>)</span><br><span class="line"><span class="meta">    .reverse</span></span><br><span class="line"><span class="meta">    .slice</span>(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line"><span class="meta">    .map</span>(e =&gt; e<span class="meta">._1</span> + <span class="string">":"</span> + e<span class="meta">._2</span>.formatted(<span class="string">"%.3f"</span>))</span><br><span class="line"><span class="meta">    .mkString</span>(<span class="string">","</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val result = df</span><br><span class="line"><span class="meta">    .groupBy</span>($<span class="string">"id1"</span>)</span><br><span class="line"><span class="meta">    .agg</span>(collect_list(struct($<span class="string">"id2"</span>, $<span class="string">"simscore"</span>)).as(<span class="string">"simids"</span>))</span><br><span class="line"><span class="meta">    .withColumn</span>(<span class="string">"simids"</span>, sortAndSlice(sort_array($<span class="string">"simids"</span>, asc = false)))</span><br><span class="line"></span><br><span class="line">result.show(<span class="number">10</span>)</span><br><span class="line">result.coalesce(<span class="number">1</span>).write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"data/tfidf"</span>)</span><br></pre></td></tr></table></figure></p><p>打印结果如下：<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+---+</span>---------------+</span><br><span class="line">|id1|         simids|</span><br><span class="line"><span class="code">+---+</span>---------------+</span><br><span class="line">|  1|0:0.894,2:0.286|</span><br><span class="line">|  2|        1:0.286|</span><br><span class="line">|  0|        1:0.894|</span><br><span class="line"><span class="code">+---+</span>---------------+</span><br></pre></td></tr></table></figure></p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;TFIDF算法介绍&quot;&gt;&lt;a href=&quot;#TFIDF算法介绍&quot; class=&quot;headerlink&quot; title=&quot;TFIDF算法介绍&quot;&gt;&lt;/a&gt;TFIDF算法介绍&lt;/h3&gt;&lt;p&gt;TF-IDF（Term Frequency–InverseDocument Freq
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="NLP" scheme="http://thinkgamer.cn/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(4)-深度信念网络（上）篇</title>
    <link href="http://thinkgamer.cn/2019/11/26/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(4)-%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8A%EF%BC%89%E7%AF%87/"/>
    <id>http://thinkgamer.cn/2019/11/26/TensorFlow/常见的五种神经网络(4)-深度信念网络（上）篇/</id>
    <published>2019-11-26T06:32:04.000Z</published>
    <updated>2019-12-06T03:14:43.931Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/103231385" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/103231385</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>常见的五种神经网络系列第三篇，主要介绍深度信念网络。内容分为上下两篇进行介绍，本文主要是深度信念网络（上）篇，主要介绍以下内容：</p><ul><li>背景</li><li>玻尔兹曼机</li><li>受限玻尔兹曼机</li></ul><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络(上篇)</li><li>常见的五种神经网络(4)-深度信念网络(下篇)</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>对于一个复杂的数据分布，我们往往只能观测到有限的局部特征，并且这些特征通常会包含一定的噪声。如果要对这个数据分布进行建模，就需要挖掘出可观测变量之间复杂的依赖关系，以及可观测变量背后隐藏的内部表示。</p><p>而深度信念网络可以有效的学习变量之间复杂的依赖关系。深度信念网络中包含很多层的隐变量，可以有效的学习数据的内部特征表示，也可以作为一种有效的非线性降维方法，这些学习到的内部特征表示包含了数据的更高级的、有价值的信息，因此十分有助于后续的分类和回归等任务。</p><p>玻尔兹曼机是生成模型的一种基础模型，和深度信念网络的共同问题是<strong>推断和学习</strong>，因为这两种模型都比较复杂，并且都包含隐变量，他们的推断和学习一般通过MCMC方法来进行近似估计。这两种模型和神经网络有很强的对应关系，在一定程度上也称为随机神经网络（Stochastic Neural Network，SNN）。</p><p>因为深度信念网络是有多层玻尔兹曼机组成的，所以本篇文章我们先来了解一下<strong>玻尔兹曼机</strong>和<strong>受限玻尔兹曼机</strong>。</p><h3 id="玻尔兹曼机"><a href="#玻尔兹曼机" class="headerlink" title="玻尔兹曼机"></a>玻尔兹曼机</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>玻尔兹曼机（Boltzmann Machine）可以看作是一种随机动力系统，每个变量的状态都以一定的概率受到其他变量的影响。玻尔兹曼机可以用概率无向图模型来描述，如下所示：</p><p><img src="https://img-blog.csdnimg.cn/20191122090749423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="一个有六个变量的玻尔兹曼机"></p><p>BM的三个性质：</p><ul><li>二值输出：每个随机变量可以用一个二值的随机变量表示</li><li>全连接：所有节点之间是全连接的</li><li>权重对称：每两个变量之间的相互影响是对称的</li></ul><p>BM中的每个变量$X$的联合概率由玻尔兹曼分布得到，即：</p><script type="math/tex; mode=display">p(x) = \frac{1}{Z} exp(\frac{-E(x)}{T})</script><p>其中$Z$为配分函数，能量函数$E(X)$的定义为：</p><script type="math/tex; mode=display">E(x) \overset{\bigtriangleup }{=} E(X=x) = -(   \sum_{i<j } w_{ij}x_ix_j + \sum_{i} b_i x_i  )</script><p>其中$w_{ij}$是两个变量之间的连接权重，$x_i \in \{0,1\}$表示状态，$b_i$是变量$x_i$的偏置。</p><p>玻尔兹曼机可以用来解决两类问题，一是搜索问题，当给定变量之间的连接权重，需要找到一组二值变量，使得整个网络的能量最低。另一类是学习问题，当给定一部分变量的观测值时，计算一组最优的权重。</p><h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><p>在玻尔兹曼机中配分函数$Z$通常难以计算，因此联合概率分布$p(x)$一般通过MCMC（马尔科夫链蒙特卡罗，Markov Chain Monte Carlo）方法来近似，生成一组服从$p(x)$分布的样本。这里介绍基于吉布斯采样的样本生成方法。</p><p><strong>1. 全条件概率</strong><br>吉布斯采样需要计算每个变量$X_i$的全条件概率$p(x_i|x_{\setminus i})$，其中$x_{\setminus i}$表示除了$X_i$外其它变量的取值。</p><p>对于玻尔兹曼机中的一个变量$X_i$，当给定其他变量$x_{\setminus i}$时，全条件概率公式$p(x_i|x_{\setminus i})$为：</p><script type="math/tex; mode=display">p(x_i=1|x_{\setminus i}) = \sigma( \frac{ \sum_{j} w_{ij}x_j +b_i }{T} )\\p(x_i=0|x_{\setminus i}) = 1- p(x_i=1|x_{\setminus i})</script><p>其中$\sigma(.)$为sigmoid函数。</p><p><strong>2. 吉布斯采样</strong></p><p>玻尔兹曼机的吉布斯采样过程为：随机选择一个变量$X_i$，然后根据其全条件概率$p(x_i|x_{\setminus i})$来设置其状态，即以$p(x_i=1|x_{\setminus i})$的概率将变量$X_i$设为1，否则全为0。在固定温度$T$的情况下，在运动不够时间之后，玻尔兹曼机会达到热平衡。此时，任何全局状态的概率服从玻尔兹曼分布$p(x)$，只与系统的能量有关，与初始状态无关。</p><p>要使得玻尔兹曼机达到热平衡，其收敛速度和温度$T$相关。当系统温度非常高$T \rightarrow \infty$时，$p(x_i|x_{\setminus i}) \rightarrow 0.5$，即每个变量状态的改变十分容易，每一种系统状态都是一样的，从而很快可以达到热平衡。当系统温度非常低$T \rightarrow 0$时，如果$\Delta E_i(x_{ \setminus i}) &gt; 0$，则$p(x_i|x_{\setminus i}) \rightarrow 1$，如果$\Delta E_i(x_{ \setminus i}) &lt; 0$，则$p(x_i|x_{\setminus i}) \rightarrow 0$，即：</p><script type="math/tex; mode=display">x_i = \left\{\begin{matrix}1 &  if \sum_{j}w_{ij}x_j + b_i  \geq 0\\ 0 & otherwise\end{matrix}\right.</script><p>因此，当$ \rightarrow 0$时，随机性方法变成了确定性方法，这时，玻尔兹曼机退化为一个Hopfield网络。Hopfield网络是一种确定性的动力系统，每次的状态更新都会使系统的能量降低；而玻尔兹曼机时一种随机性动力系统，每次的状态更新则以一定的概率使得系统的能力上升。</p><p><strong>3. 能量最小化与模拟退火</strong></p><p>要使得动力系统达到热平衡，温度$T$的选择十分关键。一个比较好的折中方法是让系统刚开始在一个比较高的温度下运行达到热平衡，然后逐渐降低直到系统在一个比较低的温度下达到热平衡。这样我们就能够得到一个能量全局最小的分布。这个过程被称为模拟退火（Simulated Annealing）。</p><p>模拟退火是一种寻找全局最优的近似方法。</p><h3 id="受限玻尔兹曼机"><a href="#受限玻尔兹曼机" class="headerlink" title="受限玻尔兹曼机"></a>受限玻尔兹曼机</h3><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>全连接的玻尔兹曼机十分复杂，虽然基于采样的方法提高了学习效率，但在权重更新的过程中仍十分低效。在实际应用中，使用比较广泛的一种带限制的版本，即受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）是一个二分图结构的无向图模型，如下所示。<br><img src="https://img-blog.csdnimg.cn/20191122145237428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="一个有7个变量的受限玻尔兹曼机"></p><p>首先玻尔兹曼机中的变量也分为隐藏变量和可观测变量。分别用可观测层和隐藏层来表示这两组变量。同一层中的节点之间没有连接，而不同层一个层中的节点与另一层中的所有节点连接，这和两层的全连接神经网络结构相同。</p><p>一个受限玻尔兹曼机由$m_1$个可观测变量和$m_2$个隐变量组成，其定义如下：</p><ul><li>可观测的随机向量$v=[v_1, …, v_{m_1}]^T$</li><li>隐藏的随机向量 $h=[h_1, … , h_{m_2}]^T$</li><li>权重矩阵$W \in R^{m_1 * m_2}$，其中每个元素$w_{ij}$为可观测变量$v_i$和隐变量$h_j$之间边的权重</li><li>偏置$a \in R^{m_1}$和$b \in R^{m_2}$，其中$a_i$为每个可观测变量$v_i$得偏置，$b_j$为每个隐变量$h_j$得偏置</li></ul><p>受限玻尔兹曼机得能量函数定义为：</p><script type="math/tex; mode=display">E(v,h) = - \sum_{i} a_iv_i - \sum_{j}b_j h_j - \sum_{i}\sum_{j}v_i w_{ij}h_j = -a^Tv -b^Th - v^T W h</script><p>受限玻尔兹曼机得联合概率分布为$p(v,h)$定义为：</p><script type="math/tex; mode=display">p(v,h) = \frac{1}{Z} exp(-E(v,h)) = \frac{1}{Z} exp(a^Tv)exp(b^Th)exp(v^TWh)</script><p>其中$Z=\sum_{v,h}exp(-E(v,h))$为配分函数。</p><h4 id="生成模型-1"><a href="#生成模型-1" class="headerlink" title="生成模型"></a>生成模型</h4><p>受限玻尔兹曼机得联合概率分布p(v,h)一般也通过吉布斯采样的方法来近似，生成一组服从$p(v,h)$分布的样本。</p><p><strong>1. 全条件概率</strong><br>吉布斯采样需要计算每个变量$V_i$和$H_j$的全条件概率。受限玻尔兹曼机中的同层的变量之间没有连接。从无向图的性质可知，在给定可观测变量时，隐变量之间相互条件独立，同样在给定隐变量时，可观测变量之间也相互条件独立，即有：</p><script type="math/tex; mode=display">p(v_i | v_{\setminus i},h) = p(v_i|h)\\p(h_j | v,h_{\setminus j}) = p(h_j|v)</script><p>其中$v_{\setminus i}$表示除变量$V_i$外其他可观测变量得取值，$h_{\setminus j}$为除变量$H_j$外其它隐变量的取值。因此，$V_i$的全条件概率只需要计算$p(v_i|h)$，而$H_j$的全条件概率只需要计算$p(h_j|v)$</p><p>在受限玻尔兹曼机中，每个可观测变量和隐变量的条件概率为：</p><script type="math/tex; mode=display">p(v_i=1|h) = \sigma (a_i + \sum_{j}w_{ij} h_j)\\p(h_j=1|v) = \sigma (b_j + \sum_{i}w_{ij} v_i)</script><p>其中$\sigma$为sigmoid函数。</p><p><strong>2. RBM中得吉布斯采样</strong><br>受限玻尔兹曼机得采样过程如下：</p><ul><li>（给定）或随机初始化一个可观测的向量$v_0$，计算隐变量得概率，并从中采样一个隐向量$h_0$</li><li>基于$h_0$，计算可观测变量得概率，并从中采样一个个可观测的向量$v_1$</li><li>重复$t$次后，获得$(v_t, h_t)$</li><li>当$t \rightarrow \infty$时，$(v_t,h_t)$的采样服从$p(v,h)$分布</li></ul><p>下图为上述采样过程的示例：<br><img src="https://img-blog.csdnimg.cn/20191122162803614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="受限玻尔兹曼机得采样过程"></p><p><strong>3. 对比散度学习算法</strong></p><p>由于首先玻尔兹曼机得特殊结构，因此可以使用一种比吉布斯采样更高效的学习算法，即对比散度（Contrastive Divergence）。对比散度算法仅需k步吉布斯采样。</p><p>为了提高效率，对比散度算法用一个训练样本作为可观测向量的初始值，然后交替对可观测向量和隐藏向量进行吉布斯采用，不需要等到收敛，只需要k步就行了。这就是CD-k算法，通常，k=1就可以学得很好。对比散度得流程如下所示：<br><img src="https://img-blog.csdnimg.cn/20191122163920824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="单步对比散度算法"></p><p><strong>4. 受限玻尔兹曼机得类型</strong><br>在具体的不同任务中，需要处理得数据类型不一定是二值得，也可能时连续值，为了能够处理这些数据，就需要根据输入或输出得数据类型来设计新的能量函数。一般来说受限玻尔兹曼机有以下三种：</p><ul><li>“伯努利-伯努利”受限玻尔兹曼机：即上面介绍得可观测变量喝隐变量都为二值类型得受限玻尔兹曼机</li><li>“高斯-伯努利”受限玻尔兹曼机：假设可观测变量为高斯分布，隐变量为伯努利分布，其能量函数定义为：<script type="math/tex; mode=display">E(v,h) = \sum_{i} \frac{(v_i - \mu_i)^2}{2 \sigma_i^2} - \sum{j} b_jh_j - \sum_{i}\sum{j} \frac{v_i}{\sigma_i}w_ijh_j</script>其中每个可观测变量$v_i$服从$(\mu_i, \sigma_i)$的高斯分布。</li><li>“伯努利-高斯”受限玻尔兹曼机：假设可观测变量为伯努利分布，隐变量为高斯分布，其能量函数定义为：<script type="math/tex; mode=display">E(v,h)=\sum_{i}a_i v_j - \sum_{j} \frac{(h_j-u_j)^2}{2\sigma_j^2} - \sum_{i}\sum_{j}v_iw_{ij}\frac{h_j}{\sigma_j}</script>其中每个隐变量$h_j$服从$(\mu_j, \sigma_j)$的高斯分布</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/103231385&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>模型的独立学习方式</title>
    <link href="http://thinkgamer.cn/2019/11/12/TensorFlow/%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%8B%AC%E7%AB%8B%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"/>
    <id>http://thinkgamer.cn/2019/11/12/TensorFlow/模型的独立学习方式/</id>
    <published>2019-11-12T12:53:23.000Z</published>
    <updated>2019-12-06T03:14:43.932Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>针对一个给定的任务，通常采取的步骤是：准确一定非规模的数据集，这些数据要和真实数据集的分布一致；然后设定一个优化目标和方法；然后在训练集上训练模型。</p><p>不同的模型往往都是从零开始训练的，一切知识都需要从训练集中得到，这就意味着每个任务都需要大量的训练数据。在实际应用中，我们面对的任务很难满足上述需求，比如训练任务和目标任务的数据分布不一致，训练数据集过少等。这时机器学习的任务就会受到限制，因此人们开始关注一些新的任务学习方式。<br>本篇文章主要介绍一些“模型独立的学习方式”，比如：集成学习、协同学习、自学习、多任务学习、迁移学习、终身学习、小样本学习、元学习等。</p><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>集成学习（Ensemble Learning）就是通过某种策略将多个模型集成起来，通过群体决策来提高决策准确率。集成学习首要的问题是如何集成多个模型，比较常用的集成策略有直接平均、加权平均等。</p><p><strong>集成学习可以分为：Boosting、Bagging、Stacking</strong>，这三种的详细区分和流程可以参考《<a href="https://item.jd.com/12671716.html" target="_blank" rel="external">推荐系统开发实战</a>》一书中第八章 点击率预估部分。本文中主要介绍集成学习中的Boosting学习和AdaBoost算法。</p><p>集成学习的思想可以采用一句古老的谚语来描述：“三个臭皮匠，顶个诸葛亮”。但是一个有效的集成需要各个基模型的差异尽可能的大。为了增加模型之间的差异性，可以采取Bagging类和Boosting类两类方法。</p><h4 id="Bagging类方法"><a href="#Bagging类方法" class="headerlink" title="Bagging类方法"></a>Bagging类方法</h4><p>Bagging类方法是通过随机构造训练样本、随机选择特征等方法来提高每个基模型的独立性，代表性方法有Bagging和随机森林。</p><ul><li>Bagging（Bootstrap Aggregating）是一个通过不同模型的训练数据集的独立性来提高不同模型之间的独立性。我们在原始训练集上进行有放回的随机采样，得到M比较小的训练集并训练M个模型，然后通过投票的方法进行模型集成。</li><li>随机森林（Random Forest）是在Bagging的基础上再引入了随机特征，进一步提升每个基模型之间的独立性。在随机森林中，每个基模型都是一棵树。</li></ul><p>随机森林的算法步骤如下：</p><ul><li>从样本集中通过重采样的方式产生n个样本</li><li>假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点</li><li>重复m次，产生m棵决策树<br>-多数投票机制来进行预测<blockquote><p>需要注意的一点是，这里m是指循环的次数，n是指样本的数目，n个样本构成训练的样本集，而m次循环中又会产生m个这样的样本集</p></blockquote></li></ul><h4 id="Boosting类方法"><a href="#Boosting类方法" class="headerlink" title="Boosting类方法"></a>Boosting类方法</h4><p>Boosting类方法是按照一定的顺序来先后训练不同的基模型，每个模型都针对前续模型的错误进行专门训练。根据前序模型的结果，来提高训练样本的权重，从而增加不同基模型之间的差异性。Boosting类方法的代表性方法有AbaBoost，GBDT，XGB，LightGBM等。</p><p>关于GBDT的介绍同样可以参考《<a href="https://item.jd.com/12671716.html" target="_blank" rel="external">推荐系统开发实战</a>》一书。</p><p>Boosting类集成模型的目标是学习一个加性模型（additive model） ，其表达式如下：</p><script type="math/tex; mode=display">F(x) = \sum_{m=1}^{M} a_m f_m(x)</script><p>其中$f_m(x)$为弱分类器，或基分类器，$a_m$为弱分类器的集成权重，$F(x)$称为强分类器。</p><p>Boosting类方法的关键是如何训练每个弱分类器$f_m(x)$以及对应的权重$a_m$。为了提高集成的效果，应尽可能使得每个弱分类器的差异尽可能大。一种有效的方法是采用迭代的策略来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。</p><p>在学习了第m个弱分类器之后，增加分错样本的权重，使得第$m+1$个弱分类器“更关注”于前边弱分类器分错的样本。这样增加每个弱分类器的差异，最终提升的集成分类器的准确率。这种方法称为AdaBoost（其实其他的Boost模型采用的也是类似的策略，根据前m-1颗树的误差迭代第m颗树）。</p><h4 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h4><p>AdaBoost算法是一种迭代式的训练算法，通过改变数据分布来提高弱分类器的差异。在每一轮训练中，增加分错样本的权重，减少对分对样本的权重，从而得到一个新的数据分布。</p><p>以两类分类为例，弱分类器$f_m(x) \in \{ +1, -1\}$，AdaBoost算法的训练过程如下所示，最初赋予每个样本同样的权重。在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器。然后根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。</p><p><img src="https://img-blog.csdnimg.cn/20191106144746194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="AdaBoost算法流程"></p><p><strong>AdaBoost算法的统计学解释</strong></p><p>AdaBoost算法可以看做是一种分步优化的加性模型，其损失函数定义为：</p><script type="math/tex; mode=display">L(F) = exp(-y F(x))\\= exp(-y \sum_{m=1}^{M} a_m f_m(x))</script><p>其中$y,f_m(x)\in \{ +1,-1\}$</p><p>假设经过$m-1$次迭代，得到：</p><script type="math/tex; mode=display">F_{m-1}(x) = \sum_{t=1}^{T} a_t f_t(x)</script><p>则第$m$次迭代的目标是找一个$a_m$和$f_m(x)$使得下面的损失函数最小。</p><script type="math/tex; mode=display">L(a_m,f_m(x)) = \sum_{n=1}^{N}exp(-y^{(n)} (F_{m-1} (x^{(n)}) + a_m f_m(x^{(n)})))</script><p>令 $w_m^{(n)}=exp(-y^{(n)} F_{m-1}(x^{(n)}))$，则损失函数可以表示为：</p><script type="math/tex; mode=display">L(a_m,f_m(x)) = \sum_{n=1}^{N} w_m^{(n)} exp(-y^{(n)} a_m f_m(x^{(n)}))</script><p>因为$y,f_m(x) \in {+1, -1}$，有：</p><script type="math/tex; mode=display">yf_m(x) = 1-2I(y\neq f_m(x))</script><p>其中$I(x)$为指示函数。</p><p>将损失函数在$f_m(x)=0$处进行二阶泰勒展开，有：</p><script type="math/tex; mode=display">L(a_m, f_m(x)) = \sum_{n=1}^{N} w_m^{(n)} (  1 - a_m y^{(n)}f_m(x^{(n)}) + \frac{1}{2}a_m^2  ) \\\propto a_m \sum_{n=1}^{N} w_n^{(n)} I(y^{(n} \neq f_m(x^{(n)})</script><p>从上式可以看出，当$a_m&gt;0$时，最优的分类器$f_m(x)$为使得在样本权重为$w_m^{(n)}, 1 \leq n \leq N$时的加权错误率最小的分类器。</p><p>在求解出$f_m(x)$之后，上述的损失函数可以写为：</p><script type="math/tex; mode=display">L(a_m, f_m(x)) = \sum_{y^{(n)}=f_m(x^{(n)})} w_m^{(n)} exp(-a_m) + \sum_{y^{(n)} \neq f_m(x^{(n)})}  w_m^{(n)} exp(a_m) \\\propto (1-\epsilon _m) exp(-a_m) + \epsilon_m exp(a_m)</script><p>其中$\epsilon_m$为分类器$f_m(x)$的加权错误率</p><script type="math/tex; mode=display">\epsilon_m = \frac { \sum_{ y^{(n)} \neq f_m(x^{(n)}) } w_m^{(n)} } {\sum_{n} w_m^{(n)}}</script><p>求上式关于$a_m$的导数并令其为0，得到</p><script type="math/tex; mode=display">a_m = \frac {1}{2} log \frac {1-\epsilon_m}{\epsilon_m}</script><p><strong>AdaBoost算法的优缺点</strong><br>优点：</p><ul><li>作为分类器精度很高</li><li>可以使用各种算法构建子分类器，AdaBoost提供的是一个框架</li><li>使用简单分类器时，计算出的结果可理解，且构造简单</li><li>不需要做特征筛选</li><li>不同担心过拟合</li></ul><p>缺点：</p><ul><li>容易收到噪声干扰</li><li>训练时间长，因为需要遍历所有特征</li><li>执行效果依赖于弱分类器的选择</li></ul><h3 id="自训练和协同训练"><a href="#自训练和协同训练" class="headerlink" title="自训练和协同训练"></a>自训练和协同训练</h3><p>监督学习虽然准确度上有一定的保证，但往往需要大量的训练数据，但在一些场景中标注数据的成本是非常高的，因此如何利用大量的无标注数据提高监督学习的效率，有着十分重要的意义。这种利用少量样本标注数据和大量样本标注数据进行学习的方式称之为半监督学习（Semi-Supervised Learning，SSL）。</p><p>本节介绍两种无监督的学习算法：自训练和协同训练。</p><h4 id="自训练"><a href="#自训练" class="headerlink" title="自训练"></a>自训练</h4><p>自训练（Slef-Training）也叫自训练（Self-teaching）或者自举法（boostStrapping）。</p><p>自训练的思路是：利用已知的标注数据训练一个模型，利用该模型去预测无标注的样本数据，然后将置信度较高的样本以及其伪标签加入训练集，然后重新训练模型，进行迭代。下图给出了自训练的算法过程。</p><p><img src="https://img-blog.csdnimg.cn/20191109150434700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="自训练算法过程"></p><p>自训练和密度估计中EM算法有一定的相似之处，通过不断地迭代来提高模型能力。但自训练的缺点是无法保证每次加入训练集的样本的伪标签是正确的。如果选择样本的伪标签是错误的，反而会损害模型的预测能力。因此自训练最关键的步骤是如何设置挑选样本的标准。</p><h4 id="协同训练"><a href="#协同训练" class="headerlink" title="协同训练"></a>协同训练</h4><p>协同训练（Co-Training）是自训练的一种改进方法，通过两个基于不同视角的分类器来相互促进。很多数据都有相对独立的不同视角。比如互联网上的每个网页都由两种视角组成：文字内容和指向其他网页的链接。如果要确定一个网页的类别，可以根据文字内容来判断，也可以根据网页之间的链条关系来判断。</p><p>假设一个样本$x=[x_1,x_2]$，其中$x_1,x_2$分别表示两种不同视角$V_1,V_2$的特征，并满足下面两个假设：</p><ul><li>（1）：条件独立性，即给定样本标签y时，两种特征条件独立$p(x_1,x_2|y)=p(x_1|y)p(x_2|y)$</li><li>（2）：充足和冗余性，即当数据充分时，每种视角的特征都可以足以单独训练出一个正确的分类器。</li></ul><p>令$y=g(x)$为需要学习的真实映射函数，$f_1$和$f_2$分别为两个视角的分类器，有：</p><script type="math/tex; mode=display">\exists f_1, f_2,    \forall x \in X,\,\,\,\,\, f_1(x_1) = f_2(x_2) = g(x)</script><p>其中$X$为样本$x$的取值空间。</p><p>由于不同视角的条件独立性，在不同视角上训练出来的模型就相当于从不同的视角来理解问题，具有一定的互补性。协同训练就是利用这种互补行来进行自训练的一种方法。首先在训练集上根据不同视角分别训练两个模型$f_1$和$f_2$然后用$f_1$和$f_2$在无标记数据集上进行预测，各选取预测置信度比较高的样本加入到训练集，重新训练两个不同视角的模型，并不断重复这个过程（需要注意的是协同算法要求两种视图时条件独立的，如果两种视图完全一样，则协同训练退化成自训练算法）。</p><p>协同训练的算法过程如下：<br><img src="https://img-blog.csdnimg.cn/20191109153310892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="协同训练的算法过程"></p><h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>一般的机器学习模型是针对单个任务进行的，不同任务的模型需要在各自的训练集上单独学习得到。而多任务学习（Multi-task learning）是指同时学习多个相关任务，让 这些任务在学习的过程中共享知识，利用多个任务之间的相关性来改进模型的性能和泛化能力。</p><p>多任务学习可以看做时一种归纳迁移学习（Inductive Transfer Learning），即通过利用包含在相关任务中的信息作为归纳偏置（Inductive Bias）来提高泛化能力。</p><p><strong>多任务学习的主要挑战在于如何设计多任务之间的共享机制</strong>，在传统的机器学习任务中很难引入共享信息，但是在神经网络中就变得简单了许多，常见的以下四种：</p><ul><li><strong>硬共享模式</strong>：让不同任务的神经网络模型共同使用一些共享模块来提取一些通用的特征，然后再针对每个不同的任务设置一些私有模块来提取一些任务特定的特征。</li><li><strong>软共享模式</strong>：不显式设置共享模块，但每个任务都可以从其他任务中“窃取”一些信息来提高自己的能力。窃取的方式包括直接复制使用其他任务的隐状态，或使用注意力机制来主动选择有用的信息。</li><li><strong>层次共享模式</strong>：一般神经网络中不同层抽取的特征类型不同，底层一般抽取一些低级的局部特征，高层抽取一些高级的抽象语义特征。因此如果多任务学习中不同任务也有级别高低之分，那么一个合理的共享模式是让低级任务在底层输出，高级任务在高层输出。</li><li><strong>共享-私有模式</strong>：一个更加分工明确的方式是将共享模块和任务特定（私有）模块的责任分开。共享模块捕捉一些跨任务的共享特征，而私有模块只捕捉和特点任务相关的特征。最终的表示由共享特征和私有特征共同构成。</li></ul><p>在多任务学习中，每个任务都可以有自己单独的训练集。为了让所有任务同时学习，我们通常会使用交替训练的方式来“近似”的实现同时学习，下图给出了四种常见的共享模式图</p><p><img src="https://img-blog.csdnimg.cn/20191112101743882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="四种常见的共享模式图"></p><p>多任务学习的流程可以分为两个阶段：</p><ul><li>（1）联合训练阶段：每次迭代时，随机挑选一个任务，然后从这个任务中随机选择一些训练样本，计算梯度并更新参数</li><li>（2）单任务精调阶段：基于多任务学习到的参数，分别在每个单独任务进行精调，其中单任务精调阶段为可选阶段。当多个任务的差异性比较大时，在每个单任务上继续优化参数可以进一步提升模型能力。</li></ul><p>假设有M个相关任务，其模型分别为$f_m(x,\theta), 1\leq m \leq M$，多任务学习的联合目标函数为所有任务损失函数的线性加权：</p><script type="math/tex; mode=display">L(\theta) = \sum_{m=1}^{M}\sum_{n=1}^{N_m} \eta_m l_m(f_m(x^{(m,n)}, \theta ), y_{(m,n)})</script><p>其中$l_m$为第m个任务的损失函数，$\eta_m$是第m个任务的权重，$\theta$表示包含了共享模块和私有模块在内的所有参数。</p><p>多任务学习中联合训练阶段的具体过程如下所示：<br><img src="https://img-blog.csdnimg.cn/20191112102456841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="多任务学习中联合训练阶段的具体过程"></p><p>多任务学习通常比单任务学习获得更好的泛化能力，主要由于以下几个原因：</p><ul><li>1.多任务学习在多个数据集上进行训练，训练集范围更大，且多任务之间具有一定的相关性，相当于是一种隐式的数据增强，可以提高模型的泛化能力。</li><li>2.多任务学习中的共享模块需要兼顾所有任务，在一定程度上避免了模型过拟合到单个任务的训练集，可以看做是一种正则化。</li><li>3.多任务学习比单任务学习可以获得更好的表示</li><li>4.在多任务学习中，每个任务都可以“选择性”利用其他任务中学习到的隐藏特征，从而提高自身的能力。</li></ul><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>标准机器学习的前提假设只训练数据和测试数据的分布是相同的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差。如何将相关任务的训练数据中学习到的可泛化知识迁移到目标任务中，就是迁移学习（Transfer Learning）要解决的问题。</p><p>迁移学习根据不同的迁移方式又分为两个类型：归纳迁移学习（Inductive Transfer Learning）和推导迁移学习（Transductive Transfer Learning）。这两个类型分别对应两个机器学习的范式：归纳学习（Inductive Learning）和转导学习（Transductive Learning）。一般的机器学习任务都是指归纳学习，即希望再训练集上学习到使得期望风险最小的模型。而转导学习的目标是学习一种在给定测试集上错误率最小的模型，在训练阶段可以利用测试集的信息。</p><h4 id="归纳迁移学习"><a href="#归纳迁移学习" class="headerlink" title="归纳迁移学习"></a>归纳迁移学习</h4><p>一般而言，归纳迁移学习要求源领域和目标领域时相关的，并且源领域$D_S$有大量的训练样本，这些样本可以是有标注的样本也可以时无标注的样本。</p><ul><li>当源领域只有大量无标注数据时，源任务可以转换为无监督学习任务，比如自编码和密度估计，通过无监督任务学习一种可迁移的表示，然后将这些表示迁移到目标任务上。</li><li><p>当源领域有大量的标注数据时，可以直接将源领域上训练的模型迁移到目标领域上。<br>归纳迁移学习一般有下面两种迁移方式：</p></li><li><p>基于特征的方式：将预训练模型的输出或者中间隐藏层的输出作为特征直接加入到目标任务学习模型中。目标任务的学习模型可以时一般的浅层分类器（比如支持向量机等）或一个新的神经网络模型。</p></li><li>精调的方式：在目标任务上复用预训练模型的部分参数，并对其参数进行精调。</li></ul><p>假设预训练的模型是一个深层神经网络，不同层的可迁移性也不尽相同。通常来说网络的低层学习一些通用的低层特征，中层或者高层学习抽象的高级语义特征，而最后几层一般学习和特定任务相关的特征。因此根据目标任务的自身特点以及和源任务的相关性，可以针对性的选择预训练模型的不同层来迁移到目标任务中。</p><p>将预训练模型迁移到目标任务中通常会比从零开始学习的方式好，主要体现在以下三点：</p><ul><li>（1）初始模型的性能一般比随机初始化的模型要好</li><li>（2）训练时模型的学习速度比从零开始学习要快，收敛性更好</li><li>（3）模型的最终性能更好，具有更好的泛化性</li></ul><p>归纳迁移学习和多任务学习也比较类似，但是有下面两点区别：</p><ul><li>（1）多任务学习是同时学习多个不同任务，而归纳迁移学习通常分为两个阶段，即源任务上的学习阶段，和目标任务上的迁移学习阶段</li><li>（2）归纳迁移学习是单向的知识迁移，希望提高模型在目标任务上的性能，而多任务学习时希望提高所有任务的性能。</li></ul><h4 id="转导迁移学习"><a href="#转导迁移学习" class="headerlink" title="转导迁移学习"></a>转导迁移学习</h4><p>转导迁移学习是一种从样本到样本的迁移，直接利用源领域和目标领域的样本进行迁移学习。转导迁移学习可以看作是一种特殊的转导学习。转导迁移学习通常假设源领域有大量的标注数据，而目标领域没有（或少量）的标注数据，但是有大量的无标注数据。目标领域的数据在训练阶段是可见的。</p><p>转导迁移学习的一个常见子问题时领域适应（Domain Adaptation），在领域适应问题中，一般假设源领域和目标领域有相同的样本空间，但是数据分布不同$p_S(x,y) \neq p_T(x,y)$。</p><p>根据贝叶斯公式，$p(x,y)=p(x|y)p(y) = p(y|x)p(x)$，因此数据分布的不一致通常由三种情况造成。</p><ul><li>（1）协变量偏移（Covariate Shift）：源领域和目标领域的输入边际分布不同$p_S(x) \neq p_T(x)$，但后验分布相同$p_S(y|x) = p_T(y|x)$，即学习任务相同$T_s = T_T$</li><li>（2）概念偏移（Concept Shift）：输入边际分布相同$p_S(x) = p_T(x)$，但后验分布不同$p_S(y|x) \ neq p_T(y|x)$，即学习任务不同$T_S \neq T_T$</li><li>（3）先验偏移（Prior Shift）：源领域和目标领域中的输出$y$先验分布不同$p_S(y) \neq p_T(y)$，条件分布相同$p_S(x|y) = p_T(x|y)$。在这样的情况下，目标领域必须提供一定数量的标注样本。</li></ul><h3 id="终生学习"><a href="#终生学习" class="headerlink" title="终生学习"></a>终生学习</h3><p>终生学习（Lifelong Learning）也叫持续学习（Continuous Learning）是指像人类一样具有持续不断的学习能力，根据历史任务中学到的经验和知识来帮助学习不断出现的新任务，并且这些经验和知识是持续累积的，不会因为新的任务而忘记旧的知识。</p><p>在终生学习中，假设一个终生学习算法已经在历史人任务$T_1, T_2, …$上学习到一个模型，当出现一个新任务$T_{m+1}$时，这个算法可以根据过去在$m$个任务上学习到的知识来帮助第$m+1$个任务，同时积累所有的$m+1$个任务上的知识。</p><p>在终生学习中，一个关键的问题是如何避免<strong>灾难性遗忘（Catastrophic Forgetting）</strong>，即按照一定顺序学习多个任务时，在学习新任务的同时不忘记先前学习到的历史知识。比如在神经网络模型中，一些参数对任务$T_A$非常重要，如果在学习任务$T_B$时被改变了，就可能给任务$T_A$造成不好的影响。</p><p>解决灾难性遗忘的方法有很多，比如弹性权重巩固方法（Elastic Weight Coonsolidation）。</p><h3 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h3><p>根据没有免费午餐定理，没有一种通用的学习算法在所有任务上都有效。因此当使用机器学习算法实现某个任务时，我们通常需要“就事论事”，根据任务的特定来选择合适的模型、损失函数、优化算法以及超参数。</p><p>而这种动态调整学习方式的能力，称为元学习（Meta-Learning），也称为学习的学习（Learning to Learn）。</p><p>元学习的目的时从已有的任务中学习一种学习方法或元知识，可以加速新任务的学习。从这个角度来说，元学习十分类似于归纳迁移学习，但元学习更侧重从多种不同的任务中归纳出一种学习方法。</p><p>这里主要介绍两种典型的元学习方法：基于优化器的元学习和模型无关的元学习。</p><h4 id="基于优化器的元学习"><a href="#基于优化器的元学习" class="headerlink" title="基于优化器的元学习"></a>基于优化器的元学习</h4><p>目前神经网络的的学习方法主要是定义一个目标损失函数$L(\theta)$，并通过梯度下降算法来最小化$L(\theta)$</p><script type="math/tex; mode=display">\theta_t \leftarrow \theta_{t-1} - \alpha \bigtriangledown L(\theta_{t-1})</script><p>其中$\theta_t$为第$t$步时的模型参数，$\bigtriangledown L(\theta_{t-1})$为梯度，$\alpha$为学习率。在不同的任务上，通常选择不同的学习绿以及不同的优化方法，比如动量法，Adam等。这些优化算法的本质区别在于更新参数的规则不同，因此一种很自然的元学习就是自动学习一种更新参数的规则，即通过另一个神经网络（比如循环神经网络）来建模梯度下降的过程。下图给出了基于优化器的元学习示例。</p><p><img src="https://img-blog.csdnimg.cn/20191112151007633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="基于优化器的元学习示例"></p><p>我们用函数$g_t(.)$来预测第$t$步时参数更新的差值$\Delta \theta_t = \theta_t - \theta_{t-1}$，函数$g_t(.)$称为优化器，输入是当前时刻的梯度值，输出时参数的更新差值$\Delta \theta_t$，这样第$t$步的更新规则可以写为：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t + g_t(\bigtriangledown L(\theta_{t}), \phi )</script><p>其中$\phi$为优化器$g_t(.)$的参数。</p><p>学习优化器$g_t(.)$的过程可以看做是一种元学习过程，其目标是找到一个适用于多个不同任务的优化器。在标准的梯度下降中，每步迭代的目标是使得$L(\theta)$下降。而在优化器的元学习中，我们希望在每步迭代的目标是$L(\theta)$最小，具体的目标函数为：</p><script type="math/tex; mode=display">L(\phi) = E_f [ \sum_{t=1}^{T} w_t L(\theta_t) ]\\\theta_t = \theta_{t-1} + g_t\\[g_t: h_t] = LSTM( \bigtriangledown L(\theta_{t-1}), h_{t-1}, \phi)</script><p>其中$T$为最大迭代次数，$w_t&gt;0$为每一步的权重，一般可以设置$w_t=1,\forall t$。由于LSTM网络可以记忆梯度的历史信息，学习到的优化器可以看做是一个高阶的优化方法。</p><h4 id="模型无关的元学习"><a href="#模型无关的元学习" class="headerlink" title="模型无关的元学习"></a>模型无关的元学习</h4><p>模型无关的元学习（Model-Agnostic Meta-Learning， MAML）是一个简单的模型无关、任务无关的元学习算法。假设所有的任务都来自一个任务空间，其分布为$p(T)$，我们可以在这个任务空间的所有任务上学习一种通用的表示，这种表示可以经过梯度下降方法在一个特定的单任务上进行精调。假设一个模型为$f(\theta)$，如果我们让这个模型适应到一个新任务$T_m$上，通过一步或多步的梯度下降更新，学习到的任务适配参数为：</p><script type="math/tex; mode=display">\theta_m ' = \theta- \alpha \bigtriangledown _\theta L_{T_m}(f_\theta)</script><p>其中$\alpha$为学习率，这里的$\theta_m’$可以理解为关于$\theta$的函数，而不是真正的参数更新。</p><p>MAML的目标是学习一个参数$\theta$使得其经过一个梯度迭代就可以在新任务上达到最好的性能。</p><script type="math/tex; mode=display">\underset{ \theta }{ min } \sum_{T_m \sim  p(T)} L_{T_m}(f(\theta'_m)) = \sum_{T_m \sim  p(T)} L_{T_m} ( f(\theta - \alpha \bigtriangledown _\theta L_{T_m} (f_\theta )) )</script><p>即在所有任务上的元优化（Meta-Optimization）也采用梯度下降来进行优化，即：</p><script type="math/tex; mode=display">\theta \leftarrow \theta - \beta \bigtriangledown _\theta \sum_{m=1}^{M} L_{T_m}(f_{\theta_m'})</script><p>其中$\beta$为元学习率，这里为一个真正的参数更新步骤。需要计算关于$\theta$的二阶梯度，但用一级近似通常也可以达到比较好的性能。</p><p>MAML的具体过程算法如下：<br><img src="https://img-blog.csdnimg.cn/20191112155840241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="MAML的具体过程算法"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>目前神经网路的学习机制主要是以监督学习为主，这种学习方式得到的模型往往是定向的，也是孤立的，每个任务的模型都是从零开始训练的，一切知识都需要从训练数据中得到，导致每个任务都需要大量的训练数据。本章主要介绍了一些和模型无关的学习方式，包括集成学习、自训练和协同训练、多任务学习、迁移学习、元学习，这些都是深度学习中研究的重点。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;针对一个给定的任务，通常采取的步骤是：准确一定非规模的数据集，这些数据要和真实数据集的分布一致；然后设定一个优化目标和方法；然后在训练集上训
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="集成学习" scheme="http://thinkgamer.cn/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="迁移学习" scheme="http://thinkgamer.cn/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="元学习" scheme="http://thinkgamer.cn/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【论文】文本相似度计算方法综述</title>
    <link href="http://thinkgamer.cn/2019/11/07/%E8%AE%BA%E6%96%87/%E3%80%90%E8%AE%BA%E6%96%87%E3%80%91%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/"/>
    <id>http://thinkgamer.cn/2019/11/07/论文/【论文】文本相似度计算方法综述/</id>
    <published>2019-11-07T07:55:25.000Z</published>
    <updated>2019-12-06T03:14:43.953Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在信息爆炸时代，人们迫切希望从海量信息中获取与自身需要和兴趣吻合度高的内容，为了满足此需求，出现了多种技术，如：搜索引擎、推荐系统、问答系统、文档分类与聚类、文献查重等，而这些应用场景的关键技术之一就是文本相似度计算技术。因此了解文本相似度的计算方法是很有必要的。</p><h1 id="文本相似度定义"><a href="#文本相似度定义" class="headerlink" title="文本相似度定义"></a>文本相似度定义</h1><p>文本相似度在不同领域被广泛讨论，由于应用场景不同，其内涵有所差异，故没有统一、公认的定义。</p><p>Lin从信息论的角度阐明相似度与文本之间的共性和差异有关，共性越大、差异越小、则相似度越高；共性越小、差异越大、则相似度越低。相似度最大的情况是文本完全相同。同时提出基于假设推论出相似度定理，如下所示：</p><script type="math/tex; mode=display">Sim(A,B) = \frac{ log P(common(A,B)) } {log P(description(A,B))}</script><p>其中，common(A,B)是A和B的共性信息，description(A,B)是描述A和B的全部信息，上述公式表达出相似度与文本共性成正相关。 由于没有限制领域，此定义被采用较多。</p><blockquote><p>相关度与相似度是容易混淆的概念，大量学者没有对此做过对比说明。相关度体现在文本共现或者以任何形式相互关联（包括上下位关系、同义关系、反义关系、部件-整体关系、值-属性关系等）反映出文本的组合特点。而相似度是相关度的一种特殊情况，包括上下位关系和同义关系。由此得出，文本的相似度越高，则相关度越大，但是相关度越大并不能说明相似度高。</p></blockquote><p>相似度一般用[0,1]表示，该实数可以通过语义距离计算获得。相似度与语义距离呈反比关系，语义距离越小，相似度越高；语义距离越大，相似度越低。通常用下面的公式表示相似度与语义距离的关系。</p><script type="math/tex; mode=display">Sim(S_A,S_B) = \frac {\alpha} { Dis(S_A,S_B) + \alpha }</script><p>其中，$Dis(S_A,S_B)$表示文本$S_A,S_B$之间的非负语义距离，$\alpha$为调节因子，保证了当语义距离为0时上述公式的意义。</p><p>文本相似度计算中还有一个重要的概念是文本表示，代表对文本的基本处理，目的是将半结构化或非结构化的文本转化为计算机可读形式。<strong>文本相似度计算方法的不同本质是文本表示方法不同</strong></p><h1 id="文本相似度计算方法"><a href="#文本相似度计算方法" class="headerlink" title="文本相似度计算方法"></a>文本相似度计算方法</h1><p><img src="https://img-blog.csdnimg.cn/20191105152528460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="文本相似度计算方法"></p><p>文本相似度计算方法可分为四大类：</p><ul><li>基于字符串的方法（String-Based）</li><li>基于语料库的方法（Corpus-Based）</li><li>基于世界知识的方法（Knowledge-Based）</li><li>其他方法</li></ul><h2 id="基于字符串的方法"><a href="#基于字符串的方法" class="headerlink" title="基于字符串的方法"></a>基于字符串的方法</h2><p>该方法从字符串匹配度出发，以字符串共现和重复程度为相似度的衡量标准。根据计算粒度不同，可以将该方法分为<strong>基于字符的方法</strong>和<strong>基于词语的方法</strong>。下图列出两种方法常见的算法以及思路</p><p><img src="https://img-blog.csdnimg.cn/20191105153106412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="基于字符串的方法"></p><p>基于字符串的方法是在字面层次上的文本比较，文本表示即为原始文本，该方法原理简单，易于实现，现已成为其他方法的计算基础。</p><p>但不足的是将字符或词语作为独立的知识单元，并未考虑词语本身的含义和词语之间的关系。以同义词为例，尽管表达不同，但具有相同的含义，而这类词语的相似度依靠基于字符串的方法并不能准确计算。</p><h2 id="基于语料库的方法"><a href="#基于语料库的方法" class="headerlink" title="基于语料库的方法"></a>基于语料库的方法</h2><p>基于语料库的方法利用语料库中获取的信息计算文本相似度。基于语料库的方法可以划分为：</p><ul><li>基于词袋模型的方法</li><li>基于神经网络的方法</li><li>基于搜索引擎的方法</li></ul><h3 id="基于词袋"><a href="#基于词袋" class="headerlink" title="基于词袋"></a>基于词袋</h3><p>词袋模型（Bag of Words Model，BOW）建立在分布假说的基础上，即“词语所处的上下文语境相似，其语义则相似”。其基本思想是：不考虑词语在文档中出现的顺序，将文档表示成一系列词语的组合。</p><p>根据考虑的语义成程度不同，基于词袋模型的方法主要包括：</p><ul><li>向量空间模型（Vector Space Model，VSM）</li><li>潜在语义分析（Latent Semantic Analysis，LSA）</li><li>概率潜在语义分析（Probabilistic Latent Semantic Analysis，PLSA）</li><li>潜在狄利克雷分布（Latent Dirichlet Allocation，LDA）</li></ul><h4 id="VSM"><a href="#VSM" class="headerlink" title="VSM"></a>VSM</h4><p>VSM模型的基本思想是将每篇文档表示成一个基于词频或者词频-逆文档频率权重的实值向量，那么N篇文档则构成n维实值空间，其中空间的每一维都对用词项，每一篇文档表示该空间的一个点或者向量。两个文档之间的相似度就是两个向量的距离，一般采用余弦相似度方法计算。</p><p>VSM有两个明显的缺点：一是该方法基于文本中的特征项进行相似度计算，当特征项较多时，产生的高维稀疏矩阵导致计算效率不高；二是向量空间模型算法的假设是文本中抽取的特征项没有关联，不符合文本语义表达。</p><h4 id="LSA，PLSA"><a href="#LSA，PLSA" class="headerlink" title="LSA，PLSA"></a>LSA，PLSA</h4><p>LSA算法的基本思想是将文本从稀疏的高维词汇空间映射到低维的潜在语义空间，在潜在的语义空间计算相似性。LSA是基于VSM提出的，两种方法都是采用空间向量表示文本，但LSA使用潜在语义空间，利用奇异值分解提高对高维的词条-文档矩阵进行处理，去除了原始向量空间的某些“噪音”，使数据不再稀疏。Hofmann在LSA的基础上引入主题层，采用期望最大化算法（EM）训练主题。</p><p>LSA本质上是通过降维提高计算准确度，但该算法复杂度比较高，可移植性差，比较之下，PLSA具备统计基础，多义词和同义词在PLSA中分别被训练到不同的主题和相同的主题，从而避免了多义词，同义词的影响，使得计算结构更加准确，但不适用于大规模文本。</p><h4 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h4><p>LDA主题模型是一个三层的贝叶斯概率网络，包含词、主题和文档三层结构。采用LDA计算文本相似性的基本思想是对文本进行主题建模，并在主题对应的词语分布中遍历抽取文本中的词语，得到文本的主题分布，通过此分布计算文本相似度。</p><p>以上三类尽管都是采用词袋模型实现文本表示，但是不同方法考虑的语义程度有所不同。基于向量空间建模的方法语义程度居中，加入潜在语义空间概念，解决了向量空间模型方法的稀疏矩阵问题并降低了多义词，同义词的影响。基于LDA的主题模型的方法语义程度最高，基于相似词语可能属于统一主题的理论，主题经过训练得到，从而保证了文本的语义性。</p><h3 id="基于神经网络"><a href="#基于神经网络" class="headerlink" title="基于神经网络"></a>基于神经网络</h3><p>基于神经网络生成词向量计算文本相似度是近些年提的比较多的。不少产生词向量的模型和工具也被提出，比如Word2Vec和GloVe等。词向量的本质是从未标记的非结构文本中训练出一种低维实数向量，这种表达方式使得类似的词语在距离上更为接近，同时较好的解决了词袋模型由于词语独立带来的维数灾难和语义不足问题。</p><p>基于神经网络方法与词袋模型方法的不同之处在于表达文本的方式。词向量是经过训练得到的低维实数向量，维数可以认为限制，实数值可根据文本距离调整，这种文本表示符合人理解文本的方式，所以基于词向量判断文本相似度的效果有进一步研究空间。</p><h3 id="基于搜索引擎"><a href="#基于搜索引擎" class="headerlink" title="基于搜索引擎"></a>基于搜索引擎</h3><p>基本原理是给定搜索关键词$x,y$，搜索引擎返回包含 $x,y$的网页数量$f(x),f(y)$以及同时包含$x,y$的网页数量$f(x,y)$，计算谷歌相似度距离如下所示:</p><script type="math/tex; mode=display">NGD(x,y) = \frac { G(x,y) - min(G(x),G(y)) } { max(G(x),G(y)}\\= \frac {max\{ log \,f(x), log\,f(y) \} - log \, f(x,y)} { log \, N - min{log \,f(x), log \, f(y)} }</script><p>但是该方法最大的不足是计算结果完全取决于搜索引擎的查询效果, 相似度因搜索引擎而异</p><h2 id="基于世界知识的方法"><a href="#基于世界知识的方法" class="headerlink" title="基于世界知识的方法"></a>基于世界知识的方法</h2><p>基于世界知识的方法是利用具有规范组织体系的知识库计算文本相似度，一般分为两种：基于本体知识和基于网络知识。</p><h3 id="基于本体知识"><a href="#基于本体知识" class="headerlink" title="基于本体知识"></a>基于本体知识</h3><p>文本相似度计算方法使用的本体不是严格的本体概念, 而指广泛的词典、叙词表、词汇表以及狭义的本体。由于本体能够准确地表示概念含义并能反映出概念之间的关系, 所以本体成为文本相似度的研究基础[7]。最常利用的本体是通用词典, 例如 WordNet、《知网》(HowNet)和《同义词词林》等, 除了词典还有一些领域本体, 例如医疗本体、电子商务本体、地理本体、农业本体等。</p><p>结合Hliaoutaki、Batet等的研究，将基于本体的文本相似度算法概括为四种：</p><ul><li>基于距离</li><li>基于内容</li><li>基于属性</li><li>混合式相似度</li></ul><p>下表列出了各种方法的基本原理、代表方法和特点<br><img src="https://img-blog.csdnimg.cn/20191105162644574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="基于本体的文本相似度算法"></p><h3 id="基于网络知识"><a href="#基于网络知识" class="headerlink" title="基于网络知识"></a>基于网络知识</h3><p>由于本体中词语数量的限制，有些学者开始转向基于网络知识方法的研究，原因是后者覆盖范围广泛、富含丰富的语义信息、更新速度相对较快，使用最多的网络知识是维基百科、百度百科。网络知识一般包括两种结构，分别是词条页面之间的链接和词条之间的层次结构。</p><p>基于网络知识的文本相似度计算方法大多利用页面链接或层次结构，能较好的反映出词条的语义关系。但其不足在于：词条与词条的信息完备程度差异较大，不能保证计算准确度，网络知识的生产方式是大众参与，导致文本缺少一定的专业性。</p><h2 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h2><p>除了基于字符串、基于语料库和基于世界知识的方法, 文本相似度计算还有一些其他方法，比如：</p><ul><li>句法分析</li><li>混合方法</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文总结了文本相似度计算的四种方法，以及他们的优缺点。作者认为今后文本相似度的计算方法趋势有三个方向，分别是：</p><ul><li>基于神经网络的方法研究将更加丰富</li><li>网络资源为文本相似度计算方法研究提供更多支持</li><li>针对特定领域以及跨领域文本的相似度计算将成为今后发展的重点</li></ul><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;在信息爆炸时代，人们迫切希望从海量信息中获取与自身需要和兴趣吻合度高的内容，为了满足此需求，出现了多种技术，如：搜索引擎、推荐系统、问答系统
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="NLP" scheme="http://thinkgamer.cn/tags/NLP/"/>
    
      <category term="论文" scheme="http://thinkgamer.cn/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="文本相似度" scheme="http://thinkgamer.cn/tags/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习中的无监督特征学习、聚类和密度估计</title>
    <link href="http://thinkgamer.cn/2019/11/05/TensorFlow/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0%E3%80%81%E8%81%9A%E7%B1%BB%E5%92%8C%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    <id>http://thinkgamer.cn/2019/11/05/TensorFlow/无监督学习中的无监督特征学习、聚类和密度估计/</id>
    <published>2019-11-05T02:58:36.000Z</published>
    <updated>2019-12-06T03:14:43.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习概述"><a href="#无监督学习概述" class="headerlink" title="无监督学习概述"></a>无监督学习概述</h2><p>无监督学习（Unsupervised Learning）是指从无标签的数据中学习出一些有用的模式，无监督学习一般直接从原始数据进行学习，不借助人工标签和反馈等信息。典型的无监督学习问题可以分为以下几类：</p><ul><li><p>无监督特征学习（Unsupervised Feature Learning）</p><blockquote><p>从无标签的训练数据中挖掘有效的特征表示，无监督特征学习一般用来进行降维，数据可视化或监督学习前期的特征预处理。</p></blockquote></li><li><p>密度估计（Density Estimation）</p><blockquote><p>是根据一组训练样本来估计样本空间的概率密度。密度估计可以分为：参数密度估计和非参数密度估计。参数密度估计是假设数据服从某个已知概率密度函数形式的分布，然后根据训练样本去估计该分布的参数。非参数密度估计是不假设服从某个概率分布，只利用训练样本对密度进行估计，可以进行任意形状的密度估计，非参数密度估计的方法包括：直方图、核密度估计等。</p></blockquote></li><li><p>聚类（Clustering）</p><blockquote><p>是将一组样本根据一定的准则划分到不同的组。一个通用的准则是组内的样本相似性要高于组间的样本相似性。常见的聚类方法包括：KMeans、谱聚类、层次聚类等。</p></blockquote></li></ul><p>聚类大家已经非常熟悉了，下文主要介绍无监督特征学习和概率密度估计。</p><h2 id="无监督特征学习"><a href="#无监督特征学习" class="headerlink" title="无监督特征学习"></a>无监督特征学习</h2><p>无监督特征学习是指从无标注的数据中自动学习有效的数据表示，从而能够帮助后续的机器学习模型达到更好的性能。无监督特征学习主要方法有：</p><ul><li>主成分分析</li><li>稀疏编码</li><li>自编码器</li></ul><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><p>主成分分析（Principal Component Analysis，PCA）是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。以下部分摘自于 <a href="https://zhuanlan.zhihu.com/p/32412043" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/32412043</a></p><h4 id="PCA中的最大可分性思想"><a href="#PCA中的最大可分性思想" class="headerlink" title="PCA中的最大可分性思想"></a>PCA中的最大可分性思想</h4><p>PCA降维，用原始样本数据中最主要的方面代替原始数据，最简单的情况是从2维降到1维，如下图所示，我们希望找到某一个维度方向，可以代表两个维度的数据，图中列了两个方向 $u_1, u_2$，那么哪个方向可以更好的代表原始数据呢？</p><p><img src="https://img-blog.csdnimg.cn/2019110418180564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="最大可分性示例"><br>从直观上看，$u_1$比$u_2$好，这就是所说的最大可分性。</p><h4 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h4><p><img src="https://img-blog.csdnimg.cn/20191104182446889.jpg" alt="基变换"></p><p>其中$p_i \in {p_1, p_2, …, p_R}$，$p_i \in R^{1<em>N}$是一个行向量，表示第i个基，$a_j \in {a_1, a_2, …, a_M}$，$a_i \in R^{N</em>1}$是一个列向量，表示第$j$个原始数据记录，特别要注意的是，这里R可以小于N，而R决定了变维后数据的维数。</p><p>从上图和文字解释我们可以得到一种矩阵相乘的物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学习矩阵相乘时，只是简单的记住了相乘的规则，但并不清楚其背后的物理意义。</p><h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><p>如何考虑一个方向或者基是最优的，看下图：</p><p><img src="https://img-blog.csdnimg.cn/20191104184311912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="周志华机器学习插图"></p><p>我们将所有的点向两条直线做投影，基于前面PCA最大可分性思想，我们要找的是降维后损失最小，可以理解为投影后数据尽可能的分开，那么在数学中去表示数据的分散使用的是方差，我们都知道方差越大，数据越分散，方差的表达式如下：</p><script type="math/tex; mode=display">Var(a) = \frac{1}{m} \sum_{i=1}^{m} (a_i - \mu)^2</script><p>其中$\mu$为样本均值，如果提前对样本做去中心化，则方差表达式为：</p><script type="math/tex; mode=display">Var(a) = \frac{1}{m} \sum_{i=1}^{m} (a_i)^2</script><p>到现在，我们知道了以下几点：</p><ul><li>对原始数据进行（线性变换）基变换可以对原始样本给出不同的表示</li><li>基的维度小于样本的维度可以起到降维的作用，</li><li>对基变换后新的样本求其方差，选取使其方差最大的基</li></ul><p>那么再考虑另外一个问题？</p><blockquote><p>上面只是说明了优化目标，但并没有给出一个可行性的操作方案或者算法，因为只说明了要什么，但没说怎么做，所以继续进行探讨。</p></blockquote><h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><p>从二维降到一维可以采用方差最大来选出能使基变换后数据分散最大的方向（基），但遇到高纬的基变换，当完成第一个方向（基）选择后，第二个投影方向应该和第一个“几乎重合在一起”，这样显然是没有用的，要有其他的约束，我们希望两个字段尽量表示更多的信息，使其不存在相关性。</p><p>数学上使用协方差表示其相关性。</p><script type="math/tex; mode=display">Cov(a,b)= \frac{1}{m} \sum_{i=1}^{m}a_i b_i</script><p>当Cov(a,b)=0时表示两个字段完全独立，也是我们优化的目标。</p><blockquote><p>注意这里的 $a_i,b_i$是经过去中心化处理的。</p></blockquote><h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>我们想要达到的目标与字段内方差及协方差有密切的关系，假如只有a、b两个字段，将他们按行组成矩阵X，表示如下：</p><p><img src="https://img-blog.csdnimg.cn/20191104190715730.png" alt="矩阵X"></p><p>然后用X乘以X的转置矩阵，并乘以系数 $\frac{1}{m}$得：</p><p><img src="https://img-blog.csdnimg.cn/20191104190820539.png" alt="在这里插入图片描述"></p><p>可见，协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差，而其他元素是a 和 b的协方差，然后会发现两者被合并到了一个矩阵内。</p><h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><p>我们的目标是使$\frac{1}{m}\sum_{i=1}^{m}a_ib_i=0$，根据上述的推导，可以看出优化目标是$C=\frac{1}{m}XX^T$等价于协方差矩阵对角化。即除对角线外的其他元素（如$\frac{1}{m} \sum_{i=1}^{m}a_i b_i$）化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达成了优化目的。</p><p>这样说可能不是很明晰，我们进一步看下原矩阵和基变换后矩阵协方差矩阵的关系：</p><p>设原始数据矩阵为X，对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p><script type="math/tex; mode=display">D=\frac{1}{m}YY^T\\= \frac{1}{m}(PX)(PX)^T\\= \frac{1}{m} PXX^TP^T\\=P(\frac{1}{m} XX^T)P^T\\= PCP^T\\=P \begin{pmatrix}\frac{1}{m} \sum_{i=1}^{m} a_i^2 & \frac{1}{m} \sum_{i=1}^{m} a_i b_i \\  \frac{1}{m} \sum_{i=1}^{m} a_ib_i  & \frac{1}{m} \sum_{i=1}^{m} b_i^2 \end{pmatrix} P^T</script><p>可见我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p>我们希望投影后的方差最大化，于是优化目标可以改写为：</p><script type="math/tex; mode=display">\underset{P}{max} \, tr(PCP^T)\\s.t. \,PP^T=I</script><p>利用拉格朗日函数可以得到：</p><script type="math/tex; mode=display">J(P) = tr(PCP^T) + \lambda(PP^T - I)</script><p>对P求导有$CP^T + \lambda P^T = 0$，整理得：</p><script type="math/tex; mode=display">CP^T = (- \lambda) P^T</script><p>于是，只需对协方差矩阵C进行特征分解，对求得的特征值进行排序，再对 $P^T = (P_1, P_2, …, P_R)$取前K列组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><h4 id="PCA算法流程"><a href="#PCA算法流程" class="headerlink" title="PCA算法流程"></a>PCA算法流程</h4><p>从上边可以看出，求样本$x_i$的$n’$维的主成分，其实就是求样本集的协方差矩阵$\frac{1}{m}XX^T$的前$n’$维个特征值对应特征向量矩阵P，然后对于每个样本$x_i$，做如下变换$y_i = P x_i$，即达到PCA降维的目的。</p><p>具体的算法流程如下：</p><ul><li>输入：n维的样本集 $X=(x_i, x_2,…,x_m)$，要降维到的维数$n’$</li><li>输出：降维后的维度Y</li></ul><ol><li>对所有的样本集去中心化 $x_i = x_i - \frac{1}{m} \sum_{j=1}^{m}x_j$</li><li>计算样本的协方差矩阵$C = \frac{1}{m}XX^T$</li><li>求出协方差矩阵对应的特征值和对应的特征向量</li><li>将特征向量按照特征值从大到小，从上到下按行排列成矩阵，取前k行组成矩阵P</li><li>$Y=PX$即为降维到K维之后的数据</li></ol><p>注意：有时候降维并不会指定维数，而是指定一个比例$t$，比如降维到原先的t比例。</p><h4 id="PCA算法总结"><a href="#PCA算法总结" class="headerlink" title="PCA算法总结"></a>PCA算法总结</h4><p>PCA算法的主要优点：</p><ul><li>仅仅需要以方差衡量信息量，不受数据集意外因素的影响</li><li>各主成分之间正交，可消除原始数据各成分间的相互影响的因素</li><li>方法设计简单，主要运算是特征值分解，易于实现</li></ul><p>PCA算法的主要缺点：</p><ul><li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的可解释性强</li><li>方差小的非主成分也可能包含对样本差异的重要信息，因降维丢弃可能会对后续数据处理有影响</li><li>当样本特征维度较大时，需要巨大的计算量（比如，10000*10000，这时候就需要SVD[奇异值分解]，SVD不仅可以得到PCA降维的结果，而且可以大大的减小计算量）</li></ul><h3 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h3><h4 id="稀疏编码（Sparse-Coding）介绍"><a href="#稀疏编码（Sparse-Coding）介绍" class="headerlink" title="稀疏编码（Sparse Coding）介绍"></a>稀疏编码（Sparse Coding）介绍</h4><p>在数学上，线性编码是指给定一组基向量$A=[a_1,a_2,…,a_p]$，将输入样本$x\in R$表示为这些基向量的线性组合</p><script type="math/tex; mode=display">x = \sum _{i=1}^{p} z_i a_i = Az</script><p>其中基向量的系数$z=[z_1,…,z_p]$称为输入样本x的编码，基向量A也称为字典（dictionary）。</p><p>编码是对d维空间中的样本x找到其在p维空间中的表示（或投影），其目标通常是编码的各个维度都是统计独立的，并且可以重构出输入样本。编码的关键是找到一组“完备”的基向量A，比如主成分分析等。但是是主成分分析得到的编码通常是稠密向量，没有稀疏性。</p><blockquote><p>如果p个基向量刚好可以支撑p维的欧式空间，则这p个基向量是完备的，如果p个基向量可以支撑d维的欧式空间，并且p&gt;d，则这p个基向量是过完备，冗余的。<br><br><br>“过完备”基向量一般指的是基向量个数远大于其支撑空间维度，因此这些基向量一般是不具备独立，正交等性质。</p></blockquote><p>给定一组N个输入向量$x^1, …, x^N$，其稀疏编码的目标函数定义为：</p><script type="math/tex; mode=display">L(A,Z)= \sum _{n=1}^{N}( || x^n - Az^n || ^2 + \eta \rho (z^n))</script><p>其中$\rho(.)$是一个稀疏性衡量函数，$\eta$是一个超参数，用来控制稀疏性的强度。</p><p>对于一个向量$z \in R$，其稀疏性定义为非零元素的比例。如果一个向量只有很少的几个非零元素，就说这个向量是稀疏的。稀疏性衡量函数$\rho(z)$是给向量z一个标量分数。z越稀疏，$\rho(z)$越小。</p><p>稀疏性衡量函数有多种选择，最直接的衡量向量z稀疏性的函数是$l_0$范式</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} I(|z_i| > 0)</script><p>但$l_0$范数不满足连续可导，因此很难进行优化，在实际中，稀疏性衡量函数通常选用$l_1$范数</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} |z_i|</script><p>或对数函数</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} log(1+z_i^2)</script><p>或指数函数</p><script type="math/tex; mode=display">\rho(z) = \sum _{i=1}^{p} -exp(-z_i^2)</script><h4 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h4><p>给定一组N个输入向量$x^1, … , x^N$，需要同时学习基向量A以及每个输入样本对应的稀疏编码$z^1, …,z^N$。</p><p>稀疏编码的训练过程一般用交替优化的方法进行（这一点和ALS很相似）。</p><p>（1）固定基向量A，对每个输入$x^n$ ，计算其对应的最优编码（原内容为减去稀疏性衡量函数，觉得不对）</p><script type="math/tex; mode=display">\underset{x^n}{min} || x^n - Az^n ||^2 + \eta \rho (z^n), \forall n \in [1,N]</script><p>（2）固定上一步得到的编码$z^1, …,z^N$，计算其最优的基向量</p><script type="math/tex; mode=display">\underset{A}{min} \sum _{i=1}^{N} ( || x^n - Az^n ||^2 ) + \lambda \frac{1}{2} ||A||^2</script><p>其中第二项为正则化项，$\lambda$为正则化项系数。</p><h4 id="稀疏编码优缺点"><a href="#稀疏编码优缺点" class="headerlink" title="稀疏编码优缺点"></a>稀疏编码优缺点</h4><p>稀疏编码的每一维都可以看作是一种特征，和基于稠密向量的分布式表示相比，稀疏编码具有更小的计算量和更好的可解释性等优点。</p><p><strong>计算量</strong> 稀疏性带来的最大好处就是可以极大的降低计算量</p><p><strong>可解释性</strong> 因为稀疏编码只有少数的非零元素，相当于将一个输入样本表示为少数几个相关的特征，这样我们可以更好的描述其特征，并易于理解</p><p><strong>特征选择</strong> 稀疏性带来的另一个好处是可以实现特征的自动选择，只选择和输入样本相关的最少特征，从而可以更好的表示输入样本，降低噪声并减轻过拟合</p><h3 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h3><p>自编码器（Auto-Encoder，AE）是通过无监督的方式来学习一组数据的有效编码。</p><p>假设有一组d维的样本$x^n \in R^d, 1 \leq n \leq N$，自编码器将这组数据映射到特征空间得到每个样本的编码$z^n \in R^p, 1 \leq n \leq N$，并且希望这组编码可以重构出原来的样本。</p><p>自编码器的结构可分为两部分：编码器（encoder）：$f: R^d -&gt; R^p$和解码器（decoder）：$R^p -&gt; R^d$</p><p>自编码器的学习目标是最小化重构误差（reconstruction errors）</p><script type="math/tex; mode=display">L = \sum_{n=1}^{N} || x^n -g(f(x^n)) ||^2 = \sum || x^n -f \cdot  g(x^n) ||^2</script><p>如果特征空间的维度p小雨原始空间的维度d，自编码器相当于是一种降维或特征抽取方法。如果$p \geq d$，一定可以找到一组或多组解使得$f \cdot g$为单位函数（Identity Function），并使得重构错误为0。但是这样的解并没有太多的意义，但是如果再加上一些附加的约束，就可以得到一些有意义的解，比如编码的稀疏性、取值范围，f和g的具体形式等。如果我们让编码只能取k个不同的值（k&lt;N），那么自编码器就可以转换为一个k类的聚类问题。</p><p>最简单的自编码器如下图所示的两层神经网络，输入层到隐藏层用来编码，隐藏层到输出层用来解码，层与层之间互相全连接。</p><p><img src="https://img-blog.csdnimg.cn/20191104162315242.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="最简单的自编码器"></p><p>对于样本x，中间隐藏层为编码：</p><script type="math/tex; mode=display">z = s(W^1 x + b^l)</script><p>输出为重构的数据</p><script type="math/tex; mode=display">x' = s(W^2 z + b^l)</script><p>其中$W,b$为网格参数，$s(.)$为激活函数。如果令$W^2$等于$W^1$的转置，即$W^2=W^{(1)T}$，称为捆绑权重（tied weights）。</p><p>给定一组样本 $x^n \in [0,1]^d, 1 \leq n \leq N$，其重构错误为：</p><script type="math/tex; mode=display">L = \sum_{n=1}^{N} || x^n -x^{'n} ||^2 + \lambda ||W||_F^2</script><p>其中$\lambda$为正则化系数，通过最小化重构误差，可以有效的学习网格的参数。</p><p>我们使用自编码器是为了得到有效的数据表示，因此在训练数据后，我们一般去掉解码器，只保留编码器，编码器的输出可以直接作为后续机器学习模型的输入。</p><h3 id="稀疏自编码器"><a href="#稀疏自编码器" class="headerlink" title="稀疏自编码器"></a>稀疏自编码器</h3><p>自编码器除了可以学习低维编码之外，也学习高维的稀疏编码。假设中间隐藏层z的维度为p，大于输入样本的维度，并让z尽量稀疏，这就是稀疏自编码器（Sparse Auto-Encoder）。和稀疏编码一样，稀疏自编码器的优点是有很高的模型可解释性，并同时进行了隐式的特征选择。</p><p>通过给自编码器中隐藏单元z加上稀疏性限制，自编码器可以学习到数据中一些有用的结构。</p><h3 id="堆叠自编码器"><a href="#堆叠自编码器" class="headerlink" title="堆叠自编码器"></a>堆叠自编码器</h3><p>对于很多数据来说，仅使用两层神经网络的自编码器还不足以获取一种好的数据表示，为了获取更好的数据表示，我们可以使用更深层的神经网络。深层神经网络作为自编码器提取的数据表示一般会更加抽象，能够很好的捕捉到数据的语义信息。在实践中经常使用逐层堆叠的方式来训练一个深层的自编码器，称为堆叠自编码器（Stacked Auto-Encoder，SAE）。堆叠自编码一般可以采用逐层训练（layer-wise training）来学习网络参数。</p><h3 id="降噪自编码器"><a href="#降噪自编码器" class="headerlink" title="降噪自编码器"></a>降噪自编码器</h3><p>降噪自编码器（Denoising Autoencoder）就是一种通过引入噪声来增加编码鲁棒性的自编码器。对于一个向量x，我们首先根据一个比例$\mu$随机将x的一些维度的值设置为0，得到一个被损坏的向量$\tilde x$。然后将被损坏的向量$\tilde x$输入给自编码器得到编码z，并重构原始的无损输入x。</p><p>下图给出了自编码器和降噪自编码器的对比，其中$f_{\theta}$为编码器，$g_{\theta^’}$为解码器，$L(x,x’)$为重构错误。</p><p><img src="https://img-blog.csdnimg.cn/20191104175727219.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="自编码器和降噪自编码器的对比"></p><p>降噪自编码器的思想十分简单，通过引入噪声来学习更鲁棒性的数据编码，并提高模型的泛化能力。</p><h2 id="概率密度估计"><a href="#概率密度估计" class="headerlink" title="概率密度估计"></a>概率密度估计</h2><p>概率密度估计（Probabilistic Density Estimation）简称密度估计（Density Estimation），是基于一些观测样本来估计一个随机变量的概率密度函数。密度估计在机器学习和数学建模中应用十分广泛。</p><p>概率密度估计分为：</p><ul><li>参数密度估计</li><li>非参数密度估计</li></ul><h3 id="参数密度估计"><a href="#参数密度估计" class="headerlink" title="参数密度估计"></a>参数密度估计</h3><p>参数密度估计（Parametric Density Estimation）是根据先验知识假设随机变量服从某种分布，然后通过训练样本来估计分布的参数。</p><p>令 $D = {\{x^n\}}_{i=1}^{N}$为某个未知分布中独立抽取的N个训练样本，假设这些样本服从一个概率分布函数$p(x|\theta)$，其对数似然函数为：</p><script type="math/tex; mode=display">log\,p(D|\theta) = \sum_{n=1}^{N}log\,p(x^n|\theta)</script><p>要估计一个参数$\theta ^{ML}$来使得：</p><script type="math/tex; mode=display">\theta ^{ML} = \underset{\theta}{arg\,max } \sum_{n=1}^{N}log\,p(x^n|\theta)</script><p>这样参数估计问题就转化为最优化问题。</p><h4 id="正态分布中的参数密度估计"><a href="#正态分布中的参数密度估计" class="headerlink" title="正态分布中的参数密度估计"></a>正态分布中的参数密度估计</h4><p>假设样本$x \in X$服从正态分布 $X \sim N(\mu,\sigma^2)$，正态分布的表达式如下：</p><script type="math/tex; mode=display">X \sim N(\mu,\sigma^2) = \frac{1}{ \sqrt{2\pi} \sigma^2} e^{- \frac{(x-\mu)^2}{2\sigma^2}}</script><p>求 $\mu,\sigma^2$的最大似然估计量。</p><p>$X$的概率密度为：</p><script type="math/tex; mode=display">f(x;\mu,\sigma^2) = \frac{1}{ \sqrt{2\pi} \sigma^2} e^{- \frac{(x-\mu)^2}{2\sigma^2}}</script><p>似然函数为：</p><script type="math/tex; mode=display">L(\mu,\sigma^2) = \prod_{i=1}^{N} \frac{1}{ \sqrt{2\pi} \sigma^2} e^{- \frac{(x-\mu)^2}{2\sigma^2}}\\= (2\pi)^{-\frac{N}{2}} (\sigma^2)^{-\frac{N}{2}} e^{(-\frac{1}{2\sigma^2} \sum_{i=1}^{N} (x_i - \mu)^2)}</script><p>对其求导可得对数似然函数为：</p><script type="math/tex; mode=display">Ln\, L =-\frac{N}{2} ln(2\pi)-\frac{N}{2} ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{N}(x_i - \mu)^2</script><p>令：</p><script type="math/tex; mode=display">\left\{\begin{matrix}\frac{\partial }{\partial \mu }ln\, L = \frac{1}{\sigma^2} (\sum_{i=1}^{N} x_i -N\mu ) =0 & \\ \\\frac{\partial }{\partial \sigma^2 }ln\, L = - \frac{N}{2\sigma^2} + \frac{1}{ (2\sigma^2)^2} \sum_{i=1}^{N}(x_i-\mu)^2 =0& \end{matrix}\right.</script><p>由前一式解得$\tilde{\mu}=\frac{1}{N}\sum_{i=1}^{N}x_i = \bar{\mu}$，代入后一式得$\tilde{\sigma^2}=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})^2$，因此得$\mu,\sigma^2$的最大似然估计为：</p><script type="math/tex; mode=display">\tilde{\mu} = \bar{X},\tilde{\sigma^2}=\frac{1}{N}(x_i - \bar{x})^2</script><h4 id="多项分布中的参数密度估计"><a href="#多项分布中的参数密度估计" class="headerlink" title="多项分布中的参数密度估计"></a>多项分布中的参数密度估计</h4><p>假设样本服从K个状态的多态分布，令onehot向量$x\in[0,1]^K$来表示第K个状态，即$x_k=1$，其余$x_{i,k \neq k}=0$，则样本x的概率密度函数为：</p><script type="math/tex; mode=display">p(x|\mu) = \prod_{k=1}^{K}\mu_k ^{x_K}</script><p>其中$\mu_k$为第k个状态的概率，并且满足$\sum_{k=1}^{K} \mu_k =1$。</p><p>数据集$D={\{x^n\}}_{n=1}^{N}$的对数似然函数为：</p><script type="math/tex; mode=display">log(D|\mu) = \sum_{n=1}^{N} \sum_{k=1}^{K} x_n ^k log (\mu _k)</script><p>多项分布的参数估计为约束优化问题，引入拉格朗日乘子$\lambda$，将原问题转化为无约束优化问题。</p><script type="math/tex; mode=display">\underset{\mu, \lambda}{ max} \sum_{n=1}^{N} \sum_{k=1}^{K} x_k ^n log(\mu_k) + \lambda (\sum_{k=1}^{K} \mu_k -1)</script><p>上式分别对$\mu_k,\lambda$求偏，并令其等于0，得到：</p><script type="math/tex; mode=display">\mu_k ^{ML} = \frac{m_k}{N}, 1 \leq N \leq K</script><p>其中$m_k = \sum_{n=1}^{N} x_k ^n$为数据集中取值为第k个状态的样本数量。</p><p>在实际应用中，参数密度估计一般存在两个问题：</p><ul><li>（1）模型选择问题，即如何选择数据分布的密度函数，实际的数据分布往往是非常复杂的，而不是简单的正态分布或者多项分布。</li><li>（2）不可观测变量问题，即我们用来训练数据的样本只包含部分的可观测变量，还有一些非常关键的变量是无法观测的，这导致我们很难估计数据的真实分布。</li><li>（3）维度灾难问题，即高维的参数估计十分困难。随着维度的增加，估计参数所需要的样本量呈指数增加。在样本不足时会出现过拟合。</li></ul><h4 id="非参数密度估计"><a href="#非参数密度估计" class="headerlink" title="非参数密度估计"></a>非参数密度估计</h4><p>非参数密度估计（Nonparametric Density Estimation）是不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数。</p><p>对于高纬空间中的一个随机向量x，假设其服从一个未知分布p(x)，则x落入空间中的小区域R的概率为：  $P=\int_{R} p(x)dx$。</p><p>给定N个训练样本$D=\{x^n\}_{n=1}^{N}$，落入区域R的样本数量K服从二项分布：</p><script type="math/tex; mode=display">P_K = \binom{N}{K}P^K(1-P)^{1-K}</script><p>其中$K/N$的期望为$E[K/N]=P$，方差为$var(K/N)=P(1-P)/N$。当N非常大时，我们可以近似认为：$P\approx \frac{K}{N}$，假设区域R足够小，其内部的概率密度是相同的，则有$P\approx p(x)V$，其中V为区域R的提及，结合前边的两个公式，可得：$p(x)\approx \frac{K}{NV}$。</p><p>根据上式，要准确的估计p(x)需要尽量使得样本数量N足够大，区域体积V尽可能的小。但在具体的应用中吗，样本数量一般有限，过小的区域导致落入该区域的样本比较少，这样估计的概率密度就不太准确。</p><p>因此在实践中估计非参数密度通常使用两种方法：</p><ul><li>（1）固定区域大小V，统计落入不同区域的数量，这种方式包括直方图和核方法两种</li><li>（2）改变区域大小，以使得落入每个区域的样本数量为K，这种方法成为K近邻方法</li></ul><h5 id="直方图方法"><a href="#直方图方法" class="headerlink" title="直方图方法"></a>直方图方法</h5><p>直方图（Histogram Method）是一种非常直观的估计连续变量密度函数的方法，可以表示为一种柱状图。</p><p>以一维随机变量为例，首先将其取值范围划分为M个连续的、不重叠的区间，每个区间的宽度为$\Delta m$，给定$N$个训练样本，我们统计这些样本落入每个区间的数量$K_m$，然后将他们归一化为密度函数。</p><script type="math/tex; mode=display">p_m = \frac {K_m}{N\Delta m},1 \leq m \leq  M</script><p>直方图的关键问题是如何选择一个合适的$\Delta m$，如果该值太小，那么落入每个区间的样本会特别少，其估计的区间密度也会有很大的随机性，如果该值过大，其估计的密度函数会变得十分平滑。下图给出了两个直方图的例子，其中蓝色表示真实的密度函数，红色表示直方图估计的密度函数。</p><p><img src="https://img-blog.csdnimg.cn/20191104090336556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="直方图估计密度函数"></p><p>直方图通常用来处理低维随机变量，可以非常快速的对数据的分布进行可视化，但其<strong>缺点</strong>是很难扩展到高维变量，假设一个d维的随机变量，如果每一维都划分为M个空间，那么整个空间的区域数量为$M^d$，直方图估计的方法会随着空间的增大而指数增长，从而形成<strong>维度灾难（Curse Of Dimensionality）</strong></p><h5 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h5><p>核密度估计（Kernel Density Estimation），也叫Parzen窗方法，是一种直方图方法的改进。</p><p>假设$R$为$d$维空间中的一个以点x为中心的“超立方体”，并定义核函数</p><script type="math/tex; mode=display">\phi (\frac{z-x}{h}) = \left\{\begin{matrix}1 \,\,\,\,\,\, if \, |z_i - x_i|< \frac{h}{2},  1 \leq i \leq d & \\ 0 \,\,\,\,\,\, else & \end{matrix}\right.</script><p>来表示一个样本是否落入该超立方体中，其中$h$为超立方体的边长，也称为核函数的密宽度。</p><p>给定$N$个训练样本$D$，落入区域$R$的样本数量$K$为：</p><script type="math/tex; mode=display">K = \sum_{n=1}^{K} \phi (\frac {x^n - x}{h})</script><p>则点$x$的密度估计为：</p><script type="math/tex; mode=display">p(x) = \frac{K}{Nh^d} =\frac{1}{Nh^d} \sum_{n=1}^{K} \phi (\frac {x^n - x}{h})</script><p>其中$h^d$表示区域$R$的体积。</p><p>除了超立方体的核函数意外之外，我们还可以选择更加平滑的核函数，比如高斯核函数：</p><script type="math/tex; mode=display">\phi (\frac {z-x}{h}) = \frac {1}{ (2\pi)^{\frac{1}{2}} h} exp(- \frac{||z-x||^2}{2h^2})</script><p>其中$h^2$可以看做是高斯核函数的方差，这样点$x$的密度估计为：</p><script type="math/tex; mode=display">p (x) = \frac{1}{N} \sum_{n=1}^{N}   \frac {1}{ (2\pi)^{\frac{1}{2}} h} exp(- \frac{||z-x||^2}{2h^2})</script><h5 id="K近邻方法"><a href="#K近邻方法" class="headerlink" title="K近邻方法"></a>K近邻方法</h5><p>核密度估计方法中的核宽度是固定的，因此同一个宽度可能对高密度的区域过大，而对低密度的区域过小。一种更加灵活的方式是设置一种可变宽度的区域，并使得落入每个区域中的样本数量固定为K。</p><p>要估计点x的密度，首先找到一个以x为中心的球体，使得落入球体的样本数量为K，然后根据公式$p(x)\approx \frac{K}{NV}$就可以计算出点x的密度。因为落入球体的样本也是离x最近的K个样本，所以这种方法也称为K近邻（K-Nearest Neughbor）方法。</p><p>在K近邻方法中，K值的选择十分重要，如果K太小，无法有效的估计密度函数，而K太大也会使局部的密度不准确，并且会增加计算开销。</p><p>K近邻方法也经常用于分类问题，称为K近邻分类器。 当K=1时为最近邻分类器。</p><p>最近邻分类器的一个性质是，当 $N \rightarrow \infty$，其分类错误率不超过最优分类器错误率的两倍。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无监督学习是一种十分重要的机器学习方法，无监督学习问题主要可以分为聚类，特征学习，密度估计等几种类型。但是无监督学习并没有像有监督学习那样取得广泛的成功，主要原因在于其缺少有效客观评价的方法，导致很难衡量一个无监督学习方法的好坏。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无监督学习概述&quot;&gt;&lt;a href=&quot;#无监督学习概述&quot; class=&quot;headerlink&quot; title=&quot;无监督学习概述&quot;&gt;&lt;/a&gt;无监督学习概述&lt;/h2&gt;&lt;p&gt;无监督学习（Unsupervised Learning）是指从无标签的数据中学习出一些有用的模式，无
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="无监督学习" scheme="http://thinkgamer.cn/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="密度估计" scheme="http://thinkgamer.cn/tags/%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>冷启动中的多避老虎机问题（Multi-Armed Bandit，MAB）</title>
    <link href="http://thinkgamer.cn/2019/10/15/RecSys/%E5%86%B7%E5%90%AF%E5%8A%A8%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%81%BF%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%88Multi-Armed%20Bandit%EF%BC%8CMAB%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2019/10/15/RecSys/冷启动中的多避老虎机问题（Multi-Armed Bandit，MAB）/</id>
    <published>2019-10-15T02:50:47.000Z</published>
    <updated>2019-12-06T03:14:43.918Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/102560272" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/102560272</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><p>推荐系统中有两个很重要的问题：EE问题和冷启动。在实际的场景中很好的解决这两个问题又很难，比如冷启动，我们可以基于热门、用户、第三方等信息进行半个性化的推荐，但很难去获得用户的真实兴趣分布。那么有没有一种算法可以很好的解决这个问题呢？答案就是：Bandit。</p><h3 id="Bandit算法与推荐系统"><a href="#Bandit算法与推荐系统" class="headerlink" title="Bandit算法与推荐系统"></a>Bandit算法与推荐系统</h3><p>在推荐系统领域里，有两个比较经典的问题常被人提起，一个是EE问题，另一个是用户冷启动问题。</p><p>EE问题又叫Exploit-Explore。</p><ul><li>Exploit表示的是对于用户已经确定的兴趣当然要迎合。</li><li>Explore表示的如果仅对用户进行兴趣投放，很快就会看厌，所以要不断的探索用户的新兴趣</li></ul><p>所以在进行物品推荐时，不仅要投其所好，还要进行适当的长尾物品挖掘。</p><p>用户冷启动问题，也就是面对新用户时，如何能够通过若干次实验，猜出用户的大致兴趣。</p><p>这两个问题本质上都是如何选择用户感兴趣的主题进行推荐，比较符合Bandit算法背后的MAB问题。</p><p>比如，用Bandit算法解决冷启动的大致思路如下：用分类或者Topic来表示每个用户兴趣，也就是MAB问题中的臂（Arm），我们可以通过几次试验，来刻画出新用户心目中对每个Topic的感兴趣概率。这里，如果用户对某个Topic感兴趣（提供了显式反馈或隐式反馈），就表示我们得到了收益，如果推给了它不感兴趣的Topic，推荐系统就表示很遗憾（regret）了。如此经历“选择-观察-更新-选择”的循环，理论上是越来越逼近用户真正感兴趣的Topic的。</p><h3 id="Bandit算法来源"><a href="#Bandit算法来源" class="headerlink" title="Bandit算法来源"></a>Bandit算法来源</h3><p>Bandit算法来源于历史悠久的赌博学，它要解决的问题是这样的：</p><p>一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, K-armed bandit problem, MAB）。</p><p>怎么解决这个问题呢？最好的办法是去试一试，不是盲目地试，而是有策略地快速试一试，这些策略就是Bandit算法。</p><p>这个多臂问题，推荐系统里很多问题都与它类似：</p><ul><li>假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。</li><li>假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？</li><li>我们的算法工程师又想出了新的模型，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？</li><li>如果只是推荐已知的用户感兴趣的物品，如何才能科学地冒险给他推荐一些新鲜的物品？</li></ul><p>Bandit算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？常见Bandit算法有哪些呢？往下看</p><h3 id="Thompson-sampling"><a href="#Thompson-sampling" class="headerlink" title="Thompson sampling"></a>Thompson sampling</h3><h4 id="1、Beta分布"><a href="#1、Beta分布" class="headerlink" title="1、Beta分布"></a>1、Beta分布</h4><p>Thompson Sampling是基于Beta分布进行的，所以首先看下什么是Beta分布？</p><p>Beta分布可以看作是一个概率的概率分布，当你不知道一个东西的具体概率是多少时，他可以给出所有概率出现的可能性。Beta是一个非固定的公式，其表示的是一组分布（这一点和距离计算中的闵可夫斯基距离类似）。</p><p>比如：</p><p>二项分布（抛n次硬币，正面出现k次的概率）</p><script type="math/tex; mode=display">P(S=k)=\binom{n}{k} p^k (1-p)^{n-k}</script><p>几何分布（抛硬币，第一次抛出正面所需的次数的概率）</p><script type="math/tex; mode=display">P(T=t)= (1-p)^{t-1} p</script><p>帕斯卡分布（抛硬币，第k次出现正面所需次数的概率）</p><script type="math/tex; mode=display">P(Y_k=t)=\binom{t-1}{k-1} p^{k-1} (1-p)^{t-k}p</script><p>去找一个统一的公式去描述这些分布，就是Beta分布：</p><script type="math/tex; mode=display">Beta(x| \alpha,\beta) = \frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^\beta</script><p>其中 $B(\alpha, \beta)$是标准化函数，他的作用是使总概率和为1，$\alpha, \beta$为形状参数，不同的参数对应的图像形状不同，他不但可以表示常见的二项分布、几何分布等，还有一个好处就是，不需要去关系某次实验结果服从什么分布，而是利用<br>$\alpha, \beta$的值就可以计算出我们想要的统计量。</p><p>常见的参数对应的图形为：</p><p><img src="https://img-blog.csdnimg.cn/20191015085055652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="Beta分布"></p><p>$Beta(\alpha, \beta)$的常见的统计量为：</p><ul><li>众数为：$\frac {\alpha-1}{\alpha + \beta -1}$</li><li>期望为：$\mu = E(x)= \frac {\alpha} {\alpha + \beta}$</li><li>方差为：$Var(x) = E(x - \mu)^2 = \frac { \alpha \beta } { (\alpha + \beta)^2 (\alpha + \beta +1) }$</li></ul><h4 id="2、Beta分布的例子"><a href="#2、Beta分布的例子" class="headerlink" title="2、Beta分布的例子"></a>2、Beta分布的例子</h4><p>网上资料中一个很常见的例子是棒球运动员的，这里进行借鉴。</p><p>棒球运动有一个指标是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。</p><p>现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。</p><p>对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。</p><p>接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取$\alpha$=81,$\beta$=219</p><p><img src="https://img-blog.csdnimg.cn/20191015090126646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="棒球运动员Beta分布例子"></p><p>之所以取这两个参数是因为：</p><ul><li>beta分布的均值是：$\frac{81} {81 + 219}=0.27$</li><li>从图中可以看到这个分布主要落在了(0.2,0.35)间，这是从经验中得出的合理的范围。</li></ul><blockquote><p>在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率。也就是说beta分布可以看作一个概率的概率分布。</p></blockquote><p>有了这样的初始值，随着运动的进行，其表达式可以表示为：</p><script type="math/tex; mode=display">Beta(\alpha_0 + hits , \beta_0 + misses)</script><p>其中 $\alpha_0, \beta_0$是一开始的参数，值为81，219。当击中一次球是 hits + 1，misses不变，当未击中时，hits不变，misses+1。这样就可以在每次击球后求其最近的平均水平了。</p><h4 id="3、Thompson-Smapling"><a href="#3、Thompson-Smapling" class="headerlink" title="3、Thompson Smapling"></a>3、Thompson Smapling</h4><p>Thompson sampling算法简单实用，简单介绍一下它的原理，要点如下：</p><ul><li>假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p。</li><li>我们不断地试验，去估计出一个置信度较高的“概率p的概率分布”就能近似解决这个问题了。</li><li>怎么能估计“概率p的概率分布”呢？ 答案是假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose。</li><li>每个臂都维护一个beta分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。</li><li>每次选择臂的方式是：计算每个臂现有的beta分布的平均水平，选择所有臂产生的随机数中最大的那个臂去摇。</li></ul><h4 id="4、TS的Python实现"><a href="#4、TS的Python实现" class="headerlink" title="4、TS的Python实现"></a>4、TS的Python实现</h4><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import numpy as <span class="built_in">np</span></span><br><span class="line">import <span class="built_in">random</span></span><br><span class="line"></span><br><span class="line">def ThompsonSampling(wins, trials):</span><br><span class="line">    pbeta = [<span class="number">0</span>] * N</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, len(trials)):</span><br><span class="line">        pbeta[i] = <span class="built_in">np</span>.<span class="built_in">random</span>.<span class="built_in">beta</span>(wins[i] + <span class="number">1</span>, trials[i] - wins[i] + <span class="number">1</span>)</span><br><span class="line">    choice = <span class="built_in">np</span>.argmax(pbeta)</span><br><span class="line">    trials[choice] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">random</span>.<span class="built_in">random</span>() &gt; <span class="number">0.5</span>:</span><br><span class="line">        wins[choice] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">T = <span class="number">10000</span>  # 实验次数</span><br><span class="line">N = <span class="number">10</span>  # 类别个数</span><br><span class="line"># 臂的选择总次数</span><br><span class="line">trials = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0</span>] * N )</span><br><span class="line"># 臂的收益</span><br><span class="line">wins = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0</span>] * N )</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, T):</span><br><span class="line">    ThompsonSampling(wins, trials)</span><br><span class="line"><span class="built_in">print</span>(trials)</span><br><span class="line"><span class="built_in">print</span>(wins)</span><br><span class="line"><span class="built_in">print</span>(wins/trials)</span><br></pre></td></tr></table></figure><h3 id="UCB"><a href="#UCB" class="headerlink" title="UCB"></a>UCB</h3><h4 id="1、UCB的原理"><a href="#1、UCB的原理" class="headerlink" title="1、UCB的原理"></a>1、UCB的原理</h4><p>UCB（Upper Confidence Bound，置信区间上界）可以理解为不确定性的程度，区间越宽，越不确定，反之就越确定，其表达式如下：</p><script type="math/tex; mode=display">score(i) = \frac {N_i}{T} + \sqrt{ \frac{2 ln T}{N_i}}</script><p>其中 Ni 表示第i个臂收益为 1 的次数，T表示选择的总次数</p><p>公式分为左右两部分，左侧（+左侧部分）表示的是候选臂i到目前为止的平均收益，反应的是它的效果。右侧（+右侧部分）叫做Bonus，本质上是均值的标准差，反应的是候选臂效果的不确定性，就是置信区间的上边界。</p><blockquote><p>统计学中的一些统计量表达的含义</p></blockquote><p>如果一个臂的收益很少，即Ni很小，那么他的不确定性就越大，在最后排序输出时就会有优势，bouns越大，候选臂的平均收益置信区间越宽，越不稳定，就需要更多的机会进行选择。反之如果平均收益很大，即+号左侧户数很大，在选择时也会有被选择的机会。</p><h4 id="2、UCB的Python实现"><a href="#2、UCB的Python实现" class="headerlink" title="2、UCB的Python实现"></a>2、UCB的Python实现</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 T = 1000 个用户，即总共进行1000次实现</span></span><br><span class="line">T = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 定义 N = 10 个标签，即 N 个 物品</span></span><br><span class="line">N = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保证结果可复现，设置随机数种子</span></span><br><span class="line">np.<span class="built_in">random</span>.seed(<span class="number">888</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个物品的累积点击率（理论概率）</span></span><br><span class="line">true_rewards = np.<span class="built_in">random</span>.uniform(low=<span class="number">0</span>, high=<span class="number">1</span>, size= N)</span><br><span class="line"><span class="comment"># true_rewards = np.array([0.5] * N)</span></span><br><span class="line"><span class="comment"># 每个物品的当前点击率</span></span><br><span class="line">now_rewards = np.zeros(N)</span><br><span class="line"><span class="comment"># 每个物品的点击次数</span></span><br><span class="line">chosen_count = np.zeros(N)</span><br><span class="line"></span><br><span class="line">total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算ucb的置信区间宽度</span></span><br><span class="line">def calculate_delta(T, <span class="keyword">item</span>):</span><br><span class="line">    <span class="keyword">if</span> chosen_count[<span class="keyword">item</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="literal">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="literal">return</span> np.<span class="built_in">sqrt</span>( <span class="number">2</span> * np.<span class="built_in">log</span>( T ) / chosen_count[<span class="keyword">item</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算UCB</span></span><br><span class="line">def ucb(t, N):</span><br><span class="line">    <span class="comment"># ucb得分</span></span><br><span class="line">    upper_bound_probs = [ now_rewards[<span class="keyword">item</span>] + calculate_delta(t,<span class="keyword">item</span>) <span class="keyword">for</span> <span class="keyword">item</span> <span class="keyword">in</span> range(N) ]</span><br><span class="line">    <span class="keyword">item</span> = np.argmax(upper_bound_probs)</span><br><span class="line">    <span class="comment"># 模拟伯努利收益</span></span><br><span class="line">    <span class="comment"># reward = sum(np.random.binomial(n =1, p = true_rewards[item], size=20000)==1 ) / 20000</span></span><br><span class="line">    reward = np.<span class="built_in">random</span>.binomial(n =<span class="number">1</span>, p = true_rewards[<span class="keyword">item</span>])</span><br><span class="line">    <span class="literal">return</span> <span class="keyword">item</span>, reward</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>,T+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 为第 t个用户推荐一个物品</span></span><br><span class="line">    <span class="keyword">item</span>, reward = ucb(t, N)</span><br><span class="line">    <span class="comment"># print("item is %s, reward is %s" % (item, reward))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 一共有多少用户接受了推荐的物品</span></span><br><span class="line">    total_reward += reward</span><br><span class="line">    chosen_count[<span class="keyword">item</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新物品的当前点击率</span></span><br><span class="line">    now_rewards[<span class="keyword">item</span>] =  ( now_rewards[<span class="keyword">item</span>] * (t<span class="number">-1</span>) + reward) /  t</span><br><span class="line">    <span class="comment"># print("更新后的物品点击率为：%s" % (now_rewards[item]))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出当前点击率 / 累积点击率</span></span><br><span class="line">    <span class="comment"># print("当前点击率为: %s" % now_rewards)</span></span><br><span class="line">    <span class="comment"># print("累积点击率为: %s" % true_rewards)</span></span><br><span class="line"></span><br><span class="line">    diff =  np.<span class="built_in">subtract</span>( true_rewards, now_rewards)</span><br><span class="line">    print(diff[<span class="number">0</span>])</span><br><span class="line">    print(total_reward)</span><br></pre></td></tr></table></figure><h4 id="3、UCB的推导"><a href="#3、UCB的推导" class="headerlink" title="3、UCB的推导"></a>3、UCB的推导</h4><p>观测 1：假设一个物品被推荐了k次，获取了k次反馈（点击 or 不点击），可以计算出物品被点击的平均概率</p><p>当k 接近于正无穷时，p’ 会接近于真实的物品被点击的概率</p><script type="math/tex; mode=display">p' = \frac {\sum reward_i}{k}</script><p>观测 2：现实中物品被点击的次数不可能达到无穷大，因此估计出的被点击的概率 p’ 和真实的点击的概率 p 总会存在一个差值 d，即：</p><script type="math/tex; mode=display">p'-d \leqslant p \leqslant p'+d</script><p>最后只需要解决差值 d 到底是怎么计算的？</p><p>首先介绍霍夫丁不等式（Chernoff-Hoeffding Bound），霍夫丁不等式假设reward_1, … , reward_n 是在[0,1]之间取值的独立同分布随机变量，用p’ 表示样本的均值，用p表示分布的均值，那么有：</p><script type="math/tex; mode=display">P\{|p'-p| \leqslant \delta \} \geqslant 1 - 2e^{-2n\delta ^2}</script><p>当 $\delta$ 取值为$\sqrt { 2In T /n}$ （其中 T 表示有物品被推荐的次数，n表示有物品被点击的次数），可以得到：</p><script type="math/tex; mode=display">P\{|p'-p| \leqslant \sqrt { \frac{2In T }{n}} \} \geqslant 1 - \frac{ 2 }{ T^4}</script><p>也就是说：</p><script type="math/tex; mode=display">p' - \sqrt { \frac{2In T }{n}} \leqslant p \leqslant p' + \sqrt { \frac{2In T }{n}}</script><p>是以 1 - 2/T^4 的概率成立的，<br>当T=2时，成立的概率为0.875<br>当T=3时，成立的概率为0.975<br>当T=4时，成立的概率为0.992<br>可以看出 $d =  \sqrt { \frac{2In T }{n}}$ 是一个不错的选择。</p><h3 id="Epsilon-Greedy"><a href="#Epsilon-Greedy" class="headerlink" title="Epsilon-Greedy"></a>Epsilon-Greedy</h3><h4 id="1、算法原理"><a href="#1、算法原理" class="headerlink" title="1、算法原理"></a>1、算法原理</h4><p>这是一个朴素的Bandit算法，有点类似模拟退火的思想：</p><ol><li>选一个（0,1）之间较小的数作为epsilon；</li><li>每次以概率epsilon做一件事：所有臂中随机选一个；</li><li>每次以概率1-epsilon 选择截止到当前，平均收益最大的那个臂。</li></ol><p>是不是简单粗暴？epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只选择收益最大的。</p><h4 id="2、Python实现"><a href="#2、Python实现" class="headerlink" title="2、Python实现"></a>2、Python实现</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedy</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, epsilon, counts, values)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.epsilon = epsilon</span><br><span class="line">        <span class="keyword">self</span>.counts = counts</span><br><span class="line">        <span class="keyword">self</span>.values = values</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(<span class="keyword">self</span>, n_arms)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.counts = [<span class="number">0</span> <span class="keyword">for</span> col <span class="keyword">in</span> range(n_arms)]</span><br><span class="line">        <span class="keyword">self</span>.values = [<span class="number">0</span>.<span class="number">0</span> <span class="keyword">for</span> col <span class="keyword">in</span> range(n_arms)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_arm</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> random.random() &gt; <span class="keyword">self</span>.<span class="symbol">epsilon:</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">self</span>.values.index( max(<span class="keyword">self</span>.values) )</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="comment"># 随机返回 self.values 中个的一个</span></span><br><span class="line">            <span class="keyword">return</span> random.randrange(len(<span class="keyword">self</span>.values))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reward</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> random.random() &gt; <span class="number">0</span>.<span class="number">5</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(<span class="keyword">self</span>, chosen_arm, reward)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.counts[chosen_arm] = <span class="keyword">self</span>.counts[chosen_arm] + <span class="number">1</span></span><br><span class="line">        n = <span class="keyword">self</span>.counts[chosen_arm]</span><br><span class="line"></span><br><span class="line">        value = <span class="keyword">self</span>.values[chosen_arm]</span><br><span class="line">        new_value = ((n - <span class="number">1</span>)  * value + reward ) / float(n)</span><br><span class="line">        <span class="keyword">self</span>.values[chosen_arm] = round(new_value,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">n_arms = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">algo = EpsilonGreedy(<span class="number">0</span>.<span class="number">1</span>, [], [])</span><br><span class="line"></span><br><span class="line">algo.initialize(n_arms)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>)<span class="symbol">:</span></span><br><span class="line">    chosen_arm = algo.select_arm()</span><br><span class="line">    reward = algo.reward()</span><br><span class="line">    algo.update(chosen_arm, reward)</span><br><span class="line"></span><br><span class="line">print(algo.counts)</span><br><span class="line">print(algo.values)</span><br></pre></td></tr></table></figure><h3 id="朴素Bandit算法"><a href="#朴素Bandit算法" class="headerlink" title="朴素Bandit算法"></a>朴素Bandit算法</h3><p>最朴素的Bandit算法就是：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。这个算法是人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。</p><p>Python实现比较简单，这里就不做演示了。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/102560272&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐算法" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="冷启动，Bandit" scheme="http://thinkgamer.cn/tags/%E5%86%B7%E5%90%AF%E5%8A%A8%EF%BC%8CBandit/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（四）之正则化</title>
    <link href="http://thinkgamer.cn/2019/09/27/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/09/27/TensorFlow/神经网络中的网络优化和正则化（四）之正则化/</id>
    <published>2019-09-27T00:15:10.000Z</published>
    <updated>2019-12-06T03:14:43.936Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍（如下所示），本篇为最后一篇主要介绍神经网络中的网络正则化。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化 </li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化</li></ul></li></ul><p>机器学习模型中的关键是泛化问题，即样本在真实数据集上的期望风险最小化，而在训练集上的经验风险最小化和期望风险并不一致。由于神经网络的拟合能力很强，其在训练集上的训练误差会降的很小，从而导致过拟合。</p><p><strong>正则化（Regularization）</strong>是一类通过限制模型复杂度，从而避免过拟合，提高模型泛化能力的一种方法，包括引入一些约束规则，增加先验，提前终止等。</p><p>在传统的机器学习模型中，提高模型泛化能力的主要方法是限制模型复杂度，比如$l_1,l_2$正则，但是在训练深层神经网络时，特别是在过度参数（OverParameterized）时，$l_1,l_2$正则化不如机器学习模型中效果明显，因此会引入其他的一些方法，比如：数据增强，提前终止，丢弃法，继承法等。</p><h3 id="l-1-l-2-正则"><a href="#l-1-l-2-正则" class="headerlink" title="$l_1,l_2$正则"></a>$l_1,l_2$正则</h3><p>$l_1,l_2$正则是机器学习中常用的正则化方法，通过约束参数的$l_1,l_2$范数来减少模型在训练数据上的过拟合现象。</p><p>通过引入$l_1,l_2$正则，优化问题变为：</p><script type="math/tex; mode=display">a\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta))+\lambda l_p(\theta)</script><p>$L$为损失函数，$N$为训练的样本数量，$f(.)$为待学习的神经网络，$\theta$为参数，$l_p$为$l_1,l_2$正则中的一个，$\lambda$为正则项系数。</p><p>带正则化的优化问题等价于下面带约束条件的优化问题：</p><script type="math/tex; mode=display">\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta))\\subject \, to \, l_p(\theta) \leq 1</script><p>下图给出了不同范数约束条件下的最优化问题示例：<br><img src="https://img-blog.csdnimg.cn/20190926205404238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="不同范数约束条件下的最优化问题示例"></p><p>上图中红线表示$l_p$范数，黑线表示$f(\theta)$的等高线（简单起见，这里用直线表示）</p><p>从上图最左侧图可以看出，$l_1$范数的约束条件往往会使最优解位于坐标轴上，从而使用最终的参数为稀疏向量，此外$l_1$范数在零点不可导，常用下式来代替：</p><script type="math/tex; mode=display">l_1(\theta) = \sum_{i} \sqrt{\theta_i ^2 + \epsilon }</script><p>其中$\epsilon$为一个非常小的常数。</p><p>一种折中的方法是<strong>弹性网络正则化（Elastic Net Regularization）</strong> ，同时加入$l_1, l_2$正则，如下：</p><script type="math/tex; mode=display">a\theta ^* = \underset{a}{ arg \,  min } \frac{1}{N} L ( y^n, f(x^n, \theta)_+\lambda_1 l_1(\theta) + \lambda_2 l_2(\theta)</script><p>其中$\lambda_1, \lambda_2$分别是正则化项的参数。</p><h3 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h3><p><strong>权重衰减（Weight Deacy）</strong> 也是一种有效的正则化方法，在每次调参时，引入一个衰减系数，表示式为：</p><script type="math/tex; mode=display">\theta_t \leftarrow (1-w)\theta_{t-1} - \alpha g_t</script><p>其中$g_t$为第t次更新时的梯度，$\alpha$为学习率，$w$为权重衰减系数，一般取值比较小，比如0.0005。在标准的随机梯度下降中，权重衰减和$l_2$正则达到的效果相同，因此，权重衰减在一些深度学习框架中用$l_2$正则来代替。但是在较为复杂的优化方法中，两者并不等价。</p><h3 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h3><p><strong>提前终止（early stop）</strong> 对于深层神经网络而言是一种简单有效的正则化方法，由于深层神经网络拟合能力很强，比较容易在训练集上过拟合，因此在实际操作时往往产出一个和训练集独立的验证集，并用在验证集上的错误来代表期望错误，当验证集上的错误不再下降时，停止迭代。</p><p>然而在实际操作中，验证集上的错误率变化曲线并不是一条平衡的曲线，很可能是先升高再降低，因此提前停止的具体停止标准需要根据实际任务上进行优化。</p><h3 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h3><p>当训练一个深层神经网络时，可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法称为 <strong>丢弃法（Dropout Method）</strong>。每次丢弃的神经元为随机的，对于每一个神经元都以一个概率p来判断要不要停留，对于每一个神经层 $y=f(Wx + b)$，我们可以引入一个丢弃函数$d(.)$使得$y=f(Wd(x)+b)$。丢弃函数的定义为：</p><script type="math/tex; mode=display">d(x) = \left\{\begin{matrix}m \odot x, When \, Train\\ px, When \,  Test\end{matrix}\right.</script><p>其中$m \in \{0,1\}^d$是丢弃掩码（dropout mask），通过以概率为p的贝努力分布随机生成，$p$可以通过一个验证集选取一个最优值，也可以设置为0.5， 这样对大部分网络和任务比较有效。在训练时，神经元的平均数量为原来的$p$倍，而在测试时，所有的神经元都可以是激活的，这会造成训练时和测试时的网络结构不一致，为了缓解这个问题，在测试时，需要将每一个神经元的输出乘以$p$，也相当于把不同的神经网络做了一个平均。</p><p>下图给出了一个网络经过dropout的示例。<br><img src="https://img-blog.csdnimg.cn/20190926170817842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="丢弃法示例"></p><p>一般来讲，对于隐藏层的神经元，丢弃率$p=0.5$时最好，这样当训练时有一半的神经元是丢弃的，随机生成的网络结构具有多样性。对于输入层的神经元，其丢弃率往往设置为更接近于1的数，使得输入变化不会太大，对输入层的神经元进行丢弃时，相当于给数据增加噪声，提高网络的鲁棒性。</p><p>丢弃法一般是针对神经元进行随机丢弃，但是也可以扩展到神经元之间的连接进行随机丢弃，或每一层进行随机丢弃。</p><p>丢弃法有两种解释：</p><p>（1）集成学习的解释<br>每做一次丢弃，相当于从原始的网络中采样得到一个子网络，如果一个神经网络有n个神经元，那么可以采样出$2^n$个子网络，每次训练都相当于是训练一个不同的子网络，这些子网络都共享最开始的参数。那么最终的网络可以看成是集成了指数级个不同风格的组合模型。</p><p>（2）贝叶斯学习的解释</p><p>丢弃法也可以解释为一个贝叶斯学习的近似，用$y=f(x,\theta)$表示一个要学习的网络，贝叶斯学习是假设参数$\theta$为随机向量，并且先验分布为$q(\theta)$，贝叶斯方法的预测为：</p><script type="math/tex; mode=display">E_{q(\theta)}[y] = \int_{q}f(x,\theta)q(\theta)d\theta \approx \frac{1}{M}\sum_{m=1}^{M}f(x, \theta_m)</script><p>其中$f(x, \theta_m)$为第m次应用丢弃方法后的网络，其参数$\theta_m$为全部参数$\theta$的一次采样。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>深层神经网络的训练需要大量的样本才能取得不错的效果，因为在数据量有限的情况下，可以通过 <strong>数据增强（Data Augmentation）</strong>来增加数据量，提高模型鲁棒性，避免过拟合。目前数据增强主要应用在图像数据上，在文本等其他类型的数据还没有太好的方法。</p><p>图像数据增强主要通过算法对图像进行转换，引入噪声方法增强数据的多样性，增强的方法主要有：</p><ul><li>转换（Rotation）：将图像按照顺时针或者逆时针方向随机旋转一定的角度；</li><li>翻转（Flip）：将图像沿水平或者垂直方向随机翻转一定的角度；</li><li>缩放（Zoom in/out）：将图像放大或者缩小一定的比例；</li><li>平移（Shift）：将图像按照水平或者垂直的方法平移一定步长；</li><li>加噪声（Noise）：加入随机噪声。</li></ul><h3 id="标签平滑"><a href="#标签平滑" class="headerlink" title="标签平滑"></a>标签平滑</h3><p>在数据增强中，可以通过给样本加入随机噪声来避免过拟合，同样也可以给样本的标签引入一定的噪声。假设在训练数据集中，有一些样本的标签是被错误标注的，那么最小化这些样本上的损失函数会导致过拟合。一种改善的正则化方法是<strong>标签平滑（label smothing）</strong>，即在输出标签中随机加入噪声，来避免模型过拟合。</p><p>一个样本$x$的标签一般用onehot向量表示，如下：</p><script type="math/tex; mode=display">y = [0,...,0,1,.....,1]^T</script><p>这种标签可以看作<strong>硬目标（hard targets）</strong>，如果使用softmax分类器并使用交叉熵损失函数，最小化损失函数会使得正确类和其他类权重差异很大。根据softmax函数的性质可以知道，如果要使得某一类的输出概率接近于1，其未归一化的得分要远大于其他类的得分，这样可能会导致其权重越来越大，并导致过拟合。i</p><p>此外如果标签是错误的，会导致严重的过拟合现象，为了改善这种情况，我们可以引入一个噪声会标签进行平滑，即假设样本以$\epsilon$的概率为其他类，平滑后的标签为：</p><script type="math/tex; mode=display">\tilde{y} =[ \frac{ \epsilon }{K-1} ,...,\frac{ \epsilon }{K-1} ,1- \epsilon,\frac{ \epsilon }{K-1},....,\frac{ \epsilon }{K-1}]^T</script><p>其中$K$为标签数量，这种标签可以看作是<strong>软目标（soft targets）</strong>。标签平滑可以避免模型的输出过拟合到硬目标上，并且通常不会降低其分类能力。</p><p>上边的标签平滑方法是给其他$K-1$个标签相同的概率$\frac{\epsilon}{K-1}$，没有考虑目标之间的相关性。一种更好的做法是按照类别相关性来赋予其他标签不同的概率，比如先训练另外一个更复杂的教师网络，并使用大网络的输出作为软目标进行训练学生网络，这种方法也称为知识精炼（Knowledge Distillation）。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>至此，神经网络中的网络优化和正则化（一）（二）（三）（四）篇已经完成，如下：</p><ul><li><a href="https://thinkgamer.blog.csdn.net/article/details/100996744" target="_blank" rel="external">神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/101026786" target="_blank" rel="external">神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/101033047" target="_blank" rel="external">神经网络中的网络优化和正则化（三）之超参数优化</a></li><li><a href="">神经网络中的网络优化和正则化（四）之正则化</a></li></ul><p>神经网络中的网络优化和正则化即是对立又统一的关系，一方面我们希望找到一个最优解使得模型误差最小，另一方面又不希望得到一个最优解，可能陷入过拟合。优化和正则化的目标是期望风险最小化。</p><p>目前在深层神经网络中泛化能力还没有很好的理论支持，在传统的机器学习上比较有效的$l_1,l_2$正则化在深层神经网络中作用也比较有限，而一些经验性的做法，比如随机梯度下降和提前终止，会更加有效。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍（如下所示），本篇为最后一篇主要介绍神经网络中的网络正则化。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（三）之超参数优化</title>
    <link href="http://thinkgamer.cn/2019/09/25/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/09/25/TensorFlow/神经网络中的网络优化和正则化（三）之超参数优化/</id>
    <published>2019-09-25T12:53:25.000Z</published>
    <updated>2019-12-06T03:14:43.935Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>公众号标题：神经网络中的优化方法之学习率衰减和动态梯度方向</p></blockquote><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化</li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化 </li></ul></li></ul><hr><p>无论是神经网络还是机器学习都会存在很多的超参数，在神经网络中，常见的超参数有：</p><ul><li>网络结构：包括神经元之间的连接关系，层数，每层的神经元数量，激活函数的类型等</li><li>优化参数：包括优化方法，学习率，小批量样本数量</li><li>正则化系数</li></ul><p><strong>超参数优化（Hyperparamter Optimization）</strong> 主要存在两方面的困难：</p><ul><li>超参数优化是一个组合优化问题，无法像一般参数那样通过梯度优化的方法来求解，也没有一种通用的优化方法</li><li>评估一组超参数配置时间代价很高，从而导致一些优化算法（比如时间演化算法）在超参数优化中难以应用</li></ul><p>对于超参数的设置，一般有三种比较简单的优化方法，人工搜索，网格搜素，随机搜索</p><hr><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>网格搜索（grid search）是一种通过尝试所有超参数的组合来寻找一组合适的超参数组合的方法。如果参数是连续，可以将其离散化。比如“学习率”，我们可以根据经验选取几个值:$\alpha \in {0.01, 0.1, 0.5, 1.0}$。</p><p>一般而言，对于连续的超参数，不能采用等间隔的方式进行划分，需要根据超参数自身的特点进行离散化。</p><p>网格搜索根据不同的参数组合在测试集上的表现，选择一组最优的参数作为结果。</p><h3 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h3><p>不同超参数对模型的影响不同，有的超参数（比如正则项系数）对模型的影响有限，有的超参数（比如学习率）对模型的影响比较大，这时候采用网格搜索就会在影响不大的超参数上浪费时间。</p><p>一种在实践中比较有效的方法是对超参数进行随机组合（比如不太重要的参数进行随机抽取，重要的参数可以按照网格搜索的方式选择），选择表现最好的参数作为结果,这就是<strong>随机搜索（random search）</strong></p><blockquote><p>网格搜索和随机搜索没有利用超参数之间的相关性，即如果模型的超参数组合比较类似，其模型的性能表现也是比较接近的，这时候网格搜索和随机搜索就比较低效。下面介绍两种自适应的超参数优化方法：贝叶斯优化和动态资源分配。</p></blockquote><hr><h3 id="动态资源分配"><a href="#动态资源分配" class="headerlink" title="动态资源分配"></a>动态资源分配</h3><p>在超参数优化中，每组超参数配置的评估代价很高，如果我们可以在较早的阶段就估计出该组超参数效果就比较差，然后提前终止该组参数的测试，从而将更多的资源留给其他。这个问题可以归结为<strong>多臂赌博机问题</strong>的一个泛化问题，即<strong>最优臂问题（best-arm problem）</strong>，即在给定有限次数的情况下，如何获取最大收益。</p><p>动态资源分配的一种有效方法是<strong>逐层减半（successive halving）</strong>，将超参数优化看作是一种非随机的最优臂问题。该方法出自2015年的一篇论文，论文下载地址为：<a href="https://arxiv.org/pdf/1502.07943.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1502.07943.pdf</a></p><p>假设要尝试N组超参数配置，总共可利用的摇臂资源次数为B，我们可以通过$T= [log_2N]-1$轮逐次减半的方法来选取最优的配置，具体计算过程如下：</p><p><img src="https://img-blog.csdnimg.cn/20190919191933874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="逐次减半"></p><blockquote><p>在逐次减半方法中，N的设置十分重要，如果N越大，得到最佳配置的机会也越大，但每组配置分配到的资源就越少，这样早期的评估结果可能不准确，反之，如果N越小，每组超参数配置的评估就会越准确，但也有可能无法得到最优的参数配置。因此如何设置N是评估“利用-探索”的一个关键因素，一种改进的方法是：HyperBrand方法，通过尝试不同的N来寻找最优的参数配置。对应的论文下载地址为：<a href="https://openreview.net/pdf?id=ry18Ww5ee" target="_blank" rel="external">https://openreview.net/pdf?id=ry18Ww5ee</a></p></blockquote><hr><h3 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h3><h4 id="贝叶斯优化背后的思想"><a href="#贝叶斯优化背后的思想" class="headerlink" title="贝叶斯优化背后的思想"></a>贝叶斯优化背后的思想</h4><p>贝叶斯优化（Bayesian optimization）是一种自适应的超参数优化方法，根据当前已经试验的超参数组合，来预测下一个可能带来最大收益的组合。</p><blockquote><p>对于同一个算法来讲，不同的超参数组合其实是对应不同的模型，而贝叶斯优化可以帮助我们在众多模型中寻找性能最优的模型，虽然我们可以使用交叉验证的思想寻找更好的超参数组合，但是不知道需要多少样本才能从一系列候选模型中选择出最优的模型。这就是为什么贝叶斯优化能够减少计算任务加速优化过程的进程，同样贝叶斯优化不依赖于人为猜测需要样本量的多少，这种优化计算是基于随机性和概率分布得到的。<br><br><br>简单来说，当我们把第一条样本送到模型中的时候，模型会根据当前的样本点构建一条直接，当把第二天样本送到模型中的时候，模型将结合这两个点并从前面的线出发绘制一条修正的线，当输送第三个样本的时候，模型绘制的就是一条非线性曲线，当样本数据增加时，模型所结合的曲线就会变得更多，这就像统计学里的抽样定理，即我们从样本参数出发估计总体参数，且希望构建出的估计量与总体参数相合，无偏估计。<br><br><br>下图为非线性目标函数曲线图，对于给定的目标函数，在输送了所有的观察样本之后，它将搜寻到最大值，即寻找令目标函数最大的参数（arg max）。<br><img src="https://img-blog.csdnimg.cn/20190920122716823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="非线性目标函数曲线图"><br>我们的目标并不是使用尽可能多的样本去完全推断未知的目标函数，而是希望能求得使目标函数最大化的参数，所以我们将注意力从曲线上移开，当目标函数组合能提升曲线形成分布时，其就可以称为采集函数（Acquisition funtion），这就是<strong>贝叶斯优化背后的思想</strong>。（灰色区域部分参考：<a href="https://www.jiqizhixin.com/articles/2017-08-18-5）" target="_blank" rel="external">https://www.jiqizhixin.com/articles/2017-08-18-5）</a></p></blockquote><h4 id="时序模型优化"><a href="#时序模型优化" class="headerlink" title="时序模型优化"></a>时序模型优化</h4><p>一种常用的贝叶斯优化方法为时序模型优化（Sequential Model-Based Optimization，SMBD），假设超参数优化的函数f(x)服从高斯过程，则$p(f(x)|x)$为一个正态分布。贝叶斯优化过程是根据已有的N组实验结果$H={x_n,y_n}, n\in(1,N)$（$y_n$为$f(x_n)$的观测值）来建模高斯过程，并计算$f(x)$的后验分布$p(f(x)|x,H)$。</p><p>为了使得$p(f(x)|x,H)$接近其真实分布，就需要对样本空间进行足够多的采样，但是超参数优化中每一个样本的生成成本都很高，需要使用尽可能少的样本来使得$p_\theta(f(x)|x,H)$接近于真实分布。因此需要定义一个收益函数（Acquisition funtion）$\alpha (x, H)$来判断一个样本能否给建模$p_\theta(f(x)|x,H)$提供更多的收益。收益越大，其修正的高斯过程会越接近目标函数的真实分布。</p><p>收益函数的定义有很多方式，一个常用的是期望改善（Expected Improvement，EI）。假设$y^* = min \left \{  y_n, 1 \leq n \leq N \right \}$是当前已有样本中的最优值，期望改善函数为：</p><script type="math/tex; mode=display">EI(x, H) = \int_{-\infty }^{ +\infty } max (y^* - y, 0) p(y|x, H) dy</script><p>期望改善是定义一个样本$x$在当前模型$p(f(x)|x,H)$下，$f(x)$超过最好结果$y^*$的期望。除了期望改善函数之外，收益函数还有其他函数的定义，比如改善概率（Probability Of Improvement），高斯过程置信上界（GP Up Confidence Bound，GP-UCB）等。</p><p>时序模型优化过程如下所示：<br><img src="https://img-blog.csdnimg.cn/20190920150457198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="时序模型优化过程"></p><p>贝叶斯优化的缺点是高斯建模过程需要计算矩阵的逆，时间复杂度为$O(n^3)$，因此不能很好的处理高维过程，深层神经网络的参数一般比较多，需要更加高效的高斯过程建模，也有一些方法将时间复杂度从$O(n^3)$降到了$O(n)$。</p><blockquote><p>至此，超参数优化部分已经介绍完成，这里并没有对超参数优化进行实现，有很多Python库已经对其进行了封装，感兴趣的可以关注下，另外贝叶斯优化在日常实践中用的比较多但是不太好理解，可以多看几遍，对比一些文章什么看下理解下。</p></blockquote><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;公众号标题：神经网络中的优化方法之学习率衰减和动态梯度方向&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化</title>
    <link href="http://thinkgamer.cn/2019/09/22/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96:%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86:%E9%80%90%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/09/22/TensorFlow/神经网络中的网络优化和正则化（二）之参数初始化:数据预处理:逐层归一化/</id>
    <published>2019-09-22T13:32:55.000Z</published>
    <updated>2019-12-06T03:14:43.935Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化</li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化 </li></ul></li></ul><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><h4 id="对称权重现象"><a href="#对称权重现象" class="headerlink" title="对称权重现象"></a>对称权重现象</h4><p>在上一篇文章中我们提到神经网络中的参数学习是基于梯度下降的，而梯度下降需要赋予一个初始的参数，所以这个参数的初始化就显得特别重要。</p><p>在感知器和逻辑回归中，一般将参数初始化为0，但是在神经网络中如果把参数初始化为0，就会导致在第一次前向计算时，所有隐藏层神经元的激活值都相同，这样会导致深层神经元没有区分性，这种现象称为<strong>对称权重现象</strong></p><p>因此如果要高质量的训练一个网络，给参数选择一个合适的初始化区间是非常重要的，一般而言，参数初始化的区间应该根据神经元的性质进行差异化的设置，如果一个神经元的输入过多，权重就不要设置太大，以避免神经元的的输出过大（当激活函数为ReLU时）或者过饱和（激活函数为Sigmoid函数时）。<br>关于神经网络中的激活函数介绍可参考：</p><blockquote><p><a href="https://blog.csdn.net/Gamer_gyt/article/details/89440152" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/89440152</a></p></blockquote><p>常见的参数初始化方法包括以下两种。</p><h4 id="Gaussian初始化"><a href="#Gaussian初始化" class="headerlink" title="Gaussian初始化"></a>Gaussian初始化</h4><p>高斯初始化是最简单的初始化方法，参数服从一个固定均值和固定方差的高斯分布进行随机初始化。</p><p>初始化一个深度网络时，一个比较好的初始化方案是保持每个神经元输入的方差是一个常量，当一个神经元的输入连接数量为n时，可以考虑其输入连接权重以$N(0,\sqrt{\frac{1}{n}})$的高斯分布进行初始化，如果同时考虑神经元的输出连接数量为m时，可以按照$N(0,\sqrt{\frac{2}{m+n}})$进行高斯分布初始化。</p><h4 id="均匀分布初始化"><a href="#均匀分布初始化" class="headerlink" title="均匀分布初始化"></a>均匀分布初始化</h4><blockquote><p>均匀初始化是指在一个给定的区间[-r,r]内采用均匀分布来初始化参数，超参数r的设置也可以根据神经元的连接数量来进行自适应调整。</p></blockquote><p><strong>Xavier初始化方法</strong>是一种自动计算超参数r的方法，参数可以在[-r,r]之间采用均匀分布进行初始化。</p><p>如果神经元激活函数为logistic函数，对于第l-1层到第l层的权重参数区间可以设置为：</p><script type="math/tex; mode=display">r = \sqrt{ \frac{6}{ n^{l-1} + n^l}}</script><p>$n^l$ 表示第l层神经元的个数，$n^{l-1}$表示l-1层神经元的个数。</p><p>如果是tanh激活函数，权重参数区间可以设置为：</p><script type="math/tex; mode=display">r =4 \sqrt{ \frac{6}{ n^{l-1} + n^l}}</script><blockquote><p>在实际经验中，Xavier初始化方法用的比较多。</p></blockquote><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="为什么要进行数据预处理"><a href="#为什么要进行数据预处理" class="headerlink" title="为什么要进行数据预处理"></a>为什么要进行数据预处理</h4><p>一般情况下，在原始数据中，数据的维度往往不一致，比如在电商数据中，某个商品点击的次数往往要远大于购买的次数，即<strong>特征的分布范围差距很大</strong>，这样在一些使用余弦相似度计算的算法中，较大的特征值就会起到绝对作用，显然这样做是极其不合理的。同样在深度神经网络中，虽然可以通过参数的调整来自适应不同范围的输入，但是这样训练的效率也是很低的。</p><p>假设一个只有一层的网络 $y=tanh(w_1x_1 + w_2 x_2 +b)$，其中$x_1 \in [0,10], x_2 \in [0,1]$。因为激活函数 tanh的导数在[-2,2]之间是敏感的，其余的值域导数接近0，因此$w_1x_1 + w_2 x_2 +b$过大或者过小都会影响训练，为了提高训练效率，我们需要把$w_1x_1 + w_2 x_2 +b$限定在[-2,2]之间，因为$x_1,x_2$的取值范围，需要把$w_1$设置的小一些，比如在[-0.1, 0.1]之间，可以想象，如果数据维度比较多的话，我们需要精心的去设置每一个参数，<strong>但是如果把特征限定在一个范围内，比如[0,1]</strong>，我们就不需要太区别对待每一个参数。</p><p>除了参数初始化之外，不同特征取值范围差异比较大时也会影响梯度下降法的搜索效率，下图（图1-1）给出了数据归一化对梯度的影响，对比等高线图可以看出，归一化后，梯度的位置方向更加接近于最优梯度的方向。</p><p><img src="https://img-blog.csdnimg.cn/20190919111801183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="数据归一化对梯度的影响"></p><h4 id="数据预处理的方法"><a href="#数据预处理的方法" class="headerlink" title="数据预处理的方法"></a>数据预处理的方法</h4><p>关于原始数据归一化的方法有很多，可以参考《推荐系统开发实战》中第四章节部分内容，写的很全面，而且有对应的代码实现。该书的购买链接：</p><blockquote><p><a href="https://item.jd.com/12671716.html" target="_blank" rel="external">点击查看详情-京东链接</a></p></blockquote><h3 id="逐层归一化"><a href="#逐层归一化" class="headerlink" title="逐层归一化"></a>逐层归一化</h3><h4 id="深层神经网络中为什么要做逐层归一化"><a href="#深层神经网络中为什么要做逐层归一化" class="headerlink" title="深层神经网络中为什么要做逐层归一化"></a>深层神经网络中为什么要做逐层归一化</h4><p>在深层神经网络中，当前层的输入是上一层的输出，因此之前层参数的变化对后续层的影响比较大，就像一栋高楼，低层很小的变化就会影响到高层。</p><p>从机器学习的角度去看，如果某个神经网络层的输入参数发生了变化，那么其参数需要重新学习，这种现象叫做<strong>内部协变量偏移（Internal Covariate Shift）</strong>。</p><blockquote><p>这里补充下机器学习中的协变量偏移（Covariate Shift）。协变量是一个统计学的概念，是影响预测结果的统计变量。在机器学习中，协变量可以看作是输入，一般的机器学习都要求输入在训练集和测试集上的分布是相似的，如果不满足这个假设，在训练集上得到的模型在测试集上表现就会比较差。<br><img src="https://img-blog.csdnimg.cn/20190919130030578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="协变量偏移"></p></blockquote><p>为了解决内部协变量偏移问题，需要对神经网络层的每一层输入做归一化，下面介绍几种常见的方法：</p><ul><li>批量归一化</li><li>层归一化</li><li>其他方法</li></ul><h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>为了减少内部协变量偏移的影响，需要对神经网络每一层的净输入$z^l$进行归一化，相当于每一层都要做一次数据预处理，从而加快收敛速度，但是因为对每一层都进行操作，所以要求归一化的效率要很高，一般使用标准归一化，将净输入$z^l$的每一维都归一到标准正态分布，其公式如下：</p><script type="math/tex; mode=display">\hat{z}^l = \frac{ z^l - E[z^l] }{ \sqrt{var(z^l) + \epsilon } }</script><p>$E[z^l]，var(z^l)$表示在当前参数下$z^l$的每一维在整个训练集上的期望和方差，因为深度神经网络采用的是下批量的梯度下降优化方法，基于全部样本计算期望和方差是不可能的，因为通常采用小批量进行估计。给定一个包含K个样本的集合，第$l$层神经元的净输入$z^{(1,l)},….z^{(K,l)}$的均值和方差为：</p><script type="math/tex; mode=display">\mu _\beta = \frac{1 }{ K } \sum_{k=1}^{K } z^{(k,l)}\\\sigma^2 _\beta = \frac{1 }{ K }\sum_{ k=1}^{K} (z^{(k,l)} - \mu _\beta)^2</script><p>对净输入$z^l$的标准归一化会使其取值集中在0附近，这样当使用sigmoid激活函数时，这个取值空间刚好接近线性变换的空间，减弱了神经网络的非线性性质。因此为了不使归一化对网络产生影响，需要对其进行缩放和平移处理，公式如下：</p><script type="math/tex; mode=display">\hat{z}^l = \frac{ z^l - \mu _\beta }{ \sqrt{\sigma^2 _\beta+ \epsilon } } \odot \gamma  + \beta</script><p>其中$\gamma  , \beta$分别代表缩放和平移的向量。</p><blockquote><p>这里需要注意的是每次小批量样本的均值和方差是净输入$z^l$的函数，而不是常量因此在计算梯度时要考虑到均值和方差产生的影响，当训练完成时，用整个数据集上的均值和方差来代替每次小样本计算得到的均值和方差。在实际实践经验中，小批量样本的均值和方差也可以使用移动平均来计算。</p></blockquote><h4 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h4><p>批量归一化的操作对象是单一神经元，因此要求选择样本批量的时候，不能太小，否则难以计算单个神经元的统计信息，另外一个神经元的输入是动态变化的，比如循环神经网络，那么就无法应用批量归一化操作。</p><p><strong>层归一化（Layer Normalization）</strong> 是和批量归一化非常类似的方法，但层归一化的操作对象是某层全部神经元。</p><p>对于深层神经网络，第$l$层神经元的净输入为$z^l$，其均值和方差为：</p><script type="math/tex; mode=display">u^l = \frac{1}{n^l} \sum_{i=1}^{ n^l} z_i^l\\\sigma ^2_l = \frac{1}{n^l} \sum_{i=1}^{ n^l} (z_i^l - u^l )</script><p>其中$n^l$为第$l$层神经元的数量。则层归一化定义为：</p><script type="math/tex; mode=display">\hat{z^l} = \frac{z^l - u^l }{ \sqrt {\sigma^2 _l + \epsilon } } \odot \gamma  + \beta</script><p>其中$\gamma ,\beta$分别代表缩放和平移的向量，和$z^l$的维度相同。</p><p><strong>循环神经网络中的层归一化</strong>为：</p><script type="math/tex; mode=display">z_t = U h_{t-1} + W x_t\\h_t = f(\hat{z^l})</script><p>其中隐藏层为$h_t$，$x_t$为第$t$时刻的净输入，$U,W$为参数。</p><blockquote><p>在标准循环网络中，循环神经层的输入一般就随着时间慢慢变大或者变小，从而引起梯度爆炸或者梯度消失，而层归一化的神经网络可以有效的缓解这种状况。</p></blockquote><h4 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h4><p>除了上面介绍的两种归一化方法之外，还有一些其他的一些归一化方法，感兴趣的可以自行搜索查看。</p><ul><li>权重归一化(Weight Normalization)</li><li>局部响应归一化</li></ul><blockquote><p>至此，神经网络中的优化方法第二部分介绍完成，主要包好了三部分内容：参数初始化，数据预处理和逐层归一化。再下一篇将会重点介绍超参数优化的方法不仅适用于深度神经网络，也适用于一般的机器学习任务。如果你觉得不错，分享一下吧！</p></blockquote><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一篇包括&lt;ul&gt;
&lt;li&gt;网络优化和正则
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向</title>
    <link href="http://thinkgamer.cn/2019/09/22/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E5%92%8C%E5%8A%A8%E6%80%81%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91/"/>
    <id>http://thinkgamer.cn/2019/09/22/TensorFlow/神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向/</id>
    <published>2019-09-22T13:25:01.000Z</published>
    <updated>2019-12-06T03:14:43.934Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。</p><ul><li>第一篇包括<ul><li>网络优化和正则化概述</li><li>优化算法介绍</li></ul></li><li>第二篇包括<ul><li>参数初始化</li><li>数据预处理</li><li>逐层归一化</li></ul></li><li>第三篇包括<ul><li>超参数优化</li></ul></li><li>第四篇包括<ul><li>网络正则化 </li></ul></li></ul><hr><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>虽然神经网络有比较强的表达能力，但是应用神经网络到机器学习任务时仍存在一些问题，主要分为：</p><ol><li>网络优化<blockquote><p>神经网络模型是一个非凸函数，再加上神经网络中的梯度消失和梯度爆炸，很难进行优化，另外网络的参数比较多，且数据量比较大导致训练效率比较低。</p></blockquote></li><li>正则化<blockquote><p>神经网络拟合能力强，容易在训练集上产生过拟合，需要一些正则化的方法来提高网络的泛化能力。</p></blockquote></li></ol><p>从大量的实践经验看主要是从网络优化和正则化两个方面提高学习效率并得到一个好的网络模型。</p><p>在低维空间的非凸优化问题中主要是存在一些局部最优点，基于梯度下降优化算法会陷入局部最优点，因此低维空间的非凸优化的难点在于如何选择合适的参数和逃离局部最优点。</p><p>深层神经网络中参数较多，其是在高维空间的非凸优化问题中，和低维空间的非凸优化有些不同，其主要难点在于如何逃离鞍点（Saddle Point），鞍点的梯度为0，但是在一些维度上是最高点，在另一些维度上是最低点，如下图所示（图1-1）：<br><img src="https://img-blog.csdnimg.cn/2019091814412421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="鞍点示例"></p><p>在高维空间中，局部最优点要求在每一维度上都是最低点，这种概率很低，假设网络有1000<br>个参数，每一维上取得局部最优点的最小概率为p，则在整个参数空间中取得局部最优点的最小概率为$p^{1000}$，这种概率很小，也就是说在整个参数空间中，大部分梯度为0的点都是鞍点。</p><hr><h3 id="优化算法介绍"><a href="#优化算法介绍" class="headerlink" title="优化算法介绍"></a>优化算法介绍</h3><p>深层神经网络的参数学习主要是通过梯度下降算法寻找一组最小结构的风险参数，梯度下降分为：</p><ul><li>批量梯度下降</li><li>随机梯度下降</li><li>小批量梯度下降</li></ul><p>根据不同的数据量和参数量，可以选择一种合适的梯度下降优化算法，除了在收敛效果和效率上的区别，这三种梯度下降优化算法还存在一些共同问题（<strong>具体会在下一篇进行详细介绍</strong>）：</p><ul><li>如何初始化参数</li><li>预处理数据</li><li>如何选择合适的学习率，避免陷入局部最优</li></ul><p>在训练深层神经网络时，通常采用小批量梯度下降算法。令$f(x,\theta)$为一个深层神经网络，$\theta$为网络参数，使用小批量梯度优化算法时，每次选择K个训练样本$I_t =\left \{ (x^t,y^t)  \right \} , t \in (1,T)$，第t次迭代时损失函数关于$\theta$的偏导数为（公式1-1）：</p><script type="math/tex; mode=display">g_t(\theta ) = \frac{ 1 }{ K } \sum_{ (x^t,y^t) \in I_t} \frac{ \partial L(y^t,f(x^t, \theta)) }{ \partial \theta }</script><p>第t次更新的梯度$g’_t$定义为（公式1-2）：</p><script type="math/tex; mode=display">g_t'(\theta)= g_t(\theta_{t-1})</script><p>使用梯度下降来更新参数（公式1-3）：</p><script type="math/tex; mode=display">\theta_t = \theta_{t-1} - \alpha g'(\theta)</script><p>一般批量较小时，需要选择较小的学习率，否则模型不会收敛。下图（图1-2）给出了在Mnist数据集上批量大小对梯度的影响。从图1-2(a)可以看出，批量大小设置的越大，下降的越明显，并且下降的比较平滑，当选择批量的大小为1时，整体损失呈下降趋势，但是局部比较震荡。从图1-2(b)可以看出，如果按整个数据集上的迭代次数（Epoch）来看损失变化情况，则是批量样本数越小，下降效果越明显。</p><p><img src="https://img-blog.csdnimg.cn/20190918153354525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="批量的大小对梯度的影响"></p><p>为了更加有效的训练深层神经网络，在标准的小批量梯度下降算法中，经常使用一些改进方法加快优化速度，常见的改进方法有两种：</p><ul><li>学习率衰减</li><li>梯度方向优化</li></ul><blockquote><p>这些改进的优化方法也同样可以应用在批量梯度下降算法和随机梯度下降算法。</p></blockquote><hr><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>在梯度下降中，学习率的设置很重要，设置过大，则不会收敛，设置过小，则收敛太慢。从经验上看，学习率在一开始要设置的大些来保证收敛速度，在收敛到局部最优点附近时要小些来避免震荡，因此比较简单的学习率调整可以通过学习率衰减（Learning Rate Decay）的方式来实现。假设初始学习率为$\alpha_0$，第t次迭代的学习率为$a_t$，常用的衰减方式为按照迭代次数进行衰减，例如</p><ul><li>逆时衰减（公式1-4）</li></ul><script type="math/tex; mode=display">a_t = a_0 \frac{1 }{ 1 + \beta t}</script><ul><li>指数衰减（公式1-5）</li></ul><script type="math/tex; mode=display">a_t = a_0\beta^t</script><ul><li>自然指数衰减（公式1-6）</li></ul><script type="math/tex; mode=display">a_t = a_0 exp(-\beta * t)</script><p>其中$\beta$为衰减率，一般为0.96</p><h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>AdaGrad（Adaptive Gradient）算法是借鉴L2正则化的思想，每次迭代时自适应的调整每个参数的学习率。AdaGrad的参数更新公式为（公式1-7）：</p><script type="math/tex; mode=display">G_t = \sum_{t=1}^{T} g_t \odot g_t\\\bigtriangleup \theta_t = - \frac{\alpha }{ \sqrt{G_t + \epsilon  } } \odot g_t\\g'_t(\theta) = g_t(\theta_{t-1}) + \bigtriangleup \theta_t</script><p>其中$\alpha$为学习率，$\epsilon$是为了保证数据稳定性而设置的非常小的常数，一般取值是 $e^{-7}$到$e^{-10}$，这里的开平方，加，除运算都是按照元素进行的操作。</p><p>在AdaGrad算法中，如果某个参数的偏导数累积比较大，其学习率相对较小，相反，如果其偏导数累积比较大，其学习率相对较大。但是整体上随着迭代次数的增加，学习率逐渐减小。</p><p>AdaGrad算法的缺点是在经过一定次数的迭代后依然没有找到最优点，由于这时候的学习率已经很小了，就很难找到最优点。</p><h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>RMSProp是Geoff Hinton提出的一种自适应学习率的方法，可以在有些情况下避免AdaGrad的学习率单调递减以至于过早衰减的缺点。</p><p>RMSProp算法首先计算的是每次迭代速度$g_t$平方的指数衰减移动平均，如下所示（公式1-8）：</p><script type="math/tex; mode=display">G_t = \beta G_{t-1}  + (1-\beta) g_t \odot g_t = (1- \beta) \sum_{t=1}^{T} \beta ^{T-t} g_t \odot g_t</script><p>其中$\beta$为衰减率，一般取值为0.9，RMSProp算法参数更新公式为（公式1-9）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = - \frac{\alpha }{ \sqrt{G_t + \epsilon  } } \odot g_t\\g'_t(\theta) = g_t(\theta_{t-1}) + \bigtriangleup \theta_t</script><p>其中$\alpha$为学习率，通常为0.001。</p><blockquote><p>从公式1-8 可以看出，RMSProp和AdaGrad的区别在于$G_t$的计算由累积方式变成了指数衰减移动平均，在迭代过程中，每个参数的学习率并不是呈衰减趋势，即可以变大，也可以变小。</p></blockquote><h4 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h4><p>AdaDelta算法也是AdaGrad算法的一个改进，和RMSProp算法类似，AdaDelta算法通过梯度平方的指数衰减移动平均来调整学习率，除此之外，AdaDelta算法还引入了每次参数更新差$\bigtriangleup \theta$的平方的指数衰减移动平均。</p><p>第t次迭代时，每次参数更新差$\bigtriangleup \theta_t , 1&lt;t&lt;T-1$的指数衰减移动平均为（公式1-10）：</p><script type="math/tex; mode=display">\bigtriangleup X^2_{t-1} =\beta _1 \bigtriangleup X^2_{t-2} + (1 - \beta) \bigtriangleup \theta_{t-1} \odot \bigtriangleup \theta_{t-1}</script><p>其中$\beta_1$为衰减率，AdaDelta算法的参数更新差值为（公式1-11）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = - \frac{\sqrt {\bigtriangleup X^2_{t-1} + \epsilon }}{ \sqrt {G_t + \epsilon}} g_t</script><blockquote><p>其中$G_t$的计算方式和RMSProp算法一样。从公式1-11可以看出，AdaDelta算法将RMSProp算法中的初始学习率$\alpha$改为动态计算的$\sqrt {\bigtriangleup X^2_{t-1} + \epsilon }$，在一定程度上减缓了学习旅率的波动。</p></blockquote><hr><h3 id="梯度方向优化"><a href="#梯度方向优化" class="headerlink" title="梯度方向优化"></a>梯度方向优化</h3><p>除了调整学习率外，还可以使用最近一段时间内的平均梯度来代替当前时刻的梯度来作为参数的更新方向，从图1-2中可以看出，在小批量梯度下降中，如果每次选取样本数量比较小，损失就会呈现震荡的方式下降，有效的缓解梯度下降中的震荡的方式是通过用梯度的移动平均来代替每次的实际梯度。并提高优化速度，这就是<strong>动量法</strong>。</p><h4 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h4><p>动量法（Momentum Method）是用之前积累的动量来替代真正的梯度，每次替代的梯度可以看作是加速度。</p><p>在第t次迭代时，计算负梯度的“加权移动平均”作为参数的更新方向，如下所示（公式1-12）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = \rho \bigtriangleup \theta_{t-1}-\alpha g_t</script><p>其中$\rho$为动量因子，通常设置为0.9，$\alpha$为学习率。</p><blockquote><p>参数的实际更新值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内梯度方向不一致时，参数更新的幅度变小，相反，参数更新的幅度变大，起到加速的作用。</p><p>一般而言，在迭代初期，梯度的更新方向比较一致，动量法会起到加速作用，可以更快的起到加速的作用，可以更快的到达最优点，在迭代后期，梯度的更新方向不一致，在收敛时比较动荡，动量法会起到减速作用，增加稳定性。从某种程度来讲，当前梯度叠加上部分的上次梯度，一定程度上可以看作二次梯度。</p></blockquote><h4 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h4><p>Nesterov加速梯度（Nesterov Accelerated Gradient， NAG）也叫Nesterov动量法（Nesterov Momentum），是一种对动量法的改进。</p><p>在动量法中，实际的参数更新方向$\bigtriangleup \theta_t$为上一步的参数更新方向$\bigtriangleup \theta_{t-1}$和当前的梯度 $-g_t$的叠加，这样，$\bigtriangleup \theta_t$可以拆分为两步进行，先根据$\bigtriangleup \theta_{t-1}$更新一次得到参数$\tilde{\theta }$，再用$g_t$进行更新，如下所示（公式1-13）：</p><script type="math/tex; mode=display">\tilde{\theta } = \theta_{t-1} + \rho \bigtriangleup \theta_{t-1} \\\theta_t = \tilde{\theta } - \alpha g_t</script><p>其中$g_t$为点$\theta_{t-1}$上的梯度，所以第二步不太合理，更合理的更新方向为$\tilde{\theta }$上的梯度，这样合并后的更新方向为（公式1-14）：</p><script type="math/tex; mode=display"> \bigtriangleup \theta_t =  \rho \bigtriangleup \theta_{t-1} -\alpha g_t(\theta_{t-1} + \rho \bigtriangleup \theta_{t-1} )</script><p>其中$g_t(\theta_{t-1} + \rho \bigtriangleup \theta_{t-1} )$表示损失函数在$\tilde{\theta } = \theta_{t-1} + \rho \bigtriangleup \theta_{t-1}$上的偏导数。</p><p>下图（图1-3）给出了动量法和 Nesterov 加速梯度在参数更新时的比较：</p><p><img src="https://img-blog.csdnimg.cn/20190918192321930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="动量法和 Nesterov 加速梯度在参数更新时的比较"></p><h4 id="AdaM算法"><a href="#AdaM算法" class="headerlink" title="AdaM算法"></a>AdaM算法</h4><p>自适应动量估计算法（Adaptive Moment Estimation，Adam）可以看作是动量法和RMSprop的结合，不但使用动量作为参数更新，而且可以自适应调整学习率（公式1-15）。</p><script type="math/tex; mode=display">M_t = \beta _1M_{t-1} + (1-\beta _1)g_t\\ G_t = \beta _2 G_{t-1} + (1-\beta _2)g_t \odot g_t</script><p>其中$\beta_1 ，\beta_2$分别为两个移动平均的衰减率，通常取值：$\beta_1=0.9,\beta_2=0.99$。</p><p>$M_t$可以看作是梯度的均值（一阶矩），$G_t$可以看作是梯度的未减去均值的方差（二阶矩）。</p><p>假设$M_t =0,G_t=0$，那么在迭代初期，$M_t，G_t$的值会比真实的均值和方差要小，特别是当$\beta_1 ，\beta_2$都接近1时，偏差会很大，因此需要对偏差进行修正，如下所示（公式1-16）：</p><script type="math/tex; mode=display">\tilde{M_t} = \frac{M_t}{ 1 - \beta^t _1}\\\tilde{G_t} = \frac{G_t}{ 1 - \beta^t _2}</script><p>Adam算法的更新差值为（公式1-17）：</p><script type="math/tex; mode=display">\bigtriangleup \theta_t = - \frac{\alpha }{\sqrt{ \tilde{G_t} + \varepsilon  }} \tilde{M_t}</script><p>其中学习率$\alpha$通常设置为0.001，并且也可以进行衰减，比如$a_t = \frac{a_0} { \sqrt{t}}$。</p><blockquote><p>Adam算法是RMSprop与动量法的结合，因此一种自然的Adam改进方法是引入Nesterov加速梯度，称为Nadam算法。</p></blockquote><h4 id="梯度截断"><a href="#梯度截断" class="headerlink" title="梯度截断"></a>梯度截断</h4><p>在深层神经网络或者循环网络中，除了梯度消失之外，梯度爆炸是影响学习效率的主要隐私，在基于梯度下降的优化过程中，如果梯度突然增大，用较大的梯度更新参数，反而会使结果远离最优点，为了避免这种情况，当梯度达到一定值的时候，要进行梯度截断（gradient clipping）。</p><p>梯度截断是一种比较简单的启发式方法，把梯度的模限定在一个范围内，当梯度的模大于或者小于某个区间时，就进行截断，一般截断的方式有以下几种：</p><ul><li>按值截断</li></ul><p>在第t次迭代时，梯度为$g_t$，给的一个区间[a,b]，如果梯度小于a时，令其为a，大于b时，令其为b。</p><ul><li>按模截断<br>将梯度的模截断到一个给定的截断阈值b。如果$||g_t||^2 \leq b$保持梯度不变，如果$||g_t||^2 &gt; b$，则$g_t= \frac{ b}{||g_t||} g_t$。</li></ul><p>截断阈值 b 是一个超参数,也可以根据一段时间内的平均梯度来自动调整。实验中发现,训练过程对阈值 b 并不十分敏感,通常一个小的阈值就可以得到很好的结果。</p><blockquote><p>在训练循环神经网络时，按模截断是避免梯度爆炸的有效方法。</p></blockquote><hr><h3 id="优化算法总结"><a href="#优化算法总结" class="headerlink" title="优化算法总结"></a>优化算法总结</h3><p>本文介绍了神经网络中的网络优化和正则化概述，以及网络优化中的加快网络优化的两种方法，这些方法大体分为两类：</p><ul><li>调整学习率，使得优化更稳定<blockquote><p>比如：AdaGrad，RMSprop，AdaDelta</p></blockquote></li><li>调整梯度方向，优化训练速度<blockquote><p>比如：动量法，Nesterov加速梯度，梯度截断</p></blockquote></li></ul><p>Adam则是RMSprop 和 动量法的结合。</p><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;神经网络中的网络优化和正则化问题介绍主要分为一，二，三，四篇进行介绍。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一篇包括&lt;ul&gt;
&lt;li&gt;网络优化和正则
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(3)-循环神经网络（下）篇</title>
    <link href="http://thinkgamer.cn/2019/09/18/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(3)-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%8B%EF%BC%89%E7%AF%87/"/>
    <id>http://thinkgamer.cn/2019/09/18/TensorFlow/常见的五种神经网络(3)-循环神经网络（下）篇/</id>
    <published>2019-09-18T00:14:54.000Z</published>
    <updated>2019-12-06T03:14:43.930Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/100943664</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>常见的五种神经网络系列第三篇，主要介绍循环神经网络，由于循环神经网络包含的内容过多，分为上中下三篇进行介绍，本文主要是循环神经网络（下）篇，主要介绍以下内容：</p><ul><li>长短期记忆网络（LSTM）</li><li>门控循环单元网络（GRU）</li><li>递归循环神经网络（RecNN）</li><li>图网络（GN）</li></ul><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><hr><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>长短期记忆（Long Short-Term Memory，LSTM）网络是循环神经网络的一个变体，可以有效的解决简单循环神经网络的梯度爆炸和梯度消失问题。</p><p>LSTM的改进包含两点：</p><ul><li>新的内部状态</li><li>门机制</li></ul><h4 id="新的内部状态"><a href="#新的内部状态" class="headerlink" title="新的内部状态"></a>新的内部状态</h4><p>LSTM网络引入一个新的内部状态（internal state）$c_t$专门进行线性的循环传递，同时（非线性）输出信息给隐藏层的外部状态$h_t$（公式3-1）。</p><script type="math/tex; mode=display">c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t}\\h_t = o_t \odot tanh(c_t)</script><p>其中 $f_t$，$i_t$，$o_t$为三个门来控制信息传递的路径，$\odot$为向量元素乘积，$c_{t-1}$为上一时刻的记忆单元，$\tilde{c_t}$是通过非线性函数得到的候选状态（公式3-2）:</p><script type="math/tex; mode=display">\tilde{c_t} = tanh( W_c x_t + U_c h_{t-1} + b_c )</script><p>在每个时刻t，LSTM网络的内部状态$c_t$记录了到当前时刻为止的历史信息。</p><h4 id="门机制"><a href="#门机制" class="headerlink" title="门机制"></a>门机制</h4><p>LSTM网络引入门机制来控制信息的传递， $f_t，i_t，o_t$分别为遗忘门，输入门，输出门。电路中门是0或1，表示关闭和开启，LSTM网络中的门是一种软门，取值在(0,1)，表示以一定比例的信息通过，其三个门的作用分别为：</p><ul><li>$f_t$：控制上一个时刻的内部状态 $c_{t-1}$需要遗忘多少信息 </li><li>$i_t$：控制当前时刻的候选状态$\tilde{c_t}$有多少信息需要保存</li><li>$o_t$：控制当前时刻的状态$c_t$有多少信息需要输出为$h_t$</li></ul><p>三个门的计算如下（公式3-3）：</p><script type="math/tex; mode=display">i_t=\sigma (W_i x_t+U_i h_{t-1} + b_i)\\f_t=\sigma (W_f x_t+U_f h_{t-1}+ b_f )\\o_t=\sigma (W_o x_t+U_o h_{t-1}+b_o)</script><p>其中$\sigma$为logsitic函数，其输出区间为(0,1)，$x_t$为当前输入，$h_{t-1}$为上一时刻的外部状态。 </p><p>下图（图3-1）给出了LSTM的循环单元结构，其计算分为三个过程：</p><ol><li>利用当前时刻的输入$x_t$和上一时刻的外部状态$h_{t-1}$计算出三个门和候选状态$\tilde{c_t}$</li><li>结合遗忘门$f_t$和输入门$i_t$来更新记忆单元$c_t$</li><li>结合输出门$o_t$将内部状态信息传递给外部状态$h_t$</li></ol><p><img src="https://img-blog.csdnimg.cn/20190917210304438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="LSTM循环单元结构"></p><p>通过LSTM循环单元，整个网络可以建立长距离的时序依赖关系，公式3-1～3-3可以简单的描述为（公式3-4）：</p><script type="math/tex; mode=display">\begin{bmatrix}\tilde{c_t}\\ o_t\\ i_t\\ f_t\end{bmatrix} = \begin{bmatrix}tanh\\ \sigma \\ \sigma \\ \sigma \end{bmatrix} (W\begin{bmatrix}x_t\\ h_{t-1}\end{bmatrix} + b)\\c_t = f_t \odot c_{t-1}+ i_t \odot \tilde{c_t}\\h_t = o_t  \odot tanh(c_t)</script><p>其中$x_t$为当前时刻的输入，$W$和$b$为网络参数。</p><blockquote><p>循环神经网络中的隐状态h存储了历史信息，可以看作是一种记忆（Memeory）。在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作一种短期记忆（Short-term Memeory）。在神经网络中，长期记忆（Long-term Memory）可以看作是网格参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆。而在LSTM网络中，记忆单元c可以在某个时刻捕捉到某个关键信息，并有能力将该信息保存一段时间，记忆单元c中保存的信息要远远长于隐状态h，但又远远短于长期记忆，因此被成为长短期记忆网络（Long Short-term Memory）。</p></blockquote><hr><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>门控单元（Gate Recurrent Unit，GRU）网络是一种比LSTM更加简单的循环神经网络。在LSTM中遗忘门和输入门是互补关系，比较冗余，GRU将遗忘门和输入门合并成一个门：更新门。同时GRU也不引入额外的记忆单元，直接在当前的状态$h_t$和上一个时刻的状态$h_{t-1}$之间引入线性依赖关系。</p><p>在GRU网络中，当前时刻的候选状态$\tilde{h_t}$为（公式3-5）：</p><script type="math/tex; mode=display">\tilde{h_t} = tanh( W_h x_h + U_h(r_t\odot h_{t-1}) + b_h )</script><blockquote><p>计算$\tilde{h_t}$时，选用tanh激活函数是因为其导数有比较大的值域，缓解梯度消失问题。</p></blockquote><p>其中$r_t \in [0,1]$ 为重置门（reset gate），用来控制候选状态$\tilde {h_t}$的计算是否依赖上一时刻的状态$h_{t-1}$，公式如下（公式3-6）：</p><script type="math/tex; mode=display">r_t = \sigma ( W_r x_t  + U_r h_{t-1} + b_r)</script><p>当 $r_t$为0 时，候选状态$\tilde{h_t}$只和当前输入$x_t$有关，和历史状态无关，当$r_t$为1时，候选状态$\tilde{h_t}$和当前输入$x_t$，历史状态$h_{t-1}$都有关，和简单循环网络一致。</p><p>GRU网络隐状态$h_t$的更新方式为（公式3-7）：</p><script type="math/tex; mode=display">h_t = z_t \odot h_{t-1}+ (1-z_t) \odot \tilde {h_t}</script><p>其中$z \in [0,1]$为更新门（update gate），用来控制当前状态需要从历史状态中保留多少信息（不经过非线性变换），以及需要从候选状态中获取多少信息。$z_t$公式如下（公式3-8）：</p><script type="math/tex; mode=display">z_t = \sigma (W_z x_t + U_z h_{t-1} + b_z)</script><ul><li>若$z_t=0$，当前状态$h_t$和历史状态$h_{t-1}$之间为非线性函数。</li><li>若$z_t=0，r=1$，GRU退化为简单循环网络</li><li>若$z_t=0，r=0$，当前状态$h_t$只和当前输入$x_t$有关，和历史状态$h_{t-1}$无关</li><li>若$z_t=1$，当前时刻状态$h_t=h_{t-1}$，和当前输入$x_t$无关</li></ul><p>GRU网络循环单元结构如下（图3-2）：</p><p><img src="https://img-blog.csdnimg.cn/20190917231241261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="GRU网络循环单元结构"></p><hr><h3 id="RecNN"><a href="#RecNN" class="headerlink" title="RecNN"></a>RecNN</h3><blockquote><p>如果将循环神经网络按时间展开，每个时刻的隐状态$h_t$看做是一个节点，那么这些节点构成一个链式结构，而链式结构是一种特殊的图结构，很容易将这种消息传递的思想扩展到任意的图结构上。</p></blockquote><p>递归神经网络（Recursive Neurnal Network，RecNN）是循环神经网络在有向无循环图上的控制，递归神经网络一般结构为树状的层次结构，如下图所示（图3-3）：</p><p><img src="https://img-blog.csdnimg.cn/20190917232047624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="递归神经网络"></p><p>以上图(a)为例，包含3个隐藏层$h_1,h_2,h_3$，其中$h_1$由两个输入$x_1,x_2$计算得到，$h_2$由两个输入$x_3,x_4$计算得到，$h_3$由两个隐藏层$h_1,h_2$计算得到。</p><p>对于一个节点$h_i$，它可以接受来自子节点集合$\pi_i$中所有节点的消息，并更新自己的状态，如下所示（公式3-9）：</p><script type="math/tex; mode=display">h_i = f(h_{\pi_i})</script><p>其中$h_{\pi_i}$表示$\pi_i$集合中所有节点状态的拼接，$f(.)$是一个和节点状态无关的非线性函数，可以为一个单层的前馈神经网络，比如图3-3(a)所表示的递归神经网络可以表示为（公式3-10）：</p><script type="math/tex; mode=display">h_1 = \sigma (W \begin{bmatrix}x_1\\ x_2\end{bmatrix}+ b) \\h_2 = \sigma (W \begin{bmatrix}x_3\\ x_4\end{bmatrix}+ b) \\h_3 = \sigma (W \begin{bmatrix}h_1\\ h_2\end{bmatrix}+ b)</script><p>其中$\sigma$表示非线性激活函数，W和b为可学习的参数，同样输出层y可以为一个分类器，比如（公式3-11）：</p><script type="math/tex; mode=display">h_3 = g (W' \begin{bmatrix}h_1\\ h_2\end{bmatrix}+ b')</script><p>其中$g(.)$为分类器，$W’$和$b’$为分类器的参数。当递归神经网络的结构退化为图3-3(b)时，就等价于简单神经循环网络。</p><p>递归神经网络主要用来建模自然语言句子的语义，给定一个句子的语法结构，可以使用递归神经网络来按照句法的组合关系来合成一个句子的语义，句子中每个短语成分可以分成一些子成分，即每个短语的语义可以由它的子成分语义组合而来，进而合成整句的语义。</p><p>同样也可以使用门机制来改进递归神经网络中的长距离依赖问题，比如树结构的长短期记忆模型就是将LSTM的思想应用到树结构的网络中，来实现更灵活的组合函数。</p><hr><h3 id="GN"><a href="#GN" class="headerlink" title="GN"></a>GN</h3><p>在实际应用中，很多数据是图结构的，比如知识图谱，社交网络，分子网络等。而前馈网络和反馈网络很难处理图结构的数据。</p><p><strong>图网络（Graph Network，GN）</strong>是将消息传递的思想扩展到图结构数据上的神经网络。</p><p>对于一个图结构$G(V,\varepsilon )$，其中$V$表示节点结合，$\varepsilon$表示边集合。每条边表示两个节点之间的依赖关系，节点之间的连接可以是有向的，也可以是无向的。图中每个节点v都用一组神经元来表示其状态$h^{(v)}$ ，初始状态可以为节点v的输入特征$x^{(v)}$，每个节点接受相邻节点的信息，来更新自己的状态，如下所示（公式3-12）：</p><script type="math/tex; mode=display">m^{(v)}_t = \sum_{u \in N(v)} f( h^{(v)}_{t-1},h^{(u)}_{t-1},e^{(u,v)} )\\h^{(v)}_t = g(h^{(v)}_{t-1},m^{(u)}_t)</script><p>其中$N(v)$表示节点v的邻居节点，$m^{(v)}_t$ 表示在t时刻节点v接受到的信息，$e^{(u,v)}$为边(v,u)上的特征。</p><p>公式3-12是一种同步更新方式，所有结构同时接受信息并更新自己的状态，而对于有向图来说，使用异步的更新方式会更有效率，比如循环神经网络或者递归神经网络，在整个图更新T次后，可以通过一个读出函数g(.)来得到整个网络的表示。</p><blockquote><p>至此，循环神经网络（上）（中）（下）篇已经介绍完毕。</p></blockquote><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100943664&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>常见的五种神经网络(3)-循环神经网络（中）篇</title>
    <link href="http://thinkgamer.cn/2019/09/11/TensorFlow/%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(3)-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%AD%EF%BC%89%E7%AF%87/"/>
    <id>http://thinkgamer.cn/2019/09/11/TensorFlow/常见的五种神经网络(3)-循环神经网络（中）篇/</id>
    <published>2019-09-10T17:16:12.000Z</published>
    <updated>2019-12-06T03:14:43.931Z</updated>
    
    <content type="html"><![CDATA[<p>转载请注明出处：<a href="https://thinkgamer.blog.csdn.net/article/details/100709422" target="_blank" rel="external">https://thinkgamer.blog.csdn.net/article/details/100709422</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a><br>公众号：搜索与推荐Wiki</p><hr><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>常见的五种神经网络系列第三种，主要介绍循环神经网络，分为上中下三篇进行介绍，本文主为（中）篇，涉及内容如下：</p><ul><li>循环神经网络中的参数学习</li><li>RNN中的长期依赖问题</li><li>常见的循环神经网络结构</li></ul><p>该系列的其他文章：</p><ul><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/89459131" target="_blank" rel="external">常见的五种神经网络(1)-前馈神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100531593" target="_blank" rel="external">常见的五种神经网络(2)-卷积神经网络</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100600661" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(上篇)</a></li><li><a href="https://blog.csdn.net/Gamer_gyt/article/details/100709422" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(中篇)</a></li><li><a href="https://thinkgamer.blog.csdn.net/article/details/100943664" target="_blank" rel="external">常见的五种神经网络(3)-循环神经网络(下篇)</a></li><li>常见的五种神经网络(4)-深度信念网络</li><li>常见的五种神经网络(5)-生成对抗网络</li></ul><hr><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p>循环神经网络的参数可以通过梯度下降方法来学习。给定一个样本(x,y)，其中$x_{1:T}=(x_1, x_2, … ,x_T)$为长度是T的输入序列，其中$y_{1:T}=(y_1, y_2, … ,y_T)$是长度为T的标签序列，在每个时刻t，都有一个监督信息$y_t$，定义时刻t的损失函数为（公式1-1）：</p><script type="math/tex; mode=display">L_t = L(y_t, g(h_t))</script><p>其中$g(h_t)$为第t时刻的输出，L为可微分的损失函数，比如交叉熵，整个序列上的损失函数为（公式1-2）：</p><script type="math/tex; mode=display">L = \sum_{t=1}^{T} L_t</script><p>整个序列的损失函数L关于参数U的梯度为（公式1-3）：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial U} = \sum _{t=1}^{T}\frac{\partial L_t}{ \partial U }</script><p>即每个时刻的损失函数$L_t$对参数U的偏导数之和。</p><p>在循环神经网络中主要有两种计算梯度的方式：</p><ul><li>随时间反向传播算法（Backpropagation Through Time，BRTT）</li><li>实时循环学习（Real-Time Recurrent Learning，RTRL）</li></ul><h4 id="随时间反向传播算法"><a href="#随时间反向传播算法" class="headerlink" title="随时间反向传播算法"></a>随时间反向传播算法</h4><p>主要通过类似前馈神经网络的错误反向传播算法来进行计算梯度。随时间反向传播算法将循环神经网络看作是一个展开的多层前馈网络，其中“每一层”对应循环网络中的每个时刻，这样循环神经网络就可以按照前馈神经网络中的反向传播算法来计算梯度。与前馈神经网络不同的是，循环神经网络中各层的参数是共享的，因此参数的真实梯度是各个层的参数梯度之和。</p><p>先计算公式1-3中第t时刻损失对参数U的偏导数 $\frac {\partial L_t}{\partial U}$，参数U和每个时刻k的净输入$z_k = Uh_{k-1} + Wx_{k} + b$有关，因此第t个时刻损失函数$L_t$关于参数$U_ij$的梯度为（公式1-4）：</p><script type="math/tex; mode=display">\frac{\partial L_t}{ \partial U_{ij}} = \sum_{k=1}^{t} tr( ( \frac{\partial L_t}{ \partial z_k} )^T \frac{\partial^+ z_k}{ \partial U_{ij}}    )\\= \sum_{k=1}^{t}  ( \frac{\partial^+ z_k}{ \partial U_{ij}}    )^T  \frac{\partial L_t}{ \partial z_k}</script><p>其中$\frac{\partial^+ z_k}{ \partial U_{ij}}$ 表示“直接”偏导数，即公式$z_k = Uh_{k-1} + Wx_{k} + b$中保持$h_{k-1}$不变，对$U_{ij}$进行求偏导数，得到（公式1-5）：</p><script type="math/tex; mode=display"> \frac{\partial^+ z_k}{ \partial U_{ij}}  = \begin{bmatrix}0\\ ... \\ [h_{k-1}]_j \\ ... \\ 0\end{bmatrix} \triangleq I_i([h_{k-1}]_j)</script><p>其中$[h_{k-1}]_j$为第$k-1$时刻隐状态的第j维，$I_i(x)$除了第j行值为x，之外全为0的向量。</p><p>定义$\delta _{t,k} = \frac{\partial L_t}{ \partial z_k }$为第t时刻损失函数对第k时刻隐藏层神经元净输入$z_k$的导数，则（公式1-6）：</p><script type="math/tex; mode=display">\delta _{t,k} = \frac{\partial L_t}{ \partial z_k }\\= \frac{ \partial h_k }{ \partial z_k} \frac{\partial z_{k+1}}{ \partial h_k } \frac{ \partial L_t }{ \partial z_{k+1} }\\= diag(f'(z_k))U^T \delta _{t,k+1}</script><p>将（公式1-6） 和 （公式 1-5） 代入（公式1-4）得到（公式1-7）：</p><script type="math/tex; mode=display">\frac{\partial L_t}{ \partial U_{ij} } = \sum_{k=1}^{ t } [\delta _{t,k}]_i [h_{k-1}]_j</script><p>将（公式1-7）写成矩阵形式为（公式1-8）：</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial U } = \sum_{k=1}^{ t } \delta _{t,k} h^T_{k-1}</script><p>下图为随时间反向传播算法示例：</p><p><img src="https://img-blog.csdnimg.cn/20190910191047315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="随时间反向传播算法示例"></p><p>将（公式1-8）代入（公式1-3）得到整个序列的损失函数$L$关于参数$U$的梯度（公式1-9）:</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial U } = \sum_{t=1}^{ T}\sum_{k=1}^{ t } \delta _{t,k} h^T_{k-1}</script><p>同理可得到$L$关于参数$W$的梯度（公式1-10）：</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial W } = \sum_{t=1}^{ T}\sum_{k=1}^{ t } \delta _{t,k} x^T_k</script><p>$L$关于参数$b$的梯度（公式1-11）：</p><script type="math/tex; mode=display">\frac{\partial L}{ \partial b } = \sum_{t=1}^{ T}\sum_{k=1}^{ t } \delta _{t,k}</script><blockquote><p>在 随时间反向传播算法中，参数的梯度需要在一个完整的“向前”计算和“向后”计算后才能得到并参数更新。</p></blockquote><h4 id="实时循环学习"><a href="#实时循环学习" class="headerlink" title="实时循环学习"></a>实时循环学习</h4><p>与随时间反向传播算法不同的是：实时循环学习（Real-Time Recurrent Learning）是通过前向传播的方式来计算梯度。</p><p>假设RNN中第 $t+1$时刻的状态$h_{t+1}$为（公式1-12）：</p><script type="math/tex; mode=display">h_{t+1} = f(z_{t+1}) = f(Uh_k + Wx_{k+1} + b)</script><p>其关于参数$U_{ij$的偏导数为（公式1-13）：</p><script type="math/tex; mode=display">\frac{ h_{t+1} }{ \partial U_{ij} } = \frac{ \partial h_{t+1} }{ \partial z_{t+1} } ( \frac{ \partial^+z_{t+1} }{ \partial  U_{ij}}   + U \frac{ \partial h_t}{ \partial U_{ij} } )\\= diag( f'(z_{t+1}) ) ( I_i ([h_t]_j)+ U \frac{ \partial h_t}{ \partial U_{ij} }  )\\=f'(z_{t+1}) \odot  ( I_i ([h_t]_j)+ U \frac{ \partial h_t}{ \partial U_{ij} }  )</script><p>其中$I_i(x)$为除了第i行之外元素全为0的向量。</p><p>RTRL自从第一个时刻开始，除了计算RNN的隐状态之外，还利用（公式1-13）依次前向计算偏导数$\frac{\partial h_1}{ \partial U_{ij}},\frac{\partial h_2}{ \partial U_{ij}},\frac{\partial h_3}{ \partial U_{ij}}…$</p><p>这样假设第t个时刻存在一个监督信息，其损失函数为$L_t$，就可以同时计算损失函数对$U_{ij}$的偏导数（公式1-14）：</p><script type="math/tex; mode=display">\frac{\partial L_t}{ \partial U_{ij}} =( \frac{\partial h_t}{ \partial U_{ij} } )^T \frac{\partial L_t}{ \partial h_t}</script><p>这样在第t个时刻就可以实时计算$L_t$关于参数U的梯度，并更新参数。参数W和b的梯度也可以按照上述方法进行计算。</p><blockquote><p>两种算法比较：RTRL算法和BPTT算法都是基于梯度求解参数，分别通过前向模式和反向模式应用链式法则来计算梯度。在RNN中一般输出维度要比输入维度少，因此BPTT算法的计算量会很小，但要保存计算过程中的梯度值，空间复杂度较高。RTRL算法不需要进行空间回传，比较适合用在在线学习或无限序列的任务中。</p></blockquote><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>在BRTT算法中，将（公式1-6）展开得到（公式1-15）：</p><script type="math/tex; mode=display">\delta _{t,k}=\prod_{i=k}^{t-1} ( diag('f(z_i ))U^T  )\delta _{t,t}</script><p>如果定义$\gamma \approx || diag(‘f(z_i ))U^T   ||$，则（公式1-16）：</p><script type="math/tex; mode=display">\delta _{t,k}=\gamma ^{t-k} \delta _{t,t}</script><p>若$\gamma &gt;1$，当$t-k \rightarrow +\infty$，$\gamma ^{t-k} \rightarrow +\infty$，会造成系统不稳定，称之为梯度爆炸（Gradient Exploding Problem），反之，若$\gamma &lt; 1$，当$t-k \rightarrow +\infty$，$\gamma ^{t-k} \rightarrow 0$，会出现和前馈神经网络类似的梯度消失问题（Gradient Vanishing Problem）。</p><blockquote><p>注意：在循环神经网络中，梯度消失指的是并不是说$\frac{ \partial L_t}{ \partial U}$的梯度消失了，而是$\frac{ \partial L_t}{ \partial h_k}$的梯度消失，当$t-k$很大时，即参数U的更新主要靠最近的几个状态来更新，长距离的状态对参数U没有影响。</p></blockquote><p>当循环神经网络中使用的激活函数是Logistic或者tanh的时候，由于其导数小于1，并且权重矩阵$||U||$也不会太大，因此，如果时间间隔t-k过大的话，也会出现梯度消失问题。所以一般采用 ReLU激活函数（关于激活函数的介绍可参考：<a href="https://blog.csdn.net/Gamer_gyt/article/details/89440152" target="_blank" rel="external">神经网络中的激活函数介绍</a>）。</p><p>虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸和梯度消失问题，实际上只能学习到短期的依赖关系，这样如果t时刻的输出$y_t$依赖于$t-k$时刻的输入$x_{t-k}$，当间隔k比较大时，简单神经网络很难建模这种长距离的依赖关系，称之为长期依赖问题（Long-Term Dependences Problem）。</p><p>改进措施：</p><ul><li>选取合适的参数</li><li>使用非饱和的激活函数</li></ul><p>循环网络的梯度爆炸问题比较容易解决，一般通过梯度截断和权重衰减来避免。而梯度消失很难解决，通常是对模型进行调优来解决。</p><h3 id="常见的循环神经网络结构"><a href="#常见的循环神经网络结构" class="headerlink" title="常见的循环神经网络结构"></a>常见的循环神经网络结构</h3><p>主要包含四种：</p><ul><li>N：N</li><li>1：N</li><li>N：1</li><li>N：M</li></ul><h4 id="N比N结构"><a href="#N比N结构" class="headerlink" title="N比N结构"></a>N比N结构</h4><p>N维输入对应N维输出，大致结构如下所示：</p><p><img src="https://img-blog.csdnimg.cn/20190910211158381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比N结构"></p><p>其常常用于处理以下问题：</p><ul><li>视频理解中获取视频每一帧标签，输入为视频解码后的图像，通过此结构，获取每一 帧的标签信息。这种场景一般用作视频理解的初期，对视频做初步的处理后， 后续可以基于这些标签信息进行语义分析，构建更为复杂的需求场景。</li><li>股票价格预测。基于历史的股票信息输入，预测下一时刻或者未来的股票走势信息。</li></ul><h4 id="1比N结构"><a href="#1比N结构" class="headerlink" title="1比N结构"></a>1比N结构</h4><p>一维输入，N维输出，大致结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190910211546198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络1比N结构"></p><p>还有一种结构是在同一信息在不同时刻输入到网络中，如下所示：<br><img src="https://img-blog.csdnimg.cn/20190910211800194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络1比N结构"></p><p>其常常用于处理以下问题：</p><ul><li>看图写描述 : 根据输入的 一张图 片，生成对这张图片的描述信息 </li><li>自动作曲 : 按照类别生成音乐 </li></ul><h4 id="N比1结构"><a href="#N比1结构" class="headerlink" title="N比1结构"></a>N比1结构</h4><p>N维输入，一维输出，大致结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190910211426311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比1结构"></p><p>其常常用于处理以下问题：</p><ul><li>视频理解中的获取视频每个场景的描述信息，或者获取整个影片的摘要信息。 </li><li>获取用户评价的情感信息，即根据用户的一句话的评论，来判断用户的喜好等情感信息 。</li></ul><h4 id="N比M结构"><a href="#N比M结构" class="headerlink" title="N比M结构"></a>N比M结构</h4><p>N维输入，M维输出，这种结构又被称为Encoder-Decoder模型，也可以称为Seq2Seq模型，这种模型的输入和输出可以不相等，该模型由两部分组成：编码部分和解码部分，大致结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190910212035184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比M结构"></p><p> c的前半部分循环神经网络为编码部分，称之为Endcoder, c可以是s3的直接输出，或者 是对s3输出做一定的变换，也可以对编码部分所有的s1、s2、s3进行变换得到，这样c中就包 含了对X1、 Xz、码的编码信息 。c的后半部分循环神经网络为解码部分，称之为Decoder。c作为之前的状态编码，作为初始值，输入到Decoder当中。 Decoder经过循环处理，最终将信息解码输出。</p><p> 除了上边所示的解码结构外，还有下图所示的结构：<br> <img src="https://img-blog.csdnimg.cn/20190910212711689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="循环神经网络N比M结构"></p><p> N比M的循环神经网络结构更具有普遍性，现实环境中有很多基于该结构落地的场景，他可以解决如下问题：</p><ul><li>机器翻译：将不同语言作为输入，输出为非输入语言的类型，这也是Encoder-Decoder的经典用法</li><li>文本摘要：输入一篇文章，输出这篇文章的摘要信息</li><li>语音识别：输入一段语音，输出这段语音信息的文字</li></ul><blockquote><p>至此，循环神经网络（中）篇已经介绍完了，在下篇中会展开介绍更多的内容，欢迎关注。</p></blockquote><hr><center>【技术服务】，详情点击查看：<a href="https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg" target="_blank" rel="external">https://mp.weixin.qq.com/s/PtX9ukKRBmazAWARprGIAg</a></center><hr><center><img src="https://img-blog.csdnimg.cn/20191108184219834.jpeg"><br>扫一扫 关注微信公众号！号主 专注于搜索和推荐系统，尝试使用算法去更好的服务于用户，包括但不局限于机器学习，深度学习，强化学习，自然语言理解，知识图谱，还不定时分享技术，资料，思考等文章！</center><hr><center><img src="https://img-blog.csdnimg.cn/20191105121139935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center><center><img src="https://img-blog.csdnimg.cn/20191105121309716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" width="80%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载请注明出处：&lt;a href=&quot;https://thinkgamer.blog.csdn.net/article/details/100709422&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://thinkgamer.blog.csdn.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
